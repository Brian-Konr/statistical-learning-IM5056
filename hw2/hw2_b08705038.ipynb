{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Learning and Deep Learning - Homework 2\n",
    "#### B08705038 資管四 郭子麟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一題 [Data Processing]\n",
    "(10%) 資料前處理是一個重要的工作，本題將利用UCI的\"Adult\" dataset <https://archive.ics.uci.edu/ml/datasets/Adult>來練習資料前處理。我們使用這個資料集的方式是用來建構預測最後一個收入欄位是'>50K'或'<=50K'。這個資料集已經先切好了Training跟Test。我們將會沿用這個切割。\n",
    "\n",
    "資料前處理包含以下工作:\n",
    "* 生成以下numpy變數: x_train(訓練特徵)、y_train(訓練標籤)、x_test(測試特徵)、y_test(測試標籤)。用一個Dictionary組織將這些變數，其中Key為變數名稱，Value為之前生成的變數內容。\n",
    "* 最後一欄為標籤，將'>50K'與'<=50K'轉成1跟0。其他欄位為特徵。\n",
    "* 把所有含有缺值的Rows刪除。\n",
    "* 所有數值欄位標準化(均數為0，變異數為1)。測試資料特徵需用訓練資料的均數與變異數標準化。\n",
    "* 所有類別欄位(如native-country與workclass)都應使用\"1-of-K\"轉換成0與1的欄位。\n",
    "* 我們只考慮在訓練資料中出現超過(含)10次的特徵值。如果一個特徵值出現不到10次，則刪除這個特徵值所對應的1-of-K欄位。\n",
    "* 你可以使用sklearn中的工具函數進行1-of-K encoding與變數標準化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 讀檔並展示 dataframe 確認讀檔成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 5 rows of train_raw:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  educational-num  \\\n",
       "0   39         State-gov   77516  Bachelors               13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors               13   \n",
       "2   38           Private  215646    HS-grad                9   \n",
       "3   53           Private  234721       11th                7   \n",
       "4   28           Private  338409  Bachelors               13   \n",
       "\n",
       "       marital-status         occupation   relationship   race  gender  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country income  \n",
       "0          2174             0              40  United-States  <=50K  \n",
       "1             0             0              13  United-States  <=50K  \n",
       "2             0             0              40  United-States  <=50K  \n",
       "3             0             0              40  United-States  <=50K  \n",
       "4             0             0              40           Cuba  <=50K  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 5 rows of train_raw:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>257302</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>154374</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>151910</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>201490</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>287927</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age     workclass  fnlwgt   education  educational-num  \\\n",
       "32556   27       Private  257302  Assoc-acdm               12   \n",
       "32557   40       Private  154374     HS-grad                9   \n",
       "32558   58       Private  151910     HS-grad                9   \n",
       "32559   22       Private  201490     HS-grad                9   \n",
       "32560   52  Self-emp-inc  287927     HS-grad                9   \n",
       "\n",
       "           marital-status         occupation relationship   race  gender  \\\n",
       "32556  Married-civ-spouse       Tech-support         Wife  White  Female   \n",
       "32557  Married-civ-spouse  Machine-op-inspct      Husband  White    Male   \n",
       "32558             Widowed       Adm-clerical    Unmarried  White  Female   \n",
       "32559       Never-married       Adm-clerical    Own-child  White    Male   \n",
       "32560  Married-civ-spouse    Exec-managerial         Wife  White  Female   \n",
       "\n",
       "       capital-gain  capital-loss  hours-per-week native-country income  \n",
       "32556             0             0              38  United-States  <=50K  \n",
       "32557             0             0              40  United-States   >50K  \n",
       "32558             0             0              40  United-States  <=50K  \n",
       "32559             0             0              20  United-States  <=50K  \n",
       "32560         15024             0              40  United-States   >50K  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 5 rows of test_raw:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt     education  educational-num      marital-status  \\\n",
       "0   25    Private  226802          11th                7       Never-married   \n",
       "1   38    Private   89814       HS-grad                9  Married-civ-spouse   \n",
       "2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse   \n",
       "3   44    Private  160323  Some-college               10  Married-civ-spouse   \n",
       "4   18        NaN  103497  Some-college               10       Never-married   \n",
       "\n",
       "          occupation relationship   race  gender  capital-gain  capital-loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
       "1    Farming-fishing      Husband  White    Male             0             0   \n",
       "2    Protective-serv      Husband  White    Male             0             0   \n",
       "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
       "4                NaN    Own-child  White  Female             0             0   \n",
       "\n",
       "   hours-per-week native-country  income  \n",
       "0              40  United-States  <=50K.  \n",
       "1              50  United-States  <=50K.  \n",
       "2              40  United-States   >50K.  \n",
       "3              40  United-States   >50K.  \n",
       "4              30  United-States  <=50K.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 5 rows of test_raw:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16276</th>\n",
       "      <td>39</td>\n",
       "      <td>Private</td>\n",
       "      <td>215419</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16277</th>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>321403</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other-relative</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16278</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>374983</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16279</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>83891</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>5455</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16280</th>\n",
       "      <td>35</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>182148</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age     workclass  fnlwgt  education  educational-num  \\\n",
       "16276   39       Private  215419  Bachelors               13   \n",
       "16277   64           NaN  321403    HS-grad                9   \n",
       "16278   38       Private  374983  Bachelors               13   \n",
       "16279   44       Private   83891  Bachelors               13   \n",
       "16280   35  Self-emp-inc  182148  Bachelors               13   \n",
       "\n",
       "           marital-status       occupation    relationship  \\\n",
       "16276            Divorced   Prof-specialty   Not-in-family   \n",
       "16277             Widowed              NaN  Other-relative   \n",
       "16278  Married-civ-spouse   Prof-specialty         Husband   \n",
       "16279            Divorced     Adm-clerical       Own-child   \n",
       "16280  Married-civ-spouse  Exec-managerial         Husband   \n",
       "\n",
       "                     race  gender  capital-gain  capital-loss  hours-per-week  \\\n",
       "16276               White  Female             0             0              36   \n",
       "16277               Black    Male             0             0              40   \n",
       "16278               White    Male             0             0              50   \n",
       "16279  Asian-Pac-Islander    Male          5455             0              40   \n",
       "16280               White    Male             0             0              60   \n",
       "\n",
       "      native-country  income  \n",
       "16276  United-States  <=50K.  \n",
       "16277  United-States  <=50K.  \n",
       "16278  United-States  <=50K.  \n",
       "16279  United-States  <=50K.  \n",
       "16280  United-States   >50K.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create header list and read data to from a dataframe\n",
    "headers = [\n",
    "  'age', \n",
    "  'workclass', \n",
    "  'fnlwgt', \n",
    "  'education', \n",
    "  'educational-num', \n",
    "  'marital-status', \n",
    "  'occupation', \n",
    "  'relationship', \n",
    "  'race', \n",
    "  'gender', \n",
    "  'capital-gain', \n",
    "  'capital-loss', \n",
    "  'hours-per-week', \n",
    "  'native-country', \n",
    "  'income'\n",
    "]\n",
    "\n",
    "# because in the dataset, ? is used to represent missing values, we need to replace it with NaN\n",
    "train_raw = pd.read_csv('adult.data', header=None, names=headers, na_values=['?'], skipinitialspace=True) # skipinitialspace=True is used to remove the space before the value\n",
    "test_raw = pd.read_csv('adult.test', header=None, names=headers, skiprows=1, na_values=['?'], skipinitialspace=True) # first line is not data, so skip it\n",
    "\n",
    "print(\"first 5 rows of train_raw:\")\n",
    "display(train_raw.head())\n",
    "\n",
    "print(\"last 5 rows of train_raw:\")\n",
    "display(train_raw.tail())\n",
    "\n",
    "print(\"first 5 rows of test_raw:\")\n",
    "display(test_raw.head())\n",
    "\n",
    "print(\"last 5 rows of test_raw:\")\n",
    "display(test_raw.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將'>50K'與'<=50K'轉成 1 跟 0 並把含有 NaN 的 row 刪除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw['income'] = train_raw['income'].map({'<=50K': 0, '>50K': 1})\n",
    "test_raw['income'] = test_raw['income'].map({'<=50K.': 0, '>50K.': 1})\n",
    "\n",
    "train_raw.dropna(inplace=True, axis=0)\n",
    "test_raw.dropna(inplace=True, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 所有數值欄位標準化(均數為0，變異數為1)。測試資料特徵需用訓練資料的均數與變異數標準化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "numeric_features = ['age', 'fnlwgt', 'educational-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "scaler = StandardScaler()\n",
    "train_raw[numeric_features] = scaler.fit_transform(train_raw[numeric_features])\n",
    "test_raw[numeric_features] = scaler.transform(test_raw[numeric_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 所有類別欄位(如native-country與workclass)都應使用\"1-of-K\"轉換成0與1的欄位。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country']\n",
    "train_raw_with_dummies = pd.get_dummies(train_raw, columns=categorical_features)\n",
    "test_raw_with_dummies = pd.get_dummies(test_raw, columns=categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 只考慮在訓練資料中出現超過(含) 10 次的特徵值。如果一個特徵值出現不到 10 次，則刪除這個特徵值所對應的 1-of-K 欄位。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features to remove: ['occupation_Armed-Forces', 'native-country_Holand-Netherlands']\n"
     ]
    }
   ],
   "source": [
    "categorical_features_dummies_to_remove = []\n",
    "categorical_features_dummies_to_remove_for_test = []\n",
    "# for all categorial columns in train_raw_with_dummies\n",
    "for col in train_raw_with_dummies.columns:\n",
    "    if col not in numeric_features and col != 'income':\n",
    "\t\t# if the feature appears less than 10 times, remove the column\n",
    "        if train_raw_with_dummies[col].sum() < 10:\n",
    "            categorical_features_dummies_to_remove.append(col)\n",
    "\n",
    "print(\"features to remove:\", categorical_features_dummies_to_remove)\n",
    "train_raw_with_dummies.drop(categorical_features_dummies_to_remove, axis=1, inplace=True)\n",
    "test_raw_with_dummies.drop(categorical_features_dummies_to_remove, axis=1, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15060, 103), (30162, 103))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_raw_with_dummies.shape, train_raw_with_dummies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成 dictionary 並將 x_train, y_train, x_test, y_test 轉成 numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30162, 102) (15060, 102)\n"
     ]
    }
   ],
   "source": [
    "y_train = train_raw_with_dummies['income'].values\n",
    "y_test = test_raw_with_dummies['income'].values\n",
    "\n",
    "# 按照 pickle 的 column 順序排列\n",
    "import pickle\n",
    "dsfile = 'adult_m50k.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)\n",
    "reordered_train_raw_with_dummies = pd.DataFrame(columns=adult50kp['columnname'])\n",
    "reordered_test_raw_with_dummies = pd.DataFrame(columns=adult50kp['columnname'])\n",
    "for col in adult50kp['columnname']:\n",
    "    reordered_train_raw_with_dummies[col] = train_raw_with_dummies[col]\n",
    "    reordered_test_raw_with_dummies[col] = test_raw_with_dummies[col]\n",
    "\n",
    "x_train = reordered_train_raw_with_dummies.to_numpy()\n",
    "x_test = reordered_test_raw_with_dummies.to_numpy()\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult50k = {\n",
    "\t'x_train': x_train,\n",
    "\t'y_train': y_train,\n",
    "\t'x_test': x_test,\n",
    "\t'y_test': y_test,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 與 pickle 檔進行比對，確認處理完的 data 與 pickle 相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train match!\n",
      "x_test match!\n",
      "y_train match!\n",
      "y_test match!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "dsfile = 'adult_m50k.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)\n",
    "\n",
    "elems = ['x_train', 'x_test', 'y_train', 'y_test']\n",
    "\n",
    "for aelem in elems:\n",
    "    cnomatch = np.sum(adult50kp[aelem] != adult50k[aelem])\n",
    "    if cnomatch == 0:\n",
    "        print(aelem, \"match!\")\n",
    "    else:\n",
    "        print(aelem, \"%d elements no match!\" % cnomatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二題 [ROC and AUC]\n",
    "(35%) Receiver operation characteristic (ROC) 曲線以及其線下面積 (Area Under Curve; AUC) 為衡量分類器預測能力常用的工具。  \n",
    "本題將練習繪製ROC以及計算AUC。在這之前我們必須載入資料，訓練模型，並進行預測："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.848340\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# load dataset\n",
    "dsfile = 'adult_m50k.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)\n",
    "    \n",
    "# train prediction model    \n",
    "c = 0.3\n",
    "lr2 = LogisticRegression(solver = 'lbfgs', C= c, max_iter = 1000)\n",
    "lr2.fit(adult50kp['x_train'], adult50kp['y_train'])\n",
    "# make prediction\n",
    "ypred = lr2.predict(adult50kp['x_test'])\n",
    "ypredprob = lr2.predict_proba(adult50kp['x_test'])\n",
    "# compute accuracy\n",
    "ncorrect = np.sum(adult50kp['y_test'] == ypred)\n",
    "accuracy_sk = ncorrect / adult50kp['y_test'].shape[0]\n",
    "print(\"Accuracy = %f\" % accuracy_sk)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.1 (17.5%): 基於 `adult50kp['y_test']` 與 `ypredprob` 繪製ROC Curve。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move a threshold on the sorted instances, and calculate corresponding confusion matrix for each threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_confusion_matrix_return_4_values(y, ypred):\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i] == 1 and ypred[i] == 1:\n",
    "            tp += 1\n",
    "        elif y[i] == 0 and ypred[i] == 0:\n",
    "            tn += 1\n",
    "        elif y[i] == 0 and ypred[i] == 1:\n",
    "            fp += 1\n",
    "        elif y[i] == 1 and ypred[i] == 0:\n",
    "            fn += 1\n",
    "    return tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp, tn, fp, fn = 1, 11360, 0, 3699\n"
     ]
    }
   ],
   "source": [
    "dict = {\n",
    "    'y_test': adult50kp['y_test'],\n",
    "    'y_pred_pos': ypredprob[:,1],\n",
    "}\n",
    "instances = pd.DataFrame(dict)\n",
    "instances.sort_values(by=['y_pred_pos'], ascending=False, inplace=True)\n",
    "instances.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# define fpr_tpr list, for every element, it is a array of [fpr, tpr]\n",
    "fpr_tpr = []\n",
    "# do first new prediction based on first threshold\n",
    "tp, fp, tn, fn = 0, 0, 0, 0\n",
    "threshold = instances['y_pred_pos'][0]\n",
    "y_pred_new = instances['y_pred_pos'].map(lambda x: 1 if x >= threshold else 0)\n",
    "tp, tn, fp, fn = cal_confusion_matrix_return_4_values(instances['y_test'], y_pred_new)\n",
    "print(f\"tp, tn, fp, fn = {tp}, {tn}, {fp}, {fn}\")\n",
    "fpr_tpr.append([fp/(fp+tn), tp/(tp+fn)])\n",
    "\n",
    "for i in range(instances.shape[0]):\n",
    "    # 每次多一個 predict positive 因此要看 true class\n",
    "    if instances['y_test'][i] > 0:\n",
    "        tp += 1\n",
    "        fn -= 1\n",
    "    else:\n",
    "        fp += 1\n",
    "        tn -= 1\n",
    "    fpr_tpr.append([fp/(fp+tn), tp/(tp+fn)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the ROC curve based on false positive rate and true positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ffb7ec4d30>]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot ROC curve by fpr_tpr\n",
    "import matplotlib.pyplot as plt\n",
    "# make graph larger\n",
    "\n",
    "# sort fpr_tpr by fpr\n",
    "fpr_tpr.sort(key=lambda x: x[0])\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "plt.plot([x[0] for x in fpr_tpr], [x[1] for x in fpr_tpr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2 (17.5%): 計算繪製出的 ROC Curve 的 AUC。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.9037639179673276\n"
     ]
    }
   ],
   "source": [
    "# count AUC based on fpr_tpr\n",
    "# sort fpr_tpr by fpr\n",
    "fpr_tpr.sort(key=lambda x: x[0])\n",
    "auc = 0\n",
    "for i in range(len(fpr_tpr)-1):\n",
    "    auc += (fpr_tpr[i+1][0] - fpr_tpr[i][0]) * (fpr_tpr[i+1][1] + fpr_tpr[i][1]) / 2\n",
    "print(f\"AUC = {auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三題 [Logistic Regression with L2 Regularization]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.1 (15%) Derive the gradient and hessian matrix for the new E(w).\n",
    "<img src=\"q3-1_handwriting.jpg\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.2 (25%) Create your mylogistic_l2 class. Train your model and show the learned $w$ as well as test accuracy for the cases below. If $w$ is too long for you, show selected $w$ for continuous-valued, binary-valued, and the constant term.  \n",
    "* Case 1: lambda = 1 for all coefficients\n",
    "* Case 2: lambda = 1 for all but the intercept, no regularization for intercept term.\n",
    "* Case 3: lambda = 1 for numerical-valued features, lambda = 0.5 for binary-valued features, no regularization for intercept term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define mylogistic_l2 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class mylogistic_l2():\n",
    "    def __init__(self, reg_vec, max_iter = 100, tol = 1e-5, add_intercept = True):\n",
    "        \"\"\"reg_vec: the regularization coefficient vector\n",
    "           max_iter: maximum number of iteration to run for the Newton method\n",
    "           tol: tolerance for the objective function\n",
    "           add_intercept: whether to add intercept (a column of ones) at last column of the feature matrix\"\"\"\n",
    "        ### Add your code here\n",
    "        self.reg_vec = np.diag(reg_vec)\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.add_intercept = add_intercept\n",
    "    \n",
    "    def __sigmoid(self, x, w):\n",
    "        \"\"\"x: feature matrix\n",
    "           w: weight vector\n",
    "           return: sigmoid(xw)\"\"\"\n",
    "        return 1 / (1 + np.exp(-x.dot(w)))\n",
    "    def fit(self, x, y, verbal = False):\n",
    "        # Add your code here\n",
    "        if self.add_intercept:\n",
    "            x = np.concatenate((x, np.ones((x.shape[0], 1))), axis=1)\n",
    "        # initialize w with closed-form ridge regression\n",
    "        # b = average of lambda\n",
    "        b = np.mean(self.reg_vec)\n",
    "        self.w = np.linalg.inv(x.T @ x + b * np.identity(x.shape[1])) @ x.T @ y\n",
    "\n",
    "        # Newton method\n",
    "        obj = np.inf # to check if converge\n",
    "        for i in range(self.max_iter):\n",
    "            current_y = self.__sigmoid(x, self.w)\n",
    "            # compute gradient and hessian\n",
    "            gradient = self.reg_vec @ self.w + x.T @ (current_y - y)\n",
    "            r = np.diag(current_y * (1 - current_y))\n",
    "            hessian = self.reg_vec + x.T @ r @ x\n",
    "\n",
    "            # update w\n",
    "            w_new = self.w - np.linalg.inv(hessian) @ gradient\n",
    "            # compute objective function\n",
    "            obj_new = (w_new.T @ self.reg_vec @ w_new) / 2 - (y @ np.log(current_y) + (1 - y) @ np.log(1 - current_y))\n",
    "            if verbal:\n",
    "                print(f\"iter {i}: obj = {obj_new}\")\n",
    "            if abs(obj_new - obj) < self.tol:\n",
    "                print(f\"Converge at iter {i}\")\n",
    "                break\n",
    "            obj = obj_new\n",
    "            self.w = w_new\n",
    "            \n",
    "    def predict(self, x):\n",
    "        \"\"\"doing prediction\"\"\"\n",
    "        ### add your code here.\n",
    "        if self.add_intercept:\n",
    "            x = np.concatenate((x, np.ones((x.shape[0], 1))), axis=1)\n",
    "        predict_prob = self.__sigmoid(x, self.w)\n",
    "        # use 0.5 as threshold\n",
    "        predict = np.where(predict_prob >= 0.5, 1, 0)\n",
    "        return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dsfile = 'adult_m50k.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)\n",
    "\n",
    "# You can access the training and test data using the following keys: 'x_train', 'x_test', 'y_train', 'y_test'. \n",
    "# In addition, the key 'columnname' map the a list of column names, and the key 'num_col' map to a list of numeric columns.\n",
    "X_train = adult50kp['x_train']\n",
    "X_test = adult50kp['x_test']\n",
    "Y_train = adult50kp['y_train']\n",
    "Y_test = adult50kp['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1. lambda = 1 for all coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set lambda_vec and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converge at iter 8\n"
     ]
    }
   ],
   "source": [
    "lambda_vec = np.ones(X_train.shape[1] + 1)\n",
    "logic1 = mylogistic_l2(reg_vec = lambda_vec, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "logic1.fit(X_train, Y_train)\n",
    "ypred = logic1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Accuracy and show weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.847875166002656\n",
      "continous-valued weights = [0.25831075 0.35295138 2.33390153 0.75114521 0.33352443 0.07923687]\n",
      "binary-valued weights = [-2.59305996e-01 -3.31058935e-02 -8.02092305e-01 -1.16328381e+00\n",
      " -1.57480242e-01  1.06974336e+00 -6.33846060e-01  1.16732407e-01\n",
      " -2.31567383e-01 -5.17122209e-01 -7.97216481e-02 -1.09949780e+00\n",
      " -2.46027090e-01  6.19694925e-02  1.26685883e-01  8.62656059e-01\n",
      " -9.18352843e-01 -6.21226177e-01 -2.00740224e-01 -7.51600981e-01\n",
      " -1.61011588e+00  5.75820911e-01  6.48995282e-01  3.53741433e-01\n",
      "  7.17218474e-01 -2.84494746e-02 -9.54820902e-04 -1.96540899e-01\n",
      " -1.46351641e-01  6.26946274e-01  4.48207080e-01  2.45945817e-02\n",
      "  4.69223656e-02 -4.91067747e-01 -2.03035424e-01 -1.63303681e-01\n",
      " -1.76623509e-02 -1.11328323e-01 -9.94618248e-02 -1.17391916e+00\n",
      "  1.80702677e-01 -6.92720011e-02  9.76496905e-01  4.60988601e-01\n",
      " -4.95440415e-01 -1.27203531e+00  4.86772406e-01 -8.98963733e-01\n",
      " -6.00542600e-02 -3.50848853e-01  4.32815220e-01  5.94120149e-01\n",
      "  5.82151924e-01 -6.20962284e-01 -5.97480379e-02  9.29035249e-02\n",
      " -1.51892101e-01 -5.38528876e-03  3.41609085e-02 -2.89088237e-01\n",
      "  1.56053911e-01  4.95401242e-01  8.90942264e-01  1.49151436e-01\n",
      "  3.42484780e-01 -3.13312160e-01 -3.55939108e-01 -3.62494608e-01\n",
      " -6.67247475e-01 -4.08831131e-01  4.47489831e-01  1.37768930e-01\n",
      "  1.41351233e-01 -1.16015422e-01 -5.61032710e-02 -9.34583043e-01\n",
      " -2.92596528e-02 -2.99012958e-01 -1.50511251e-01  3.52331870e-01\n",
      " -7.85846535e-01  5.80200206e-01  4.97042310e-01 -1.90320741e-01\n",
      " -3.47718541e-04  1.74993803e-01 -4.88202696e-01 -3.12259617e-01\n",
      " -1.02643023e+00 -7.22310843e-01  1.44672470e+00  1.15520747e+00\n",
      " -6.80202911e-01 -1.21195631e+00 -7.98338514e-01 -5.34648493e-01]\n",
      "intercept term weight = -1.3455248934916817\n"
     ]
    }
   ],
   "source": [
    "acc = np.sum(ypred == Y_test) / Y_test.shape[0]\n",
    "print(f\"Accuracy = {acc}\")\n",
    "# print weights\n",
    "print(f\"continous-valued weights = {logic1.w[:6]}\")\n",
    "print(f\"binary-valued weights = {logic1.w[6:logic1.w.shape[0]-1]}\")\n",
    "print(f\"intercept term weight = {logic1.w[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2. lambda = 1 for all but the intercept, no regularization for intercept term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set lambda_vec and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: obj = 21193.18699600823\n",
      "iter 1: obj = 11777.772834219251\n",
      "iter 2: obj = 10294.32952392708\n",
      "iter 3: obj = 9850.222617639063\n",
      "iter 4: obj = 9778.168346189175\n",
      "iter 5: obj = 9773.250947924738\n",
      "iter 6: obj = 9773.112183506511\n",
      "iter 7: obj = 9773.111183721507\n",
      "iter 8: obj = 9773.11118355854\n",
      "Converge at iter 8\n"
     ]
    }
   ],
   "source": [
    "feature_num = X_train.shape[1]\n",
    "lambda_vec_case_2 = np.ones(feature_num + 1)\n",
    "# set lambda for intercept term to 0\n",
    "lambda_vec_case_2[-1] = 0\n",
    "logic2 = mylogistic_l2(reg_vec = lambda_vec_case_2, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "logic2.fit(X_train, Y_train, verbal = True)\n",
    "ypred_2 = logic2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Accuracy and show weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8477423638778221\n",
      "continous-valued weights = [0.25833063 0.35307341 2.33348256 0.7378757  0.33385106 0.07926886]\n",
      "binary-valued weights = [-0.04219572  0.19987643 -0.58360967 -0.93671318  0.0754847   1.28715744\n",
      " -0.37140327  0.39422898  0.04305748 -0.26147348  0.19559029 -0.42695771\n",
      "  0.42695771  0.16424528  0.22840772  0.96472553 -0.81743779 -0.52074423\n",
      " -0.09910239 -0.64944042 -1.55235099  0.6786798   0.75066429  0.45541098\n",
      "  0.81857112  0.07308911  0.0728464  -0.11752645 -0.06282948  0.67242506\n",
      "  0.5040869   0.08799091  0.11435013 -0.38483984 -0.10196309 -0.05145374\n",
      "  0.10741777 -0.01997934  0.01717544 -1.16567808  0.30082277  0.02715464\n",
      "  1.00831207  0.50210397 -0.45756662 -1.24002555  0.52780939 -0.86832688\n",
      " -0.02771494 -0.31412701  0.47343435  0.62981111  0.62405658 -0.5867506\n",
      " -0.0296708   0.12414401 -0.14376238  0.02434194  0.0621604  -0.24843986\n",
      "  0.19459429  0.52620501  0.93165615  0.18707696  0.37950109 -0.28749402\n",
      " -0.31137357 -0.33290534 -0.65117786 -0.38160106  0.48879121  0.17662205\n",
      "  0.17410342 -0.07343502 -0.0314651  -0.89846776  0.00653561 -0.27232555\n",
      " -0.12442075  0.39697177 -0.75318727  0.61067658  0.70544004  0.01789988\n",
      "  0.2090388   0.382747   -0.2795817  -0.10453082 -0.9310132  -0.52642475\n",
      "  1.61398956  1.367359   -0.49235221 -1.0149365  -0.60567592 -0.34195918]\n",
      "intercept term weight = -3.175085794493925\n"
     ]
    }
   ],
   "source": [
    "acc_2 = np.sum(ypred_2 == Y_test) / Y_test.shape[0]\n",
    "print(f\"Accuracy = {acc_2}\")\n",
    "# print weights\n",
    "print(f\"continous-valued weights = {logic2.w[:6]}\")\n",
    "print(f\"binary-valued weights = {logic2.w[6:logic2.w.shape[0]-1]}\")\n",
    "print(f\"intercept term weight = {logic2.w[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3. lambda = 1 for numerical-valued features, lambda = 0.5 for binary-valued features, no regularization for intercept term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set lambda_vec and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: obj = 21191.857158290903\n",
      "iter 1: obj = 11774.11800250116\n",
      "iter 2: obj = 10288.554764617063\n",
      "iter 3: obj = 9842.712612741932\n",
      "iter 4: obj = 9769.874060371954\n",
      "iter 5: obj = 9764.427044817206\n",
      "iter 6: obj = 9764.224448421459\n",
      "iter 7: obj = 9764.222592728347\n",
      "iter 8: obj = 9764.222592337484\n",
      "Converge at iter 8\n"
     ]
    }
   ],
   "source": [
    "feature_num = X_train.shape[1]\n",
    "lambda_vec_case_3 = np.ones(feature_num + 1)\n",
    "for i in range(feature_num):\n",
    "    if i < 6:\n",
    "        lambda_vec_case_3[i] = 1\n",
    "    else:\n",
    "        lambda_vec_case_3[i] = 0.5\n",
    "# set lambda for intercept term to 0\n",
    "lambda_vec_case_3[-1] = 0\n",
    "\n",
    "logic3 = mylogistic_l2(reg_vec = lambda_vec_case_3, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "logic3.fit(X_train, Y_train, verbal = True)\n",
    "ypred_3 = logic3.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Accuracy and show weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.847675962815405\n",
      "continous-valued weights = [0.25851661 0.3533387  2.33562764 0.7825921  0.33439916 0.07940036]\n",
      "binary-valued weights = [-0.08347988  0.23309138 -0.59278097 -0.92248498  0.11139578  1.25425868\n",
      " -0.38299462  0.41291781  0.04136013 -0.26411462  0.19283129 -0.42890321\n",
      "  0.42890321  0.23635124  0.30021363  1.03810523 -0.75216084 -0.45341369\n",
      " -0.02691155 -0.58252688 -2.00075405  0.75127893  0.82696619  0.52830707\n",
      "  0.89488996  0.14510376  0.18253094 -0.02583999  0.00991404  0.89862004\n",
      "  0.68517002  0.23294385  0.24519931 -0.38363083 -0.08029608 -0.06493444\n",
      "  0.0453608   0.03743376 -0.01295908 -2.09374319  0.25763304  0.06659781\n",
      "  1.18748312  0.55059265 -0.47576613 -1.45842154  0.5822242  -1.0627833\n",
      " -0.00957212 -0.31704572  0.52485137  0.73044517  0.67457228 -0.63624179\n",
      " -0.00967268  0.17339113 -0.2364757   0.0375474   0.10120874 -0.24679341\n",
      "  0.23800627  0.64228457  1.00567032  0.23258941  0.42267607 -0.35336167\n",
      " -0.29178766 -0.38125401 -0.96291964 -0.45007954  0.512985    0.22019382\n",
      "  0.22640627 -0.04989103 -0.01836864 -0.95953334  0.01656804 -0.32741555\n",
      " -0.14011404  0.42856024 -0.84476926  0.75121645  0.76670733  0.07638783\n",
      "  0.26824615  0.44314098 -0.22058151 -0.04631789 -1.28758289 -0.57187069\n",
      "  1.82502291  1.39622515 -0.54691698 -1.05894078 -0.6555147  -0.38800491]\n",
      "intercept term weight = -3.36269037726637\n"
     ]
    }
   ],
   "source": [
    "acc_3 = np.sum(ypred_3 == Y_test) / Y_test.shape[0]\n",
    "print(f\"Accuracy = {acc_3}\")\n",
    "# print weights\n",
    "print(f\"continous-valued weights = {logic3.w[:6]}\")\n",
    "print(f\"binary-valued weights = {logic3.w[6:logic3.w.shape[0]-1]}\")\n",
    "print(f\"intercept term weight = {logic3.w[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.3 (10%) Further split the training data into subtraining (90%) and tuning (10%) to search for the best hyperparameters. Set the regularization coefficient for the constant term to zero. Allow different regularizations for continuous-valued and binary-valued features. Let $a_1$ and $a_2$ denote the regularization coefficients for continuous-valued and binary-valued features. Search the best $a_1$ and $a_2$ and report the test accuracy using the best hyper-parameters. You should follow the following procedure to search for the best hyperparameters. \n",
    "1. Choose a set of grids among a reasonable range. For example, 10 grids in [0.01, 100]. \n",
    "2. Conduct grid search with the constraint that $a_1 = a_2$. Record the best value $a_1^*$ and $a_2^*$.\n",
    "3. Fix $a_1 = a_1^*$, and search $a_2$ for the best value, call the result the new $a_2^*$. \n",
    "4. Fix $a_2 = a_2^*$, and search $a_1$ for the best value.\n",
    "5. Report the selected $a_1$ and $a_2$.\n",
    "6. Train a model using the selected hyper-parameters, and report the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split the training data into subtraining and tuning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_subtrain, x_tune, y_subtrain, y_tune = train_test_split(X_train, Y_train, test_size = 0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using np.logspace to generate a set of grids and conduct procedure 2 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grids = [1.00000000e-02 2.78255940e-02 7.74263683e-02 2.15443469e-01\n",
      " 5.99484250e-01 1.66810054e+00 4.64158883e+00 1.29154967e+01\n",
      " 3.59381366e+01 1.00000000e+02]\n",
      "Converge at iter 11\n",
      "for candidate_coef = 0.01, accuracy = 0.8452104739807756\n",
      "Converge at iter 10\n",
      "for candidate_coef = 0.027825594022071243, accuracy = 0.8452104739807756\n",
      "Converge at iter 9\n",
      "for candidate_coef = 0.0774263682681127, accuracy = 0.8452104739807756\n",
      "Converge at iter 9\n",
      "for candidate_coef = 0.21544346900318834, accuracy = 0.8452104739807756\n",
      "Converge at iter 8\n",
      "for candidate_coef = 0.5994842503189409, accuracy = 0.8452104739807756\n",
      "Converge at iter 8\n",
      "for candidate_coef = 1.6681005372000592, accuracy = 0.8462048392442824\n",
      "Converge at iter 8\n",
      "for candidate_coef = 4.6415888336127775, accuracy = 0.8458733841564468\n",
      "Converge at iter 8\n",
      "for candidate_coef = 12.915496650148826, accuracy = 0.8455419290686113\n",
      "Converge at iter 8\n",
      "for candidate_coef = 35.93813663804626, accuracy = 0.8452104739807756\n",
      "Converge at iter 7\n",
      "for candidate_coef = 100.0, accuracy = 0.8442161087172688\n",
      "best regularization coefficient for a1 and a2 are 1.6681005372000592\n"
     ]
    }
   ],
   "source": [
    "grids = np.logspace(start = -2, stop = 2, num = 10) # from 0.01 to 100\n",
    "print(f\"grids = {grids}\")\n",
    "# conduct grid search with constraint that a1 = a2, record the best a1* and a2*\n",
    "acc_list = []\n",
    "for candidate_coef in grids:\n",
    "    # a1 is regularization coefficient for continuous-valued features\n",
    "    # a2 is regularization coefficient for binary-valued features\n",
    "    # first six features are continuous-valued, the rest except last one are binary-valued\n",
    "    lambda_vec = np.ones(x_subtrain.shape[1] + 1) * candidate_coef\n",
    "    lambda_vec[-1] = 0\n",
    "    logic = mylogistic_l2(reg_vec = lambda_vec, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "    logic.fit(x_subtrain, y_subtrain)\n",
    "    ypred = logic.predict(x_tune)\n",
    "    acc = np.sum(ypred == y_tune) / y_tune.shape[0]\n",
    "    print(f\"for candidate_coef = {candidate_coef}, accuracy = {acc}\")\n",
    "    acc_list.append(acc)\n",
    "\n",
    "best_coef = grids[np.argmax(acc_list)]\n",
    "print(f\"best regularization coefficient for a1 and a2 are {best_coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best a1 and a2 are 1.6681005372000592"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix $a_1 = a_1^*$, and search $a_2$ for the best value, call the result the new $a_2^*$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converge at iter 11\n",
      "for candidate_coef = 0.01, accuracy = 0.8455419290686113\n",
      "Converge at iter 10\n",
      "for candidate_coef = 0.027825594022071243, accuracy = 0.8455419290686113\n",
      "Converge at iter 9\n",
      "for candidate_coef = 0.0774263682681127, accuracy = 0.8452104739807756\n",
      "Converge at iter 9\n",
      "for candidate_coef = 0.21544346900318834, accuracy = 0.8452104739807756\n",
      "Converge at iter 8\n",
      "for candidate_coef = 0.5994842503189409, accuracy = 0.8452104739807756\n",
      "Converge at iter 8\n",
      "for candidate_coef = 1.6681005372000592, accuracy = 0.8462048392442824\n",
      "Converge at iter 8\n",
      "for candidate_coef = 4.6415888336127775, accuracy = 0.8462048392442824\n",
      "Converge at iter 8\n",
      "for candidate_coef = 12.915496650148826, accuracy = 0.8452104739807756\n",
      "Converge at iter 8\n",
      "for candidate_coef = 35.93813663804626, accuracy = 0.8458733841564468\n",
      "Converge at iter 7\n",
      "for candidate_coef = 100.0, accuracy = 0.8445475638051044\n",
      "best regularization coefficient for a2 under a1 = 1.6681005372000592 is 1.6681005372000592\n"
     ]
    }
   ],
   "source": [
    "a1 = best_coef\n",
    "acc_list_for_searching_a2 = []\n",
    "# Fix $a_1 = a_1^*$, and search $a_2$ for the best value, call the result the new $a_2^*$. \n",
    "for candidate_coef in grids:\n",
    "    lambda_vec = np.ones(x_subtrain.shape[1] + 1) * a1\n",
    "    lambda_vec[6:lambda_vec.shape[0]-1] = candidate_coef\n",
    "    lambda_vec[-1] = 0\n",
    "    logic = mylogistic_l2(reg_vec = lambda_vec, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "    logic.fit(x_subtrain, y_subtrain)\n",
    "    ypred = logic.predict(x_tune)\n",
    "    acc = np.sum(ypred == y_tune) / y_tune.shape[0]\n",
    "    print(f\"for candidate_coef = {candidate_coef}, accuracy = {acc}\")\n",
    "    acc_list_for_searching_a2.append(acc)\n",
    "\n",
    "best_a2_coef = grids[np.argmax(acc_list_for_searching_a2)]\n",
    "print(f\"best regularization coefficient for a2 under a1 = {a1} is {best_a2_coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new a2 after fixing a1 is still 1.6681005372000592"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix $a_2 = a_2^*$, and search $a_1$ for the best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converge at iter 8\n",
      "for candidate_coef = 0.01, accuracy = 0.8462048392442824\n",
      "Converge at iter 8\n",
      "for candidate_coef = 0.027825594022071243, accuracy = 0.8462048392442824\n",
      "Converge at iter 8\n",
      "for candidate_coef = 0.0774263682681127, accuracy = 0.8462048392442824\n",
      "Converge at iter 8\n",
      "for candidate_coef = 0.21544346900318834, accuracy = 0.8462048392442824\n",
      "Converge at iter 8\n",
      "for candidate_coef = 0.5994842503189409, accuracy = 0.8458733841564468\n",
      "Converge at iter 8\n",
      "for candidate_coef = 1.6681005372000592, accuracy = 0.8462048392442824\n",
      "Converge at iter 8\n",
      "for candidate_coef = 4.6415888336127775, accuracy = 0.8458733841564468\n",
      "Converge at iter 8\n",
      "for candidate_coef = 12.915496650148826, accuracy = 0.8462048392442824\n",
      "Converge at iter 8\n",
      "for candidate_coef = 35.93813663804626, accuracy = 0.8462048392442824\n",
      "Converge at iter 8\n",
      "for candidate_coef = 100.0, accuracy = 0.8458733841564468\n",
      "best regularization coefficient for a1 under a2 = 1.6681005372000592 is 0.01\n"
     ]
    }
   ],
   "source": [
    "a2 = best_a2_coef\n",
    "acc_list_for_searching_a1 = []\n",
    "for candidate_coef in grids:\n",
    "    lambda_vec = np.ones(x_subtrain.shape[1] + 1) * a2\n",
    "    lambda_vec[0:6] = candidate_coef\n",
    "    lambda_vec[-1] = 0\n",
    "    logic = mylogistic_l2(reg_vec = lambda_vec, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "    logic.fit(x_subtrain, y_subtrain)\n",
    "    ypred = logic.predict(x_tune)\n",
    "    acc = np.sum(ypred == y_tune) / y_tune.shape[0]\n",
    "    print(f\"for candidate_coef = {candidate_coef}, accuracy = {acc}\")\n",
    "    acc_list_for_searching_a1.append(acc)\n",
    "\n",
    "best_a1_coef = grids[np.argmax(acc_list_for_searching_a1)]\n",
    "print(f\"best regularization coefficient for a1 under a2 = {a2} is {best_a1_coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new a1 after fixing a2 becomes 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### final: a1: 0.01, a2: 1.6681005372000592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: obj = 19078.509591469803\n",
      "iter 1: obj = 10601.511010599377\n",
      "iter 2: obj = 9269.825175761382\n",
      "iter 3: obj = 8872.33971528803\n",
      "iter 4: obj = 8808.744594533786\n",
      "iter 5: obj = 8804.564410288674\n",
      "iter 6: obj = 8804.437790026763\n",
      "iter 7: obj = 8804.436719294863\n",
      "iter 8: obj = 8804.43671913156\n",
      "Converge at iter 8\n",
      "for a1 = 0.01, a2 = 1.6681005372000592, accuracy = 0.8462048392442824\n"
     ]
    }
   ],
   "source": [
    "a1 = best_a1_coef\n",
    "a2 = best_a2_coef\n",
    "lambda_vec = np.ones(x_subtrain.shape[1] + 1) * a1\n",
    "lambda_vec[6:lambda_vec.shape[0]-1] = a2\n",
    "lambda_vec[-1] = 0\n",
    "logic = mylogistic_l2(reg_vec = lambda_vec, max_iter = 1000, tol = 1e-5, add_intercept = True)\n",
    "logic.fit(x_subtrain, y_subtrain, verbal = True)\n",
    "ypred = logic.predict(x_tune)\n",
    "acc = np.sum(ypred == y_tune) / y_tune.shape[0]\n",
    "print(f\"for a1 = {a1}, a2 = {a2}, accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.4 (5%) Use sklearn.linear_model.LogisticRegression to train and test the model (including hyperparameter tuning). Compare the estimated parameters and test accuracy with those from your own models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for candidate_coef = 0.01, accuracy = 0.8442161087172688\n",
      "for candidate_coef = 0.027825594022071243, accuracy = 0.8452104739807756\n",
      "for candidate_coef = 0.0774263682681127, accuracy = 0.8455419290686113\n",
      "for candidate_coef = 0.21544346900318834, accuracy = 0.8458733841564468\n",
      "for candidate_coef = 0.5994842503189409, accuracy = 0.8462048392442824\n",
      "for candidate_coef = 1.6681005372000592, accuracy = 0.8452104739807756\n",
      "for candidate_coef = 4.6415888336127775, accuracy = 0.8452104739807756\n",
      "for candidate_coef = 12.915496650148826, accuracy = 0.8452104739807756\n",
      "for candidate_coef = 35.93813663804626, accuracy = 0.8452104739807756\n",
      "for candidate_coef = 100.0, accuracy = 0.8452104739807756\n",
      "best regularization coefficient for sklearn logistic regression is 0.5994842503189409\n",
      "for sklearn logistic regression with C = 0.5994842503189409, accuracy = 0.8462048392442824\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# use grids for hyperparameter tuning\n",
    "sklearn_logistic_acc_list = []\n",
    "for candidate_coef in grids:\n",
    "    logic = LogisticRegression(C = candidate_coef, fit_intercept = True, max_iter = 1000, tol = 1e-5)\n",
    "    logic.fit(x_subtrain, y_subtrain)\n",
    "    ypred = logic.predict(x_tune)\n",
    "    acc = np.sum(ypred == y_tune) / y_tune.shape[0]\n",
    "    sklearn_logistic_acc_list.append(acc)\n",
    "    print(f\"for candidate_coef = {candidate_coef}, accuracy = {acc}\")\n",
    "\n",
    "best_coef = grids[np.argmax(sklearn_logistic_acc_list)]\n",
    "print(f\"best regularization coefficient for sklearn logistic regression is {best_coef}\")\n",
    "\n",
    "# use the best hyperparameter to train the model\n",
    "logic = LogisticRegression(C = best_coef, fit_intercept = True, max_iter = 1000, tol = 1e-5)\n",
    "logic.fit(x_subtrain, y_subtrain)\n",
    "ypred = logic.predict(x_tune)\n",
    "acc_for_sk_logic = np.sum(ypred == y_tune) / y_tune.shape[0]\n",
    "print(f\"for sklearn logistic regression with C = {best_coef}, accuracy = {acc_for_sk_logic }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Difference\n",
    "1. The best accuracy gained from `mylogistic_l2` is pretty close to the accuracy gained from `sklearn.linear_model.LogisticRegression`\n",
    "2. When a1 = $0.01$ and a2 = $1.6681005372000592$, `mylogistic_l2` has the best accuracy; However in `sklearn.linear_model.LogisticRegression`, the best accuracy happens when the regularization coefficient is set to $0.5994842503189409$\n",
    "3. Although `mylogistic_l2` and `sklearn.linear_model.LogisticRegression` are nearly the same in best accuracy from tuning, they have a huge difference in computing time. `sklearn.linear_model.LogisticRegression` only needs $20.9$ seconds to finish $10$ rounds of hyperparameter tuning, but `mylogistic_l2` spends almost $60$ to $80$ seconds just for one model fit, thus $10$ rounds of tuning will take $500$ to $800$ seconds. It proves that our model is good at accuracy but doesn't perform well on efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('python39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1f0ca08df83ebe1228368c35aef2dd5bdef6efbf7330e5bb8a931893862d7fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
