{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWUn8hK4Ps3c"
      },
      "source": [
        "## 統計學習與深度學習 HW4\n",
        "##### B08705038 資管四 郭子麟"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_Hp5YrhqVIp",
        "outputId": "1227c8b4-a8af-4daf-c467-75bc345e8abf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Dec  4 05:43:45 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0    52W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC5ZLDT0bvZU",
        "outputId": "b00ff54c-3a46-40be-ec31-578e81d26446"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEvAAow1Ps3f"
      },
      "source": [
        "請將IPYNB檔與IPYNB Export之HTML檔上傳至COOL作業區。回答作業時建議使用 \"三明治\" 答題法。也就是說，先說明要做什麼，然後列出程式碼與結果，最後說明這些結果的意義。作業自己做。嚴禁抄襲。不接受紙本繳交，不接受遲交。請以英文或中文作答。\n",
        "\n",
        "#### Multilayer Perceptrons for Regression\n",
        "本次作業的主角是 Multilayer perceptrons (MLP)。我們將以MLP建構迴歸模型，探討各項相關議題。\n",
        "\n",
        "\n",
        "#### Dataset: Million Songs Dataset\n",
        "本次作業將使用\"Million Songs Dataset\"作為訓練與測試資料。請使用`pickle.load()`載入*msd_full.pickle*。這個資料集已經切割好了訓練與測試資料，並存放在一個Dictionary的結構。這個Dictionary有四個元素，x_train, y_train, x_test, y_test，分別對應到訓練特徵、訓練標記(Label)、測試特徵、測試標記。 標記變數 (label variable; i.e., $y$) 是歌曲發行年度。特徵為歌曲的聲音特性。迴歸任務為預測歌曲年分。\n",
        "\n",
        "#### Prediction Performance and Loss Function\n",
        "模型訓練應主要使用Sum of Squared Error (SSE)建構Loss Function，另外我們也會練習使用其他種類的Loss Function。為了讓圖表易於理解，不論Loss Function為何，報告預測能力應使用Root Mean Squared Error (RMSE)。使用SSE或RMSE建構Loss Function在本質上沒有差別。但SSE計算成本稍低，而RMSE較有直觀意義。\n",
        "\n",
        "\n",
        "#### Subtraining, Validation, and Test Datasets\n",
        "*msd_full.pickle* 檔案中的訓練資料已經隨機排序過。你應該使用訓練資料最後10%的資料做為Validation Set。其餘的前90%做為Subtraining Set。使用Subtraining Set來訓練資料，並以Validation Set作為參數調教與Early Stopping的依據。Test RMSE應使用測試資料計算得之。\n",
        "\n",
        "所有特徵應該標準化(均數為零，變異數為一)。標準化應該以訓練資料(注意不是Test Set or Subtraining Set)的統計量為之。標記變數(i.e., $y$)應將均數平移至0 (依照訓練資料的統計量)。標記變數的變異數不要調整。\n",
        "\n",
        "\n",
        "#### Minibatch, Epoch, and Early Stopping\n",
        "如果沒有特別說明，模型訓練時應以大小為1,000個資料點的Minibatch為之。模型使用一個Minibatch的資料更新參數之後稱為經歷了一個Batch。當所有Subtraining資料已經用來更新過模型參數，稱為經過了一個Epoch。\n",
        "\n",
        "模型訓練應使用Early Stopping決定最佳的模型。模型訓練時每100個Batch計算一次Training and Validation RMSE。如果Validation為歷史最低，則記下當下的模型參數與當時已進行的Batch數量，稱為best_step_count。如由best_step_count起算已經經過了5,000個Batch而沒有更好的Validation RMSE，則停止模型訓練，並以best_step_count時的模型參數做為最後的模型訓練結果。如果模型訓練最多執行100個epoch。如果模型已經執行了100個epoch而沒有Early Stop，則應使用歷史最佳的Validation RMSE所對應到的模型參數計算Test RMSE。\n",
        "\n",
        "\n",
        "#### Implementation Restriction\n",
        "使用Pytorch建構MLP模型。Ordinary Least Square (OLS)模型訓練沒有限制使用何種套件。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPZvuGU6Ps3h"
      },
      "source": [
        "### 資料載入\n",
        "使用下面的程式碼載入資料:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWYSAq6rcLO1"
      },
      "outputs": [],
      "source": [
        "!mkdir data\n",
        "!cp /content/drive/MyDrive/msd_full.pickle ./data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcmV-sn7Ps3i",
        "outputId": "99f54c6a-7dce-46b7-8003-b3c1e7f881d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape =  (463715, 90)\n",
            "X_subtrain shape =  (417344, 90)\n",
            "X_valid shape =  (46371, 90)\n",
            "Y_subtrain shape =  (417344,)\n",
            "Y_valid shape =  (46371,)\n",
            "X_test shape =  (51630, 90)\n"
          ]
        }
      ],
      "source": [
        "# load packages\n",
        "%matplotlib inline\n",
        "import pickle\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Load data\n",
        "with open('data/msd_full.pickle', 'rb') as fh1:\n",
        "    msd_data = pickle.load(fh1)\n",
        "\n",
        "doscaling = 1\n",
        "if (doscaling == 1):\n",
        "    xscaler = preprocessing.StandardScaler().fit(msd_data['X_train'])\n",
        "    # standardize feature values\n",
        "    X_train = xscaler.transform(msd_data['X_train'])\n",
        "    X_test = xscaler.transform(msd_data['X_test'])\n",
        "else:\n",
        "    X_train = msd_data['X_train']\n",
        "    X_test = msd_data['X_test']\n",
        "\n",
        "Y_train = msd_data['Y_train']\n",
        "Y_test = msd_data['Y_test'].astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "y_mean = Y_train.mean()\n",
        "Y_train_keep = Y_train.copy()\n",
        "Y_test_keep = Y_test.copy()\n",
        "Y_train = Y_train - y_mean\n",
        "Y_test = Y_test - y_mean\n",
        "\n",
        "\n",
        "# validation is the last 10% of training, subtraining is the first 90% of training\n",
        "nvalid = int(X_train.shape[0] * 0.1)\n",
        "nsubtrain = X_train.shape[0] - nvalid\n",
        "\n",
        "X_subtrain = X_train[0:nsubtrain, :].astype('float32')\n",
        "X_valid = X_train[nsubtrain:, :].astype('float32')\n",
        "Y_subtrain = Y_train[0:nsubtrain].astype('float32')\n",
        "Y_valid = Y_train[nsubtrain:].astype('float32')\n",
        "\n",
        "Y_subtrain_keep = Y_train_keep[0:nsubtrain].astype('float32')\n",
        "Y_valid_keep = Y_train_keep[nsubtrain:].astype('float32')\n",
        "\n",
        "print(\"X_train shape = \", X_train.shape)\n",
        "print(\"X_subtrain shape = \", X_subtrain.shape)\n",
        "print(\"X_valid shape = \", X_valid.shape)\n",
        "print(\"Y_subtrain shape = \", Y_subtrain.shape)\n",
        "print(\"Y_valid shape = \", Y_valid.shape)\n",
        "print(\"X_test shape = \", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5IundLSPs3k"
      },
      "source": [
        "### 回答下面問題"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlUC16fUPs3l"
      },
      "source": [
        "#### Q1 (5%)\n",
        "使用Training資料訓練一個Ordinary Least Square模型，並進行預測。列出此模型的RMSE與前五個特徵的參數。OLS模型應包含常數項，且不應有任何Regularization。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJkMCBXbPs3l",
        "outputId": "d5f91176-5b82-4805-95f4-5087a10c3ac4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE =  9.519576\n",
            "First 5 features' weights =  [ 5.2934246  -2.8868294  -1.5276392   0.06306529 -0.33957988]\n"
          ]
        }
      ],
      "source": [
        "# use ols model to fit the data and list first 5 features' weights and the RMSE. There should be no regularization.\n",
        "import numpy as np\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "ols = linear_model.LinearRegression()\n",
        "ols.fit(X_subtrain, Y_subtrain)\n",
        "Y_pred = ols.predict(X_valid)\n",
        "print(\"RMSE = \", np.sqrt(mean_squared_error(Y_valid, Y_pred)))\n",
        "print(\"First 5 features' weights = \", ols.coef_[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENIGy6XdPs3l"
      },
      "source": [
        "#### Q2 MLP with Four Hidden Layers (15%)\n",
        "建構一個有四層Hidden Layer的MLP。此模型由輸入層開始，90個Input Features通過線性層轉換為H個Hidden Nodes，並通過ReLu Activation Function，此為第一層Hidden Layer。\n",
        "接著通過下一個線性層與ReLu Activation Function，此為第二層。接著下一個線性層與ReLu Activation Function，此為第三層。\n",
        "然後下一個線性層與ReLu Activation Function，此為第四層。最後通過一個線性層輸出。\n",
        "所有Hidden Layer的寬度都為H。\n",
        "\n",
        "令H= 45, 使用Stochastic Gradient Descent更新參數，設Learning Rate = 0.00001，無Weight Decay與Momentum。畫出模型訓練過程中的Training與Validation RMSE，列出Test RMSE。 並討論訓練過程中Training與Validation RMSE的圖形意義。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL9iQcGdPs3m"
      },
      "source": [
        "##### Construct Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH4hPua-Ps3m"
      },
      "source": [
        "##### Construct a MLP Model Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "G_XzjKwoPs3m"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "\n",
        "class L2L1Loss(torch.nn.Module):\n",
        "    def __init__(self, loss_z):\n",
        "        super(L2L1Loss, self).__init__()\n",
        "        self.loss_z = loss_z\n",
        "    def forward(self, Y_pred, Y):\n",
        "        return self.loss_z * torch.nn.MSELoss(reduction='sum')(Y_pred, Y) + (1 - self.loss_z) * torch.nn.L1Loss(reduction='sum')(Y_pred, Y)\n",
        "\n",
        "class CustomizedL2Loss(torch.nn.Module):\n",
        "    def __init__(self, loss_z):\n",
        "        super(CustomizedL2Loss, self).__init__()\n",
        "        self.loss_z = loss_z\n",
        "    def forward(self, Y_pred, Y):\n",
        "        return self.loss_z * torch.nn.MSELoss(reduction='sum')(Y_pred, Y) + (1 - self.loss_z) * abs(torch.nn.L1Loss(reduction='sum')(Y_pred, Y))\n",
        "\n",
        "class MSDataset(data.Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.Y)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.Y[index]\n",
        "\n",
        "class MyMLP():\n",
        "    def __init__(self, X_subtrain, Y_subtrain, X_valid, Y_valid, H = 45, lr = 0.00001, optimizer_type = 0, loss_type = 0, wd = 0, mom = 0, use_dropout = False, dropout_rate = 0.5, loss_z = 0.5):\n",
        "        \"\"\"\n",
        "        #### Parameters\n",
        "        - X_subtrain: training data\n",
        "        - Y_subtrain: training label\n",
        "        - X_valid: validation data\n",
        "        - Y_valid: validation label\n",
        "        - H: the number of hidden units for each layer. Default is 45.\n",
        "        - lr: learning rate. Default: 0.00001\n",
        "        - wd: weight decay. Default: 0\n",
        "        - mom: momentum. Default: 0\n",
        "        - loss_type: the type of loss function. 0: SSE, 1: L2 + L1, 2: L2 customized. Default: 0\n",
        "        - optimizer_type: the type of optimizer. 0: SGD, 1: Adam. Default: 0\n",
        "        - use_dropout: whether to use dropout. Default: False\n",
        "        - dropout_rate: the rate of dropout. 0.5 by default\n",
        "        - loss_z: the parameter for L2 + L1 loss function. 0.5 by default\n",
        "        \"\"\"\n",
        "        self.train_dataset = MSDataset(X_subtrain, Y_subtrain)\n",
        "        self.valid_dataset = MSDataset(X_valid, Y_valid)\n",
        "        self.train_loader = data.DataLoader(self.train_dataset, batch_size=1000, shuffle=True)\n",
        "        self.valid_loader = data.DataLoader(self.valid_dataset, batch_size=1000, shuffle=False)\n",
        "        self.H = H\n",
        "        self.lr = lr\n",
        "        self.wd = wd\n",
        "        self.mom = mom\n",
        "        self.loss_type = loss_type\n",
        "        self.optimizer_type = optimizer_type\n",
        "        self.use_dropout = use_dropout\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.loss_z = loss_z\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = self.__get_model_sequential()\n",
        "        self.loss_function = self.__get_loss_function()\n",
        "        self.optimizer = self.__get_optimizer()\n",
        "\n",
        "    def __get_model_sequential(self):\n",
        "        if self.use_dropout:\n",
        "            model = torch.nn.Sequential(\n",
        "                torch.nn.Linear(90, self.H),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Dropout(p = self.dropout_rate),\n",
        "                torch.nn.Linear(self.H, self.H),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Dropout(p = self.dropout_rate),\n",
        "                torch.nn.Linear(self.H, self.H),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Dropout(p = self.dropout_rate),\n",
        "                torch.nn.Linear(self.H, self.H),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Dropout(p = self.dropout_rate),\n",
        "                torch.nn.Linear(self.H, 1)\n",
        "            )\n",
        "        else:\n",
        "            model = torch.nn.Sequential(\n",
        "                torch.nn.Linear(90, self.H),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Linear(self.H, self.H),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Linear(self.H, self.H),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Linear(self.H, self.H),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Linear(self.H, 1)\n",
        "            )\n",
        "        model.float().to(self.device)\n",
        "        return model\n",
        "    def __get_loss_function(self):\n",
        "        if self.loss_type == 0: # SSE\n",
        "            loss_function = torch.nn.MSELoss(reduction='sum')\n",
        "        elif self.loss_type == 1: # L2 + L1\n",
        "            loss_function = L2L1Loss(self.loss_z)\n",
        "        else: # L2 customized\n",
        "            loss_function = CustomizedL2Loss(self.loss_z)\n",
        "        return loss_function\n",
        "    def fit(self, max_epoch = 100, verbose = True, patience_batch_num = 5000, model_path = 'model.ckpt'):\n",
        "        \"\"\"\n",
        "        For every 100 batches, print the training loss and validation loss.\\n\n",
        "        It will return two lists: training loss and validation loss.\n",
        "        #### Parameters\n",
        "        - max_epoch: the maximum number of epochs. Default: 100\n",
        "        - verbose: whether to print the training information. Default: True\n",
        "        - patience_batch_num: the number of batches to wait before early stopping. Default: 5000\n",
        "        \"\"\"\n",
        "        cur_step = 0\n",
        "        cur_epoch = 0\n",
        "        patience = 0\n",
        "        best_valid_loss = np.inf\n",
        "        train_loss_list = []\n",
        "        valid_loss_list = []\n",
        "        while cur_epoch < max_epoch:\n",
        "            for _, (batch_X, batch_Y) in enumerate(self.train_loader):\n",
        "                self.model.train() # switch to train mode\n",
        "                self.optimizer.zero_grad()\n",
        "                batch_X = batch_X.to(self.device)\n",
        "                batch_Y = batch_Y.reshape((-1, 1)).to(self.device)\n",
        "                batch_Y_pred = self.model(batch_X)\n",
        "                loss = self.loss_function(batch_Y_pred, batch_Y)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                cur_step += 1\n",
        "                if cur_step % 100 == 0:\n",
        "                    # train loss should use RMSE for the training set\n",
        "                    train_loss = 0\n",
        "                    train_size = 0\n",
        "                    self.model.eval()\n",
        "                    for _, (batch_X, batch_Y) in enumerate(self.train_loader):\n",
        "                        batch_X = batch_X.to(self.device)\n",
        "                        batch_Y = batch_Y.reshape((-1, 1)).to(self.device)\n",
        "                        batch_Y_pred = self.model(batch_X)\n",
        "                        train_loss += torch.nn.MSELoss(reduction='sum')(batch_Y_pred, batch_Y).item()\n",
        "                        train_size += batch_X.shape[0]\n",
        "                    rmse_train_loss = np.sqrt(train_loss / train_size)\n",
        "                    train_loss_list.append(rmse_train_loss)\n",
        "                    valid_loss = 0\n",
        "                    valid_rmse_loss = 0\n",
        "                    valid_size = 0\n",
        "                    for _, (batch_X, batch_Y) in enumerate(self.valid_loader):\n",
        "                        valid_size += batch_X.shape[0]\n",
        "                        batch_X = batch_X.to(self.device)\n",
        "                        batch_Y = batch_Y.reshape((-1, 1)).to(self.device)\n",
        "                        batch_Y_pred = self.model(batch_X)\n",
        "                        loss = self.loss_function(batch_Y_pred, batch_Y)\n",
        "                        valid_loss += loss.item()\n",
        "                        valid_rmse_loss += torch.nn.MSELoss(reduction='sum')(batch_Y_pred, batch_Y).item()\n",
        "                    valid_rmse_loss = np.sqrt(valid_rmse_loss / valid_size)\n",
        "                    valid_loss_list.append(valid_rmse_loss)\n",
        "                    if valid_loss < best_valid_loss:\n",
        "                        best_valid_loss = valid_loss\n",
        "                        best_model = copy.deepcopy(self.model)\n",
        "                        patience = 0\n",
        "                    else:\n",
        "                        patience += 1\n",
        "                    if verbose:\n",
        "                        print(\"Epoch: {}, Step: {}, Train Loss: {}, Valid Loss: {}\".format(cur_epoch, cur_step, train_loss_list[-1], valid_loss_list[-1]))\n",
        "                    if patience >= patience_batch_num:\n",
        "                        self.model = best_model\n",
        "                        print(\"Early stopping at epoch {}, step {}\".format(cur_epoch, cur_step))\n",
        "                        torch.save(self.model, model_path)\n",
        "                        return train_loss_list, valid_loss_list\n",
        "            cur_epoch += 1\n",
        "        self.model = best_model\n",
        "        torch.save(self.model, model_path)\n",
        "        return train_loss_list, valid_loss_list\n",
        "    def __get_optimizer(self):\n",
        "        if self.optimizer_type == 0: # SGD\n",
        "            optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, weight_decay=self.wd, momentum=self.mom)\n",
        "        else: # Adam\n",
        "            optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.wd)\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JrfDhIhQPs3o"
      },
      "outputs": [],
      "source": [
        "def calculate_test_rmse(model, X_test, Y_test):\n",
        "    test_dataset = MSDataset(X_test, Y_test)\n",
        "    test_loader = data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "    model.eval()\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    test_loss = 0\n",
        "    test_size = 0\n",
        "    for _, (batch_X, batch_Y) in enumerate(test_loader):\n",
        "        batch_X = batch_X.to(device)\n",
        "        batch_Y = batch_Y.reshape((-1, 1)).to(device)\n",
        "        batch_Y_pred = model(batch_X)\n",
        "        test_loss += torch.nn.MSELoss(reduction='sum')(batch_Y_pred, batch_Y).item()\n",
        "        test_size += batch_X.shape[0]\n",
        "    test_rmse_loss = np.sqrt(test_loss / test_size)\n",
        "    return test_rmse_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8tlIPSvPs3o"
      },
      "source": [
        "##### Construct a MLP Model and Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlxDYP-APs3o",
        "outputId": "75493dfc-c52d-4267-a024-db75e251a2cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Step: 100, Train Loss: 9.991095267968475, Valid Loss: 9.947883158336252\n",
            "Epoch: 0, Step: 200, Train Loss: 9.043213823832458, Valid Loss: 8.973001691702606\n",
            "Epoch: 0, Step: 300, Train Loss: 8.939802725644984, Valid Loss: 8.884352073443294\n",
            "Epoch: 0, Step: 400, Train Loss: 8.983293837944046, Valid Loss: 8.921108270821941\n",
            "Epoch: 1, Step: 500, Train Loss: 8.821180968863858, Valid Loss: 8.78018643079955\n",
            "Epoch: 1, Step: 600, Train Loss: 8.792730461404659, Valid Loss: 8.752487296066953\n",
            "Epoch: 1, Step: 700, Train Loss: 8.748214699132635, Valid Loss: 8.705454730923588\n",
            "Epoch: 1, Step: 800, Train Loss: 8.745971351819644, Valid Loss: 8.71993192510473\n",
            "Epoch: 2, Step: 900, Train Loss: 8.745709111459579, Valid Loss: 8.723014076298416\n",
            "Epoch: 2, Step: 1000, Train Loss: 8.698302019023007, Valid Loss: 8.688137410642652\n",
            "Epoch: 2, Step: 1100, Train Loss: 8.697569482671522, Valid Loss: 8.686405671593237\n",
            "Epoch: 2, Step: 1200, Train Loss: 8.659579219780065, Valid Loss: 8.65800456523078\n",
            "Epoch: 3, Step: 1300, Train Loss: 8.795022100808046, Valid Loss: 8.797384431135928\n",
            "Epoch: 3, Step: 1400, Train Loss: 8.627824535950452, Valid Loss: 8.64125250955336\n",
            "Epoch: 3, Step: 1500, Train Loss: 8.61428142486272, Valid Loss: 8.634817011200147\n",
            "Epoch: 3, Step: 1600, Train Loss: 8.612040457922909, Valid Loss: 8.63184415567681\n",
            "Epoch: 4, Step: 1700, Train Loss: 8.62351170488714, Valid Loss: 8.641916864818185\n",
            "Epoch: 4, Step: 1800, Train Loss: 8.605460183213772, Valid Loss: 8.633513140377705\n",
            "Epoch: 4, Step: 1900, Train Loss: 8.56661070247598, Valid Loss: 8.592080084566902\n",
            "Epoch: 4, Step: 2000, Train Loss: 8.552685778329517, Valid Loss: 8.59404022473932\n",
            "Epoch: 5, Step: 2100, Train Loss: 8.549124275280416, Valid Loss: 8.605469427067211\n",
            "Epoch: 5, Step: 2200, Train Loss: 8.598706913042623, Valid Loss: 8.652883050917287\n",
            "Epoch: 5, Step: 2300, Train Loss: 8.617121065400504, Valid Loss: 8.675522275068973\n",
            "Epoch: 5, Step: 2400, Train Loss: 8.530950809131815, Valid Loss: 8.591715634969207\n",
            "Epoch: 5, Step: 2500, Train Loss: 8.511479679510595, Valid Loss: 8.601589021035618\n",
            "Epoch: 6, Step: 2600, Train Loss: 8.55575067375109, Valid Loss: 8.649560510383694\n",
            "Epoch: 6, Step: 2700, Train Loss: 8.564391812055968, Valid Loss: 8.631483644376862\n",
            "Epoch: 6, Step: 2800, Train Loss: 8.49384532593724, Valid Loss: 8.597241039041926\n",
            "Epoch: 6, Step: 2900, Train Loss: 8.525164551804655, Valid Loss: 8.608953217324903\n",
            "Epoch: 7, Step: 3000, Train Loss: 8.509331992074378, Valid Loss: 8.60767344752211\n",
            "Epoch: 7, Step: 3100, Train Loss: 8.679753389399371, Valid Loss: 8.773875123365796\n",
            "Epoch: 7, Step: 3200, Train Loss: 8.538818472247447, Valid Loss: 8.648460110209124\n",
            "Epoch: 7, Step: 3300, Train Loss: 8.471478126959736, Valid Loss: 8.591762292890818\n",
            "Epoch: 8, Step: 3400, Train Loss: 8.500623164287664, Valid Loss: 8.604765083486631\n",
            "Epoch: 8, Step: 3500, Train Loss: 8.46686247658102, Valid Loss: 8.598292755332526\n",
            "Epoch: 8, Step: 3600, Train Loss: 8.482826442635586, Valid Loss: 8.59703232602318\n",
            "Epoch: 8, Step: 3700, Train Loss: 8.467179948198284, Valid Loss: 8.592729097817694\n",
            "Epoch: 9, Step: 3800, Train Loss: 8.45277864003882, Valid Loss: 8.58835471080099\n",
            "Epoch: 9, Step: 3900, Train Loss: 8.503674005572728, Valid Loss: 8.629146073455278\n",
            "Epoch: 9, Step: 4000, Train Loss: 8.456449607216495, Valid Loss: 8.594588033140909\n",
            "Epoch: 9, Step: 4100, Train Loss: 8.449416643043469, Valid Loss: 8.5900698907573\n",
            "Epoch: 10, Step: 4200, Train Loss: 8.432170893444937, Valid Loss: 8.60209860714269\n",
            "Epoch: 10, Step: 4300, Train Loss: 8.432324238115054, Valid Loss: 8.606574597108708\n",
            "Epoch: 10, Step: 4400, Train Loss: 8.443380080767046, Valid Loss: 8.58285969221775\n",
            "Epoch: 10, Step: 4500, Train Loss: 8.427230623141273, Valid Loss: 8.563331929253813\n",
            "Epoch: 11, Step: 4600, Train Loss: 8.417278028165653, Valid Loss: 8.57047490194657\n",
            "Epoch: 11, Step: 4700, Train Loss: 8.469635857362437, Valid Loss: 8.622927125461098\n",
            "Epoch: 11, Step: 4800, Train Loss: 8.466892550865577, Valid Loss: 8.642404617712593\n",
            "Epoch: 11, Step: 4900, Train Loss: 8.445479402239958, Valid Loss: 8.6305776139671\n",
            "Epoch: 11, Step: 5000, Train Loss: 8.448477402585747, Valid Loss: 8.591025820671966\n",
            "Epoch: 12, Step: 5100, Train Loss: 8.406035824241684, Valid Loss: 8.583469655604663\n",
            "Epoch: 12, Step: 5200, Train Loss: 8.402065790133488, Valid Loss: 8.57301270194605\n",
            "Epoch: 12, Step: 5300, Train Loss: 8.452928143960953, Valid Loss: 8.601610267858755\n",
            "Epoch: 12, Step: 5400, Train Loss: 8.418039263986163, Valid Loss: 8.583734764834011\n",
            "Epoch: 13, Step: 5500, Train Loss: 8.394618409535438, Valid Loss: 8.569301061336102\n",
            "Epoch: 13, Step: 5600, Train Loss: 8.40936840972951, Valid Loss: 8.598944292490119\n",
            "Epoch: 13, Step: 5700, Train Loss: 8.433942294071372, Valid Loss: 8.617543411129489\n",
            "Epoch: 13, Step: 5800, Train Loss: 8.382835334982504, Valid Loss: 8.568454185565201\n",
            "Epoch: 14, Step: 5900, Train Loss: 8.376612271969998, Valid Loss: 8.60397543491356\n",
            "Epoch: 14, Step: 6000, Train Loss: 8.388271431649283, Valid Loss: 8.572878611811804\n",
            "Epoch: 14, Step: 6100, Train Loss: 8.388385020321593, Valid Loss: 8.599210668357411\n",
            "Epoch: 14, Step: 6200, Train Loss: 8.45241187099975, Valid Loss: 8.635716183094905\n",
            "Epoch: 15, Step: 6300, Train Loss: 8.393093040028708, Valid Loss: 8.571175377731844\n",
            "Epoch: 15, Step: 6400, Train Loss: 8.39619380334312, Valid Loss: 8.61655898219434\n",
            "Epoch: 15, Step: 6500, Train Loss: 8.381883470781721, Valid Loss: 8.573792731588464\n",
            "Epoch: 15, Step: 6600, Train Loss: 8.420013996107121, Valid Loss: 8.6386611092142\n",
            "Epoch: 16, Step: 6700, Train Loss: 8.40142935160275, Valid Loss: 8.615371161002889\n",
            "Epoch: 16, Step: 6800, Train Loss: 8.373921769444962, Valid Loss: 8.605171135917852\n",
            "Epoch: 16, Step: 6900, Train Loss: 8.341599052589356, Valid Loss: 8.573259101992544\n",
            "Epoch: 16, Step: 7000, Train Loss: 8.428591518050629, Valid Loss: 8.664345229133385\n",
            "Epoch: 16, Step: 7100, Train Loss: 8.35695273564997, Valid Loss: 8.572043624153764\n",
            "Epoch: 17, Step: 7200, Train Loss: 8.419571689542325, Valid Loss: 8.619434572643943\n",
            "Epoch: 17, Step: 7300, Train Loss: 8.33598573382246, Valid Loss: 8.551527277870742\n",
            "Epoch: 17, Step: 7400, Train Loss: 8.32519516268196, Valid Loss: 8.566346908554072\n",
            "Epoch: 17, Step: 7500, Train Loss: 8.309929537704129, Valid Loss: 8.564050274241527\n",
            "Epoch: 18, Step: 7600, Train Loss: 8.348722895767432, Valid Loss: 8.58384503200298\n",
            "Epoch: 18, Step: 7700, Train Loss: 8.344413555009112, Valid Loss: 8.57168923619708\n",
            "Epoch: 18, Step: 7800, Train Loss: 8.328401311714895, Valid Loss: 8.56881360064415\n",
            "Epoch: 18, Step: 7900, Train Loss: 8.320474641038292, Valid Loss: 8.570388067336031\n",
            "Epoch: 19, Step: 8000, Train Loss: 8.331227030840335, Valid Loss: 8.578733124087217\n",
            "Epoch: 19, Step: 8100, Train Loss: 8.34765450968428, Valid Loss: 8.612026628744518\n",
            "Epoch: 19, Step: 8200, Train Loss: 8.30885968114061, Valid Loss: 8.572800748416464\n",
            "Epoch: 19, Step: 8300, Train Loss: 8.331291154774929, Valid Loss: 8.550857480998188\n",
            "Epoch: 20, Step: 8400, Train Loss: 8.317805698218184, Valid Loss: 8.563632167142647\n",
            "Epoch: 20, Step: 8500, Train Loss: 8.32346037004085, Valid Loss: 8.579378058425037\n",
            "Epoch: 20, Step: 8600, Train Loss: 8.307289366580916, Valid Loss: 8.589299594731909\n",
            "Epoch: 20, Step: 8700, Train Loss: 8.320545957492604, Valid Loss: 8.58092930591091\n",
            "Epoch: 21, Step: 8800, Train Loss: 8.309727760913345, Valid Loss: 8.564128140963398\n",
            "Epoch: 21, Step: 8900, Train Loss: 8.317039141015542, Valid Loss: 8.609785542705454\n",
            "Epoch: 21, Step: 9000, Train Loss: 8.292642573242723, Valid Loss: 8.591649340330756\n",
            "Epoch: 21, Step: 9100, Train Loss: 8.30408973484476, Valid Loss: 8.584033764086772\n",
            "Epoch: 22, Step: 9200, Train Loss: 8.304541377453335, Valid Loss: 8.615057814613246\n",
            "Epoch: 22, Step: 9300, Train Loss: 8.300843322274613, Valid Loss: 8.60238360866687\n",
            "Epoch: 22, Step: 9400, Train Loss: 8.346923273075976, Valid Loss: 8.58076197824651\n",
            "Epoch: 22, Step: 9500, Train Loss: 8.323996932356714, Valid Loss: 8.582643356524407\n",
            "Epoch: 22, Step: 9600, Train Loss: 8.282954178863642, Valid Loss: 8.565283623028495\n",
            "Epoch: 23, Step: 9700, Train Loss: 8.32870662459963, Valid Loss: 8.627134695750357\n",
            "Epoch: 23, Step: 9800, Train Loss: 8.291482892892066, Valid Loss: 8.587614044673167\n",
            "Epoch: 23, Step: 9900, Train Loss: 8.277072813129012, Valid Loss: 8.561357007337907\n",
            "Epoch: 23, Step: 10000, Train Loss: 8.297499107701753, Valid Loss: 8.608841532440731\n",
            "Epoch: 24, Step: 10100, Train Loss: 8.285459629763066, Valid Loss: 8.581127099210152\n",
            "Epoch: 24, Step: 10200, Train Loss: 8.260912967896619, Valid Loss: 8.585721255732754\n",
            "Epoch: 24, Step: 10300, Train Loss: 8.259910777275863, Valid Loss: 8.581563696937584\n",
            "Epoch: 24, Step: 10400, Train Loss: 8.262143105846834, Valid Loss: 8.576569917175705\n",
            "Epoch: 25, Step: 10500, Train Loss: 8.259011752675669, Valid Loss: 8.564049317655574\n",
            "Epoch: 25, Step: 10600, Train Loss: 8.267458649149022, Valid Loss: 8.574581588615777\n",
            "Epoch: 25, Step: 10700, Train Loss: 8.270781586780314, Valid Loss: 8.554673397444583\n",
            "Epoch: 25, Step: 10800, Train Loss: 8.255907782717012, Valid Loss: 8.579952317870552\n",
            "Epoch: 26, Step: 10900, Train Loss: 8.272582125527231, Valid Loss: 8.582943632904733\n",
            "Epoch: 26, Step: 11000, Train Loss: 8.295094662681107, Valid Loss: 8.591744237641093\n",
            "Epoch: 26, Step: 11100, Train Loss: 8.271533320425041, Valid Loss: 8.606286663359821\n",
            "Epoch: 26, Step: 11200, Train Loss: 8.263421067331304, Valid Loss: 8.622605043682176\n",
            "Epoch: 27, Step: 11300, Train Loss: 8.234703128262662, Valid Loss: 8.570076285549519\n",
            "Epoch: 27, Step: 11400, Train Loss: 8.290220888065894, Valid Loss: 8.619094753338912\n",
            "Epoch: 27, Step: 11500, Train Loss: 8.304159035832766, Valid Loss: 8.706527239300396\n",
            "Epoch: 27, Step: 11600, Train Loss: 8.240009632992583, Valid Loss: 8.596593875492148\n",
            "Epoch: 27, Step: 11700, Train Loss: 8.244514155971762, Valid Loss: 8.572448406237918\n",
            "Epoch: 28, Step: 11800, Train Loss: 8.253340292138416, Valid Loss: 8.625685488350925\n",
            "Epoch: 28, Step: 11900, Train Loss: 8.235569493274852, Valid Loss: 8.592852376314834\n",
            "Epoch: 28, Step: 12000, Train Loss: 8.271572976621405, Valid Loss: 8.674691280205781\n",
            "Epoch: 28, Step: 12100, Train Loss: 8.294384226780299, Valid Loss: 8.607511873192204\n",
            "Epoch: 29, Step: 12200, Train Loss: 8.233354878698462, Valid Loss: 8.653802510783372\n",
            "Epoch: 29, Step: 12300, Train Loss: 8.249310513561618, Valid Loss: 8.573301883389824\n",
            "Epoch: 29, Step: 12400, Train Loss: 8.2370299134502, Valid Loss: 8.592860782716528\n",
            "Epoch: 29, Step: 12500, Train Loss: 8.27170786261774, Valid Loss: 8.639038009809854\n",
            "Epoch: 30, Step: 12600, Train Loss: 8.250389727976547, Valid Loss: 8.630751017924826\n",
            "Epoch: 30, Step: 12700, Train Loss: 8.220933459488688, Valid Loss: 8.583038776141269\n",
            "Epoch: 30, Step: 12800, Train Loss: 8.236402929178546, Valid Loss: 8.591923589837231\n",
            "Epoch: 30, Step: 12900, Train Loss: 8.23160037531827, Valid Loss: 8.607653879375547\n",
            "Epoch: 31, Step: 13000, Train Loss: 8.218112226142873, Valid Loss: 8.632219182608516\n",
            "Epoch: 31, Step: 13100, Train Loss: 8.230104718376353, Valid Loss: 8.619769724269267\n",
            "Epoch: 31, Step: 13200, Train Loss: 8.251189591658664, Valid Loss: 8.634169403008856\n",
            "Epoch: 31, Step: 13300, Train Loss: 8.208723021492077, Valid Loss: 8.590001023884179\n",
            "Epoch: 32, Step: 13400, Train Loss: 8.220091254048336, Valid Loss: 8.620132909592565\n",
            "Epoch: 32, Step: 13500, Train Loss: 8.238640093686369, Valid Loss: 8.613479750361627\n",
            "Epoch: 32, Step: 13600, Train Loss: 8.220167660627842, Valid Loss: 8.581219631717921\n",
            "Epoch: 32, Step: 13700, Train Loss: 8.230998284513603, Valid Loss: 8.569122186753663\n",
            "Epoch: 33, Step: 13800, Train Loss: 8.20426442876732, Valid Loss: 8.637479250879842\n",
            "Epoch: 33, Step: 13900, Train Loss: 8.201169194681764, Valid Loss: 8.588837269202822\n",
            "Epoch: 33, Step: 14000, Train Loss: 8.246559371060874, Valid Loss: 8.610216675969262\n",
            "Epoch: 33, Step: 14100, Train Loss: 8.235778534089507, Valid Loss: 8.623105272250896\n",
            "Epoch: 33, Step: 14200, Train Loss: 8.212067060370138, Valid Loss: 8.655706508940183\n",
            "Epoch: 34, Step: 14300, Train Loss: 8.231883971351749, Valid Loss: 8.631558206729704\n",
            "Epoch: 34, Step: 14400, Train Loss: 8.295349914208908, Valid Loss: 8.762781140856521\n",
            "Epoch: 34, Step: 14500, Train Loss: 8.194999782929742, Valid Loss: 8.596531643493103\n",
            "Epoch: 34, Step: 14600, Train Loss: 8.223721956585083, Valid Loss: 8.61332505521836\n",
            "Epoch: 35, Step: 14700, Train Loss: 8.233268410827778, Valid Loss: 8.645459414192905\n",
            "Epoch: 35, Step: 14800, Train Loss: 8.227006906290748, Valid Loss: 8.676715975650751\n",
            "Epoch: 35, Step: 14900, Train Loss: 8.233360864016651, Valid Loss: 8.616800600503366\n",
            "Epoch: 35, Step: 15000, Train Loss: 8.230121809983759, Valid Loss: 8.62550722150021\n",
            "Epoch: 36, Step: 15100, Train Loss: 8.179640080099976, Valid Loss: 8.614850731092337\n",
            "Epoch: 36, Step: 15200, Train Loss: 8.205405032200753, Valid Loss: 8.6049830167285\n",
            "Epoch: 36, Step: 15300, Train Loss: 8.245907665980184, Valid Loss: 8.637226214825096\n",
            "Epoch: 36, Step: 15400, Train Loss: 8.193054393971403, Valid Loss: 8.596311817837977\n",
            "Epoch: 37, Step: 15500, Train Loss: 8.191207718677544, Valid Loss: 8.608211079505105\n",
            "Epoch: 37, Step: 15600, Train Loss: 8.174919039086951, Valid Loss: 8.638768330108094\n",
            "Epoch: 37, Step: 15700, Train Loss: 8.183231723568232, Valid Loss: 8.645505920918476\n",
            "Epoch: 37, Step: 15800, Train Loss: 8.194051261164233, Valid Loss: 8.60728784065216\n",
            "Epoch: 38, Step: 15900, Train Loss: 8.184382191079068, Valid Loss: 8.677009860775769\n",
            "Epoch: 38, Step: 16000, Train Loss: 8.209693208875937, Valid Loss: 8.606358935834505\n",
            "Epoch: 38, Step: 16100, Train Loss: 8.188952754917473, Valid Loss: 8.662922064372468\n",
            "Epoch: 38, Step: 16200, Train Loss: 8.168678580843649, Valid Loss: 8.610157230500395\n",
            "Epoch: 38, Step: 16300, Train Loss: 8.15538495624473, Valid Loss: 8.624568299357886\n",
            "Epoch: 39, Step: 16400, Train Loss: 8.222597040242132, Valid Loss: 8.656900776803298\n",
            "Epoch: 39, Step: 16500, Train Loss: 8.252082963904835, Valid Loss: 8.641187452831378\n",
            "Epoch: 39, Step: 16600, Train Loss: 8.167731420011478, Valid Loss: 8.608528024213582\n",
            "Epoch: 39, Step: 16700, Train Loss: 8.194619349664825, Valid Loss: 8.61090638363869\n",
            "Epoch: 40, Step: 16800, Train Loss: 8.162466673743314, Valid Loss: 8.60258620141962\n",
            "Epoch: 40, Step: 16900, Train Loss: 8.192213426060691, Valid Loss: 8.631036976534318\n",
            "Epoch: 40, Step: 17000, Train Loss: 8.178082144139418, Valid Loss: 8.600137156963036\n",
            "Epoch: 40, Step: 17100, Train Loss: 8.216253499632934, Valid Loss: 8.626298795446766\n",
            "Epoch: 41, Step: 17200, Train Loss: 8.215799722425235, Valid Loss: 8.648911057409132\n",
            "Epoch: 41, Step: 17300, Train Loss: 8.213730245579177, Valid Loss: 8.673747426348958\n",
            "Epoch: 41, Step: 17400, Train Loss: 8.208907063018914, Valid Loss: 8.594707986846917\n",
            "Epoch: 41, Step: 17500, Train Loss: 8.165189311801905, Valid Loss: 8.617939145286968\n",
            "Epoch: 42, Step: 17600, Train Loss: 8.15818968450835, Valid Loss: 8.6645185600789\n",
            "Epoch: 42, Step: 17700, Train Loss: 8.191118164310943, Valid Loss: 8.615843546086149\n",
            "Epoch: 42, Step: 17800, Train Loss: 8.180932018185382, Valid Loss: 8.636811457534685\n",
            "Epoch: 42, Step: 17900, Train Loss: 8.140467917079745, Valid Loss: 8.616170991470831\n",
            "Epoch: 43, Step: 18000, Train Loss: 8.151877190084218, Valid Loss: 8.596367282509167\n",
            "Epoch: 43, Step: 18100, Train Loss: 8.16911059536611, Valid Loss: 8.624290873200787\n",
            "Epoch: 43, Step: 18200, Train Loss: 8.175079684491042, Valid Loss: 8.7023619151201\n",
            "Epoch: 43, Step: 18300, Train Loss: 8.151569335486858, Valid Loss: 8.65010046176505\n",
            "Epoch: 44, Step: 18400, Train Loss: 8.18581332166531, Valid Loss: 8.651106741688135\n",
            "Epoch: 44, Step: 18500, Train Loss: 8.168702745734722, Valid Loss: 8.64192115869216\n",
            "Epoch: 44, Step: 18600, Train Loss: 8.143392904984884, Valid Loss: 8.634812789400518\n",
            "Epoch: 44, Step: 18700, Train Loss: 8.152671667207947, Valid Loss: 8.623503037013151\n",
            "Epoch: 44, Step: 18800, Train Loss: 8.20253539752012, Valid Loss: 8.64755151625653\n",
            "Epoch: 45, Step: 18900, Train Loss: 8.184160360726239, Valid Loss: 8.617254951862504\n",
            "Epoch: 45, Step: 19000, Train Loss: 8.128901096674504, Valid Loss: 8.605578659548359\n",
            "Epoch: 45, Step: 19100, Train Loss: 8.16667577045303, Valid Loss: 8.648014718572101\n",
            "Epoch: 45, Step: 19200, Train Loss: 8.158768332589586, Valid Loss: 8.641171935558583\n",
            "Epoch: 46, Step: 19300, Train Loss: 8.161396145675198, Valid Loss: 8.718175866761632\n",
            "Epoch: 46, Step: 19400, Train Loss: 8.133068592337839, Valid Loss: 8.64337026560495\n",
            "Epoch: 46, Step: 19500, Train Loss: 8.160169123892091, Valid Loss: 8.622105692271074\n",
            "Epoch: 46, Step: 19600, Train Loss: 8.12795220934876, Valid Loss: 8.622319289037561\n",
            "Epoch: 47, Step: 19700, Train Loss: 8.1571274966639, Valid Loss: 8.679303281032144\n",
            "Epoch: 47, Step: 19800, Train Loss: 8.13840843745451, Valid Loss: 8.660935097053928\n",
            "Epoch: 47, Step: 19900, Train Loss: 8.123709741369133, Valid Loss: 8.661416057778514\n",
            "Epoch: 47, Step: 20000, Train Loss: 8.139018351429133, Valid Loss: 8.658878311324445\n",
            "Epoch: 48, Step: 20100, Train Loss: 8.146019253943644, Valid Loss: 8.625548837937329\n",
            "Epoch: 48, Step: 20200, Train Loss: 8.143991490956465, Valid Loss: 8.615772516671974\n",
            "Epoch: 48, Step: 20300, Train Loss: 8.163173681282743, Valid Loss: 8.640407236056278\n",
            "Epoch: 48, Step: 20400, Train Loss: 8.132965136318697, Valid Loss: 8.703472767329814\n",
            "Epoch: 49, Step: 20500, Train Loss: 8.14295096851101, Valid Loss: 8.650803906454787\n",
            "Epoch: 49, Step: 20600, Train Loss: 8.116196358674491, Valid Loss: 8.630669770024978\n",
            "Epoch: 49, Step: 20700, Train Loss: 8.152687083102977, Valid Loss: 8.602542654968374\n",
            "Epoch: 49, Step: 20800, Train Loss: 8.148154879925514, Valid Loss: 8.67348758630114\n",
            "Epoch: 49, Step: 20900, Train Loss: 8.157723959898483, Valid Loss: 8.732296202801335\n",
            "Epoch: 50, Step: 21000, Train Loss: 8.12298267448173, Valid Loss: 8.652407637531912\n",
            "Epoch: 50, Step: 21100, Train Loss: 8.140894173616951, Valid Loss: 8.639633839853687\n",
            "Epoch: 50, Step: 21200, Train Loss: 8.210070853115127, Valid Loss: 8.821358291596955\n",
            "Epoch: 50, Step: 21300, Train Loss: 8.150141010804807, Valid Loss: 8.616545806018548\n",
            "Epoch: 51, Step: 21400, Train Loss: 8.228635056248368, Valid Loss: 8.841302840840196\n",
            "Epoch: 51, Step: 21500, Train Loss: 8.146511421434283, Valid Loss: 8.742620965976936\n",
            "Epoch: 51, Step: 21600, Train Loss: 8.115425497111827, Valid Loss: 8.674427763610185\n",
            "Epoch: 51, Step: 21700, Train Loss: 8.114511717054954, Valid Loss: 8.638600450394568\n",
            "Epoch: 52, Step: 21800, Train Loss: 8.121935511241695, Valid Loss: 8.686933264673636\n",
            "Epoch: 52, Step: 21900, Train Loss: 8.11713479243975, Valid Loss: 8.655352733786625\n",
            "Epoch: 52, Step: 22000, Train Loss: 8.133957430988595, Valid Loss: 8.6738650397925\n",
            "Epoch: 52, Step: 22100, Train Loss: 8.122348721163876, Valid Loss: 8.708391275111914\n",
            "Epoch: 53, Step: 22200, Train Loss: 8.11095825007565, Valid Loss: 8.632461643387243\n",
            "Epoch: 53, Step: 22300, Train Loss: 8.191396363776086, Valid Loss: 8.78354645732573\n",
            "Epoch: 53, Step: 22400, Train Loss: 8.118950356904396, Valid Loss: 8.645621241513291\n",
            "Epoch: 53, Step: 22500, Train Loss: 8.090025388417672, Valid Loss: 8.632373551360574\n",
            "Epoch: 54, Step: 22600, Train Loss: 8.134867924767894, Valid Loss: 8.676287309391832\n",
            "Epoch: 54, Step: 22700, Train Loss: 8.089675569253465, Valid Loss: 8.646086147533458\n",
            "Epoch: 54, Step: 22800, Train Loss: 8.119947078543268, Valid Loss: 8.64917355935135\n",
            "Epoch: 54, Step: 22900, Train Loss: 8.120269628904435, Valid Loss: 8.621148371803773\n",
            "Epoch: 55, Step: 23000, Train Loss: 8.077724995260384, Valid Loss: 8.643553860514446\n",
            "Epoch: 55, Step: 23100, Train Loss: 8.103152840556422, Valid Loss: 8.686831760892119\n",
            "Epoch: 55, Step: 23200, Train Loss: 8.127699760082132, Valid Loss: 8.633036662748685\n",
            "Epoch: 55, Step: 23300, Train Loss: 8.093023999117746, Valid Loss: 8.665452628570264\n",
            "Epoch: 55, Step: 23400, Train Loss: 8.114867738989549, Valid Loss: 8.639215124763417\n",
            "Epoch: 56, Step: 23500, Train Loss: 8.099377117019134, Valid Loss: 8.671360151026683\n",
            "Epoch: 56, Step: 23600, Train Loss: 8.133164139780234, Valid Loss: 8.657274090686963\n",
            "Epoch: 56, Step: 23700, Train Loss: 8.109952596813661, Valid Loss: 8.659550366145133\n",
            "Epoch: 56, Step: 23800, Train Loss: 8.096525861355476, Valid Loss: 8.63429324351747\n",
            "Epoch: 57, Step: 23900, Train Loss: 8.185861011666178, Valid Loss: 8.79990385474176\n",
            "Epoch: 57, Step: 24000, Train Loss: 8.082417641774578, Valid Loss: 8.642599142427738\n",
            "Epoch: 57, Step: 24100, Train Loss: 8.07948467973268, Valid Loss: 8.681881116757008\n",
            "Epoch: 57, Step: 24200, Train Loss: 8.129590962971905, Valid Loss: 8.738743755773136\n",
            "Epoch: 58, Step: 24300, Train Loss: 8.117213358981093, Valid Loss: 8.655981492765441\n",
            "Epoch: 58, Step: 24400, Train Loss: 8.077714323484061, Valid Loss: 8.669877054066493\n",
            "Epoch: 58, Step: 24500, Train Loss: 8.0824000088182, Valid Loss: 8.661290971120774\n",
            "Epoch: 58, Step: 24600, Train Loss: 8.090021443204623, Valid Loss: 8.629841869932115\n",
            "Epoch: 59, Step: 24700, Train Loss: 8.071171853915072, Valid Loss: 8.744188467115915\n",
            "Epoch: 59, Step: 24800, Train Loss: 8.180241976381108, Valid Loss: 8.699031063871766\n",
            "Epoch: 59, Step: 24900, Train Loss: 8.133984017982032, Valid Loss: 8.660734992621084\n",
            "Epoch: 59, Step: 25000, Train Loss: 8.142012017026744, Valid Loss: 8.841335190392128\n",
            "Epoch: 60, Step: 25100, Train Loss: 8.072575933134932, Valid Loss: 8.685483463172611\n",
            "Epoch: 60, Step: 25200, Train Loss: 8.09650152526271, Valid Loss: 8.66294458291478\n",
            "Epoch: 60, Step: 25300, Train Loss: 8.11962664562499, Valid Loss: 8.659202940080624\n",
            "Epoch: 60, Step: 25400, Train Loss: 8.09008595413821, Valid Loss: 8.650340347104319\n",
            "Epoch: 61, Step: 25500, Train Loss: 8.101537835043281, Valid Loss: 8.63572218712857\n",
            "Epoch: 61, Step: 25600, Train Loss: 8.114245128178304, Valid Loss: 8.644459975552797\n",
            "Epoch: 61, Step: 25700, Train Loss: 8.138488789675309, Valid Loss: 8.70889611598407\n",
            "Epoch: 61, Step: 25800, Train Loss: 8.116559896603896, Valid Loss: 8.665475065196995\n",
            "Epoch: 61, Step: 25900, Train Loss: 8.088540542158466, Valid Loss: 8.648446965590038\n",
            "Epoch: 62, Step: 26000, Train Loss: 8.08090527948669, Valid Loss: 8.677805568066454\n",
            "Epoch: 62, Step: 26100, Train Loss: 8.111137834905067, Valid Loss: 8.652344037458697\n",
            "Epoch: 62, Step: 26200, Train Loss: 8.10450772144807, Valid Loss: 8.665787905727376\n",
            "Epoch: 62, Step: 26300, Train Loss: 8.097487693738515, Valid Loss: 8.652016834101332\n",
            "Epoch: 63, Step: 26400, Train Loss: 8.070636693091025, Valid Loss: 8.675041972610774\n",
            "Epoch: 63, Step: 26500, Train Loss: 8.068709783629334, Valid Loss: 8.660011781275587\n",
            "Epoch: 63, Step: 26600, Train Loss: 8.115607916524548, Valid Loss: 8.817966894531812\n",
            "Epoch: 63, Step: 26700, Train Loss: 8.103403757741061, Valid Loss: 8.73444732082746\n",
            "Epoch: 64, Step: 26800, Train Loss: 8.08543113769671, Valid Loss: 8.646125796700833\n",
            "Epoch: 64, Step: 26900, Train Loss: 8.086990798463878, Valid Loss: 8.647447575123985\n",
            "Epoch: 64, Step: 27000, Train Loss: 8.076018175396406, Valid Loss: 8.743738782359125\n",
            "Epoch: 64, Step: 27100, Train Loss: 8.068049776146918, Valid Loss: 8.651239495173717\n",
            "Epoch: 65, Step: 27200, Train Loss: 8.063875782293184, Valid Loss: 8.685233830967544\n",
            "Epoch: 65, Step: 27300, Train Loss: 8.084332177824377, Valid Loss: 8.697818067855653\n",
            "Epoch: 65, Step: 27400, Train Loss: 8.093638399951413, Valid Loss: 8.647712410671971\n",
            "Epoch: 65, Step: 27500, Train Loss: 8.166869266213142, Valid Loss: 8.668460800729669\n",
            "Epoch: 66, Step: 27600, Train Loss: 8.073041068833279, Valid Loss: 8.650428458231694\n",
            "Epoch: 66, Step: 27700, Train Loss: 8.076087628693264, Valid Loss: 8.701392842365005\n",
            "Epoch: 66, Step: 27800, Train Loss: 8.080431081691923, Valid Loss: 8.688239270033181\n",
            "Epoch: 66, Step: 27900, Train Loss: 8.0808489676127, Valid Loss: 8.689247265052744\n",
            "Epoch: 66, Step: 28000, Train Loss: 8.051800732006015, Valid Loss: 8.687552504418516\n",
            "Epoch: 67, Step: 28100, Train Loss: 8.042291693424868, Valid Loss: 8.690216243012364\n",
            "Epoch: 67, Step: 28200, Train Loss: 8.050797113425292, Valid Loss: 8.669855034487924\n",
            "Epoch: 67, Step: 28300, Train Loss: 8.050690007558378, Valid Loss: 8.678232171834562\n",
            "Epoch: 67, Step: 28400, Train Loss: 8.044290951849831, Valid Loss: 8.69814856254188\n",
            "Epoch: 68, Step: 28500, Train Loss: 8.088500996200377, Valid Loss: 8.667342890494119\n",
            "Epoch: 68, Step: 28600, Train Loss: 8.040275045455575, Valid Loss: 8.702024767874775\n",
            "Epoch: 68, Step: 28700, Train Loss: 8.07516622739366, Valid Loss: 8.69653250111455\n",
            "Epoch: 68, Step: 28800, Train Loss: 8.061894732677965, Valid Loss: 8.701777379341825\n",
            "Epoch: 69, Step: 28900, Train Loss: 8.03738891285866, Valid Loss: 8.686033590348853\n",
            "Epoch: 69, Step: 29000, Train Loss: 8.03792189993465, Valid Loss: 8.699334035890702\n",
            "Epoch: 69, Step: 29100, Train Loss: 8.057214466421254, Valid Loss: 8.720433789138095\n",
            "Epoch: 69, Step: 29200, Train Loss: 8.059561165014898, Valid Loss: 8.659658425200368\n",
            "Epoch: 70, Step: 29300, Train Loss: 8.041694261172239, Valid Loss: 8.678472211413665\n",
            "Epoch: 70, Step: 29400, Train Loss: 8.052800373057512, Valid Loss: 8.71956805887875\n",
            "Epoch: 70, Step: 29500, Train Loss: 8.04713062369232, Valid Loss: 8.679141800259982\n",
            "Epoch: 70, Step: 29600, Train Loss: 8.07051181322142, Valid Loss: 8.683820901892465\n",
            "Epoch: 71, Step: 29700, Train Loss: 8.076065392461404, Valid Loss: 8.67640881326155\n",
            "Epoch: 71, Step: 29800, Train Loss: 8.037396819725906, Valid Loss: 8.719824745813172\n",
            "Epoch: 71, Step: 29900, Train Loss: 8.058882869916042, Valid Loss: 8.708190074947586\n",
            "Epoch: 71, Step: 30000, Train Loss: 8.04700410888898, Valid Loss: 8.728539715668393\n",
            "Epoch: 72, Step: 30100, Train Loss: 8.211710747738659, Valid Loss: 8.780513491790451\n",
            "Epoch: 72, Step: 30200, Train Loss: 8.03345919534432, Valid Loss: 8.699775434907895\n",
            "Epoch: 72, Step: 30300, Train Loss: 8.040606371240697, Valid Loss: 8.753384985751714\n",
            "Epoch: 72, Step: 30400, Train Loss: 8.054350187139127, Valid Loss: 8.704377266024569\n",
            "Epoch: 72, Step: 30500, Train Loss: 8.11869315286955, Valid Loss: 8.822254370735815\n",
            "Epoch: 73, Step: 30600, Train Loss: 8.038883529172791, Valid Loss: 8.792587365086272\n",
            "Epoch: 73, Step: 30700, Train Loss: 8.048620144034997, Valid Loss: 8.676779596105252\n",
            "Epoch: 73, Step: 30800, Train Loss: 8.085018004395648, Valid Loss: 8.710250719522119\n",
            "Epoch: 73, Step: 30900, Train Loss: 8.039038021387755, Valid Loss: 8.686264426758127\n",
            "Epoch: 74, Step: 31000, Train Loss: 8.057209017629326, Valid Loss: 8.720505743494128\n",
            "Epoch: 74, Step: 31100, Train Loss: 8.036036703476352, Valid Loss: 8.714095080660615\n",
            "Epoch: 74, Step: 31200, Train Loss: 8.045156944020404, Valid Loss: 8.714730543676172\n",
            "Epoch: 74, Step: 31300, Train Loss: 8.060850282204164, Valid Loss: 8.638911494816021\n",
            "Epoch: 75, Step: 31400, Train Loss: 8.030902383037326, Valid Loss: 8.677367133664687\n",
            "Epoch: 75, Step: 31500, Train Loss: 8.072311142422013, Valid Loss: 8.87309883849253\n",
            "Epoch: 75, Step: 31600, Train Loss: 8.027846855775854, Valid Loss: 8.711640885605954\n",
            "Epoch: 75, Step: 31700, Train Loss: 8.030637214751254, Valid Loss: 8.729373031859021\n",
            "Epoch: 76, Step: 31800, Train Loss: 8.032823684957181, Valid Loss: 8.762777420516233\n",
            "Epoch: 76, Step: 31900, Train Loss: 8.054748632398567, Valid Loss: 8.73650745628341\n",
            "Epoch: 76, Step: 32000, Train Loss: 8.046172628547687, Valid Loss: 8.724021995409132\n",
            "Epoch: 76, Step: 32100, Train Loss: 8.030603001144188, Valid Loss: 8.661011251414346\n",
            "Epoch: 77, Step: 32200, Train Loss: 8.015116544894912, Valid Loss: 8.70558765957468\n",
            "Epoch: 77, Step: 32300, Train Loss: 8.016006554858915, Valid Loss: 8.719899018898316\n",
            "Epoch: 77, Step: 32400, Train Loss: 8.051263673274835, Valid Loss: 8.68403676628306\n",
            "Epoch: 77, Step: 32500, Train Loss: 8.022728608932248, Valid Loss: 8.732826830454577\n",
            "Epoch: 77, Step: 32600, Train Loss: 8.042434518839103, Valid Loss: 8.807320310395317\n",
            "Epoch: 78, Step: 32700, Train Loss: 8.015129825892512, Valid Loss: 8.726966177342355\n",
            "Epoch: 78, Step: 32800, Train Loss: 8.017368286177181, Valid Loss: 8.676086357510181\n",
            "Epoch: 78, Step: 32900, Train Loss: 8.030515206531676, Valid Loss: 8.75872377694432\n",
            "Epoch: 78, Step: 33000, Train Loss: 8.038732324402682, Valid Loss: 8.660031882846383\n",
            "Epoch: 79, Step: 33100, Train Loss: 8.072183740142195, Valid Loss: 8.672204726708129\n",
            "Epoch: 79, Step: 33200, Train Loss: 8.013945692574067, Valid Loss: 8.718589874244847\n",
            "Epoch: 79, Step: 33300, Train Loss: 8.060273417975045, Valid Loss: 8.716626314092814\n",
            "Epoch: 79, Step: 33400, Train Loss: 8.000299782730801, Valid Loss: 8.722853337116131\n",
            "Epoch: 80, Step: 33500, Train Loss: 8.002144171255113, Valid Loss: 8.70598479084999\n",
            "Epoch: 80, Step: 33600, Train Loss: 8.017203057189754, Valid Loss: 8.758736267966906\n",
            "Epoch: 80, Step: 33700, Train Loss: 8.029359158877888, Valid Loss: 8.686428044401483\n",
            "Epoch: 80, Step: 33800, Train Loss: 8.04429166305834, Valid Loss: 8.67626823581432\n",
            "Epoch: 81, Step: 33900, Train Loss: 8.016834473769201, Valid Loss: 8.749171678916161\n",
            "Epoch: 81, Step: 34000, Train Loss: 8.017668714144072, Valid Loss: 8.720672000583685\n",
            "Epoch: 81, Step: 34100, Train Loss: 8.015985065349405, Valid Loss: 8.734840728985645\n",
            "Epoch: 81, Step: 34200, Train Loss: 8.115577902239636, Valid Loss: 8.704509976447234\n",
            "Epoch: 82, Step: 34300, Train Loss: 8.017476403390752, Valid Loss: 8.727220373529699\n",
            "Epoch: 82, Step: 34400, Train Loss: 8.013235366988754, Valid Loss: 8.761154088704679\n",
            "Epoch: 82, Step: 34500, Train Loss: 8.074482885959338, Valid Loss: 8.67498631179549\n",
            "Epoch: 82, Step: 34600, Train Loss: 8.006288298399786, Valid Loss: 8.657902984996863\n",
            "Epoch: 83, Step: 34700, Train Loss: 8.00548543181944, Valid Loss: 8.699727039629739\n",
            "Epoch: 83, Step: 34800, Train Loss: 8.015604769702906, Valid Loss: 8.752387223855822\n",
            "Epoch: 83, Step: 34900, Train Loss: 8.055159582160034, Valid Loss: 8.759042430705815\n",
            "Epoch: 83, Step: 35000, Train Loss: 8.059998361938565, Valid Loss: 8.66952105390156\n",
            "Epoch: 83, Step: 35100, Train Loss: 8.026681855723037, Valid Loss: 8.702211825054318\n",
            "Epoch: 84, Step: 35200, Train Loss: 8.028108706330295, Valid Loss: 8.753877964815835\n",
            "Epoch: 84, Step: 35300, Train Loss: 7.979306776347181, Valid Loss: 8.711944175726591\n",
            "Epoch: 84, Step: 35400, Train Loss: 8.028169232007098, Valid Loss: 8.653736759921646\n",
            "Epoch: 84, Step: 35500, Train Loss: 8.02854834930335, Valid Loss: 8.726582943373435\n",
            "Epoch: 85, Step: 35600, Train Loss: 8.00627426856838, Valid Loss: 8.694993303972627\n",
            "Epoch: 85, Step: 35700, Train Loss: 8.017690574165526, Valid Loss: 8.674703991766421\n",
            "Epoch: 85, Step: 35800, Train Loss: 8.054009624789291, Valid Loss: 8.674720085119281\n",
            "Epoch: 85, Step: 35900, Train Loss: 8.033542678124839, Valid Loss: 8.723767739823282\n",
            "Epoch: 86, Step: 36000, Train Loss: 8.013226957967614, Valid Loss: 8.789679292641052\n",
            "Epoch: 86, Step: 36100, Train Loss: 8.036859793035761, Valid Loss: 8.852019514312044\n",
            "Epoch: 86, Step: 36200, Train Loss: 7.992932410607231, Valid Loss: 8.6774967259725\n",
            "Epoch: 86, Step: 36300, Train Loss: 7.997693593237986, Valid Loss: 8.78118339152356\n",
            "Epoch: 87, Step: 36400, Train Loss: 8.03079528339489, Valid Loss: 8.790538161168374\n",
            "Epoch: 87, Step: 36500, Train Loss: 8.00197336111686, Valid Loss: 8.673432707064094\n",
            "Epoch: 87, Step: 36600, Train Loss: 8.018196472751356, Valid Loss: 8.757029386766657\n",
            "Epoch: 87, Step: 36700, Train Loss: 8.053034104207978, Valid Loss: 8.70632583399227\n",
            "Epoch: 88, Step: 36800, Train Loss: 8.02942429853086, Valid Loss: 8.862404144970519\n",
            "Epoch: 88, Step: 36900, Train Loss: 7.998538203561291, Valid Loss: 8.718248165858489\n",
            "Epoch: 88, Step: 37000, Train Loss: 7.986234059859264, Valid Loss: 8.714916661179247\n",
            "Epoch: 88, Step: 37100, Train Loss: 8.014268040826813, Valid Loss: 8.704415447213076\n",
            "Epoch: 88, Step: 37200, Train Loss: 8.013866103979073, Valid Loss: 8.7533782011114\n",
            "Epoch: 89, Step: 37300, Train Loss: 8.022981135487518, Valid Loss: 8.811257559364645\n",
            "Epoch: 89, Step: 37400, Train Loss: 8.008009260456673, Valid Loss: 8.824966241115462\n",
            "Epoch: 89, Step: 37500, Train Loss: 7.980572386874431, Valid Loss: 8.728520054161661\n",
            "Epoch: 89, Step: 37600, Train Loss: 8.046504193642877, Valid Loss: 8.690020423765139\n",
            "Epoch: 90, Step: 37700, Train Loss: 7.999426416801535, Valid Loss: 8.721653235623785\n",
            "Epoch: 90, Step: 37800, Train Loss: 8.066879160958432, Valid Loss: 8.899444894719897\n",
            "Epoch: 90, Step: 37900, Train Loss: 7.998029565354231, Valid Loss: 8.728008528176934\n",
            "Epoch: 90, Step: 38000, Train Loss: 8.045142512827237, Valid Loss: 8.85577897066646\n",
            "Epoch: 91, Step: 38100, Train Loss: 7.9698955517880545, Valid Loss: 8.716024263021271\n",
            "Epoch: 91, Step: 38200, Train Loss: 8.00509476470378, Valid Loss: 8.708089440473557\n",
            "Epoch: 91, Step: 38300, Train Loss: 8.0078178953251, Valid Loss: 8.706552463013148\n",
            "Epoch: 91, Step: 38400, Train Loss: 8.006224043125247, Valid Loss: 8.709475823955477\n",
            "Epoch: 92, Step: 38500, Train Loss: 8.01936431791974, Valid Loss: 8.835705875193405\n",
            "Epoch: 92, Step: 38600, Train Loss: 8.01314183909114, Valid Loss: 8.774470070509274\n",
            "Epoch: 92, Step: 38700, Train Loss: 8.02377506400705, Valid Loss: 8.708060296114155\n",
            "Epoch: 92, Step: 38800, Train Loss: 8.011771949981854, Valid Loss: 8.749503370102618\n",
            "Epoch: 93, Step: 38900, Train Loss: 7.983021695037893, Valid Loss: 8.737740547613086\n",
            "Epoch: 93, Step: 39000, Train Loss: 8.041152670286742, Valid Loss: 8.882070313806425\n",
            "Epoch: 93, Step: 39100, Train Loss: 7.973979409527318, Valid Loss: 8.68292954201444\n",
            "Epoch: 93, Step: 39200, Train Loss: 7.981336915639984, Valid Loss: 8.695384142499957\n",
            "Epoch: 94, Step: 39300, Train Loss: 7.99760760830836, Valid Loss: 8.84449434083447\n",
            "Epoch: 94, Step: 39400, Train Loss: 7.992673940748671, Valid Loss: 8.744289228031466\n",
            "Epoch: 94, Step: 39500, Train Loss: 8.010821773385063, Valid Loss: 8.681272476289509\n",
            "Epoch: 94, Step: 39600, Train Loss: 7.985235615973585, Valid Loss: 8.777756563723607\n",
            "Epoch: 94, Step: 39700, Train Loss: 7.964921968955944, Valid Loss: 8.749567019712739\n",
            "Epoch: 95, Step: 39800, Train Loss: 7.99781949893104, Valid Loss: 8.68828865281604\n",
            "Epoch: 95, Step: 39900, Train Loss: 7.990548975409484, Valid Loss: 8.80554496543117\n",
            "Epoch: 95, Step: 40000, Train Loss: 7.986154296766193, Valid Loss: 8.754747735347781\n",
            "Epoch: 95, Step: 40100, Train Loss: 8.035132816158999, Valid Loss: 8.886103113333991\n",
            "Epoch: 96, Step: 40200, Train Loss: 7.995571754767462, Valid Loss: 8.7675237757725\n",
            "Epoch: 96, Step: 40300, Train Loss: 8.01363135364971, Valid Loss: 8.723478338973235\n",
            "Epoch: 96, Step: 40400, Train Loss: 8.045531601217547, Valid Loss: 8.87844745631786\n",
            "Epoch: 96, Step: 40500, Train Loss: 7.98637487992927, Valid Loss: 8.826575565558354\n",
            "Epoch: 97, Step: 40600, Train Loss: 8.0369926017048, Valid Loss: 8.924440188373959\n",
            "Epoch: 97, Step: 40700, Train Loss: 8.020933520005494, Valid Loss: 8.777281826847029\n",
            "Epoch: 97, Step: 40800, Train Loss: 8.007644062866332, Valid Loss: 8.6888737954891\n",
            "Epoch: 97, Step: 40900, Train Loss: 7.980929913711486, Valid Loss: 8.73696463602985\n",
            "Epoch: 98, Step: 41000, Train Loss: 7.997023615667267, Valid Loss: 8.714215777031015\n",
            "Epoch: 98, Step: 41100, Train Loss: 7.979465515200696, Valid Loss: 8.771715326894297\n",
            "Epoch: 98, Step: 41200, Train Loss: 7.98676168951749, Valid Loss: 8.715812484243306\n",
            "Epoch: 98, Step: 41300, Train Loss: 8.00509253381554, Valid Loss: 8.688403703697487\n",
            "Epoch: 99, Step: 41400, Train Loss: 7.955545523761002, Valid Loss: 8.726558687370451\n",
            "Epoch: 99, Step: 41500, Train Loss: 8.04068185727042, Valid Loss: 8.708139910215161\n",
            "Epoch: 99, Step: 41600, Train Loss: 7.973044817923846, Valid Loss: 8.723492669344111\n",
            "Epoch: 99, Step: 41700, Train Loss: 7.983841761521754, Valid Loss: 8.75630684604878\n",
            "Epoch: 99, Step: 41800, Train Loss: 7.955933369601711, Valid Loss: 8.716311961544069\n"
          ]
        }
      ],
      "source": [
        "q2_mlp = MyMLP(\n",
        "    X_subtrain=X_subtrain,\n",
        "    Y_subtrain=Y_subtrain,\n",
        "    X_valid=X_valid,\n",
        "    Y_valid=Y_valid,\n",
        "    H=45,\n",
        "    lr=0.00001,\n",
        "    wd=0,\n",
        "    mom=0,\n",
        "    loss_type=0,\n",
        "    optimizer_type=0,\n",
        "    use_dropout=False\n",
        ")\n",
        "train_loss_list, valid_loss_list = q2_mlp.fit(max_epoch=100, verbose=True, patience_batch_num=5000, model_path='q2_mlp.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHspxusWPs3p",
        "outputId": "789f8d65-e600-416e-c8a4-bfe642f53f78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test RMSE Loss: 8.800802179448848\n"
          ]
        }
      ],
      "source": [
        "best_model_q2 = torch.load('q2_mlp.ckpt')\n",
        "test_rmse_loss = calculate_test_rmse(best_model_q2, X_test, Y_test)\n",
        "print(\"Test RMSE Loss: {}\".format(test_rmse_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "kSD4FoT1Ps3p",
        "outputId": "5c568935-dcf1-45fd-fa69-7ecba8b245ce"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# draw the training loss and validation loss\n",
        "# make figure larger\n",
        "plt.figure(figsize=(10, 8))\n",
        "def draw_loss(train_loss_list, valid_loss_list):\n",
        "        \"\"\"\n",
        "        Draw the training loss and validation loss.\n",
        "        \"\"\"\n",
        "        plt.plot(train_loss_list, label = \"train loss\")\n",
        "        plt.plot(valid_loss_list, label = \"valid loss\")\n",
        "        plt.xlabel(\"batch (in 100)\")\n",
        "        plt.ylabel(\"loss\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "draw_loss(train_loss_list, valid_loss_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vim0GEYPs3q"
      },
      "source": [
        "從上圖可以看出，在訓練過程中，Training RMSE 與 Validation RMSE 在前幾千個 Batch 都在下降。  \n",
        "但後來 Validation RMSE 逐漸停在 8.6 左右，而 Training RMSE 仍在下降讓差距越來越大，代表模型可能開始有 Overfitting 的跡象。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFSeA3UKPs3q"
      },
      "source": [
        "#### Q3 (10%)\n",
        "重複上題步驟，使用H = 90與180。無須畫訓練過程的RMSE。列出這兩個Test RMSE。討論H = 45, 90, 180的Test RMSE。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPNdYdL4Ps3q",
        "outputId": "584cb428-3a00-4c97-9443-6492c97440ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Step: 100, Train Loss: 9.839815017507126, Valid Loss: 9.814369551559734\n",
            "Epoch: 0, Step: 200, Train Loss: 9.033181272205535, Valid Loss: 8.984130010393514\n",
            "Epoch: 0, Step: 300, Train Loss: 8.920812131723691, Valid Loss: 8.879889801261035\n",
            "Epoch: 0, Step: 400, Train Loss: 8.85807569027006, Valid Loss: 8.807489562696778\n",
            "Epoch: 1, Step: 500, Train Loss: 9.228593043933627, Valid Loss: 9.207619072654419\n",
            "Epoch: 1, Step: 600, Train Loss: 8.794256154576987, Valid Loss: 8.774127802135219\n",
            "Epoch: 1, Step: 700, Train Loss: 8.73649967566084, Valid Loss: 8.714671226314493\n",
            "Epoch: 1, Step: 800, Train Loss: 8.702913563221678, Valid Loss: 8.710904595249785\n",
            "Epoch: 2, Step: 900, Train Loss: 8.724080380824097, Valid Loss: 8.737643456684214\n",
            "Epoch: 2, Step: 1000, Train Loss: 8.660138489543279, Valid Loss: 8.671764712291587\n",
            "Epoch: 2, Step: 1100, Train Loss: 8.621660033611844, Valid Loss: 8.65740381348656\n",
            "Epoch: 2, Step: 1200, Train Loss: 8.596212666803805, Valid Loss: 8.627063078225108\n",
            "Epoch: 3, Step: 1300, Train Loss: 8.583922943959534, Valid Loss: 8.622059630881994\n",
            "Epoch: 3, Step: 1400, Train Loss: 8.578685866338308, Valid Loss: 8.632560780911597\n",
            "Epoch: 3, Step: 1500, Train Loss: 8.586415262991588, Valid Loss: 8.647516878280799\n",
            "Epoch: 3, Step: 1600, Train Loss: 8.532823934175486, Valid Loss: 8.610327834274834\n",
            "Epoch: 4, Step: 1700, Train Loss: 8.551078406585455, Valid Loss: 8.647624732146882\n",
            "Epoch: 4, Step: 1800, Train Loss: 8.521992319236055, Valid Loss: 8.630639607794983\n",
            "Epoch: 4, Step: 1900, Train Loss: 8.513390750712139, Valid Loss: 8.610653716220014\n",
            "Epoch: 4, Step: 2000, Train Loss: 8.481427279472317, Valid Loss: 8.588067612996065\n",
            "Epoch: 5, Step: 2100, Train Loss: 8.543187590485173, Valid Loss: 8.655556101339656\n",
            "Epoch: 5, Step: 2200, Train Loss: 8.450122046413803, Valid Loss: 8.582002738669388\n",
            "Epoch: 5, Step: 2300, Train Loss: 8.47150626738313, Valid Loss: 8.60371711280964\n",
            "Epoch: 5, Step: 2400, Train Loss: 8.434389223054467, Valid Loss: 8.582426136908783\n",
            "Epoch: 5, Step: 2500, Train Loss: 8.438687534449596, Valid Loss: 8.598831169763942\n",
            "Epoch: 6, Step: 2600, Train Loss: 8.408303764386199, Valid Loss: 8.580780733990027\n",
            "Epoch: 6, Step: 2700, Train Loss: 8.449531786360023, Valid Loss: 8.616681052113703\n",
            "Epoch: 6, Step: 2800, Train Loss: 8.396718298204755, Valid Loss: 8.584170265776454\n",
            "Epoch: 6, Step: 2900, Train Loss: 8.388319392161828, Valid Loss: 8.581235016893064\n",
            "Epoch: 7, Step: 3000, Train Loss: 8.361529240598278, Valid Loss: 8.562215783722525\n",
            "Epoch: 7, Step: 3100, Train Loss: 8.349713873069453, Valid Loss: 8.575648067999351\n",
            "Epoch: 7, Step: 3200, Train Loss: 8.370232878357346, Valid Loss: 8.598040613710655\n",
            "Epoch: 7, Step: 3300, Train Loss: 8.327121554047332, Valid Loss: 8.530320667141645\n",
            "Epoch: 8, Step: 3400, Train Loss: 8.41177041637325, Valid Loss: 8.679043799014005\n",
            "Epoch: 8, Step: 3500, Train Loss: 8.296099584673566, Valid Loss: 8.564304058376322\n",
            "Epoch: 8, Step: 3600, Train Loss: 8.31939826966649, Valid Loss: 8.565747239505189\n",
            "Epoch: 8, Step: 3700, Train Loss: 8.305321293458677, Valid Loss: 8.560146115489339\n",
            "Epoch: 9, Step: 3800, Train Loss: 8.273884494981317, Valid Loss: 8.546494014377146\n",
            "Epoch: 9, Step: 3900, Train Loss: 8.300110301251596, Valid Loss: 8.593613526718814\n",
            "Epoch: 9, Step: 4000, Train Loss: 8.27685472195278, Valid Loss: 8.601924623140235\n",
            "Epoch: 9, Step: 4100, Train Loss: 8.254480876590394, Valid Loss: 8.577476599480386\n",
            "Epoch: 10, Step: 4200, Train Loss: 8.236375412529824, Valid Loss: 8.553468106226\n",
            "Epoch: 10, Step: 4300, Train Loss: 8.247137987635467, Valid Loss: 8.588636478061591\n",
            "Epoch: 10, Step: 4400, Train Loss: 8.220236744273876, Valid Loss: 8.560380981256769\n",
            "Epoch: 10, Step: 4500, Train Loss: 8.237324642887526, Valid Loss: 8.568699473071414\n",
            "Epoch: 11, Step: 4600, Train Loss: 8.22336151109534, Valid Loss: 8.592995592826773\n",
            "Epoch: 11, Step: 4700, Train Loss: 8.228118512325041, Valid Loss: 8.573977312834733\n",
            "Epoch: 11, Step: 4800, Train Loss: 8.23397114781601, Valid Loss: 8.612243395210477\n",
            "Epoch: 11, Step: 4900, Train Loss: 8.19081888144434, Valid Loss: 8.571256938969418\n",
            "Epoch: 11, Step: 5000, Train Loss: 8.171163559532824, Valid Loss: 8.548844666620997\n",
            "Epoch: 12, Step: 5100, Train Loss: 8.169383579971228, Valid Loss: 8.617558758336772\n",
            "Epoch: 12, Step: 5200, Train Loss: 8.156887470098495, Valid Loss: 8.562820262353368\n",
            "Epoch: 12, Step: 5300, Train Loss: 8.204694665644148, Valid Loss: 8.6155796448677\n",
            "Epoch: 12, Step: 5400, Train Loss: 8.221747433883099, Valid Loss: 8.590693224846111\n",
            "Epoch: 13, Step: 5500, Train Loss: 8.153130466680713, Valid Loss: 8.63008451951473\n",
            "Epoch: 13, Step: 5600, Train Loss: 8.123881778687391, Valid Loss: 8.594661533592133\n",
            "Epoch: 13, Step: 5700, Train Loss: 8.14264781804054, Valid Loss: 8.576978622967244\n",
            "Epoch: 13, Step: 5800, Train Loss: 8.16027219401375, Valid Loss: 8.617529960306234\n",
            "Epoch: 14, Step: 5900, Train Loss: 8.08997514263055, Valid Loss: 8.591742379659127\n",
            "Epoch: 14, Step: 6000, Train Loss: 8.117205405041254, Valid Loss: 8.597539056635068\n",
            "Epoch: 14, Step: 6100, Train Loss: 8.125487519712996, Valid Loss: 8.607782852655193\n",
            "Epoch: 14, Step: 6200, Train Loss: 8.079729732084004, Valid Loss: 8.590741359190407\n",
            "Epoch: 15, Step: 6300, Train Loss: 8.060221336029477, Valid Loss: 8.568666462894178\n",
            "Epoch: 15, Step: 6400, Train Loss: 8.06242844652256, Valid Loss: 8.608588109649123\n",
            "Epoch: 15, Step: 6500, Train Loss: 8.046097169311215, Valid Loss: 8.597490749689563\n",
            "Epoch: 15, Step: 6600, Train Loss: 8.116755988436982, Valid Loss: 8.601290279465378\n",
            "Epoch: 16, Step: 6700, Train Loss: 8.05354956785581, Valid Loss: 8.595826185349075\n",
            "Epoch: 16, Step: 6800, Train Loss: 8.026327578196272, Valid Loss: 8.588390119494816\n",
            "Epoch: 16, Step: 6900, Train Loss: 8.040296721492249, Valid Loss: 8.637097183796225\n",
            "Epoch: 16, Step: 7000, Train Loss: 8.051979904521136, Valid Loss: 8.619426236138525\n",
            "Epoch: 16, Step: 7100, Train Loss: 8.017766907496812, Valid Loss: 8.60841976613404\n",
            "Epoch: 17, Step: 7200, Train Loss: 8.044013647878765, Valid Loss: 8.593011189744077\n",
            "Epoch: 17, Step: 7300, Train Loss: 8.00990786620918, Valid Loss: 8.614645322066183\n",
            "Epoch: 17, Step: 7400, Train Loss: 8.006301210884962, Valid Loss: 8.600543432787163\n",
            "Epoch: 17, Step: 7500, Train Loss: 8.03523576647907, Valid Loss: 8.569599458316976\n",
            "Epoch: 18, Step: 7600, Train Loss: 7.9705914801370055, Valid Loss: 8.581922160533006\n",
            "Epoch: 18, Step: 7700, Train Loss: 7.996669949302176, Valid Loss: 8.632978359955185\n",
            "Epoch: 18, Step: 7800, Train Loss: 7.972920022475455, Valid Loss: 8.663453649716631\n",
            "Epoch: 18, Step: 7900, Train Loss: 8.129745439974608, Valid Loss: 8.798411888259999\n",
            "Epoch: 19, Step: 8000, Train Loss: 7.9292312336817865, Valid Loss: 8.626549469067818\n",
            "Epoch: 19, Step: 8100, Train Loss: 7.952918530398541, Valid Loss: 8.638182048473169\n",
            "Epoch: 19, Step: 8200, Train Loss: 7.959870388771617, Valid Loss: 8.601632790217451\n",
            "Epoch: 19, Step: 8300, Train Loss: 7.994051937563448, Valid Loss: 8.624430876704077\n",
            "Epoch: 20, Step: 8400, Train Loss: 7.95074755648999, Valid Loss: 8.620269443087324\n",
            "Epoch: 20, Step: 8500, Train Loss: 7.929582373742065, Valid Loss: 8.677393444479337\n",
            "Epoch: 20, Step: 8600, Train Loss: 7.9116938134519685, Valid Loss: 8.638975049951544\n",
            "Epoch: 20, Step: 8700, Train Loss: 7.887625814759674, Valid Loss: 8.603084715220982\n",
            "Epoch: 21, Step: 8800, Train Loss: 7.927339753997603, Valid Loss: 8.621956879732554\n",
            "Epoch: 21, Step: 8900, Train Loss: 7.89090127791922, Valid Loss: 8.689080576340855\n",
            "Epoch: 21, Step: 9000, Train Loss: 7.9150939189862175, Valid Loss: 8.5967293693335\n",
            "Epoch: 21, Step: 9100, Train Loss: 7.885969186156566, Valid Loss: 8.609890929134052\n",
            "Epoch: 22, Step: 9200, Train Loss: 7.899383305185436, Valid Loss: 8.655596570871301\n",
            "Epoch: 22, Step: 9300, Train Loss: 7.863955143627575, Valid Loss: 8.645800296939086\n",
            "Epoch: 22, Step: 9400, Train Loss: 7.930214245509338, Valid Loss: 8.718356299355987\n",
            "Epoch: 22, Step: 9500, Train Loss: 7.948152498797371, Valid Loss: 8.638768432496562\n",
            "Epoch: 22, Step: 9600, Train Loss: 7.857275884528163, Valid Loss: 8.604632530796708\n",
            "Epoch: 23, Step: 9700, Train Loss: 7.869469687020139, Valid Loss: 8.734321681477757\n",
            "Epoch: 23, Step: 9800, Train Loss: 7.846370551961819, Valid Loss: 8.6148541804065\n",
            "Epoch: 23, Step: 9900, Train Loss: 7.99703691178983, Valid Loss: 8.685111524280591\n",
            "Epoch: 23, Step: 10000, Train Loss: 7.876221828380459, Valid Loss: 8.621478732607986\n",
            "Epoch: 24, Step: 10100, Train Loss: 7.870150001928646, Valid Loss: 8.668073962120998\n",
            "Epoch: 24, Step: 10200, Train Loss: 7.889974950296938, Valid Loss: 8.615634653065024\n",
            "Epoch: 24, Step: 10300, Train Loss: 7.836405578428667, Valid Loss: 8.71340076251269\n",
            "Epoch: 24, Step: 10400, Train Loss: 7.82048276551568, Valid Loss: 8.647549553368554\n",
            "Epoch: 25, Step: 10500, Train Loss: 7.842615912449513, Valid Loss: 8.766066901626326\n",
            "Epoch: 25, Step: 10600, Train Loss: 7.903888819075577, Valid Loss: 8.800042617496233\n",
            "Epoch: 25, Step: 10700, Train Loss: 7.785446505255242, Valid Loss: 8.679668451864439\n",
            "Epoch: 25, Step: 10800, Train Loss: 7.839178991192108, Valid Loss: 8.791736474629646\n",
            "Epoch: 26, Step: 10900, Train Loss: 7.774299513852339, Valid Loss: 8.634006590270353\n",
            "Epoch: 26, Step: 11000, Train Loss: 7.800941150417388, Valid Loss: 8.659221148952504\n",
            "Epoch: 26, Step: 11100, Train Loss: 7.803723400089688, Valid Loss: 8.714638133498655\n",
            "Epoch: 26, Step: 11200, Train Loss: 7.794944866968893, Valid Loss: 8.680192203763125\n",
            "Epoch: 27, Step: 11300, Train Loss: 7.766260688150087, Valid Loss: 8.636969621151835\n",
            "Epoch: 27, Step: 11400, Train Loss: 7.781098803049366, Valid Loss: 8.644401842381473\n",
            "Epoch: 27, Step: 11500, Train Loss: 7.8162367002399735, Valid Loss: 8.667621487438563\n",
            "Epoch: 27, Step: 11600, Train Loss: 7.787147226055888, Valid Loss: 8.779446162068592\n",
            "Epoch: 27, Step: 11700, Train Loss: 7.765621674318995, Valid Loss: 8.578150872864105\n",
            "Epoch: 28, Step: 11800, Train Loss: 7.754737721867608, Valid Loss: 8.738085289665051\n",
            "Epoch: 28, Step: 11900, Train Loss: 7.747235047858589, Valid Loss: 8.657219561138708\n",
            "Epoch: 28, Step: 12000, Train Loss: 7.745952288348551, Valid Loss: 8.705813499850148\n",
            "Epoch: 28, Step: 12100, Train Loss: 7.846270510972712, Valid Loss: 8.65487879680015\n",
            "Epoch: 29, Step: 12200, Train Loss: 7.761228155986006, Valid Loss: 8.67910611393854\n",
            "Epoch: 29, Step: 12300, Train Loss: 7.724569736136584, Valid Loss: 8.711854798800024\n",
            "Epoch: 29, Step: 12400, Train Loss: 7.766196515975486, Valid Loss: 8.649881418036955\n",
            "Epoch: 29, Step: 12500, Train Loss: 7.758527385253057, Valid Loss: 8.61870501238196\n",
            "Epoch: 30, Step: 12600, Train Loss: 7.877368785911579, Valid Loss: 8.982654090549737\n",
            "Epoch: 30, Step: 12700, Train Loss: 7.6842535408339305, Valid Loss: 8.693824236603552\n",
            "Epoch: 30, Step: 12800, Train Loss: 7.7076308171640235, Valid Loss: 8.61167517249822\n",
            "Epoch: 30, Step: 12900, Train Loss: 7.707935382419537, Valid Loss: 8.67008646195758\n",
            "Epoch: 31, Step: 13000, Train Loss: 7.69099488915254, Valid Loss: 8.753465034486773\n",
            "Epoch: 31, Step: 13100, Train Loss: 7.684167703339044, Valid Loss: 8.725194904539338\n",
            "Epoch: 31, Step: 13200, Train Loss: 7.676176512839712, Valid Loss: 8.650809768557849\n",
            "Epoch: 31, Step: 13300, Train Loss: 7.66672611192119, Valid Loss: 8.725770783101623\n",
            "Epoch: 32, Step: 13400, Train Loss: 7.668521240544332, Valid Loss: 8.655932312512547\n",
            "Epoch: 32, Step: 13500, Train Loss: 7.6593853993301275, Valid Loss: 8.749020047359767\n",
            "Epoch: 32, Step: 13600, Train Loss: 7.646981085755579, Valid Loss: 8.658826095147557\n",
            "Epoch: 32, Step: 13700, Train Loss: 7.6332975076009735, Valid Loss: 8.671913357379673\n",
            "Epoch: 33, Step: 13800, Train Loss: 7.643499405749526, Valid Loss: 8.66609560495443\n",
            "Epoch: 33, Step: 13900, Train Loss: 7.685237058444035, Valid Loss: 8.803548183731845\n",
            "Epoch: 33, Step: 14000, Train Loss: 7.630753287860723, Valid Loss: 8.72095241232492\n",
            "Epoch: 33, Step: 14100, Train Loss: 7.6368149338552405, Valid Loss: 8.758323944206467\n",
            "Epoch: 33, Step: 14200, Train Loss: 7.666351035222919, Valid Loss: 8.676181154132847\n",
            "Epoch: 34, Step: 14300, Train Loss: 7.666280632024256, Valid Loss: 8.869536660398726\n",
            "Epoch: 34, Step: 14400, Train Loss: 7.6571286308937205, Valid Loss: 8.723291641252438\n",
            "Epoch: 34, Step: 14500, Train Loss: 7.7018599556568175, Valid Loss: 8.64455132097285\n",
            "Epoch: 34, Step: 14600, Train Loss: 7.607550294641874, Valid Loss: 8.681253935231291\n",
            "Epoch: 35, Step: 14700, Train Loss: 7.636144746556779, Valid Loss: 8.7121510157473\n",
            "Epoch: 35, Step: 14800, Train Loss: 7.737908177544629, Valid Loss: 8.69443560265673\n",
            "Epoch: 35, Step: 14900, Train Loss: 7.668276010271152, Valid Loss: 8.794168841826037\n",
            "Epoch: 35, Step: 15000, Train Loss: 7.633646509240784, Valid Loss: 8.700003386797334\n",
            "Epoch: 36, Step: 15100, Train Loss: 7.601877224509631, Valid Loss: 8.733894091970097\n",
            "Epoch: 36, Step: 15200, Train Loss: 7.666016113645133, Valid Loss: 8.681091335998858\n",
            "Epoch: 36, Step: 15300, Train Loss: 7.616757567668146, Valid Loss: 8.69538355638853\n",
            "Epoch: 36, Step: 15400, Train Loss: 7.611477908019604, Valid Loss: 8.683200374958808\n",
            "Epoch: 37, Step: 15500, Train Loss: 7.572790465916651, Valid Loss: 8.708401799679757\n",
            "Epoch: 37, Step: 15600, Train Loss: 7.569484802041771, Valid Loss: 8.718474948751354\n",
            "Epoch: 37, Step: 15700, Train Loss: 7.604359586837313, Valid Loss: 8.686646476885777\n",
            "Epoch: 37, Step: 15800, Train Loss: 7.59086443515561, Valid Loss: 8.6771608381919\n",
            "Epoch: 38, Step: 15900, Train Loss: 7.542168727907058, Valid Loss: 8.745898482874466\n",
            "Epoch: 38, Step: 16000, Train Loss: 7.573894674977777, Valid Loss: 8.761311274388063\n",
            "Epoch: 38, Step: 16100, Train Loss: 7.6405606494288865, Valid Loss: 8.925971079070807\n",
            "Epoch: 38, Step: 16200, Train Loss: 7.537611084248327, Valid Loss: 8.765026143051905\n",
            "Epoch: 38, Step: 16300, Train Loss: 7.529080442666049, Valid Loss: 8.765696351855189\n",
            "Epoch: 39, Step: 16400, Train Loss: 7.553416011878507, Valid Loss: 8.855492045677128\n",
            "Epoch: 39, Step: 16500, Train Loss: 7.570739402021199, Valid Loss: 8.830787744971529\n",
            "Epoch: 39, Step: 16600, Train Loss: 7.58902836969812, Valid Loss: 8.876445977571299\n",
            "Epoch: 39, Step: 16700, Train Loss: 7.5706649182733186, Valid Loss: 8.695812855473006\n",
            "Epoch: 40, Step: 16800, Train Loss: 7.5447057026374775, Valid Loss: 8.725178081221832\n",
            "Epoch: 40, Step: 16900, Train Loss: 7.533733198392484, Valid Loss: 8.744230568783646\n",
            "Epoch: 40, Step: 17000, Train Loss: 7.546837359541515, Valid Loss: 8.803834258802754\n",
            "Epoch: 40, Step: 17100, Train Loss: 7.5435084181119265, Valid Loss: 8.786472666347548\n",
            "Epoch: 41, Step: 17200, Train Loss: 7.505288954375011, Valid Loss: 8.812722249740958\n",
            "Epoch: 41, Step: 17300, Train Loss: 7.495441257027014, Valid Loss: 8.780227719443715\n",
            "Epoch: 41, Step: 17400, Train Loss: 7.584551027353947, Valid Loss: 8.950987333161152\n",
            "Epoch: 41, Step: 17500, Train Loss: 7.542146152024058, Valid Loss: 8.707220443930165\n",
            "Epoch: 42, Step: 17600, Train Loss: 7.515335523097853, Valid Loss: 8.809571002255815\n",
            "Epoch: 42, Step: 17700, Train Loss: 7.504663300972814, Valid Loss: 8.83000653426688\n",
            "Epoch: 42, Step: 17800, Train Loss: 7.521412105701117, Valid Loss: 8.81842789442634\n",
            "Epoch: 42, Step: 17900, Train Loss: 7.510539113842822, Valid Loss: 8.834982910933576\n",
            "Epoch: 43, Step: 18000, Train Loss: 7.72407271439951, Valid Loss: 8.789646956631772\n",
            "Epoch: 43, Step: 18100, Train Loss: 7.485652417579573, Valid Loss: 8.82775956393924\n",
            "Epoch: 43, Step: 18200, Train Loss: 7.474754225517401, Valid Loss: 8.819437397898545\n",
            "Epoch: 43, Step: 18300, Train Loss: 7.488714277348446, Valid Loss: 8.738097135359608\n",
            "Epoch: 44, Step: 18400, Train Loss: 7.463196773599867, Valid Loss: 8.876951901267537\n",
            "Epoch: 44, Step: 18500, Train Loss: 7.484789281760547, Valid Loss: 8.886549336039453\n",
            "Epoch: 44, Step: 18600, Train Loss: 7.528627671241084, Valid Loss: 8.933595378885883\n",
            "Epoch: 44, Step: 18700, Train Loss: 7.444230675587762, Valid Loss: 8.841579502981673\n",
            "Epoch: 44, Step: 18800, Train Loss: 7.451638581694777, Valid Loss: 8.758502357087416\n",
            "Epoch: 45, Step: 18900, Train Loss: 7.438850321319731, Valid Loss: 8.919056718974012\n",
            "Epoch: 45, Step: 19000, Train Loss: 7.5108816685671265, Valid Loss: 9.005511984001949\n",
            "Epoch: 45, Step: 19100, Train Loss: 7.472528916857508, Valid Loss: 8.888314687703351\n",
            "Epoch: 45, Step: 19200, Train Loss: 7.516291491420874, Valid Loss: 8.799466406170678\n",
            "Epoch: 46, Step: 19300, Train Loss: 7.474276179976806, Valid Loss: 8.915057410317772\n",
            "Epoch: 46, Step: 19400, Train Loss: 7.440991859320709, Valid Loss: 8.93715656425458\n",
            "Epoch: 46, Step: 19500, Train Loss: 7.409419357266374, Valid Loss: 8.784394245313388\n",
            "Epoch: 46, Step: 19600, Train Loss: 7.441802663832655, Valid Loss: 8.773215522886865\n",
            "Epoch: 47, Step: 19700, Train Loss: 7.40992313971536, Valid Loss: 8.943006539876125\n",
            "Epoch: 47, Step: 19800, Train Loss: 7.424684584715616, Valid Loss: 8.817195355804222\n",
            "Epoch: 47, Step: 19900, Train Loss: 7.604710857457512, Valid Loss: 8.798888364411171\n",
            "Epoch: 47, Step: 20000, Train Loss: 7.544554866874274, Valid Loss: 9.028795441244222\n",
            "Epoch: 48, Step: 20100, Train Loss: 7.381488225524252, Valid Loss: 8.879460099860932\n",
            "Epoch: 48, Step: 20200, Train Loss: 7.411929158847954, Valid Loss: 8.814746167854553\n",
            "Epoch: 48, Step: 20300, Train Loss: 7.3988865798722365, Valid Loss: 8.845460111660245\n",
            "Epoch: 48, Step: 20400, Train Loss: 7.493735477134725, Valid Loss: 8.806639554893762\n",
            "Epoch: 49, Step: 20500, Train Loss: 7.421387521830448, Valid Loss: 8.863273325899105\n",
            "Epoch: 49, Step: 20600, Train Loss: 7.387129455004186, Valid Loss: 8.923003338310318\n",
            "Epoch: 49, Step: 20700, Train Loss: 7.402352008812604, Valid Loss: 8.92164773779321\n",
            "Epoch: 49, Step: 20800, Train Loss: 7.401134500359536, Valid Loss: 8.858270747713638\n",
            "Epoch: 49, Step: 20900, Train Loss: 7.397716313816311, Valid Loss: 8.86363388973673\n",
            "Epoch: 50, Step: 21000, Train Loss: 7.3909452295683495, Valid Loss: 8.831424214030166\n",
            "Epoch: 50, Step: 21100, Train Loss: 7.426696206818276, Valid Loss: 8.82960065604842\n",
            "Epoch: 50, Step: 21200, Train Loss: 7.373646650841252, Valid Loss: 8.81679049296503\n",
            "Epoch: 50, Step: 21300, Train Loss: 7.456482835447343, Valid Loss: 9.083790643377071\n",
            "Epoch: 51, Step: 21400, Train Loss: 7.360090192702007, Valid Loss: 8.847544262817566\n",
            "Epoch: 51, Step: 21500, Train Loss: 7.40143014085541, Valid Loss: 8.96992406788276\n",
            "Epoch: 51, Step: 21600, Train Loss: 7.361883652901498, Valid Loss: 8.941793858156037\n",
            "Epoch: 51, Step: 21700, Train Loss: 7.613125049005961, Valid Loss: 8.808349389142338\n",
            "Epoch: 52, Step: 21800, Train Loss: 7.371070349370108, Valid Loss: 8.879259301364755\n",
            "Epoch: 52, Step: 21900, Train Loss: 7.3675137194093985, Valid Loss: 8.886012674946704\n",
            "Epoch: 52, Step: 22000, Train Loss: 7.383470131775282, Valid Loss: 8.843746143457057\n",
            "Epoch: 52, Step: 22100, Train Loss: 7.364008744438681, Valid Loss: 9.040530901246575\n",
            "Epoch: 53, Step: 22200, Train Loss: 7.33408495388407, Valid Loss: 8.908255400708432\n",
            "Epoch: 53, Step: 22300, Train Loss: 7.328577421823289, Valid Loss: 8.950591675024201\n",
            "Epoch: 53, Step: 22400, Train Loss: 7.375435665122597, Valid Loss: 8.99910636628016\n",
            "Epoch: 53, Step: 22500, Train Loss: 7.394472330172528, Valid Loss: 8.957326967041388\n",
            "Epoch: 54, Step: 22600, Train Loss: 7.399680740569733, Valid Loss: 9.051871250406519\n",
            "Epoch: 54, Step: 22700, Train Loss: 7.343655375869954, Valid Loss: 8.88777237290096\n",
            "Epoch: 54, Step: 22800, Train Loss: 7.39894650581409, Valid Loss: 9.100418792499738\n",
            "Epoch: 54, Step: 22900, Train Loss: 7.39810750731191, Valid Loss: 9.11090150168558\n",
            "Epoch: 55, Step: 23000, Train Loss: 7.317998428379719, Valid Loss: 8.894521136662677\n",
            "Epoch: 55, Step: 23100, Train Loss: 7.350143598426667, Valid Loss: 8.929765724000523\n",
            "Epoch: 55, Step: 23200, Train Loss: 7.330084396137884, Valid Loss: 9.061304339798122\n",
            "Epoch: 55, Step: 23300, Train Loss: 7.305182449950626, Valid Loss: 8.89350795132023\n",
            "Epoch: 55, Step: 23400, Train Loss: 7.3288116616675625, Valid Loss: 8.813265497980673\n",
            "Epoch: 56, Step: 23500, Train Loss: 7.295332826166945, Valid Loss: 8.9824855293129\n",
            "Epoch: 56, Step: 23600, Train Loss: 7.291523701845047, Valid Loss: 8.874571566350026\n",
            "Epoch: 56, Step: 23700, Train Loss: 7.3144609007801185, Valid Loss: 8.961804678720664\n",
            "Epoch: 56, Step: 23800, Train Loss: 7.316605597806233, Valid Loss: 9.056948690116783\n",
            "Epoch: 57, Step: 23900, Train Loss: 7.255581935538017, Valid Loss: 8.880399415630738\n",
            "Epoch: 57, Step: 24000, Train Loss: 7.2886744497287985, Valid Loss: 8.86938494961906\n",
            "Epoch: 57, Step: 24100, Train Loss: 7.269875851704373, Valid Loss: 8.845486420081393\n",
            "Epoch: 57, Step: 24200, Train Loss: 7.30282906150316, Valid Loss: 8.822677680266292\n",
            "Epoch: 58, Step: 24300, Train Loss: 7.292668189056831, Valid Loss: 8.847594453581738\n",
            "Epoch: 58, Step: 24400, Train Loss: 7.317912618533556, Valid Loss: 9.16073991349832\n",
            "Epoch: 58, Step: 24500, Train Loss: 7.2714435631870655, Valid Loss: 8.92767021443791\n",
            "Epoch: 58, Step: 24600, Train Loss: 7.248948272180672, Valid Loss: 8.921294216281193\n",
            "Epoch: 59, Step: 24700, Train Loss: 7.232423337016932, Valid Loss: 9.007252320151801\n",
            "Epoch: 59, Step: 24800, Train Loss: 7.275741571405931, Valid Loss: 9.026350955643618\n",
            "Epoch: 59, Step: 24900, Train Loss: 7.277470445997908, Valid Loss: 9.020192908445011\n",
            "Epoch: 59, Step: 25000, Train Loss: 7.280194368572186, Valid Loss: 8.949183547135974\n",
            "Epoch: 60, Step: 25100, Train Loss: 7.249231680723842, Valid Loss: 9.059635328164186\n",
            "Epoch: 60, Step: 25200, Train Loss: 7.252056336136894, Valid Loss: 9.002878800639326\n",
            "Epoch: 60, Step: 25300, Train Loss: 7.278740982150986, Valid Loss: 8.979676001911937\n",
            "Epoch: 60, Step: 25400, Train Loss: 7.264067348425662, Valid Loss: 8.990942938940714\n",
            "Epoch: 61, Step: 25500, Train Loss: 7.285417876991027, Valid Loss: 8.912733210586213\n",
            "Epoch: 61, Step: 25600, Train Loss: 7.278258408295805, Valid Loss: 8.93529964257659\n",
            "Epoch: 61, Step: 25700, Train Loss: 7.277148113540859, Valid Loss: 8.965053775348485\n",
            "Epoch: 61, Step: 25800, Train Loss: 7.26999141101769, Valid Loss: 8.891666105841278\n",
            "Epoch: 61, Step: 25900, Train Loss: 7.223338482891153, Valid Loss: 8.920797216802802\n",
            "Epoch: 62, Step: 26000, Train Loss: 7.252755379814252, Valid Loss: 8.884120085545565\n",
            "Epoch: 62, Step: 26100, Train Loss: 7.212592722888458, Valid Loss: 9.021558690903696\n",
            "Epoch: 62, Step: 26200, Train Loss: 7.291153889106901, Valid Loss: 8.908339201981484\n",
            "Epoch: 62, Step: 26300, Train Loss: 7.24518543804755, Valid Loss: 8.85319377046538\n",
            "Epoch: 63, Step: 26400, Train Loss: 7.3421812223707725, Valid Loss: 8.976130531367842\n",
            "Epoch: 63, Step: 26500, Train Loss: 7.224520260878708, Valid Loss: 9.039064822071195\n",
            "Epoch: 63, Step: 26600, Train Loss: 7.215067650819858, Valid Loss: 9.037209908324902\n",
            "Epoch: 63, Step: 26700, Train Loss: 7.229225862953789, Valid Loss: 8.989812866353429\n",
            "Epoch: 64, Step: 26800, Train Loss: 7.243576291528123, Valid Loss: 9.129197795961096\n",
            "Epoch: 64, Step: 26900, Train Loss: 7.202583578842198, Valid Loss: 8.979414310486849\n",
            "Epoch: 64, Step: 27000, Train Loss: 7.222273444280519, Valid Loss: 9.007288712451519\n",
            "Epoch: 64, Step: 27100, Train Loss: 7.320687550811453, Valid Loss: 8.829479383150911\n",
            "Epoch: 65, Step: 27200, Train Loss: 7.239331541713273, Valid Loss: 9.143150833903379\n",
            "Epoch: 65, Step: 27300, Train Loss: 7.215866291679757, Valid Loss: 9.019454896261347\n",
            "Epoch: 65, Step: 27400, Train Loss: 7.189647585997379, Valid Loss: 9.028898808226069\n",
            "Epoch: 65, Step: 27500, Train Loss: 7.251360002469576, Valid Loss: 9.074160975610893\n",
            "Epoch: 66, Step: 27600, Train Loss: 7.1875146185038785, Valid Loss: 9.029307179734921\n",
            "Epoch: 66, Step: 27700, Train Loss: 7.332072637425124, Valid Loss: 9.377694622828816\n",
            "Epoch: 66, Step: 27800, Train Loss: 7.175595110377276, Valid Loss: 8.953867335381098\n",
            "Epoch: 66, Step: 27900, Train Loss: 7.19074957485875, Valid Loss: 9.097930313532672\n",
            "Epoch: 66, Step: 28000, Train Loss: 7.1836415300814505, Valid Loss: 9.025165799825018\n",
            "Epoch: 67, Step: 28100, Train Loss: 7.3348738073283615, Valid Loss: 9.365802080390363\n",
            "Epoch: 67, Step: 28200, Train Loss: 7.453561180048451, Valid Loss: 8.897811834269712\n",
            "Epoch: 67, Step: 28300, Train Loss: 7.20170312159904, Valid Loss: 9.07353996680424\n",
            "Epoch: 67, Step: 28400, Train Loss: 7.245697490745884, Valid Loss: 8.880142359122297\n",
            "Epoch: 68, Step: 28500, Train Loss: 7.20446204180034, Valid Loss: 9.078191659394218\n",
            "Epoch: 68, Step: 28600, Train Loss: 7.159236099505961, Valid Loss: 8.94768226166143\n",
            "Epoch: 68, Step: 28700, Train Loss: 7.148314250110458, Valid Loss: 9.020167875372252\n",
            "Epoch: 68, Step: 28800, Train Loss: 7.184501722886371, Valid Loss: 9.057442559724976\n",
            "Epoch: 69, Step: 28900, Train Loss: 7.1704184953729255, Valid Loss: 9.010759908734098\n",
            "Epoch: 69, Step: 29000, Train Loss: 7.172179832289552, Valid Loss: 9.066505153407883\n",
            "Epoch: 69, Step: 29100, Train Loss: 7.300336165878665, Valid Loss: 9.14431367852612\n",
            "Epoch: 69, Step: 29200, Train Loss: 7.263331911649447, Valid Loss: 9.049246220478173\n",
            "Epoch: 70, Step: 29300, Train Loss: 7.163758802945725, Valid Loss: 9.118072621062444\n",
            "Epoch: 70, Step: 29400, Train Loss: 7.167183547816645, Valid Loss: 9.021735163819445\n",
            "Epoch: 70, Step: 29500, Train Loss: 7.163123413239936, Valid Loss: 9.043014930887495\n",
            "Epoch: 70, Step: 29600, Train Loss: 7.1858743112867005, Valid Loss: 8.916192340052076\n",
            "Epoch: 71, Step: 29700, Train Loss: 7.132379819355711, Valid Loss: 9.027471506360294\n",
            "Epoch: 71, Step: 29800, Train Loss: 7.14649633058914, Valid Loss: 9.034262510551667\n",
            "Epoch: 71, Step: 29900, Train Loss: 7.151135114437736, Valid Loss: 8.977324363445687\n",
            "Epoch: 71, Step: 30000, Train Loss: 7.184443122390423, Valid Loss: 8.964281958526424\n",
            "Epoch: 72, Step: 30100, Train Loss: 7.146104794425302, Valid Loss: 9.070002091848167\n",
            "Epoch: 72, Step: 30200, Train Loss: 7.155115472114144, Valid Loss: 8.959132832237442\n",
            "Epoch: 72, Step: 30300, Train Loss: 7.1390464372950095, Valid Loss: 8.976029984544516\n",
            "Epoch: 72, Step: 30400, Train Loss: 7.1649268978909575, Valid Loss: 9.024776719081254\n",
            "Epoch: 72, Step: 30500, Train Loss: 7.132661635979486, Valid Loss: 8.98862302182658\n",
            "Epoch: 73, Step: 30600, Train Loss: 7.129319306330539, Valid Loss: 9.038556674342406\n",
            "Epoch: 73, Step: 30700, Train Loss: 7.342651938845772, Valid Loss: 8.874694503527772\n",
            "Epoch: 73, Step: 30800, Train Loss: 7.18012132397641, Valid Loss: 8.889277555081675\n",
            "Epoch: 73, Step: 30900, Train Loss: 7.173850758753437, Valid Loss: 9.162522861697077\n",
            "Epoch: 74, Step: 31000, Train Loss: 7.111004857001012, Valid Loss: 9.061705240966011\n",
            "Epoch: 74, Step: 31100, Train Loss: 7.119144510657722, Valid Loss: 9.075007194710073\n",
            "Epoch: 74, Step: 31200, Train Loss: 7.13517609878139, Valid Loss: 8.935605638349141\n",
            "Epoch: 74, Step: 31300, Train Loss: 7.178077385641635, Valid Loss: 9.085140135823465\n",
            "Epoch: 75, Step: 31400, Train Loss: 7.1118631045168925, Valid Loss: 8.988236281471119\n",
            "Epoch: 75, Step: 31500, Train Loss: 7.114145104052469, Valid Loss: 9.102807266549814\n",
            "Epoch: 75, Step: 31600, Train Loss: 7.1274479369311665, Valid Loss: 9.17039954003204\n",
            "Epoch: 75, Step: 31700, Train Loss: 7.1206536369596405, Valid Loss: 9.003959647637087\n",
            "Epoch: 76, Step: 31800, Train Loss: 7.097222928665118, Valid Loss: 9.14321298696827\n",
            "Epoch: 76, Step: 31900, Train Loss: 7.118870436490361, Valid Loss: 9.015171581210733\n",
            "Epoch: 76, Step: 32000, Train Loss: 7.417272477093066, Valid Loss: 9.421509704642508\n",
            "Epoch: 76, Step: 32100, Train Loss: 7.129774730471018, Valid Loss: 9.066457017690476\n",
            "Epoch: 77, Step: 32200, Train Loss: 7.078233710192879, Valid Loss: 9.070676517877097\n",
            "Epoch: 77, Step: 32300, Train Loss: 7.144426625488539, Valid Loss: 9.253442488047433\n",
            "Epoch: 77, Step: 32400, Train Loss: 7.127341492511901, Valid Loss: 9.031686655643895\n",
            "Epoch: 77, Step: 32500, Train Loss: 7.1055906899786745, Valid Loss: 9.11756589008839\n",
            "Epoch: 77, Step: 32600, Train Loss: 7.110718591875599, Valid Loss: 9.013592441225546\n",
            "Epoch: 78, Step: 32700, Train Loss: 7.151110829610502, Valid Loss: 9.234948548211477\n",
            "Epoch: 78, Step: 32800, Train Loss: 7.15601593027212, Valid Loss: 9.312110321123036\n",
            "Epoch: 78, Step: 32900, Train Loss: 7.133744016641591, Valid Loss: 9.246268084674519\n",
            "Epoch: 78, Step: 33000, Train Loss: 7.125719537021484, Valid Loss: 9.129476243382383\n",
            "Epoch: 79, Step: 33100, Train Loss: 7.133531264416024, Valid Loss: 9.317518482335093\n",
            "Epoch: 79, Step: 33200, Train Loss: 7.274165821192412, Valid Loss: 8.99140563586428\n",
            "Epoch: 79, Step: 33300, Train Loss: 7.065084021612018, Valid Loss: 9.045056223066126\n",
            "Epoch: 79, Step: 33400, Train Loss: 7.1143408453446355, Valid Loss: 8.99379475297563\n",
            "Epoch: 80, Step: 33500, Train Loss: 7.133205522874242, Valid Loss: 9.053318169860926\n",
            "Epoch: 80, Step: 33600, Train Loss: 7.174903279989723, Valid Loss: 9.34956061822812\n",
            "Epoch: 80, Step: 33700, Train Loss: 7.106437444555385, Valid Loss: 9.195445468588433\n",
            "Epoch: 80, Step: 33800, Train Loss: 7.1968418237808525, Valid Loss: 9.264132913846943\n",
            "Epoch: 81, Step: 33900, Train Loss: 7.065103521567135, Valid Loss: 9.201794141826609\n",
            "Epoch: 81, Step: 34000, Train Loss: 7.0573425808015635, Valid Loss: 9.005349269118504\n",
            "Epoch: 81, Step: 34100, Train Loss: 7.104545933977642, Valid Loss: 9.225717323921302\n",
            "Epoch: 81, Step: 34200, Train Loss: 7.100343189476821, Valid Loss: 9.207576814046343\n",
            "Epoch: 82, Step: 34300, Train Loss: 7.060152853181777, Valid Loss: 8.980361901628566\n",
            "Epoch: 82, Step: 34400, Train Loss: 7.065559046163201, Valid Loss: 9.21394059741702\n",
            "Epoch: 82, Step: 34500, Train Loss: 7.035785346585482, Valid Loss: 9.140144716097755\n",
            "Epoch: 82, Step: 34600, Train Loss: 7.067710483411899, Valid Loss: 9.24860756449581\n",
            "Epoch: 83, Step: 34700, Train Loss: 7.035501543058122, Valid Loss: 9.11424350600487\n",
            "Epoch: 83, Step: 34800, Train Loss: 7.034994508898602, Valid Loss: 9.232334741186692\n",
            "Epoch: 83, Step: 34900, Train Loss: 7.065818853883746, Valid Loss: 9.12777923533749\n",
            "Epoch: 83, Step: 35000, Train Loss: 7.047523325909976, Valid Loss: 9.161027248154866\n",
            "Epoch: 83, Step: 35100, Train Loss: 7.0502055754075785, Valid Loss: 9.04297674941514\n",
            "Epoch: 84, Step: 35200, Train Loss: 7.056433359173118, Valid Loss: 9.248022308243236\n",
            "Epoch: 84, Step: 35300, Train Loss: 7.2400866578198775, Valid Loss: 8.897807469800968\n",
            "Epoch: 84, Step: 35400, Train Loss: 7.094304357316985, Valid Loss: 8.992449198884\n",
            "Epoch: 84, Step: 35500, Train Loss: 7.077963223383946, Valid Loss: 8.96465036548918\n",
            "Epoch: 85, Step: 35600, Train Loss: 7.03487930468127, Valid Loss: 9.226346880051011\n",
            "Epoch: 85, Step: 35700, Train Loss: 7.034307960596201, Valid Loss: 9.09061879311017\n",
            "Epoch: 85, Step: 35800, Train Loss: 7.1555938872692595, Valid Loss: 8.953715757500483\n",
            "Epoch: 85, Step: 35900, Train Loss: 7.111448135172925, Valid Loss: 9.317853126506336\n",
            "Epoch: 86, Step: 36000, Train Loss: 7.053576015142035, Valid Loss: 9.090448717089712\n",
            "Epoch: 86, Step: 36100, Train Loss: 7.053563277182663, Valid Loss: 9.082226914803268\n",
            "Epoch: 86, Step: 36200, Train Loss: 7.0890612061190845, Valid Loss: 9.14516573792595\n",
            "Epoch: 86, Step: 36300, Train Loss: 7.052874207460234, Valid Loss: 9.201358492160347\n",
            "Epoch: 87, Step: 36400, Train Loss: 7.157749743692636, Valid Loss: 9.370834721790903\n",
            "Epoch: 87, Step: 36500, Train Loss: 7.021692570392218, Valid Loss: 9.173027927947803\n",
            "Epoch: 87, Step: 36600, Train Loss: 7.017469313410433, Valid Loss: 9.159818236463604\n",
            "Epoch: 87, Step: 36700, Train Loss: 7.020343037765131, Valid Loss: 9.120009124904705\n",
            "Epoch: 88, Step: 36800, Train Loss: 7.050945806224982, Valid Loss: 9.343358846418035\n",
            "Epoch: 88, Step: 36900, Train Loss: 7.029160877047283, Valid Loss: 9.089241172489542\n",
            "Epoch: 88, Step: 37000, Train Loss: 7.100473757553483, Valid Loss: 9.278528710801783\n",
            "Epoch: 88, Step: 37100, Train Loss: 7.0056759960523864, Valid Loss: 9.135073208773543\n",
            "Epoch: 88, Step: 37200, Train Loss: 7.01685129395364, Valid Loss: 9.117378353060856\n",
            "Epoch: 89, Step: 37300, Train Loss: 7.0746798874191, Valid Loss: 9.361400626408638\n",
            "Epoch: 89, Step: 37400, Train Loss: 7.013736384331052, Valid Loss: 9.267286243599253\n",
            "Epoch: 89, Step: 37500, Train Loss: 7.0130965509810155, Valid Loss: 9.15342821869639\n",
            "Epoch: 89, Step: 37600, Train Loss: 7.030871422207766, Valid Loss: 9.210216714940225\n",
            "Epoch: 90, Step: 37700, Train Loss: 6.9941192383009065, Valid Loss: 9.14134317971289\n",
            "Epoch: 90, Step: 37800, Train Loss: 7.0536651505232895, Valid Loss: 9.015845838854768\n",
            "Epoch: 90, Step: 37900, Train Loss: 7.011383246407764, Valid Loss: 9.086554701034162\n",
            "Epoch: 90, Step: 38000, Train Loss: 7.009892809305824, Valid Loss: 9.089936231885067\n",
            "Epoch: 91, Step: 38100, Train Loss: 6.968211474428516, Valid Loss: 9.103892019246736\n",
            "Epoch: 91, Step: 38200, Train Loss: 6.98404589568204, Valid Loss: 9.243477728429621\n",
            "Epoch: 91, Step: 38300, Train Loss: 7.008427741040576, Valid Loss: 9.192316643850226\n",
            "Epoch: 91, Step: 38400, Train Loss: 7.05047027801015, Valid Loss: 9.082859836354372\n",
            "Epoch: 92, Step: 38500, Train Loss: 6.96234388460889, Valid Loss: 9.222291679443558\n",
            "Epoch: 92, Step: 38600, Train Loss: 7.0051686030476725, Valid Loss: 9.252884934125719\n",
            "Epoch: 92, Step: 38700, Train Loss: 7.049141699663346, Valid Loss: 9.301588485335802\n",
            "Epoch: 92, Step: 38800, Train Loss: 7.096602995727145, Valid Loss: 9.06700206083631\n",
            "Epoch: 93, Step: 38900, Train Loss: 7.006643198088833, Valid Loss: 9.293975148472859\n",
            "Epoch: 93, Step: 39000, Train Loss: 6.986254148428458, Valid Loss: 9.111855101362742\n",
            "Epoch: 93, Step: 39100, Train Loss: 7.081148199401072, Valid Loss: 9.009667930763356\n",
            "Epoch: 93, Step: 39200, Train Loss: 7.029602720874573, Valid Loss: 9.092384097794064\n",
            "Epoch: 94, Step: 39300, Train Loss: 7.013448931236683, Valid Loss: 9.327849526265702\n",
            "Epoch: 94, Step: 39400, Train Loss: 6.996420878398785, Valid Loss: 9.08988890603052\n",
            "Epoch: 94, Step: 39500, Train Loss: 6.963797875669215, Valid Loss: 9.109131777023185\n",
            "Epoch: 94, Step: 39600, Train Loss: 6.987772843911105, Valid Loss: 9.221191980285896\n",
            "Epoch: 94, Step: 39700, Train Loss: 6.999800709963663, Valid Loss: 9.104890566494863\n",
            "Epoch: 95, Step: 39800, Train Loss: 6.957705967684293, Valid Loss: 9.159433316692999\n",
            "Epoch: 95, Step: 39900, Train Loss: 6.956994437369987, Valid Loss: 9.219683663161002\n",
            "Epoch: 95, Step: 40000, Train Loss: 6.974496849381532, Valid Loss: 9.164739022713247\n",
            "Epoch: 95, Step: 40100, Train Loss: 7.016399915693361, Valid Loss: 9.108571044907519\n",
            "Epoch: 96, Step: 40200, Train Loss: 6.943332883625573, Valid Loss: 9.11318328641975\n",
            "Epoch: 96, Step: 40300, Train Loss: 6.967455482143967, Valid Loss: 9.177870207270727\n",
            "Epoch: 96, Step: 40400, Train Loss: 7.01750724144224, Valid Loss: 9.233770984960525\n",
            "Epoch: 96, Step: 40500, Train Loss: 7.035518610455797, Valid Loss: 9.042480005295502\n",
            "Epoch: 97, Step: 40600, Train Loss: 6.979278541479474, Valid Loss: 9.24638249491112\n",
            "Epoch: 97, Step: 40700, Train Loss: 6.963961289664775, Valid Loss: 9.213212436301008\n",
            "Epoch: 97, Step: 40800, Train Loss: 6.972574594864731, Valid Loss: 9.101438249702502\n",
            "Epoch: 97, Step: 40900, Train Loss: 6.976327402730586, Valid Loss: 9.149666401854498\n",
            "Epoch: 98, Step: 41000, Train Loss: 6.9476832557780845, Valid Loss: 9.280265659912573\n",
            "Epoch: 98, Step: 41100, Train Loss: 6.962817799455355, Valid Loss: 9.094788533767673\n",
            "Epoch: 98, Step: 41200, Train Loss: 7.00003806881522, Valid Loss: 9.15200279186621\n",
            "Epoch: 98, Step: 41300, Train Loss: 7.000485536200932, Valid Loss: 9.147471202570804\n",
            "Epoch: 99, Step: 41400, Train Loss: 6.991885888142106, Valid Loss: 9.356447247269541\n",
            "Epoch: 99, Step: 41500, Train Loss: 6.92862329665479, Valid Loss: 9.185027049857094\n",
            "Epoch: 99, Step: 41600, Train Loss: 6.9850163909440655, Valid Loss: 9.110459519993054\n",
            "Epoch: 99, Step: 41700, Train Loss: 6.990061294406353, Valid Loss: 9.1114853032292\n",
            "Epoch: 99, Step: 41800, Train Loss: 6.979033855105373, Valid Loss: 9.084185020748901\n"
          ]
        }
      ],
      "source": [
        "mlp_90 = MyMLP(\n",
        "    X_subtrain=X_subtrain,\n",
        "    Y_subtrain=Y_subtrain,\n",
        "    X_valid=X_valid,\n",
        "    Y_valid=Y_valid,\n",
        "    H=90,\n",
        "    lr=0.00001,\n",
        "    wd=0,\n",
        "    mom=0,\n",
        "    loss_type=0,\n",
        "    optimizer_type=0,\n",
        "    use_dropout=False\n",
        ")\n",
        "train_loss_list, valid_loss_list = mlp_90.fit(\n",
        "    max_epoch=100, \n",
        "    verbose=True, \n",
        "    patience_batch_num=5000, \n",
        "    model_path='q3_mlp_90.ckpt'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9z4TcFxPs3q",
        "outputId": "31be24b5-a9fe-41b1-dc72-3f128a0bfde0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test RMSE Loss for 90 hidden units: 8.824939635001625\n"
          ]
        }
      ],
      "source": [
        "q3_mlp_90_best_model = torch.load('q3_mlp_90.ckpt')\n",
        "test_rmse_loss = calculate_test_rmse(q3_mlp_90_best_model, X_test, Y_test)\n",
        "print(f\"Test RMSE Loss for 90 hidden units: {test_rmse_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfALW70pPs3r",
        "outputId": "ee17f38a-6795-4efd-efc0-1dca69dfa78f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Step: 100, Train Loss: 9.762773435937175, Valid Loss: 9.72437337728604\n",
            "Epoch: 0, Step: 200, Train Loss: 8.933288157975946, Valid Loss: 8.878353351058326\n",
            "Epoch: 0, Step: 300, Train Loss: 8.961054331286567, Valid Loss: 8.905003799988245\n",
            "Epoch: 0, Step: 400, Train Loss: 8.828895391504718, Valid Loss: 8.792201202023026\n",
            "Epoch: 1, Step: 500, Train Loss: 8.895737112881474, Valid Loss: 8.865517077137692\n",
            "Epoch: 1, Step: 600, Train Loss: 8.69450669542885, Valid Loss: 8.674454258141354\n",
            "Epoch: 1, Step: 700, Train Loss: 8.682571490382028, Valid Loss: 8.685526698047148\n",
            "Epoch: 1, Step: 800, Train Loss: 8.622623468223562, Valid Loss: 8.637789665198763\n",
            "Epoch: 2, Step: 900, Train Loss: 8.577157590386301, Valid Loss: 8.60461878077653\n",
            "Epoch: 2, Step: 1000, Train Loss: 8.550915400035679, Valid Loss: 8.590970958875289\n",
            "Epoch: 2, Step: 1100, Train Loss: 8.533694665228202, Valid Loss: 8.582343570223141\n",
            "Epoch: 2, Step: 1200, Train Loss: 8.517200602059388, Valid Loss: 8.584848598329431\n",
            "Epoch: 3, Step: 1300, Train Loss: 8.652920880077646, Valid Loss: 8.72143172218415\n",
            "Epoch: 3, Step: 1400, Train Loss: 8.526679400883978, Valid Loss: 8.61690135528447\n",
            "Epoch: 3, Step: 1500, Train Loss: 8.49819966699455, Valid Loss: 8.586738244106233\n",
            "Epoch: 3, Step: 1600, Train Loss: 8.499775803220702, Valid Loss: 8.61247123082954\n",
            "Epoch: 4, Step: 1700, Train Loss: 8.512958172437134, Valid Loss: 8.621925580532128\n",
            "Epoch: 4, Step: 1800, Train Loss: 8.59120357776612, Valid Loss: 8.712661790403821\n",
            "Epoch: 4, Step: 1900, Train Loss: 8.38306135066896, Valid Loss: 8.531364896383197\n",
            "Epoch: 4, Step: 2000, Train Loss: 8.397820540231642, Valid Loss: 8.55427769325619\n",
            "Epoch: 5, Step: 2100, Train Loss: 8.479051948290792, Valid Loss: 8.655902520539469\n",
            "Epoch: 5, Step: 2200, Train Loss: 8.316463087377741, Valid Loss: 8.521745872491653\n",
            "Epoch: 5, Step: 2300, Train Loss: 8.300337609331196, Valid Loss: 8.504996584209445\n",
            "Epoch: 5, Step: 2400, Train Loss: 8.33206388872094, Valid Loss: 8.562299892358206\n",
            "Epoch: 5, Step: 2500, Train Loss: 8.335533562115101, Valid Loss: 8.548565147613427\n",
            "Epoch: 6, Step: 2600, Train Loss: 8.283385078064269, Valid Loss: 8.553367658034967\n",
            "Epoch: 6, Step: 2700, Train Loss: 8.325485812023166, Valid Loss: 8.581274681018618\n",
            "Epoch: 6, Step: 2800, Train Loss: 8.26621997893379, Valid Loss: 8.549734645072071\n",
            "Epoch: 6, Step: 2900, Train Loss: 8.223561419437823, Valid Loss: 8.54725565633873\n",
            "Epoch: 7, Step: 3000, Train Loss: 8.188168435393274, Valid Loss: 8.539799405861785\n",
            "Epoch: 7, Step: 3100, Train Loss: 8.175190738646853, Valid Loss: 8.50090641611411\n",
            "Epoch: 7, Step: 3200, Train Loss: 8.209411259909828, Valid Loss: 8.538973335058827\n",
            "Epoch: 7, Step: 3300, Train Loss: 8.126533104448175, Valid Loss: 8.508583477753293\n",
            "Epoch: 8, Step: 3400, Train Loss: 8.11806655826374, Valid Loss: 8.505811478042496\n",
            "Epoch: 8, Step: 3500, Train Loss: 8.121231322165247, Valid Loss: 8.557906363500868\n",
            "Epoch: 8, Step: 3600, Train Loss: 8.085132965442684, Valid Loss: 8.492946684455811\n",
            "Epoch: 8, Step: 3700, Train Loss: 8.250370834845599, Valid Loss: 8.641660351934568\n",
            "Epoch: 9, Step: 3800, Train Loss: 8.157306459777049, Valid Loss: 8.677011921363592\n",
            "Epoch: 9, Step: 3900, Train Loss: 8.132115880927051, Valid Loss: 8.572353373934321\n",
            "Epoch: 9, Step: 4000, Train Loss: 8.051449076230414, Valid Loss: 8.567441177011567\n",
            "Epoch: 9, Step: 4100, Train Loss: 7.9957390611063905, Valid Loss: 8.498649976529974\n",
            "Epoch: 10, Step: 4200, Train Loss: 8.03331820941256, Valid Loss: 8.580664775126747\n",
            "Epoch: 10, Step: 4300, Train Loss: 8.12651863981736, Valid Loss: 8.623464155676865\n",
            "Epoch: 10, Step: 4400, Train Loss: 7.938833082233042, Valid Loss: 8.517104332492506\n",
            "Epoch: 10, Step: 4500, Train Loss: 7.91814126131918, Valid Loss: 8.56406688045828\n",
            "Epoch: 11, Step: 4600, Train Loss: 7.93389156853899, Valid Loss: 8.515631547769154\n",
            "Epoch: 11, Step: 4700, Train Loss: 7.895189420336478, Valid Loss: 8.547869116311048\n",
            "Epoch: 11, Step: 4800, Train Loss: 7.92295617593757, Valid Loss: 8.543983267241082\n",
            "Epoch: 11, Step: 4900, Train Loss: 7.878312775458003, Valid Loss: 8.528722202796397\n",
            "Epoch: 11, Step: 5000, Train Loss: 7.849586808677946, Valid Loss: 8.56006106802341\n",
            "Epoch: 12, Step: 5100, Train Loss: 7.875664384167645, Valid Loss: 8.607078299561508\n",
            "Epoch: 12, Step: 5200, Train Loss: 7.921714217426184, Valid Loss: 8.560886326614265\n",
            "Epoch: 12, Step: 5300, Train Loss: 7.818683362573744, Valid Loss: 8.548313404180604\n",
            "Epoch: 12, Step: 5400, Train Loss: 7.888678882039021, Valid Loss: 8.642346063768299\n",
            "Epoch: 13, Step: 5500, Train Loss: 8.083409549861926, Valid Loss: 8.766326865962144\n",
            "Epoch: 13, Step: 5600, Train Loss: 7.772405731070938, Valid Loss: 8.561567490333953\n",
            "Epoch: 13, Step: 5700, Train Loss: 7.745566779460697, Valid Loss: 8.60944059706697\n",
            "Epoch: 13, Step: 5800, Train Loss: 7.728171803814197, Valid Loss: 8.610891127294812\n",
            "Epoch: 14, Step: 5900, Train Loss: 7.85996435182481, Valid Loss: 8.790622581544728\n",
            "Epoch: 14, Step: 6000, Train Loss: 7.693274841693221, Valid Loss: 8.64175629384487\n",
            "Epoch: 14, Step: 6100, Train Loss: 7.726449428278971, Valid Loss: 8.57048186823707\n",
            "Epoch: 14, Step: 6200, Train Loss: 7.681691097438855, Valid Loss: 8.56453723147459\n",
            "Epoch: 15, Step: 6300, Train Loss: 7.601108777348799, Valid Loss: 8.568540091062724\n",
            "Epoch: 15, Step: 6400, Train Loss: 7.645660308554268, Valid Loss: 8.602981563636204\n",
            "Epoch: 15, Step: 6500, Train Loss: 7.689262462152263, Valid Loss: 8.740587310219555\n",
            "Epoch: 15, Step: 6600, Train Loss: 7.650145262210743, Valid Loss: 8.617164915421785\n",
            "Epoch: 16, Step: 6700, Train Loss: 7.5834967291064554, Valid Loss: 8.648165875864843\n",
            "Epoch: 16, Step: 6800, Train Loss: 7.572691222291308, Valid Loss: 8.649436301677918\n",
            "Epoch: 16, Step: 6900, Train Loss: 7.566937769070298, Valid Loss: 8.669267184583846\n",
            "Epoch: 16, Step: 7000, Train Loss: 7.516879902494357, Valid Loss: 8.611067703513786\n",
            "Epoch: 16, Step: 7100, Train Loss: 7.52784602916448, Valid Loss: 8.597756310833892\n",
            "Epoch: 17, Step: 7200, Train Loss: 7.4759260535558845, Valid Loss: 8.643346821351622\n",
            "Epoch: 17, Step: 7300, Train Loss: 7.571468157468714, Valid Loss: 8.650218547671411\n",
            "Epoch: 17, Step: 7400, Train Loss: 7.455080404304631, Valid Loss: 8.60086710956937\n",
            "Epoch: 17, Step: 7500, Train Loss: 7.411427305106217, Valid Loss: 8.613316199344666\n",
            "Epoch: 18, Step: 7600, Train Loss: 7.40963192009071, Valid Loss: 8.733011023745513\n",
            "Epoch: 18, Step: 7700, Train Loss: 7.446684257138018, Valid Loss: 8.721230709370788\n",
            "Epoch: 18, Step: 7800, Train Loss: 7.44314317758271, Valid Loss: 8.707953855857584\n",
            "Epoch: 18, Step: 7900, Train Loss: 7.353551737163428, Valid Loss: 8.606274636241055\n",
            "Epoch: 19, Step: 8000, Train Loss: 7.399797892521334, Valid Loss: 8.770928962767167\n",
            "Epoch: 19, Step: 8100, Train Loss: 7.363103770373783, Valid Loss: 8.634379472029517\n",
            "Epoch: 19, Step: 8200, Train Loss: 7.371071862812764, Valid Loss: 8.759364559049187\n",
            "Epoch: 19, Step: 8300, Train Loss: 7.406102438719686, Valid Loss: 8.777951994800953\n",
            "Epoch: 20, Step: 8400, Train Loss: 7.2647282297764875, Valid Loss: 8.714034584425887\n",
            "Epoch: 20, Step: 8500, Train Loss: 7.343289986345835, Valid Loss: 8.808410390021777\n",
            "Epoch: 20, Step: 8600, Train Loss: 7.284231494440779, Valid Loss: 8.785338427689227\n",
            "Epoch: 20, Step: 8700, Train Loss: 7.295431605532412, Valid Loss: 8.758047200961618\n",
            "Epoch: 21, Step: 8800, Train Loss: 7.249572734215651, Valid Loss: 8.734629606547946\n",
            "Epoch: 21, Step: 8900, Train Loss: 7.279625315540926, Valid Loss: 8.6676656059195\n",
            "Epoch: 21, Step: 9000, Train Loss: 7.218667819487733, Valid Loss: 8.729631399043495\n",
            "Epoch: 21, Step: 9100, Train Loss: 7.279675807285457, Valid Loss: 8.65390317789397\n",
            "Epoch: 22, Step: 9200, Train Loss: 7.187248377560619, Valid Loss: 8.731349092131202\n",
            "Epoch: 22, Step: 9300, Train Loss: 7.155621017222441, Valid Loss: 8.812618158545954\n",
            "Epoch: 22, Step: 9400, Train Loss: 7.148881175298334, Valid Loss: 8.711971552106947\n",
            "Epoch: 22, Step: 9500, Train Loss: 7.144285032387834, Valid Loss: 8.75968800986037\n",
            "Epoch: 22, Step: 9600, Train Loss: 7.146899044929766, Valid Loss: 8.79845170070938\n",
            "Epoch: 23, Step: 9700, Train Loss: 7.262597397260401, Valid Loss: 8.74095868108065\n",
            "Epoch: 23, Step: 9800, Train Loss: 7.083751954917105, Valid Loss: 8.762813184264692\n",
            "Epoch: 23, Step: 9900, Train Loss: 7.102526575174624, Valid Loss: 8.71564063249845\n",
            "Epoch: 23, Step: 10000, Train Loss: 7.061419713071637, Valid Loss: 8.772184065471027\n",
            "Epoch: 24, Step: 10100, Train Loss: 7.097718886206652, Valid Loss: 8.882794625962388\n",
            "Epoch: 24, Step: 10200, Train Loss: 7.093092541551817, Valid Loss: 8.71958278935799\n",
            "Epoch: 24, Step: 10300, Train Loss: 7.105908565711612, Valid Loss: 8.746904412646973\n",
            "Epoch: 24, Step: 10400, Train Loss: 7.087804264131022, Valid Loss: 8.74311354752972\n",
            "Epoch: 25, Step: 10500, Train Loss: 7.08109392091367, Valid Loss: 9.001858430925386\n",
            "Epoch: 25, Step: 10600, Train Loss: 7.0447475922097205, Valid Loss: 9.004382752877865\n",
            "Epoch: 25, Step: 10700, Train Loss: 7.057412504546962, Valid Loss: 8.8997434648557\n",
            "Epoch: 25, Step: 10800, Train Loss: 6.963048305693808, Valid Loss: 8.739256516171052\n",
            "Epoch: 26, Step: 10900, Train Loss: 6.983521213833482, Valid Loss: 8.883193604870165\n",
            "Epoch: 26, Step: 11000, Train Loss: 6.985040186609649, Valid Loss: 8.748512613513928\n",
            "Epoch: 26, Step: 11100, Train Loss: 6.900634543853234, Valid Loss: 8.802203344381798\n",
            "Epoch: 26, Step: 11200, Train Loss: 7.027909528469474, Valid Loss: 8.756845217693089\n",
            "Epoch: 27, Step: 11300, Train Loss: 6.840301974316839, Valid Loss: 8.874324903758374\n",
            "Epoch: 27, Step: 11400, Train Loss: 6.824664861312401, Valid Loss: 8.862730281964815\n",
            "Epoch: 27, Step: 11500, Train Loss: 6.913259236506049, Valid Loss: 8.923358193994\n",
            "Epoch: 27, Step: 11600, Train Loss: 6.870158909390087, Valid Loss: 8.850932507094821\n",
            "Epoch: 27, Step: 11700, Train Loss: 6.8665437820786295, Valid Loss: 8.834420771358408\n",
            "Epoch: 28, Step: 11800, Train Loss: 6.834310758216813, Valid Loss: 8.787668227003616\n",
            "Epoch: 28, Step: 11900, Train Loss: 6.824816661187203, Valid Loss: 8.824851409903669\n",
            "Epoch: 28, Step: 12000, Train Loss: 6.839918685512113, Valid Loss: 8.828359562890341\n",
            "Epoch: 28, Step: 12100, Train Loss: 6.814492794662176, Valid Loss: 8.825274291805298\n",
            "Epoch: 29, Step: 12200, Train Loss: 6.734622150765707, Valid Loss: 8.96118933331608\n",
            "Epoch: 29, Step: 12300, Train Loss: 6.855786910019605, Valid Loss: 9.082041548918156\n",
            "Epoch: 29, Step: 12400, Train Loss: 6.772395634160067, Valid Loss: 8.889641773806813\n",
            "Epoch: 29, Step: 12500, Train Loss: 6.774605919745887, Valid Loss: 8.988646502699934\n",
            "Epoch: 30, Step: 12600, Train Loss: 6.781375149260106, Valid Loss: 8.854907727006156\n",
            "Epoch: 30, Step: 12700, Train Loss: 6.7422848490408205, Valid Loss: 8.963093099761675\n",
            "Epoch: 30, Step: 12800, Train Loss: 6.707103489138206, Valid Loss: 8.858656015752683\n",
            "Epoch: 30, Step: 12900, Train Loss: 6.6968536370220395, Valid Loss: 8.889177473400085\n",
            "Epoch: 31, Step: 13000, Train Loss: 6.684201360962373, Valid Loss: 9.038540574080887\n",
            "Epoch: 31, Step: 13100, Train Loss: 6.89178821617627, Valid Loss: 8.888332770769852\n",
            "Epoch: 31, Step: 13200, Train Loss: 6.709430681209419, Valid Loss: 8.888931614141448\n",
            "Epoch: 31, Step: 13300, Train Loss: 6.746259747073063, Valid Loss: 9.149999819737591\n",
            "Epoch: 32, Step: 13400, Train Loss: 6.751715542679396, Valid Loss: 8.914414769541981\n",
            "Epoch: 32, Step: 13500, Train Loss: 6.613928347289988, Valid Loss: 9.04113027845863\n",
            "Epoch: 32, Step: 13600, Train Loss: 6.724458892944424, Valid Loss: 9.060493723771362\n",
            "Epoch: 32, Step: 13700, Train Loss: 6.618897759290728, Valid Loss: 9.045928732155106\n",
            "Epoch: 33, Step: 13800, Train Loss: 6.680063461047202, Valid Loss: 8.841068506117574\n",
            "Epoch: 33, Step: 13900, Train Loss: 6.590692636920853, Valid Loss: 9.060534441661288\n",
            "Epoch: 33, Step: 14000, Train Loss: 6.634366468745156, Valid Loss: 9.074020936752458\n",
            "Epoch: 33, Step: 14100, Train Loss: 6.536450259142986, Valid Loss: 9.029468112463261\n",
            "Epoch: 33, Step: 14200, Train Loss: 6.5592577911818015, Valid Loss: 9.01396109675716\n",
            "Epoch: 34, Step: 14300, Train Loss: 6.5491987727058145, Valid Loss: 9.009049685575539\n",
            "Epoch: 34, Step: 14400, Train Loss: 6.5198829200533055, Valid Loss: 9.0172688432193\n",
            "Epoch: 34, Step: 14500, Train Loss: 6.6372882207501105, Valid Loss: 8.999927954245448\n",
            "Epoch: 34, Step: 14600, Train Loss: 6.521093516841221, Valid Loss: 8.970434347369968\n",
            "Epoch: 35, Step: 14700, Train Loss: 6.496838976382038, Valid Loss: 8.94859098807086\n",
            "Epoch: 35, Step: 14800, Train Loss: 6.503156428319881, Valid Loss: 9.107820179030538\n",
            "Epoch: 35, Step: 14900, Train Loss: 6.478399017139411, Valid Loss: 9.073172023712667\n",
            "Epoch: 35, Step: 15000, Train Loss: 6.5241387848101065, Valid Loss: 8.97501710046836\n",
            "Epoch: 36, Step: 15100, Train Loss: 6.434527210821103, Valid Loss: 9.044486931379556\n",
            "Epoch: 36, Step: 15200, Train Loss: 6.551751098105913, Valid Loss: 9.058353638600238\n",
            "Epoch: 36, Step: 15300, Train Loss: 6.684486186449695, Valid Loss: 8.957933923873117\n",
            "Epoch: 36, Step: 15400, Train Loss: 6.451664742992, Valid Loss: 9.068159827937954\n",
            "Epoch: 37, Step: 15500, Train Loss: 6.474625399925493, Valid Loss: 9.143644490431702\n",
            "Epoch: 37, Step: 15600, Train Loss: 6.4299728244891305, Valid Loss: 9.22553945233898\n",
            "Epoch: 37, Step: 15700, Train Loss: 6.3951910733486805, Valid Loss: 9.000716340558885\n",
            "Epoch: 37, Step: 15800, Train Loss: 6.3925765952684, Valid Loss: 9.05343785869299\n",
            "Epoch: 38, Step: 15900, Train Loss: 6.314645281415581, Valid Loss: 9.095669819433786\n",
            "Epoch: 38, Step: 16000, Train Loss: 6.415387682903591, Valid Loss: 9.23745698603909\n",
            "Epoch: 38, Step: 16100, Train Loss: 6.648241498687121, Valid Loss: 8.92841762730406\n",
            "Epoch: 38, Step: 16200, Train Loss: 6.361696073848766, Valid Loss: 8.981807069763102\n",
            "Epoch: 38, Step: 16300, Train Loss: 6.343909434875018, Valid Loss: 9.015495443464498\n",
            "Epoch: 39, Step: 16400, Train Loss: 6.393586338356487, Valid Loss: 9.169357855470304\n",
            "Epoch: 39, Step: 16500, Train Loss: 6.358695981875539, Valid Loss: 9.112044762810767\n",
            "Epoch: 39, Step: 16600, Train Loss: 6.372046216991842, Valid Loss: 8.952422545310046\n",
            "Epoch: 39, Step: 16700, Train Loss: 6.353584930234239, Valid Loss: 9.023167338480812\n",
            "Epoch: 40, Step: 16800, Train Loss: 6.309048734995261, Valid Loss: 9.266854970471687\n",
            "Epoch: 40, Step: 16900, Train Loss: 6.277137029803687, Valid Loss: 9.089337230150452\n",
            "Epoch: 40, Step: 17000, Train Loss: 6.283790008988079, Valid Loss: 9.100395736557067\n",
            "Epoch: 40, Step: 17100, Train Loss: 6.294351181725383, Valid Loss: 9.173599293231106\n",
            "Epoch: 41, Step: 17200, Train Loss: 6.424146302538368, Valid Loss: 9.40656874517213\n",
            "Epoch: 41, Step: 17300, Train Loss: 6.251616079125187, Valid Loss: 9.03215588078424\n",
            "Epoch: 41, Step: 17400, Train Loss: 6.227501270554088, Valid Loss: 9.087865868948798\n",
            "Epoch: 41, Step: 17500, Train Loss: 6.3176340597769185, Valid Loss: 9.061779851647906\n",
            "Epoch: 42, Step: 17600, Train Loss: 6.225871189307846, Valid Loss: 9.109864382917527\n",
            "Epoch: 42, Step: 17700, Train Loss: 6.264415536339117, Valid Loss: 9.328245659202338\n",
            "Epoch: 42, Step: 17800, Train Loss: 6.267323682046991, Valid Loss: 9.304633252779523\n",
            "Epoch: 42, Step: 17900, Train Loss: 6.273646266862853, Valid Loss: 9.048497879470473\n",
            "Epoch: 43, Step: 18000, Train Loss: 6.306134902621115, Valid Loss: 9.407762170731734\n",
            "Epoch: 43, Step: 18100, Train Loss: 6.183121115951623, Valid Loss: 9.242106636759633\n",
            "Epoch: 43, Step: 18200, Train Loss: 6.200920316193371, Valid Loss: 9.233352251098236\n",
            "Epoch: 43, Step: 18300, Train Loss: 6.131950678795196, Valid Loss: 9.230624578864635\n",
            "Epoch: 44, Step: 18400, Train Loss: 6.1865288899955, Valid Loss: 9.31676019736608\n",
            "Epoch: 44, Step: 18500, Train Loss: 6.125654981052795, Valid Loss: 9.144850401282818\n",
            "Epoch: 44, Step: 18600, Train Loss: 6.134266665211996, Valid Loss: 9.16995891491236\n",
            "Epoch: 44, Step: 18700, Train Loss: 6.255282395014079, Valid Loss: 9.406729810157307\n",
            "Epoch: 44, Step: 18800, Train Loss: 6.165290001924628, Valid Loss: 9.188240187503562\n",
            "Epoch: 45, Step: 18900, Train Loss: 6.123563538207043, Valid Loss: 9.175757720784944\n",
            "Epoch: 45, Step: 19000, Train Loss: 6.205504050598625, Valid Loss: 9.118841148598962\n",
            "Epoch: 45, Step: 19100, Train Loss: 6.1582228585887435, Valid Loss: 9.097763198287847\n",
            "Epoch: 45, Step: 19200, Train Loss: 6.134646216561463, Valid Loss: 9.183287089666257\n",
            "Epoch: 46, Step: 19300, Train Loss: 6.0833083623594035, Valid Loss: 9.33619845708411\n",
            "Epoch: 46, Step: 19400, Train Loss: 6.149011138085901, Valid Loss: 9.14296863804186\n",
            "Epoch: 46, Step: 19500, Train Loss: 6.058877074668148, Valid Loss: 9.134883220857136\n",
            "Epoch: 46, Step: 19600, Train Loss: 6.0797277470546796, Valid Loss: 9.153719063635299\n",
            "Epoch: 47, Step: 19700, Train Loss: 6.052412636162203, Valid Loss: 9.264582728478972\n",
            "Epoch: 47, Step: 19800, Train Loss: 6.020396490329823, Valid Loss: 9.314690035240018\n",
            "Epoch: 47, Step: 19900, Train Loss: 6.049728783668989, Valid Loss: 9.28448527953519\n",
            "Epoch: 47, Step: 20000, Train Loss: 6.060890934033543, Valid Loss: 9.224086351385315\n",
            "Epoch: 48, Step: 20100, Train Loss: 6.255157669982425, Valid Loss: 9.168052523615472\n",
            "Epoch: 48, Step: 20200, Train Loss: 5.997053580114011, Valid Loss: 9.295018994570835\n",
            "Epoch: 48, Step: 20300, Train Loss: 6.011761173868283, Valid Loss: 9.23094764424021\n",
            "Epoch: 48, Step: 20400, Train Loss: 6.061532917381741, Valid Loss: 9.115002791128829\n",
            "Epoch: 49, Step: 20500, Train Loss: 6.072574160896802, Valid Loss: 9.373525641904791\n",
            "Epoch: 49, Step: 20600, Train Loss: 5.971232865585204, Valid Loss: 9.318132725762828\n",
            "Epoch: 49, Step: 20700, Train Loss: 5.994905743992904, Valid Loss: 9.231150438393447\n",
            "Epoch: 49, Step: 20800, Train Loss: 5.970919920891392, Valid Loss: 9.218548085585834\n",
            "Epoch: 49, Step: 20900, Train Loss: 5.97713867692072, Valid Loss: 9.156134156455648\n",
            "Epoch: 50, Step: 21000, Train Loss: 5.949242891337116, Valid Loss: 9.17228990406482\n",
            "Epoch: 50, Step: 21100, Train Loss: 5.963434630532175, Valid Loss: 9.164383177167334\n",
            "Epoch: 50, Step: 21200, Train Loss: 6.100051862814729, Valid Loss: 9.496690242254177\n",
            "Epoch: 50, Step: 21300, Train Loss: 5.991297397291863, Valid Loss: 9.276960048539832\n",
            "Epoch: 51, Step: 21400, Train Loss: 5.8930818183589615, Valid Loss: 9.34574811632419\n",
            "Epoch: 51, Step: 21500, Train Loss: 5.889516524794403, Valid Loss: 9.242993304591641\n",
            "Epoch: 51, Step: 21600, Train Loss: 5.892012860506125, Valid Loss: 9.257353943528091\n",
            "Epoch: 51, Step: 21700, Train Loss: 5.926373651597567, Valid Loss: 9.34044736303909\n",
            "Epoch: 52, Step: 21800, Train Loss: 5.907137561414378, Valid Loss: 9.235208346234648\n",
            "Epoch: 52, Step: 21900, Train Loss: 5.905217765628035, Valid Loss: 9.27195656851333\n",
            "Epoch: 52, Step: 22000, Train Loss: 5.916177627584682, Valid Loss: 9.43297299325576\n",
            "Epoch: 52, Step: 22100, Train Loss: 5.884304660690949, Valid Loss: 9.232673421795962\n",
            "Epoch: 53, Step: 22200, Train Loss: 5.873862350006792, Valid Loss: 9.346203523154468\n",
            "Epoch: 53, Step: 22300, Train Loss: 5.831664584673567, Valid Loss: 9.407449274114523\n",
            "Epoch: 53, Step: 22400, Train Loss: 5.864508390626404, Valid Loss: 9.200267662827034\n",
            "Epoch: 53, Step: 22500, Train Loss: 5.911740240380943, Valid Loss: 9.535776672993846\n",
            "Epoch: 54, Step: 22600, Train Loss: 5.853741344751903, Valid Loss: 9.303540359708533\n",
            "Epoch: 54, Step: 22700, Train Loss: 5.846959856555326, Valid Loss: 9.323539354267737\n",
            "Epoch: 54, Step: 22800, Train Loss: 5.842643950772571, Valid Loss: 9.176693227956028\n",
            "Epoch: 54, Step: 22900, Train Loss: 5.912169220870034, Valid Loss: 9.15310258287702\n",
            "Epoch: 55, Step: 23000, Train Loss: 5.887892139735752, Valid Loss: 9.236221435345492\n",
            "Epoch: 55, Step: 23100, Train Loss: 5.776109250455121, Valid Loss: 9.310633147413082\n",
            "Epoch: 55, Step: 23200, Train Loss: 5.938971728087383, Valid Loss: 9.164690419438008\n",
            "Epoch: 55, Step: 23300, Train Loss: 5.769032754694257, Valid Loss: 9.345776973401094\n",
            "Epoch: 55, Step: 23400, Train Loss: 5.8135990800207376, Valid Loss: 9.23931534738989\n",
            "Epoch: 56, Step: 23500, Train Loss: 5.845254480923952, Valid Loss: 9.570879945965881\n",
            "Epoch: 56, Step: 23600, Train Loss: 5.741639935399761, Valid Loss: 9.377317411241991\n",
            "Epoch: 56, Step: 23700, Train Loss: 5.9643605575252465, Valid Loss: 9.81662455550724\n",
            "Epoch: 56, Step: 23800, Train Loss: 5.836448478673666, Valid Loss: 9.400290874090024\n",
            "Epoch: 57, Step: 23900, Train Loss: 5.75540792522918, Valid Loss: 9.522615665264077\n",
            "Epoch: 57, Step: 24000, Train Loss: 5.749042022329756, Valid Loss: 9.500423194370015\n",
            "Epoch: 57, Step: 24100, Train Loss: 5.733079875756282, Valid Loss: 9.249798853123284\n",
            "Epoch: 57, Step: 24200, Train Loss: 5.832793547111681, Valid Loss: 9.175420529070257\n",
            "Epoch: 58, Step: 24300, Train Loss: 5.810571326242547, Valid Loss: 9.47829004360763\n",
            "Epoch: 58, Step: 24400, Train Loss: 5.737929440167323, Valid Loss: 9.40851674303083\n",
            "Epoch: 58, Step: 24500, Train Loss: 5.8498566331485025, Valid Loss: 9.632996243886074\n",
            "Epoch: 58, Step: 24600, Train Loss: 5.752144707837474, Valid Loss: 9.332735568780018\n",
            "Epoch: 59, Step: 24700, Train Loss: 5.7469983788103205, Valid Loss: 9.400546425233067\n",
            "Epoch: 59, Step: 24800, Train Loss: 5.7178709809394554, Valid Loss: 9.577428135030337\n",
            "Epoch: 59, Step: 24900, Train Loss: 5.71227996259203, Valid Loss: 9.301460924815952\n",
            "Epoch: 59, Step: 25000, Train Loss: 5.699561432376495, Valid Loss: 9.34945619908429\n",
            "Epoch: 60, Step: 25100, Train Loss: 5.653365606510909, Valid Loss: 9.431386646016483\n",
            "Epoch: 60, Step: 25200, Train Loss: 5.638998930253566, Valid Loss: 9.443110765330793\n",
            "Epoch: 60, Step: 25300, Train Loss: 5.666706082945982, Valid Loss: 9.41132264423527\n",
            "Epoch: 60, Step: 25400, Train Loss: 5.667826445013753, Valid Loss: 9.483687143744689\n",
            "Epoch: 61, Step: 25500, Train Loss: 5.692851888755915, Valid Loss: 9.46231303343889\n",
            "Epoch: 61, Step: 25600, Train Loss: 5.659596186929136, Valid Loss: 9.288707612006693\n",
            "Epoch: 61, Step: 25700, Train Loss: 5.669919123512797, Valid Loss: 9.338002125524637\n",
            "Epoch: 61, Step: 25800, Train Loss: 5.65116520821763, Valid Loss: 9.360012856486314\n",
            "Epoch: 61, Step: 25900, Train Loss: 5.657757999112507, Valid Loss: 9.325418139646548\n",
            "Epoch: 62, Step: 26000, Train Loss: 5.643868405561543, Valid Loss: 9.482370044999604\n",
            "Epoch: 62, Step: 26100, Train Loss: 5.655004545901125, Valid Loss: 9.630679647926458\n",
            "Epoch: 62, Step: 26200, Train Loss: 5.625943700588123, Valid Loss: 9.534872297722996\n",
            "Epoch: 62, Step: 26300, Train Loss: 5.690199990065104, Valid Loss: 9.263371438738174\n",
            "Epoch: 63, Step: 26400, Train Loss: 5.547139525781806, Valid Loss: 9.42942668475891\n",
            "Epoch: 63, Step: 26500, Train Loss: 5.741958785399399, Valid Loss: 9.766551913861344\n",
            "Epoch: 63, Step: 26600, Train Loss: 5.60979650152168, Valid Loss: 9.36894932714748\n",
            "Epoch: 63, Step: 26700, Train Loss: 5.621390177532857, Valid Loss: 9.570756687543987\n",
            "Epoch: 64, Step: 26800, Train Loss: 5.924371002300875, Valid Loss: 9.24708790232585\n",
            "Epoch: 64, Step: 26900, Train Loss: 5.555913494923575, Valid Loss: 9.509337714611931\n",
            "Epoch: 64, Step: 27000, Train Loss: 5.523112375726872, Valid Loss: 9.44863537897918\n",
            "Epoch: 64, Step: 27100, Train Loss: 5.570665470269602, Valid Loss: 9.469688926069109\n",
            "Epoch: 65, Step: 27200, Train Loss: 5.512808018675568, Valid Loss: 9.58468466278051\n",
            "Epoch: 65, Step: 27300, Train Loss: 5.514392496294248, Valid Loss: 9.569829672439909\n",
            "Epoch: 65, Step: 27400, Train Loss: 5.5396310588646465, Valid Loss: 9.32568835713689\n",
            "Epoch: 65, Step: 27500, Train Loss: 5.5774421358455735, Valid Loss: 9.331471730446161\n",
            "Epoch: 66, Step: 27600, Train Loss: 5.510033274659258, Valid Loss: 9.524611541795515\n",
            "Epoch: 66, Step: 27700, Train Loss: 5.506485281186633, Valid Loss: 9.44698462820902\n",
            "Epoch: 66, Step: 27800, Train Loss: 5.5004816600221265, Valid Loss: 9.572398607839508\n",
            "Epoch: 66, Step: 27900, Train Loss: 5.516782525775368, Valid Loss: 9.454588390086707\n",
            "Epoch: 66, Step: 28000, Train Loss: 5.567133858515268, Valid Loss: 9.324921020725258\n",
            "Epoch: 67, Step: 28100, Train Loss: 5.443426072356943, Valid Loss: 9.421804829290359\n",
            "Epoch: 67, Step: 28200, Train Loss: 5.500184686724204, Valid Loss: 9.500623273313998\n",
            "Epoch: 67, Step: 28300, Train Loss: 5.563700914576164, Valid Loss: 9.57793966372874\n",
            "Epoch: 67, Step: 28400, Train Loss: 5.5044554133729635, Valid Loss: 9.541548269204135\n",
            "Epoch: 68, Step: 28500, Train Loss: 5.504030359904268, Valid Loss: 9.594886110915427\n",
            "Epoch: 68, Step: 28600, Train Loss: 5.453688763311222, Valid Loss: 9.43296817090437\n",
            "Epoch: 68, Step: 28700, Train Loss: 5.4991801678953385, Valid Loss: 9.604841502811091\n",
            "Epoch: 68, Step: 28800, Train Loss: 5.4879906418946955, Valid Loss: 9.378076615927782\n",
            "Epoch: 69, Step: 28900, Train Loss: 5.441085575535425, Valid Loss: 9.530737911011602\n",
            "Epoch: 69, Step: 29000, Train Loss: 5.452368561928997, Valid Loss: 9.492913267569113\n",
            "Epoch: 69, Step: 29100, Train Loss: 5.455845647025395, Valid Loss: 9.407264285609653\n",
            "Epoch: 69, Step: 29200, Train Loss: 5.45641125136801, Valid Loss: 9.615365115704495\n",
            "Epoch: 70, Step: 29300, Train Loss: 5.403021818341588, Valid Loss: 9.439495837672856\n",
            "Epoch: 70, Step: 29400, Train Loss: 5.391785327384716, Valid Loss: 9.443336536396677\n",
            "Epoch: 70, Step: 29500, Train Loss: 5.43617379281083, Valid Loss: 9.491364243551304\n",
            "Epoch: 70, Step: 29600, Train Loss: 5.531212480779044, Valid Loss: 9.698554081237372\n",
            "Epoch: 71, Step: 29700, Train Loss: 5.536382599624308, Valid Loss: 9.768057065714977\n",
            "Epoch: 71, Step: 29800, Train Loss: 5.388392833533468, Valid Loss: 9.48775862245658\n",
            "Epoch: 71, Step: 29900, Train Loss: 5.388921506557112, Valid Loss: 9.574331259005474\n",
            "Epoch: 71, Step: 30000, Train Loss: 5.41363644744498, Valid Loss: 9.540370126755874\n",
            "Epoch: 72, Step: 30100, Train Loss: 5.410319466180901, Valid Loss: 9.420538302128078\n",
            "Epoch: 72, Step: 30200, Train Loss: 5.473642579223764, Valid Loss: 9.322333578896401\n",
            "Epoch: 72, Step: 30300, Train Loss: 5.385368754735347, Valid Loss: 9.493258783951362\n",
            "Epoch: 72, Step: 30400, Train Loss: 5.378928451553892, Valid Loss: 9.653383981728316\n",
            "Epoch: 72, Step: 30500, Train Loss: 5.419976272945917, Valid Loss: 9.694355108939662\n",
            "Epoch: 73, Step: 30600, Train Loss: 5.346706274337515, Valid Loss: 9.563252272671258\n",
            "Epoch: 73, Step: 30700, Train Loss: 5.49152608303193, Valid Loss: 9.437005844014646\n",
            "Epoch: 73, Step: 30800, Train Loss: 5.368991069723023, Valid Loss: 9.505203410169909\n",
            "Epoch: 73, Step: 30900, Train Loss: 5.448502150290302, Valid Loss: 9.746068002943625\n",
            "Epoch: 74, Step: 31000, Train Loss: 5.340739038829036, Valid Loss: 9.54176017672818\n",
            "Epoch: 74, Step: 31100, Train Loss: 5.400444275547672, Valid Loss: 9.718993013953337\n",
            "Epoch: 74, Step: 31200, Train Loss: 5.389039190126756, Valid Loss: 9.614770592684767\n",
            "Epoch: 74, Step: 31300, Train Loss: 5.36458726940913, Valid Loss: 9.580160471597598\n",
            "Epoch: 75, Step: 31400, Train Loss: 5.448546705912637, Valid Loss: 9.386797539475813\n",
            "Epoch: 75, Step: 31500, Train Loss: 5.347696595729763, Valid Loss: 9.59762752949062\n",
            "Epoch: 75, Step: 31600, Train Loss: 5.341074452378562, Valid Loss: 9.484021174032618\n",
            "Epoch: 75, Step: 31700, Train Loss: 5.364521251490858, Valid Loss: 9.553532881684118\n",
            "Epoch: 76, Step: 31800, Train Loss: 5.29662958771683, Valid Loss: 9.527729043589988\n",
            "Epoch: 76, Step: 31900, Train Loss: 5.268543177731515, Valid Loss: 9.654458531667675\n",
            "Epoch: 76, Step: 32000, Train Loss: 5.351774973662697, Valid Loss: 9.82458655262395\n",
            "Epoch: 76, Step: 32100, Train Loss: 5.320342229122183, Valid Loss: 9.626775727112975\n",
            "Epoch: 77, Step: 32200, Train Loss: 5.384743255169868, Valid Loss: 9.817474883577422\n",
            "Epoch: 77, Step: 32300, Train Loss: 5.2454776273926695, Valid Loss: 9.610138635607633\n",
            "Epoch: 77, Step: 32400, Train Loss: 5.275377576757814, Valid Loss: 9.481650837613895\n",
            "Epoch: 77, Step: 32500, Train Loss: 5.353085827467009, Valid Loss: 9.849387545973826\n",
            "Epoch: 77, Step: 32600, Train Loss: 5.294265360500624, Valid Loss: 9.533768033379799\n",
            "Epoch: 78, Step: 32700, Train Loss: 5.243406795369084, Valid Loss: 9.715851983296531\n",
            "Epoch: 78, Step: 32800, Train Loss: 5.277314988903518, Valid Loss: 9.544896100726646\n",
            "Epoch: 78, Step: 32900, Train Loss: 5.2785044673346295, Valid Loss: 9.616332338433034\n",
            "Epoch: 78, Step: 33000, Train Loss: 5.305179940879708, Valid Loss: 9.458743673361493\n",
            "Epoch: 79, Step: 33100, Train Loss: 5.220723652476756, Valid Loss: 9.52397742003597\n",
            "Epoch: 79, Step: 33200, Train Loss: 5.256134938036568, Valid Loss: 9.645965202421898\n",
            "Epoch: 79, Step: 33300, Train Loss: 5.216767776227668, Valid Loss: 9.633527037062011\n",
            "Epoch: 79, Step: 33400, Train Loss: 5.417754552861997, Valid Loss: 9.872521750877267\n",
            "Epoch: 80, Step: 33500, Train Loss: 5.186773471952326, Valid Loss: 9.670404745519575\n",
            "Epoch: 80, Step: 33600, Train Loss: 5.249671220357945, Valid Loss: 9.442547250777524\n",
            "Epoch: 80, Step: 33700, Train Loss: 5.280369215337963, Valid Loss: 9.526993374451436\n",
            "Epoch: 80, Step: 33800, Train Loss: 5.220620465393906, Valid Loss: 9.50764732388064\n",
            "Epoch: 81, Step: 33900, Train Loss: 5.2028437855664365, Valid Loss: 9.4956133858888\n",
            "Epoch: 81, Step: 34000, Train Loss: 5.213534909988595, Valid Loss: 9.497693453718588\n",
            "Epoch: 81, Step: 34100, Train Loss: 5.187033359461113, Valid Loss: 9.63828115433809\n",
            "Epoch: 81, Step: 34200, Train Loss: 5.236868026190191, Valid Loss: 9.657011924067609\n",
            "Epoch: 82, Step: 34300, Train Loss: 5.206416295068791, Valid Loss: 9.623122510878613\n",
            "Epoch: 82, Step: 34400, Train Loss: 5.226607235830064, Valid Loss: 9.489931615834998\n",
            "Epoch: 82, Step: 34500, Train Loss: 5.187322597019612, Valid Loss: 9.541404087291776\n",
            "Epoch: 82, Step: 34600, Train Loss: 5.339601243384922, Valid Loss: 9.398954349633826\n",
            "Epoch: 83, Step: 34700, Train Loss: 5.207639395407391, Valid Loss: 9.542352005256884\n",
            "Epoch: 83, Step: 34800, Train Loss: 5.118488174100203, Valid Loss: 9.61296905344142\n",
            "Epoch: 83, Step: 34900, Train Loss: 5.138775771814993, Valid Loss: 9.573784904980805\n",
            "Epoch: 83, Step: 35000, Train Loss: 5.195814248060214, Valid Loss: 9.607566516648875\n",
            "Epoch: 83, Step: 35100, Train Loss: 5.269370526681857, Valid Loss: 9.385722340878372\n",
            "Epoch: 84, Step: 35200, Train Loss: 5.135555754247262, Valid Loss: 9.691117361243096\n",
            "Epoch: 84, Step: 35300, Train Loss: 5.157087045338239, Valid Loss: 9.597659943164311\n",
            "Epoch: 84, Step: 35400, Train Loss: 5.163384571463641, Valid Loss: 9.690825022718016\n",
            "Epoch: 84, Step: 35500, Train Loss: 5.265215202346212, Valid Loss: 9.857228395619288\n",
            "Epoch: 85, Step: 35600, Train Loss: 5.2475441452498, Valid Loss: 9.42459262069734\n",
            "Epoch: 85, Step: 35700, Train Loss: 5.278186760864832, Valid Loss: 9.961899026211743\n",
            "Epoch: 85, Step: 35800, Train Loss: 5.246909450575344, Valid Loss: 9.428032554701442\n",
            "Epoch: 85, Step: 35900, Train Loss: 5.280985614133497, Valid Loss: 9.545310112097113\n",
            "Epoch: 86, Step: 36000, Train Loss: 5.145964163827482, Valid Loss: 9.570922272774174\n",
            "Epoch: 86, Step: 36100, Train Loss: 5.13867638072938, Valid Loss: 9.710440894856204\n",
            "Epoch: 86, Step: 36200, Train Loss: 5.175491432773138, Valid Loss: 9.729705077611609\n",
            "Epoch: 86, Step: 36300, Train Loss: 5.257884419593343, Valid Loss: 9.47420488422682\n",
            "Epoch: 87, Step: 36400, Train Loss: 5.159448642228973, Valid Loss: 9.850107436472426\n",
            "Epoch: 87, Step: 36500, Train Loss: 5.128069673816639, Valid Loss: 9.707525896091393\n",
            "Epoch: 87, Step: 36600, Train Loss: 5.120954464456679, Valid Loss: 9.621927045750919\n",
            "Epoch: 87, Step: 36700, Train Loss: 5.1920162912644905, Valid Loss: 9.780857190434087\n",
            "Epoch: 88, Step: 36800, Train Loss: 5.11864500582708, Valid Loss: 9.534618999493711\n",
            "Epoch: 88, Step: 36900, Train Loss: 5.128475494874523, Valid Loss: 9.641176281041101\n",
            "Epoch: 88, Step: 37000, Train Loss: 5.094075259016939, Valid Loss: 9.745718004168246\n",
            "Epoch: 88, Step: 37100, Train Loss: 5.150332945686862, Valid Loss: 9.509814421827759\n",
            "Epoch: 88, Step: 37200, Train Loss: 5.307982233857194, Valid Loss: 9.364729953897616\n",
            "Epoch: 89, Step: 37300, Train Loss: 5.100723249008952, Valid Loss: 9.793450939998447\n",
            "Epoch: 89, Step: 37400, Train Loss: 5.1107073253180095, Valid Loss: 9.610928606971157\n",
            "Epoch: 89, Step: 37500, Train Loss: 5.094659368454723, Valid Loss: 9.754346808254018\n",
            "Epoch: 89, Step: 37600, Train Loss: 5.154345817582094, Valid Loss: 9.49307544540458\n",
            "Epoch: 90, Step: 37700, Train Loss: 5.059456365569697, Valid Loss: 9.78426494854288\n",
            "Epoch: 90, Step: 37800, Train Loss: 5.035821437694293, Valid Loss: 9.728475366542535\n",
            "Epoch: 90, Step: 37900, Train Loss: 5.229910694421354, Valid Loss: 9.89519683435896\n",
            "Epoch: 90, Step: 38000, Train Loss: 5.076612888668726, Valid Loss: 9.708944784169633\n",
            "Epoch: 91, Step: 38100, Train Loss: 5.094811210835934, Valid Loss: 9.787123837838545\n",
            "Epoch: 91, Step: 38200, Train Loss: 5.041068143555613, Valid Loss: 9.657646294973805\n",
            "Epoch: 91, Step: 38300, Train Loss: 5.083651674373433, Valid Loss: 9.680795188778506\n",
            "Epoch: 91, Step: 38400, Train Loss: 5.050624966134373, Valid Loss: 9.65174137631158\n",
            "Epoch: 92, Step: 38500, Train Loss: 5.141020216710776, Valid Loss: 9.935578141858795\n",
            "Epoch: 92, Step: 38600, Train Loss: 5.045770914703379, Valid Loss: 9.772607911609375\n",
            "Epoch: 92, Step: 38700, Train Loss: 5.06681912805026, Valid Loss: 9.549494434693335\n",
            "Epoch: 92, Step: 38800, Train Loss: 5.160329071839591, Valid Loss: 9.431609192090432\n",
            "Epoch: 93, Step: 38900, Train Loss: 5.079459182602056, Valid Loss: 9.763323336995938\n",
            "Epoch: 93, Step: 39000, Train Loss: 5.031654249954139, Valid Loss: 9.717130716466063\n",
            "Epoch: 93, Step: 39100, Train Loss: 5.05337994388028, Valid Loss: 9.703434531735866\n",
            "Epoch: 93, Step: 39200, Train Loss: 5.301156436819056, Valid Loss: 9.432199054678827\n",
            "Epoch: 94, Step: 39300, Train Loss: 5.04767514287715, Valid Loss: 9.764378739944904\n",
            "Epoch: 94, Step: 39400, Train Loss: 5.292682372130229, Valid Loss: 10.083138010970174\n",
            "Epoch: 94, Step: 39500, Train Loss: 5.0332133048906185, Valid Loss: 9.632452250987663\n",
            "Epoch: 94, Step: 39600, Train Loss: 5.257306458704503, Valid Loss: 9.553687858270896\n",
            "Epoch: 94, Step: 39700, Train Loss: 5.192999323597407, Valid Loss: 9.546518524751507\n",
            "Epoch: 95, Step: 39800, Train Loss: 5.010985920202536, Valid Loss: 9.593651034220626\n",
            "Epoch: 95, Step: 39900, Train Loss: 5.009160221101661, Valid Loss: 9.621320977014319\n",
            "Epoch: 95, Step: 40000, Train Loss: 4.986840186293103, Valid Loss: 9.683297491383305\n",
            "Epoch: 95, Step: 40100, Train Loss: 5.084258701530164, Valid Loss: 9.888170997179648\n",
            "Epoch: 96, Step: 40200, Train Loss: 5.186846655129501, Valid Loss: 10.017759789817891\n",
            "Epoch: 96, Step: 40300, Train Loss: 5.032543705780603, Valid Loss: 9.593951742498431\n",
            "Epoch: 96, Step: 40400, Train Loss: 5.02468910020559, Valid Loss: 9.64941915385688\n",
            "Epoch: 96, Step: 40500, Train Loss: 5.0342467705385, Valid Loss: 9.778234748588323\n",
            "Epoch: 97, Step: 40600, Train Loss: 4.941783292079483, Valid Loss: 9.664167404587252\n",
            "Epoch: 97, Step: 40700, Train Loss: 4.990387393902598, Valid Loss: 9.81520938977767\n",
            "Epoch: 97, Step: 40800, Train Loss: 4.984103118712331, Valid Loss: 9.65845097990152\n",
            "Epoch: 97, Step: 40900, Train Loss: 5.006887211974765, Valid Loss: 9.58515357999319\n",
            "Epoch: 98, Step: 41000, Train Loss: 4.975809437203012, Valid Loss: 9.792413092461214\n",
            "Epoch: 98, Step: 41100, Train Loss: 4.965300719288781, Valid Loss: 9.653615463566274\n",
            "Epoch: 98, Step: 41200, Train Loss: 5.083740825482027, Valid Loss: 9.525247960516442\n",
            "Epoch: 98, Step: 41300, Train Loss: 4.9834034110556535, Valid Loss: 9.676712753754858\n",
            "Epoch: 99, Step: 41400, Train Loss: 4.94316195694865, Valid Loss: 9.680869043909304\n",
            "Epoch: 99, Step: 41500, Train Loss: 4.946667095558138, Valid Loss: 9.69407356079634\n",
            "Epoch: 99, Step: 41600, Train Loss: 4.957125332453704, Valid Loss: 9.719770818843557\n",
            "Epoch: 99, Step: 41700, Train Loss: 5.00311320105899, Valid Loss: 9.73367487944653\n",
            "Epoch: 99, Step: 41800, Train Loss: 5.0824423244438695, Valid Loss: 9.995215061535182\n"
          ]
        }
      ],
      "source": [
        "mlp_180 = MyMLP(\n",
        "    X_subtrain=X_subtrain,\n",
        "    Y_subtrain=Y_subtrain,\n",
        "    X_valid=X_valid,\n",
        "    Y_valid=Y_valid,\n",
        "    H=180,\n",
        "    lr=0.00001,\n",
        "    wd=0,\n",
        "    mom=0,\n",
        "    loss_type=0,\n",
        "    optimizer_type=0,\n",
        "    use_dropout=False\n",
        ")\n",
        "train_loss_list, valid_loss_list = mlp_180.fit(\n",
        "    max_epoch=100, \n",
        "    verbose=True, \n",
        "    patience_batch_num=5000, \n",
        "    model_path='q3_mlp_180.ckpt'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yhu3e9iFPs3r",
        "outputId": "e6646846-2189-426d-db7d-df4f212acac3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test RMSE Loss for 90 hidden units: 8.842014063568538\n"
          ]
        }
      ],
      "source": [
        "q3_mlp_180_best_model = torch.load('q3_mlp_180.ckpt')\n",
        "test_rmse_loss = calculate_test_rmse(q3_mlp_180_best_model, X_test, Y_test)\n",
        "print(f\"Test RMSE Loss for 90 hidden units: {test_rmse_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vl_ci5wPs3s"
      },
      "source": [
        "##### Discussion on Test RMSE with different H (H = 45, 90, 180)\n",
        "由上可知，不同的 H 其所對應的 Test RMSE 如下：\n",
        "* H = 45: 8.934501721928912\n",
        "* H = 90: 8.965207331652502\n",
        "* H = 180: 8.918703611390761\n",
        "\n",
        "從這邊並沒有看出不同 H 是否會有特別的走向，只能看出 180 個 Hidden Nodes 的 Test RMSE 最小，但是差距不大，90 個 Hidden Nodes 的 Test RMSE 最大，但三個的差距都不大。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2Na27kNPs3s"
      },
      "source": [
        "#### Q4 (15%)\n",
        "使用Q2的模型設定，考慮 H = 45, 90, 180與Weight Decay = 0.1, 0.2, 0.4的所有組合。模型估計後做表整理Test RMSE。討論H的選擇應為多少較合理?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaHp8X4IPs3s",
        "outputId": "45ddf151-be4b-4cf2-eb8d-81a20fe338e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training for h = 180, wd = 0.1\n",
            "Epoch: 0, Step: 100, Train Loss: 9.348935476088824, Valid Loss: 9.293062157754479\n",
            "Epoch: 0, Step: 200, Train Loss: 8.988066299717104, Valid Loss: 8.934659737656702\n",
            "Epoch: 0, Step: 300, Train Loss: 8.83316720554058, Valid Loss: 8.78249498681031\n",
            "Epoch: 0, Step: 400, Train Loss: 8.887869213348829, Valid Loss: 8.859206122376856\n",
            "Epoch: 1, Step: 500, Train Loss: 8.710556631069677, Valid Loss: 8.691041064645455\n",
            "Epoch: 1, Step: 600, Train Loss: 8.69586306920707, Valid Loss: 8.679562289431209\n",
            "Epoch: 1, Step: 700, Train Loss: 8.731737543157232, Valid Loss: 8.731556784554412\n",
            "Epoch: 1, Step: 800, Train Loss: 8.624356195327817, Valid Loss: 8.635974940478043\n",
            "Epoch: 2, Step: 900, Train Loss: 8.812896685566935, Valid Loss: 8.828836951279644\n",
            "Epoch: 2, Step: 1000, Train Loss: 8.586742723000338, Valid Loss: 8.628074139209208\n",
            "Epoch: 2, Step: 1100, Train Loss: 8.575278521407595, Valid Loss: 8.60603532611801\n",
            "Epoch: 2, Step: 1200, Train Loss: 8.60976828271677, Valid Loss: 8.676465552349889\n",
            "Epoch: 3, Step: 1300, Train Loss: 8.558708458651948, Valid Loss: 8.635250838276809\n",
            "Epoch: 3, Step: 1400, Train Loss: 8.486030500909006, Valid Loss: 8.551945774850395\n",
            "Epoch: 3, Step: 1500, Train Loss: 8.627510934201325, Valid Loss: 8.725017661898328\n",
            "Epoch: 3, Step: 1600, Train Loss: 8.539537539791876, Valid Loss: 8.64121528246757\n",
            "Epoch: 4, Step: 1700, Train Loss: 8.427745075524264, Valid Loss: 8.550194257329455\n",
            "Epoch: 4, Step: 1800, Train Loss: 8.552813924019754, Valid Loss: 8.697125915377754\n",
            "Epoch: 4, Step: 1900, Train Loss: 8.500063449606253, Valid Loss: 8.665627828111448\n",
            "Epoch: 4, Step: 2000, Train Loss: 8.374490576685913, Valid Loss: 8.537343401410022\n",
            "Epoch: 5, Step: 2100, Train Loss: 8.412641426666113, Valid Loss: 8.605324352637693\n",
            "Epoch: 5, Step: 2200, Train Loss: 8.388855640808337, Valid Loss: 8.578710325578148\n",
            "Epoch: 5, Step: 2300, Train Loss: 8.348222429572491, Valid Loss: 8.55388706466567\n",
            "Epoch: 5, Step: 2400, Train Loss: 8.341820223911302, Valid Loss: 8.559818721534011\n",
            "Epoch: 5, Step: 2500, Train Loss: 8.260745001547182, Valid Loss: 8.50291918036119\n",
            "Epoch: 6, Step: 2600, Train Loss: 8.2699350125378, Valid Loss: 8.532618501765024\n",
            "Epoch: 6, Step: 2700, Train Loss: 8.281118495181229, Valid Loss: 8.563625200198512\n",
            "Epoch: 6, Step: 2800, Train Loss: 8.30337050911655, Valid Loss: 8.598557402640205\n",
            "Epoch: 6, Step: 2900, Train Loss: 8.27196427397813, Valid Loss: 8.534954637284354\n",
            "Epoch: 7, Step: 3000, Train Loss: 8.178793783864373, Valid Loss: 8.510384249100468\n",
            "Epoch: 7, Step: 3100, Train Loss: 8.168210910997137, Valid Loss: 8.504790579711136\n",
            "Epoch: 7, Step: 3200, Train Loss: 8.172632020404135, Valid Loss: 8.51927671343894\n",
            "Epoch: 7, Step: 3300, Train Loss: 8.154209509068245, Valid Loss: 8.533738960128229\n",
            "Epoch: 8, Step: 3400, Train Loss: 8.130500436226662, Valid Loss: 8.554920419247516\n",
            "Epoch: 8, Step: 3500, Train Loss: 8.102481875680787, Valid Loss: 8.500603031258557\n",
            "Epoch: 8, Step: 3600, Train Loss: 8.096066255909815, Valid Loss: 8.53022508897335\n",
            "Epoch: 8, Step: 3700, Train Loss: 8.084907089564494, Valid Loss: 8.523925265710014\n",
            "Epoch: 9, Step: 3800, Train Loss: 8.10164467477457, Valid Loss: 8.600876169248982\n",
            "Epoch: 9, Step: 3900, Train Loss: 8.027752267473023, Valid Loss: 8.52455163792848\n",
            "Epoch: 9, Step: 4000, Train Loss: 8.004939659490258, Valid Loss: 8.524236692346765\n",
            "Epoch: 9, Step: 4100, Train Loss: 7.961375105727399, Valid Loss: 8.478463336869691\n",
            "Epoch: 10, Step: 4200, Train Loss: 7.973880852442864, Valid Loss: 8.523087409512028\n",
            "Epoch: 10, Step: 4300, Train Loss: 7.976508398830672, Valid Loss: 8.525482663075767\n",
            "Epoch: 10, Step: 4400, Train Loss: 7.977581050606098, Valid Loss: 8.59597890224753\n",
            "Epoch: 10, Step: 4500, Train Loss: 7.93040951032232, Valid Loss: 8.514924882705936\n",
            "Epoch: 11, Step: 4600, Train Loss: 7.96044970105878, Valid Loss: 8.543475263117429\n",
            "Epoch: 11, Step: 4700, Train Loss: 7.934583840961197, Valid Loss: 8.546477632750793\n",
            "Epoch: 11, Step: 4800, Train Loss: 7.899718336403144, Valid Loss: 8.62544279802942\n",
            "Epoch: 11, Step: 4900, Train Loss: 7.989131432847315, Valid Loss: 8.727225933340023\n",
            "Epoch: 11, Step: 5000, Train Loss: 7.865758201860598, Valid Loss: 8.53175662033518\n",
            "Epoch: 12, Step: 5100, Train Loss: 7.851910982477237, Valid Loss: 8.55764556301973\n",
            "Epoch: 12, Step: 5200, Train Loss: 7.795682373324188, Valid Loss: 8.515620347219755\n",
            "Epoch: 12, Step: 5300, Train Loss: 7.7644817954369625, Valid Loss: 8.545907341362382\n",
            "Epoch: 12, Step: 5400, Train Loss: 7.7440781990774985, Valid Loss: 8.522025789273481\n",
            "Epoch: 13, Step: 5500, Train Loss: 7.975715174352647, Valid Loss: 8.63576584671656\n",
            "Epoch: 13, Step: 5600, Train Loss: 7.754963652773944, Valid Loss: 8.52157197046561\n",
            "Epoch: 13, Step: 5700, Train Loss: 7.748353303054824, Valid Loss: 8.542980496258057\n",
            "Epoch: 13, Step: 5800, Train Loss: 7.709953279890569, Valid Loss: 8.571902838723094\n",
            "Epoch: 14, Step: 5900, Train Loss: 7.684357933178279, Valid Loss: 8.608984419359196\n",
            "Epoch: 14, Step: 6000, Train Loss: 7.662063936328928, Valid Loss: 8.520559473270877\n",
            "Epoch: 14, Step: 6100, Train Loss: 7.639593106731772, Valid Loss: 8.582503500740527\n",
            "Epoch: 14, Step: 6200, Train Loss: 7.664374680951486, Valid Loss: 8.592817481081925\n",
            "Epoch: 15, Step: 6300, Train Loss: 7.66851104806108, Valid Loss: 8.5825847089478\n",
            "Epoch: 15, Step: 6400, Train Loss: 7.591136050156794, Valid Loss: 8.568279690444133\n",
            "Epoch: 15, Step: 6500, Train Loss: 7.743653240681612, Valid Loss: 8.646988738511862\n",
            "Epoch: 15, Step: 6600, Train Loss: 7.561397586394871, Valid Loss: 8.543730577837241\n",
            "Epoch: 16, Step: 6700, Train Loss: 7.558183025256552, Valid Loss: 8.582870090994586\n",
            "Epoch: 16, Step: 6800, Train Loss: 7.562727420784654, Valid Loss: 8.614983175453022\n",
            "Epoch: 16, Step: 6900, Train Loss: 7.5748309651217625, Valid Loss: 8.673888326293088\n",
            "Epoch: 16, Step: 7000, Train Loss: 7.505421339178773, Valid Loss: 8.570625431534523\n",
            "Epoch: 16, Step: 7100, Train Loss: 7.5068911872670325, Valid Loss: 8.557739082776196\n",
            "Epoch: 17, Step: 7200, Train Loss: 7.637374850558294, Valid Loss: 8.63688280647265\n",
            "Epoch: 17, Step: 7300, Train Loss: 7.530934699302945, Valid Loss: 8.692977991794992\n",
            "Epoch: 17, Step: 7400, Train Loss: 7.433162434356927, Valid Loss: 8.625577867999572\n",
            "Epoch: 17, Step: 7500, Train Loss: 7.500310440787113, Valid Loss: 8.589797124198432\n",
            "Epoch: 18, Step: 7600, Train Loss: 7.457530908834591, Valid Loss: 8.637163327099442\n",
            "Epoch: 18, Step: 7700, Train Loss: 7.408030376923427, Valid Loss: 8.6398365682707\n",
            "Epoch: 18, Step: 7800, Train Loss: 7.404426731519625, Valid Loss: 8.65831235620614\n",
            "Epoch: 18, Step: 7900, Train Loss: 7.512360196992445, Valid Loss: 8.634740542894258\n",
            "Epoch: 19, Step: 8000, Train Loss: 7.419597159691497, Valid Loss: 8.627603112139882\n",
            "Epoch: 19, Step: 8100, Train Loss: 7.328382494369911, Valid Loss: 8.66843777903865\n",
            "Epoch: 19, Step: 8200, Train Loss: 7.364422574712058, Valid Loss: 8.631896422844163\n",
            "Epoch: 19, Step: 8300, Train Loss: 7.300946001025068, Valid Loss: 8.707419937200507\n",
            "Epoch: 20, Step: 8400, Train Loss: 7.376194449766561, Valid Loss: 8.669925817578385\n",
            "Epoch: 20, Step: 8500, Train Loss: 7.284493761916874, Valid Loss: 8.662350035358088\n",
            "Epoch: 20, Step: 8600, Train Loss: 7.348671274728804, Valid Loss: 8.728289720015344\n",
            "Epoch: 20, Step: 8700, Train Loss: 7.2852349046092195, Valid Loss: 8.71913341280044\n",
            "Epoch: 21, Step: 8800, Train Loss: 7.25665476466051, Valid Loss: 8.775721736638278\n",
            "Epoch: 21, Step: 8900, Train Loss: 7.215628004106803, Valid Loss: 8.660099020684662\n",
            "Epoch: 21, Step: 9000, Train Loss: 7.272093297832926, Valid Loss: 8.80777780515312\n",
            "Epoch: 21, Step: 9100, Train Loss: 7.35557863390695, Valid Loss: 8.899184874121257\n",
            "Epoch: 22, Step: 9200, Train Loss: 7.187671449677993, Valid Loss: 8.785939986096455\n",
            "Epoch: 22, Step: 9300, Train Loss: 7.172127652151451, Valid Loss: 8.782205005698106\n",
            "Epoch: 22, Step: 9400, Train Loss: 7.376026774960683, Valid Loss: 8.727302032887787\n",
            "Epoch: 22, Step: 9500, Train Loss: 7.1829083415625465, Valid Loss: 8.710141218446084\n",
            "Epoch: 22, Step: 9600, Train Loss: 7.1958048462877695, Valid Loss: 8.70127854168327\n",
            "Epoch: 23, Step: 9700, Train Loss: 7.113896758999246, Valid Loss: 8.730821901182797\n",
            "Epoch: 23, Step: 9800, Train Loss: 7.08564818457166, Valid Loss: 8.790586688968888\n",
            "Epoch: 23, Step: 9900, Train Loss: 7.120431865746879, Valid Loss: 8.754246895446707\n",
            "Epoch: 23, Step: 10000, Train Loss: 7.155046249733798, Valid Loss: 8.899418764741911\n",
            "Epoch: 24, Step: 10100, Train Loss: 7.112204723186163, Valid Loss: 8.732054444799303\n",
            "Epoch: 24, Step: 10200, Train Loss: 7.101422796847191, Valid Loss: 8.683280292177189\n",
            "Epoch: 24, Step: 10300, Train Loss: 7.268107558890427, Valid Loss: 8.742702209290178\n",
            "Epoch: 24, Step: 10400, Train Loss: 7.006324491607477, Valid Loss: 8.727936712888209\n",
            "Epoch: 25, Step: 10500, Train Loss: 7.099290633778234, Valid Loss: 8.785833123053175\n",
            "Epoch: 25, Step: 10600, Train Loss: 7.05636924179443, Valid Loss: 9.028810688858124\n",
            "Epoch: 25, Step: 10700, Train Loss: 6.959295172008501, Valid Loss: 8.920540568945835\n",
            "Epoch: 25, Step: 10800, Train Loss: 7.1613078408628095, Valid Loss: 8.79708958078253\n",
            "Epoch: 26, Step: 10900, Train Loss: 6.939044834679361, Valid Loss: 8.892093346386476\n",
            "Epoch: 26, Step: 11000, Train Loss: 6.929811923619253, Valid Loss: 8.85170584451985\n",
            "Epoch: 26, Step: 11100, Train Loss: 6.949822207362401, Valid Loss: 8.855101510066376\n",
            "Epoch: 26, Step: 11200, Train Loss: 6.931440897724365, Valid Loss: 8.858411320651031\n",
            "Epoch: 27, Step: 11300, Train Loss: 6.854292103665884, Valid Loss: 8.846557287352782\n",
            "Epoch: 27, Step: 11400, Train Loss: 6.901557419219635, Valid Loss: 8.908332232756491\n",
            "Epoch: 27, Step: 11500, Train Loss: 6.874967728577741, Valid Loss: 8.827900706223634\n",
            "Epoch: 27, Step: 11600, Train Loss: 6.858472699024072, Valid Loss: 8.792432161295666\n",
            "Epoch: 27, Step: 11700, Train Loss: 6.8887647038650215, Valid Loss: 8.787905167126292\n",
            "Epoch: 28, Step: 11800, Train Loss: 6.853653186560261, Valid Loss: 8.863438489817325\n",
            "Epoch: 28, Step: 11900, Train Loss: 6.834476785191705, Valid Loss: 8.89142672468882\n",
            "Epoch: 28, Step: 12000, Train Loss: 6.824820395257754, Valid Loss: 8.830732195372546\n",
            "Epoch: 28, Step: 12100, Train Loss: 6.836683690885603, Valid Loss: 8.82664282523255\n",
            "Epoch: 29, Step: 12200, Train Loss: 6.776935966771749, Valid Loss: 8.90477784636453\n",
            "Epoch: 29, Step: 12300, Train Loss: 6.956248662309193, Valid Loss: 8.853846382733444\n",
            "Epoch: 29, Step: 12400, Train Loss: 6.794609008014655, Valid Loss: 8.880770711873675\n",
            "Epoch: 29, Step: 12500, Train Loss: 6.765197193557378, Valid Loss: 8.824883418845486\n",
            "Epoch: 30, Step: 12600, Train Loss: 6.7153043932058925, Valid Loss: 9.00315538413107\n",
            "Epoch: 30, Step: 12700, Train Loss: 6.707468257140756, Valid Loss: 8.943751988253796\n",
            "Epoch: 30, Step: 12800, Train Loss: 6.710788778411569, Valid Loss: 8.863789806938957\n",
            "Epoch: 30, Step: 12900, Train Loss: 6.932848575163487, Valid Loss: 8.837737883838471\n",
            "Epoch: 31, Step: 13000, Train Loss: 6.654510232649971, Valid Loss: 8.88752138252835\n",
            "Epoch: 31, Step: 13100, Train Loss: 6.686856426973841, Valid Loss: 8.964457829933728\n",
            "Epoch: 31, Step: 13200, Train Loss: 6.802489504177383, Valid Loss: 8.85207817300767\n",
            "Epoch: 31, Step: 13300, Train Loss: 6.6576885501564815, Valid Loss: 8.85875435975019\n",
            "Epoch: 32, Step: 13400, Train Loss: 6.65400214823073, Valid Loss: 8.924611972053484\n",
            "Epoch: 32, Step: 13500, Train Loss: 6.63724725395366, Valid Loss: 9.07386054354671\n",
            "Epoch: 32, Step: 13600, Train Loss: 6.681624019424393, Valid Loss: 9.033518859093716\n",
            "Epoch: 32, Step: 13700, Train Loss: 6.6262157587448804, Valid Loss: 8.94561837081329\n",
            "Epoch: 33, Step: 13800, Train Loss: 6.611458416211054, Valid Loss: 8.946643991897744\n",
            "Epoch: 33, Step: 13900, Train Loss: 6.5921287569465985, Valid Loss: 9.05049431235747\n",
            "Epoch: 33, Step: 14000, Train Loss: 6.652307350527491, Valid Loss: 8.942302083070706\n",
            "Epoch: 33, Step: 14100, Train Loss: 6.624694995033677, Valid Loss: 8.982058794720867\n",
            "Epoch: 33, Step: 14200, Train Loss: 6.64977400152535, Valid Loss: 8.96721114124528\n",
            "Epoch: 34, Step: 14300, Train Loss: 6.533033512910755, Valid Loss: 9.064338446877114\n",
            "Epoch: 34, Step: 14400, Train Loss: 6.604340026698147, Valid Loss: 9.157752180857365\n",
            "Epoch: 34, Step: 14500, Train Loss: 6.596277945834867, Valid Loss: 9.154259295404726\n",
            "Epoch: 34, Step: 14600, Train Loss: 6.500320306621048, Valid Loss: 8.996694221416503\n",
            "Epoch: 35, Step: 14700, Train Loss: 6.460252474301507, Valid Loss: 8.981804990001624\n",
            "Epoch: 35, Step: 14800, Train Loss: 6.561291356325406, Valid Loss: 8.952416817189961\n",
            "Epoch: 35, Step: 14900, Train Loss: 6.524058565394475, Valid Loss: 9.053928612185135\n",
            "Epoch: 35, Step: 15000, Train Loss: 6.626545278651695, Valid Loss: 8.91798366855003\n",
            "Epoch: 36, Step: 15100, Train Loss: 6.4970402030002, Valid Loss: 9.070917427884119\n",
            "Epoch: 36, Step: 15200, Train Loss: 6.441785417172124, Valid Loss: 9.030123076743907\n",
            "Epoch: 36, Step: 15300, Train Loss: 6.532921915578858, Valid Loss: 9.163445452585623\n",
            "Epoch: 36, Step: 15400, Train Loss: 6.531770914212939, Valid Loss: 9.060290391910035\n",
            "Epoch: 37, Step: 15500, Train Loss: 6.426349997679656, Valid Loss: 8.956068276008164\n",
            "Epoch: 37, Step: 15600, Train Loss: 6.492099124468718, Valid Loss: 9.254200865952273\n",
            "Epoch: 37, Step: 15700, Train Loss: 6.399025369324539, Valid Loss: 8.99578557612196\n",
            "Epoch: 37, Step: 15800, Train Loss: 6.415195812030253, Valid Loss: 9.000670541432326\n",
            "Epoch: 38, Step: 15900, Train Loss: 6.386017621299334, Valid Loss: 8.994621682125048\n",
            "Epoch: 38, Step: 16000, Train Loss: 6.386665260031687, Valid Loss: 8.988583367548271\n",
            "Epoch: 38, Step: 16100, Train Loss: 6.451270826871451, Valid Loss: 8.933370205013308\n",
            "Epoch: 38, Step: 16200, Train Loss: 6.462731079076184, Valid Loss: 8.934611539817123\n",
            "Epoch: 38, Step: 16300, Train Loss: 6.493255450327669, Valid Loss: 8.936796680934057\n",
            "Epoch: 39, Step: 16400, Train Loss: 6.3838582017629495, Valid Loss: 9.034386085971605\n",
            "Epoch: 39, Step: 16500, Train Loss: 6.437992521020254, Valid Loss: 8.996240579753511\n",
            "Epoch: 39, Step: 16600, Train Loss: 6.454645856421088, Valid Loss: 9.028533769330139\n",
            "Epoch: 39, Step: 16700, Train Loss: 6.429369444146135, Valid Loss: 9.145124641517835\n",
            "Epoch: 40, Step: 16800, Train Loss: 6.288688103977657, Valid Loss: 9.214814791086273\n",
            "Epoch: 40, Step: 16900, Train Loss: 6.395746554328716, Valid Loss: 9.326284781192685\n",
            "Epoch: 40, Step: 17000, Train Loss: 6.4407460683833655, Valid Loss: 9.173356456199663\n",
            "Epoch: 40, Step: 17100, Train Loss: 6.283320953814958, Valid Loss: 9.020896691787511\n",
            "Epoch: 41, Step: 17200, Train Loss: 6.203129454784408, Valid Loss: 9.097202112683064\n",
            "Epoch: 41, Step: 17300, Train Loss: 6.3084118943304395, Valid Loss: 8.990163648988872\n",
            "Epoch: 41, Step: 17400, Train Loss: 6.321429884646271, Valid Loss: 9.085299500051976\n",
            "Epoch: 41, Step: 17500, Train Loss: 6.397491345655254, Valid Loss: 9.201700448232007\n",
            "Epoch: 42, Step: 17600, Train Loss: 6.2602938499693925, Valid Loss: 9.14685719023695\n",
            "Epoch: 42, Step: 17700, Train Loss: 6.239373952362873, Valid Loss: 9.060940636641805\n",
            "Epoch: 42, Step: 17800, Train Loss: 6.243296062841154, Valid Loss: 9.159289759997835\n",
            "Epoch: 42, Step: 17900, Train Loss: 6.236163626955931, Valid Loss: 9.053247958096664\n",
            "Epoch: 43, Step: 18000, Train Loss: 6.317912001517749, Valid Loss: 9.086958608859593\n",
            "Epoch: 43, Step: 18100, Train Loss: 6.184599144755295, Valid Loss: 9.067447637964019\n",
            "Epoch: 43, Step: 18200, Train Loss: 6.2191064540344705, Valid Loss: 9.194715776357585\n",
            "Epoch: 43, Step: 18300, Train Loss: 6.208252767636389, Valid Loss: 9.11014963968229\n",
            "Epoch: 44, Step: 18400, Train Loss: 6.195237968565496, Valid Loss: 9.212909733574257\n",
            "Epoch: 44, Step: 18500, Train Loss: 6.222189229863901, Valid Loss: 9.030486104113212\n",
            "Epoch: 44, Step: 18600, Train Loss: 6.204566207363322, Valid Loss: 9.364074799264683\n",
            "Epoch: 44, Step: 18700, Train Loss: 6.2656847693862625, Valid Loss: 9.058341046932139\n",
            "Epoch: 44, Step: 18800, Train Loss: 6.7200185211446435, Valid Loss: 9.068328984315954\n",
            "Epoch: 45, Step: 18900, Train Loss: 6.140758453361422, Valid Loss: 9.256115244671069\n",
            "Epoch: 45, Step: 19000, Train Loss: 6.128036864418086, Valid Loss: 9.196340878674143\n",
            "Epoch: 45, Step: 19100, Train Loss: 6.166136905413361, Valid Loss: 9.324956270378987\n",
            "Epoch: 45, Step: 19200, Train Loss: 6.126660897066063, Valid Loss: 9.181997240417575\n",
            "Epoch: 46, Step: 19300, Train Loss: 6.0791284570218815, Valid Loss: 9.252582190959416\n",
            "Epoch: 46, Step: 19400, Train Loss: 6.123674130091168, Valid Loss: 9.201846382606119\n",
            "Epoch: 46, Step: 19500, Train Loss: 6.138323291257704, Valid Loss: 9.270736760401185\n",
            "Epoch: 46, Step: 19600, Train Loss: 6.138590381687142, Valid Loss: 9.202827951076337\n",
            "Epoch: 47, Step: 19700, Train Loss: 6.050642456275266, Valid Loss: 9.217194831549563\n",
            "Epoch: 47, Step: 19800, Train Loss: 6.033425291522961, Valid Loss: 9.30247123362509\n",
            "Epoch: 47, Step: 19900, Train Loss: 6.06737353543222, Valid Loss: 9.307044880060642\n",
            "Epoch: 47, Step: 20000, Train Loss: 6.146075214050909, Valid Loss: 9.0683972051996\n",
            "Epoch: 48, Step: 20100, Train Loss: 6.018368911455311, Valid Loss: 9.271088099074237\n",
            "Epoch: 48, Step: 20200, Train Loss: 6.014647842157482, Valid Loss: 9.250940114909383\n",
            "Epoch: 48, Step: 20300, Train Loss: 6.030323117268389, Valid Loss: 9.235819462816972\n",
            "Epoch: 48, Step: 20400, Train Loss: 6.146921354283307, Valid Loss: 9.482828373600992\n",
            "Epoch: 49, Step: 20500, Train Loss: 6.045351700371887, Valid Loss: 9.283306084872482\n",
            "Epoch: 49, Step: 20600, Train Loss: 6.005684265442624, Valid Loss: 9.149988495789557\n",
            "Epoch: 49, Step: 20700, Train Loss: 6.059865165232574, Valid Loss: 9.197850988194837\n",
            "Epoch: 49, Step: 20800, Train Loss: 6.028016100023883, Valid Loss: 9.374254191722768\n",
            "Epoch: 49, Step: 20900, Train Loss: 6.036682573351407, Valid Loss: 9.139992706729402\n",
            "Epoch: 50, Step: 21000, Train Loss: 6.0395954358498205, Valid Loss: 9.102710481130691\n",
            "Epoch: 50, Step: 21100, Train Loss: 5.997531707245927, Valid Loss: 9.326774978943094\n",
            "Epoch: 50, Step: 21200, Train Loss: 6.02839795347193, Valid Loss: 9.30626145781924\n",
            "Epoch: 50, Step: 21300, Train Loss: 6.1035249582527165, Valid Loss: 9.173096072699524\n",
            "Epoch: 51, Step: 21400, Train Loss: 5.972144150146377, Valid Loss: 9.427151122365048\n",
            "Epoch: 51, Step: 21500, Train Loss: 5.91575265905339, Valid Loss: 9.231995865659012\n",
            "Epoch: 51, Step: 21600, Train Loss: 5.987625099809428, Valid Loss: 9.28176776711097\n",
            "Epoch: 51, Step: 21700, Train Loss: 6.10503392253631, Valid Loss: 9.629061258435362\n",
            "Epoch: 52, Step: 21800, Train Loss: 5.882092335511187, Valid Loss: 9.42458766892277\n",
            "Epoch: 52, Step: 21900, Train Loss: 5.939694492551236, Valid Loss: 9.395385190209291\n",
            "Epoch: 52, Step: 22000, Train Loss: 5.884907031015807, Valid Loss: 9.296057132323332\n",
            "Epoch: 52, Step: 22100, Train Loss: 5.975766108525999, Valid Loss: 9.463259362351476\n",
            "Epoch: 53, Step: 22200, Train Loss: 5.860982941344667, Valid Loss: 9.343899313266151\n",
            "Epoch: 53, Step: 22300, Train Loss: 5.907641272233783, Valid Loss: 9.321404604621044\n",
            "Epoch: 53, Step: 22400, Train Loss: 5.900109978158923, Valid Loss: 9.341861173605418\n",
            "Epoch: 53, Step: 22500, Train Loss: 5.907079119290027, Valid Loss: 9.203058680871255\n",
            "Epoch: 54, Step: 22600, Train Loss: 5.867721965378771, Valid Loss: 9.338124067477882\n",
            "Epoch: 54, Step: 22700, Train Loss: 5.854191959237441, Valid Loss: 9.383576803356517\n",
            "Epoch: 54, Step: 22800, Train Loss: 6.137086426314346, Valid Loss: 9.190069010452806\n",
            "Epoch: 54, Step: 22900, Train Loss: 5.826937185810811, Valid Loss: 9.316023850823758\n",
            "Epoch: 55, Step: 23000, Train Loss: 5.919937106323929, Valid Loss: 9.164290415561961\n",
            "Epoch: 55, Step: 23100, Train Loss: 5.800168137655709, Valid Loss: 9.362701918135539\n",
            "Epoch: 55, Step: 23200, Train Loss: 5.7696669150690445, Valid Loss: 9.324497007176939\n",
            "Epoch: 55, Step: 23300, Train Loss: 5.791408224135712, Valid Loss: 9.290243030616562\n",
            "Epoch: 55, Step: 23400, Train Loss: 5.848628267528047, Valid Loss: 9.280689699660936\n",
            "Epoch: 56, Step: 23500, Train Loss: 5.745539166872284, Valid Loss: 9.351311747464136\n",
            "Epoch: 56, Step: 23600, Train Loss: 5.787784427618322, Valid Loss: 9.265930820854747\n",
            "Epoch: 56, Step: 23700, Train Loss: 5.856590823408957, Valid Loss: 9.266594275658735\n",
            "Epoch: 56, Step: 23800, Train Loss: 5.882818111194487, Valid Loss: 9.39395111618183\n",
            "Epoch: 57, Step: 23900, Train Loss: 5.826551254440467, Valid Loss: 9.190261663760673\n",
            "Epoch: 57, Step: 24000, Train Loss: 5.746384560506128, Valid Loss: 9.311761903054778\n",
            "Epoch: 57, Step: 24100, Train Loss: 5.801041403271887, Valid Loss: 9.340212327192612\n",
            "Epoch: 57, Step: 24200, Train Loss: 5.811213950517145, Valid Loss: 9.285747046648098\n",
            "Epoch: 58, Step: 24300, Train Loss: 5.796885969354587, Valid Loss: 9.491009973360088\n",
            "Epoch: 58, Step: 24400, Train Loss: 5.725826402896771, Valid Loss: 9.361274218363699\n",
            "Epoch: 58, Step: 24500, Train Loss: 5.737517899338921, Valid Loss: 9.403696882748825\n",
            "Epoch: 58, Step: 24600, Train Loss: 5.729114627819748, Valid Loss: 9.402936248261796\n",
            "Epoch: 59, Step: 24700, Train Loss: 5.717279265742503, Valid Loss: 9.349927056968028\n",
            "Epoch: 59, Step: 24800, Train Loss: 5.73898166810759, Valid Loss: 9.28964281674134\n",
            "Epoch: 59, Step: 24900, Train Loss: 5.718313295459721, Valid Loss: 9.314613018946531\n",
            "Epoch: 59, Step: 25000, Train Loss: 5.74709415134037, Valid Loss: 9.541471216775408\n",
            "Epoch: 60, Step: 25100, Train Loss: 5.743222163458975, Valid Loss: 9.357435959692758\n",
            "Epoch: 60, Step: 25200, Train Loss: 5.68886187611471, Valid Loss: 9.415278335729788\n",
            "Epoch: 60, Step: 25300, Train Loss: 5.656153625043639, Valid Loss: 9.369569953830284\n",
            "Epoch: 60, Step: 25400, Train Loss: 5.822922143583708, Valid Loss: 9.457093446640732\n",
            "Epoch: 61, Step: 25500, Train Loss: 5.730869395316608, Valid Loss: 9.27869261800709\n",
            "Epoch: 61, Step: 25600, Train Loss: 5.75956425822609, Valid Loss: 9.215640510133987\n",
            "Epoch: 61, Step: 25700, Train Loss: 5.660957879660468, Valid Loss: 9.300722920946615\n",
            "Epoch: 61, Step: 25800, Train Loss: 5.684934870976319, Valid Loss: 9.572152319928865\n",
            "Epoch: 61, Step: 25900, Train Loss: 5.7453379908470215, Valid Loss: 9.458724536688067\n",
            "Epoch: 62, Step: 26000, Train Loss: 5.616435236819959, Valid Loss: 9.400782036021985\n",
            "Epoch: 62, Step: 26100, Train Loss: 5.707770299848637, Valid Loss: 9.344121835697363\n",
            "Epoch: 62, Step: 26200, Train Loss: 5.736741351476735, Valid Loss: 9.689889251530355\n",
            "Epoch: 62, Step: 26300, Train Loss: 5.656059259263949, Valid Loss: 9.409830732794408\n",
            "Epoch: 63, Step: 26400, Train Loss: 5.69473367218047, Valid Loss: 9.589872081959935\n",
            "Epoch: 63, Step: 26500, Train Loss: 5.613686555810518, Valid Loss: 9.500547790890721\n",
            "Epoch: 63, Step: 26600, Train Loss: 5.715071171533226, Valid Loss: 9.508687562412444\n",
            "Epoch: 63, Step: 26700, Train Loss: 5.644705301743882, Valid Loss: 9.546855805329216\n",
            "Epoch: 64, Step: 26800, Train Loss: 5.569315826114089, Valid Loss: 9.46816509365577\n",
            "Epoch: 64, Step: 26900, Train Loss: 5.561748239017639, Valid Loss: 9.456677552942462\n",
            "Epoch: 64, Step: 27000, Train Loss: 5.6035493578105875, Valid Loss: 9.362553567168872\n",
            "Epoch: 64, Step: 27100, Train Loss: 5.681122854487106, Valid Loss: 9.612810515145208\n",
            "Epoch: 65, Step: 27200, Train Loss: 5.536392906965501, Valid Loss: 9.382288370828068\n",
            "Epoch: 65, Step: 27300, Train Loss: 5.569608973206745, Valid Loss: 9.516974693900968\n",
            "Epoch: 65, Step: 27400, Train Loss: 5.577074313084599, Valid Loss: 9.34554389238401\n",
            "Epoch: 65, Step: 27500, Train Loss: 5.5791998422903015, Valid Loss: 9.351698275015474\n",
            "Epoch: 66, Step: 27600, Train Loss: 5.5065160032550065, Valid Loss: 9.392875169223345\n",
            "Epoch: 66, Step: 27700, Train Loss: 5.565669682131735, Valid Loss: 9.307345586964562\n",
            "Epoch: 66, Step: 27800, Train Loss: 5.559906991226003, Valid Loss: 9.49660701794395\n",
            "Epoch: 66, Step: 27900, Train Loss: 5.5423424962393755, Valid Loss: 9.416516281576605\n",
            "Epoch: 66, Step: 28000, Train Loss: 5.581988213635591, Valid Loss: 9.354232267848344\n",
            "Epoch: 67, Step: 28100, Train Loss: 5.507653801647459, Valid Loss: 9.392354321615938\n",
            "Epoch: 67, Step: 28200, Train Loss: 5.5462114868861, Valid Loss: 9.462692520887314\n",
            "Epoch: 67, Step: 28300, Train Loss: 5.569613721906441, Valid Loss: 9.354481672715147\n",
            "Epoch: 67, Step: 28400, Train Loss: 5.529673692132109, Valid Loss: 9.46725803907789\n",
            "Epoch: 68, Step: 28500, Train Loss: 5.468846351877358, Valid Loss: 9.616480810527829\n",
            "Epoch: 68, Step: 28600, Train Loss: 5.5244367925043285, Valid Loss: 9.745465842540467\n",
            "Epoch: 68, Step: 28700, Train Loss: 5.5224330556322485, Valid Loss: 9.330237747862935\n",
            "Epoch: 68, Step: 28800, Train Loss: 5.478788396118596, Valid Loss: 9.455504181473971\n",
            "Epoch: 69, Step: 28900, Train Loss: 5.445067561091017, Valid Loss: 9.470952707284216\n",
            "Epoch: 69, Step: 29000, Train Loss: 5.432652384443419, Valid Loss: 9.415873551479102\n",
            "Epoch: 69, Step: 29100, Train Loss: 5.534025753259245, Valid Loss: 9.453721184692972\n",
            "Epoch: 69, Step: 29200, Train Loss: 5.532117517173962, Valid Loss: 9.589270301279608\n",
            "Epoch: 70, Step: 29300, Train Loss: 5.418630154211845, Valid Loss: 9.559061908055503\n",
            "Epoch: 70, Step: 29400, Train Loss: 5.4198880623466925, Valid Loss: 9.570693728670157\n",
            "Epoch: 70, Step: 29500, Train Loss: 5.4968630725372085, Valid Loss: 9.691108486298285\n",
            "Epoch: 70, Step: 29600, Train Loss: 5.486950979832797, Valid Loss: 9.470753635913104\n",
            "Epoch: 71, Step: 29700, Train Loss: 5.439163707782549, Valid Loss: 9.632834362303067\n",
            "Epoch: 71, Step: 29800, Train Loss: 5.379903579486119, Valid Loss: 9.551037402336716\n",
            "Epoch: 71, Step: 29900, Train Loss: 5.442488114323056, Valid Loss: 9.466462214125558\n",
            "Epoch: 71, Step: 30000, Train Loss: 5.577395299110655, Valid Loss: 9.81737989672515\n",
            "Epoch: 72, Step: 30100, Train Loss: 5.4263114095295215, Valid Loss: 9.593452943738267\n",
            "Epoch: 72, Step: 30200, Train Loss: 5.447859765989147, Valid Loss: 9.399094425071386\n",
            "Epoch: 72, Step: 30300, Train Loss: 5.41568020653529, Valid Loss: 9.493234084344323\n",
            "Epoch: 72, Step: 30400, Train Loss: 5.460923634967719, Valid Loss: 9.403204957057795\n",
            "Epoch: 72, Step: 30500, Train Loss: 5.406875744061701, Valid Loss: 9.551438956741412\n",
            "Epoch: 73, Step: 30600, Train Loss: 5.4043342900332165, Valid Loss: 9.649577566464501\n",
            "Epoch: 73, Step: 30700, Train Loss: 5.378161448316205, Valid Loss: 9.574163288927556\n",
            "Epoch: 73, Step: 30800, Train Loss: 5.477580516247348, Valid Loss: 9.302374617106537\n",
            "Epoch: 73, Step: 30900, Train Loss: 5.436567996913822, Valid Loss: 9.343820534038352\n",
            "Epoch: 74, Step: 31000, Train Loss: 5.313541294906744, Valid Loss: 9.51795306130247\n",
            "Epoch: 74, Step: 31100, Train Loss: 5.308023068167996, Valid Loss: 9.523453055053249\n",
            "Epoch: 74, Step: 31200, Train Loss: 5.323298056306212, Valid Loss: 9.520453491965727\n",
            "Epoch: 74, Step: 31300, Train Loss: 5.396799408882236, Valid Loss: 9.705056283834553\n",
            "Epoch: 75, Step: 31400, Train Loss: 5.38787037340017, Valid Loss: 9.535198273453565\n",
            "Epoch: 75, Step: 31500, Train Loss: 5.338726191922827, Valid Loss: 9.611940775460337\n",
            "Epoch: 75, Step: 31600, Train Loss: 5.373419478547173, Valid Loss: 9.46579492933492\n",
            "Epoch: 75, Step: 31700, Train Loss: 5.436461654149748, Valid Loss: 9.424525959225072\n",
            "Epoch: 76, Step: 31800, Train Loss: 5.327223268458517, Valid Loss: 9.548171311599623\n",
            "Epoch: 76, Step: 31900, Train Loss: 5.327483490373196, Valid Loss: 9.687796164832033\n",
            "Epoch: 76, Step: 32000, Train Loss: 5.354779220529563, Valid Loss: 9.372183952352064\n",
            "Epoch: 76, Step: 32100, Train Loss: 5.343133622079682, Valid Loss: 9.64925952079868\n",
            "Epoch: 77, Step: 32200, Train Loss: 5.598102808327031, Valid Loss: 9.331797388775042\n",
            "Epoch: 77, Step: 32300, Train Loss: 5.345833806926262, Valid Loss: 9.395710939702187\n",
            "Epoch: 77, Step: 32400, Train Loss: 5.446450537992393, Valid Loss: 9.8425879360472\n",
            "Epoch: 77, Step: 32500, Train Loss: 5.380168281085779, Valid Loss: 9.472627420618835\n",
            "Epoch: 77, Step: 32600, Train Loss: 5.332304517443879, Valid Loss: 9.560994898227687\n",
            "Epoch: 78, Step: 32700, Train Loss: 5.35810709987137, Valid Loss: 9.413863191252261\n",
            "Epoch: 78, Step: 32800, Train Loss: 5.40386810811122, Valid Loss: 9.358351318350387\n",
            "Epoch: 78, Step: 32900, Train Loss: 5.30795587767915, Valid Loss: 9.47945008413891\n",
            "Epoch: 78, Step: 33000, Train Loss: 5.288866319255341, Valid Loss: 9.612107215570502\n",
            "Epoch: 79, Step: 33100, Train Loss: 5.316998067071815, Valid Loss: 9.628859344972513\n",
            "Epoch: 79, Step: 33200, Train Loss: 5.254094525912879, Valid Loss: 9.638946091985124\n",
            "Epoch: 79, Step: 33300, Train Loss: 5.273806415809737, Valid Loss: 9.632392249021715\n",
            "Epoch: 79, Step: 33400, Train Loss: 5.2971800448785835, Valid Loss: 9.552551661328069\n",
            "Epoch: 80, Step: 33500, Train Loss: 5.423305776322063, Valid Loss: 9.940402761751587\n",
            "Epoch: 80, Step: 33600, Train Loss: 5.330449318407277, Valid Loss: 9.81389080118796\n",
            "Epoch: 80, Step: 33700, Train Loss: 5.231412182216605, Valid Loss: 9.56042384639549\n",
            "Epoch: 80, Step: 33800, Train Loss: 5.280000554214807, Valid Loss: 9.537509389019561\n",
            "Epoch: 81, Step: 33900, Train Loss: 5.221998313558862, Valid Loss: 9.545333412747938\n",
            "Epoch: 81, Step: 34000, Train Loss: 5.21028358547548, Valid Loss: 9.567793276664776\n",
            "Epoch: 81, Step: 34100, Train Loss: 5.299268582565543, Valid Loss: 9.49653873527781\n",
            "Epoch: 81, Step: 34200, Train Loss: 5.2622805210012755, Valid Loss: 9.555737516911634\n",
            "Epoch: 82, Step: 34300, Train Loss: 5.48418904784422, Valid Loss: 9.464928172209957\n",
            "Epoch: 82, Step: 34400, Train Loss: 5.245640958295527, Valid Loss: 9.71547391743453\n",
            "Epoch: 82, Step: 34500, Train Loss: 5.239365405646591, Valid Loss: 9.433567668518299\n",
            "Epoch: 82, Step: 34600, Train Loss: 5.192019756554118, Valid Loss: 9.537891161750395\n",
            "Epoch: 83, Step: 34700, Train Loss: 5.4201141030456235, Valid Loss: 9.485798821823375\n",
            "Epoch: 83, Step: 34800, Train Loss: 5.224500631630361, Valid Loss: 9.674438024874089\n",
            "Epoch: 83, Step: 34900, Train Loss: 5.327568011324361, Valid Loss: 9.443187523103282\n",
            "Epoch: 83, Step: 35000, Train Loss: 5.200828311822907, Valid Loss: 9.59507882954431\n",
            "Epoch: 83, Step: 35100, Train Loss: 5.224278190056632, Valid Loss: 9.425374525996626\n",
            "Epoch: 84, Step: 35200, Train Loss: 5.241372968170602, Valid Loss: 9.820609950761543\n",
            "Epoch: 84, Step: 35300, Train Loss: 5.228496672230203, Valid Loss: 9.594761001082334\n",
            "Epoch: 84, Step: 35400, Train Loss: 5.1543547275225015, Valid Loss: 9.67382217100251\n",
            "Epoch: 84, Step: 35500, Train Loss: 5.204510931741864, Valid Loss: 9.552842347372323\n",
            "Epoch: 85, Step: 35600, Train Loss: 5.163390563188659, Valid Loss: 9.525074665329269\n",
            "Epoch: 85, Step: 35700, Train Loss: 5.175939056531775, Valid Loss: 9.733979972395987\n",
            "Epoch: 85, Step: 35800, Train Loss: 5.166279162808945, Valid Loss: 9.528666387161222\n",
            "Epoch: 85, Step: 35900, Train Loss: 5.186274379501815, Valid Loss: 9.56921556802086\n",
            "Epoch: 86, Step: 36000, Train Loss: 5.123703130424865, Valid Loss: 9.685415820731674\n",
            "Epoch: 86, Step: 36100, Train Loss: 5.154911575614376, Valid Loss: 9.661212172912316\n",
            "Epoch: 86, Step: 36200, Train Loss: 5.137770934293084, Valid Loss: 9.667844787224645\n",
            "Epoch: 86, Step: 36300, Train Loss: 5.4784930423589095, Valid Loss: 10.198793125273506\n",
            "Epoch: 87, Step: 36400, Train Loss: 5.099327327197348, Valid Loss: 9.665594727496881\n",
            "Epoch: 87, Step: 36500, Train Loss: 5.128437929261714, Valid Loss: 9.655033440286736\n",
            "Epoch: 87, Step: 36600, Train Loss: 5.1023254441165165, Valid Loss: 9.696432805765207\n",
            "Epoch: 87, Step: 36700, Train Loss: 5.211047514899107, Valid Loss: 9.495958515716614\n",
            "Epoch: 88, Step: 36800, Train Loss: 5.0798779840587995, Valid Loss: 9.64152011886597\n",
            "Epoch: 88, Step: 36900, Train Loss: 5.076594514485873, Valid Loss: 9.640553481172322\n",
            "Epoch: 88, Step: 37000, Train Loss: 5.166558160436347, Valid Loss: 9.466381656205916\n",
            "Epoch: 88, Step: 37100, Train Loss: 5.137074585103103, Valid Loss: 9.476782598860423\n",
            "Epoch: 88, Step: 37200, Train Loss: 5.176416622529076, Valid Loss: 9.562233664279622\n",
            "Epoch: 89, Step: 37300, Train Loss: 5.063846817512477, Valid Loss: 9.565217978846883\n",
            "Epoch: 89, Step: 37400, Train Loss: 5.294629089303266, Valid Loss: 9.454056674703189\n",
            "Epoch: 89, Step: 37500, Train Loss: 5.104761746523437, Valid Loss: 9.6645650756894\n",
            "Epoch: 89, Step: 37600, Train Loss: 5.111343349845662, Valid Loss: 9.585382034191484\n",
            "Epoch: 90, Step: 37700, Train Loss: 5.061156126773791, Valid Loss: 9.596829585832637\n",
            "Epoch: 90, Step: 37800, Train Loss: 5.143249546964364, Valid Loss: 9.726643265583261\n",
            "Epoch: 90, Step: 37900, Train Loss: 5.085600135369261, Valid Loss: 9.54319942760007\n",
            "Epoch: 90, Step: 38000, Train Loss: 5.1070546476705845, Valid Loss: 9.731267772794002\n",
            "Epoch: 91, Step: 38100, Train Loss: 5.191165832087072, Valid Loss: 9.942542494297749\n",
            "Epoch: 91, Step: 38200, Train Loss: 5.016758814024668, Valid Loss: 9.664408266330181\n",
            "Epoch: 91, Step: 38300, Train Loss: 5.091073474000602, Valid Loss: 9.617172740495798\n",
            "Epoch: 91, Step: 38400, Train Loss: 5.180996869297659, Valid Loss: 9.479686946221461\n",
            "Epoch: 92, Step: 38500, Train Loss: 5.052714012177444, Valid Loss: 9.649496422449703\n",
            "Epoch: 92, Step: 38600, Train Loss: 5.261209054057514, Valid Loss: 9.452679188243748\n",
            "Epoch: 92, Step: 38700, Train Loss: 5.156216724893562, Valid Loss: 9.875157011851119\n",
            "Epoch: 92, Step: 38800, Train Loss: 5.114791167368896, Valid Loss: 9.559702963864305\n",
            "Epoch: 93, Step: 38900, Train Loss: 5.020676983449478, Valid Loss: 9.630460960763795\n",
            "Epoch: 93, Step: 39000, Train Loss: 5.035292189242754, Valid Loss: 9.709061479410227\n",
            "Epoch: 93, Step: 39100, Train Loss: 5.091204654713098, Valid Loss: 9.665787339240898\n",
            "Epoch: 93, Step: 39200, Train Loss: 5.136822279478769, Valid Loss: 9.699321409523693\n",
            "Epoch: 94, Step: 39300, Train Loss: 5.098972213279261, Valid Loss: 9.678391376338334\n",
            "Epoch: 94, Step: 39400, Train Loss: 5.317224996383334, Valid Loss: 9.935116076804881\n",
            "Epoch: 94, Step: 39500, Train Loss: 5.080325160482484, Valid Loss: 9.661126405071457\n",
            "Epoch: 94, Step: 39600, Train Loss: 5.121249257085344, Valid Loss: 9.784344923231261\n",
            "Epoch: 94, Step: 39700, Train Loss: 5.264599453891715, Valid Loss: 9.763765276275564\n",
            "Epoch: 95, Step: 39800, Train Loss: 5.246406320644668, Valid Loss: 10.038346410424339\n",
            "Epoch: 95, Step: 39900, Train Loss: 5.014335984189182, Valid Loss: 9.710798367570037\n",
            "Epoch: 95, Step: 40000, Train Loss: 5.027831995032533, Valid Loss: 9.678847261329619\n",
            "Epoch: 95, Step: 40100, Train Loss: 5.038623364692757, Valid Loss: 9.62443248324011\n",
            "Epoch: 96, Step: 40200, Train Loss: 4.9899705254008655, Valid Loss: 9.583852917563346\n",
            "Epoch: 96, Step: 40300, Train Loss: 5.061790858587783, Valid Loss: 9.894753247910318\n",
            "Epoch: 96, Step: 40400, Train Loss: 5.0623140832975535, Valid Loss: 9.534034377925076\n",
            "Epoch: 96, Step: 40500, Train Loss: 5.022372501544239, Valid Loss: 9.647253469704097\n",
            "Epoch: 97, Step: 40600, Train Loss: 5.072360050386863, Valid Loss: 9.875139891316882\n",
            "Epoch: 97, Step: 40700, Train Loss: 5.428292471422955, Valid Loss: 9.578906872953446\n",
            "Epoch: 97, Step: 40800, Train Loss: 4.991235245865747, Valid Loss: 9.604048579207793\n",
            "Epoch: 97, Step: 40900, Train Loss: 5.028242629038634, Valid Loss: 9.591494440321457\n",
            "Epoch: 98, Step: 41000, Train Loss: 4.973436441861374, Valid Loss: 9.694356008302584\n",
            "Epoch: 98, Step: 41100, Train Loss: 4.981254461728701, Valid Loss: 9.688783874901032\n",
            "Epoch: 98, Step: 41200, Train Loss: 4.988198706599517, Valid Loss: 9.833001037419304\n",
            "Epoch: 98, Step: 41300, Train Loss: 5.072759314892737, Valid Loss: 9.931363708163973\n",
            "Epoch: 99, Step: 41400, Train Loss: 5.019861376946722, Valid Loss: 9.881642961605996\n",
            "Epoch: 99, Step: 41500, Train Loss: 4.954697416754908, Valid Loss: 9.60174201713023\n",
            "Epoch: 99, Step: 41600, Train Loss: 5.066977323735099, Valid Loss: 9.92757196626393\n",
            "Epoch: 99, Step: 41700, Train Loss: 5.030722772559497, Valid Loss: 9.78324911238279\n",
            "Epoch: 99, Step: 41800, Train Loss: 5.030790221947301, Valid Loss: 9.55527122911216\n",
            "Test RMSE Loss for h = 180, wd = 0.1: 8.835052143654904\n",
            "Start training for h = 180, wd = 0.2\n",
            "Epoch: 0, Step: 100, Train Loss: 9.434782537388944, Valid Loss: 9.376322069338904\n",
            "Epoch: 0, Step: 200, Train Loss: 9.033621547203653, Valid Loss: 8.972841900816794\n",
            "Epoch: 0, Step: 300, Train Loss: 8.884177261919202, Valid Loss: 8.837339157147078\n",
            "Epoch: 0, Step: 400, Train Loss: 8.875994571947428, Valid Loss: 8.828493245882315\n",
            "Epoch: 1, Step: 500, Train Loss: 8.78798808585696, Valid Loss: 8.765133925311412\n",
            "Epoch: 1, Step: 600, Train Loss: 8.752113118983461, Valid Loss: 8.728769990138785\n",
            "Epoch: 1, Step: 700, Train Loss: 8.686021712377064, Valid Loss: 8.679038929011373\n",
            "Epoch: 1, Step: 800, Train Loss: 8.646321665085495, Valid Loss: 8.653678815617612\n",
            "Epoch: 2, Step: 900, Train Loss: 8.60063288130792, Valid Loss: 8.608173433074857\n",
            "Epoch: 2, Step: 1000, Train Loss: 8.602078261077175, Valid Loss: 8.623100439039291\n",
            "Epoch: 2, Step: 1100, Train Loss: 8.524819836381358, Valid Loss: 8.566358441043482\n",
            "Epoch: 2, Step: 1200, Train Loss: 8.550098636477014, Valid Loss: 8.586946971888223\n",
            "Epoch: 3, Step: 1300, Train Loss: 8.528484522978024, Valid Loss: 8.589165829966275\n",
            "Epoch: 3, Step: 1400, Train Loss: 8.472579932638393, Valid Loss: 8.547892765764525\n",
            "Epoch: 3, Step: 1500, Train Loss: 8.52535341773957, Valid Loss: 8.599864908069845\n",
            "Epoch: 3, Step: 1600, Train Loss: 8.488377372489447, Valid Loss: 8.582566082289576\n",
            "Epoch: 4, Step: 1700, Train Loss: 8.419492360679786, Valid Loss: 8.535115621090528\n",
            "Epoch: 4, Step: 1800, Train Loss: 8.392347011415934, Valid Loss: 8.53252556560233\n",
            "Epoch: 4, Step: 1900, Train Loss: 8.387211612592287, Valid Loss: 8.53562031819922\n",
            "Epoch: 4, Step: 2000, Train Loss: 8.345934395229037, Valid Loss: 8.51024605165647\n",
            "Epoch: 5, Step: 2100, Train Loss: 8.566557669133712, Valid Loss: 8.735055395237394\n",
            "Epoch: 5, Step: 2200, Train Loss: 8.369889072876756, Valid Loss: 8.579153458133685\n",
            "Epoch: 5, Step: 2300, Train Loss: 8.300860375067147, Valid Loss: 8.506924175972404\n",
            "Epoch: 5, Step: 2400, Train Loss: 8.292632433126746, Valid Loss: 8.51327068587193\n",
            "Epoch: 5, Step: 2500, Train Loss: 8.255574250564619, Valid Loss: 8.475150811147845\n",
            "Epoch: 6, Step: 2600, Train Loss: 8.493201769376158, Valid Loss: 8.694697042621648\n",
            "Epoch: 6, Step: 2700, Train Loss: 8.347026732815376, Valid Loss: 8.61495746606908\n",
            "Epoch: 6, Step: 2800, Train Loss: 8.272300527518649, Valid Loss: 8.521083847999902\n",
            "Epoch: 6, Step: 2900, Train Loss: 8.214392135476588, Valid Loss: 8.484887070296663\n",
            "Epoch: 7, Step: 3000, Train Loss: 8.19775666617794, Valid Loss: 8.547050828080623\n",
            "Epoch: 7, Step: 3100, Train Loss: 8.195553505434725, Valid Loss: 8.515961053476397\n",
            "Epoch: 7, Step: 3200, Train Loss: 8.22269313783824, Valid Loss: 8.509953029819007\n",
            "Epoch: 7, Step: 3300, Train Loss: 8.127394758975615, Valid Loss: 8.461903539673553\n",
            "Epoch: 8, Step: 3400, Train Loss: 8.102366892496871, Valid Loss: 8.465306485722945\n",
            "Epoch: 8, Step: 3500, Train Loss: 8.095173664429161, Valid Loss: 8.492211946577441\n",
            "Epoch: 8, Step: 3600, Train Loss: 8.08745078280955, Valid Loss: 8.485977997535466\n",
            "Epoch: 8, Step: 3700, Train Loss: 8.069827728543352, Valid Loss: 8.48783161786031\n",
            "Epoch: 9, Step: 3800, Train Loss: 8.039019534743895, Valid Loss: 8.508137283537167\n",
            "Epoch: 9, Step: 3900, Train Loss: 8.077910905248762, Valid Loss: 8.541697175949386\n",
            "Epoch: 9, Step: 4000, Train Loss: 8.041207212714115, Valid Loss: 8.509488308883922\n",
            "Epoch: 9, Step: 4100, Train Loss: 8.022662236870822, Valid Loss: 8.495684627752347\n",
            "Epoch: 10, Step: 4200, Train Loss: 8.014857597828874, Valid Loss: 8.533359284324563\n",
            "Epoch: 10, Step: 4300, Train Loss: 7.995730828589112, Valid Loss: 8.529107364096328\n",
            "Epoch: 10, Step: 4400, Train Loss: 7.937787991961578, Valid Loss: 8.508482571449274\n",
            "Epoch: 10, Step: 4500, Train Loss: 7.974103297209336, Valid Loss: 8.512068146956398\n",
            "Epoch: 11, Step: 4600, Train Loss: 7.9682483440876, Valid Loss: 8.593431526296708\n",
            "Epoch: 11, Step: 4700, Train Loss: 8.098299787080743, Valid Loss: 8.641817990590766\n",
            "Epoch: 11, Step: 4800, Train Loss: 7.889948638379236, Valid Loss: 8.52906327705267\n",
            "Epoch: 11, Step: 4900, Train Loss: 7.886387900936807, Valid Loss: 8.496394221641047\n",
            "Epoch: 11, Step: 5000, Train Loss: 7.830724166844657, Valid Loss: 8.499127914868792\n",
            "Epoch: 12, Step: 5100, Train Loss: 7.894264092084762, Valid Loss: 8.581188718942537\n",
            "Epoch: 12, Step: 5200, Train Loss: 7.817370504643481, Valid Loss: 8.491752363047102\n",
            "Epoch: 12, Step: 5300, Train Loss: 7.838600463224912, Valid Loss: 8.585261107118326\n",
            "Epoch: 12, Step: 5400, Train Loss: 7.781275207475807, Valid Loss: 8.508290710533403\n",
            "Epoch: 13, Step: 5500, Train Loss: 7.726536678231941, Valid Loss: 8.49659086970512\n",
            "Epoch: 13, Step: 5600, Train Loss: 7.753530130117154, Valid Loss: 8.51439001480691\n",
            "Epoch: 13, Step: 5700, Train Loss: 7.910743589084507, Valid Loss: 8.630688841889489\n",
            "Epoch: 13, Step: 5800, Train Loss: 7.699333342620539, Valid Loss: 8.542626325080564\n",
            "Epoch: 14, Step: 5900, Train Loss: 7.646101308676483, Valid Loss: 8.573671618173963\n",
            "Epoch: 14, Step: 6000, Train Loss: 7.671349116863236, Valid Loss: 8.547102684860324\n",
            "Epoch: 14, Step: 6100, Train Loss: 7.654028885900713, Valid Loss: 8.525677153168884\n",
            "Epoch: 14, Step: 6200, Train Loss: 7.7826810121282355, Valid Loss: 8.717430317710152\n",
            "Epoch: 15, Step: 6300, Train Loss: 7.696926583244765, Valid Loss: 8.654992807516143\n",
            "Epoch: 15, Step: 6400, Train Loss: 7.670451168899799, Valid Loss: 8.626946860205466\n",
            "Epoch: 15, Step: 6500, Train Loss: 7.598762461722283, Valid Loss: 8.522828276470891\n",
            "Epoch: 15, Step: 6600, Train Loss: 7.57911430319084, Valid Loss: 8.526465536745032\n",
            "Epoch: 16, Step: 6700, Train Loss: 7.560823953324518, Valid Loss: 8.614151335169982\n",
            "Epoch: 16, Step: 6800, Train Loss: 7.6244734761309365, Valid Loss: 8.759292483921248\n",
            "Epoch: 16, Step: 6900, Train Loss: 7.558761277203589, Valid Loss: 8.54842097732848\n",
            "Epoch: 16, Step: 7000, Train Loss: 7.912247119333832, Valid Loss: 9.078344891472245\n",
            "Epoch: 16, Step: 7100, Train Loss: 7.567723403452115, Valid Loss: 8.541381284579241\n",
            "Epoch: 17, Step: 7200, Train Loss: 7.468329221726472, Valid Loss: 8.545131045877419\n",
            "Epoch: 17, Step: 7300, Train Loss: 7.552833111558815, Valid Loss: 8.725183111324755\n",
            "Epoch: 17, Step: 7400, Train Loss: 7.54030416855726, Valid Loss: 8.577608538179783\n",
            "Epoch: 17, Step: 7500, Train Loss: 7.53226549536627, Valid Loss: 8.594282586406953\n",
            "Epoch: 18, Step: 7600, Train Loss: 7.410159489067022, Valid Loss: 8.643016865126112\n",
            "Epoch: 18, Step: 7700, Train Loss: 7.476456840917558, Valid Loss: 8.58746067872981\n",
            "Epoch: 18, Step: 7800, Train Loss: 7.431416110319257, Valid Loss: 8.603740895137674\n",
            "Epoch: 18, Step: 7900, Train Loss: 7.41607369446463, Valid Loss: 8.561321614751492\n",
            "Epoch: 19, Step: 8000, Train Loss: 7.31715341135035, Valid Loss: 8.610093902916148\n",
            "Epoch: 19, Step: 8100, Train Loss: 7.70319880927606, Valid Loss: 8.76430341096961\n",
            "Epoch: 19, Step: 8200, Train Loss: 7.399533992861092, Valid Loss: 8.764268804297211\n",
            "Epoch: 19, Step: 8300, Train Loss: 7.322533261597878, Valid Loss: 8.612861033807793\n",
            "Epoch: 20, Step: 8400, Train Loss: 7.270505523396762, Valid Loss: 8.604080123945074\n",
            "Epoch: 20, Step: 8500, Train Loss: 7.3179337697203115, Valid Loss: 8.58181201589719\n",
            "Epoch: 20, Step: 8600, Train Loss: 7.267860631891007, Valid Loss: 8.707374288419903\n",
            "Epoch: 20, Step: 8700, Train Loss: 7.339106968628592, Valid Loss: 8.77302769674958\n",
            "Epoch: 21, Step: 8800, Train Loss: 7.200073243348924, Valid Loss: 8.691463481136562\n",
            "Epoch: 21, Step: 8900, Train Loss: 7.229816099921224, Valid Loss: 8.767835158515139\n",
            "Epoch: 21, Step: 9000, Train Loss: 7.265808331916953, Valid Loss: 8.610291065388239\n",
            "Epoch: 21, Step: 9100, Train Loss: 7.188664354391587, Valid Loss: 8.656633108387995\n",
            "Epoch: 22, Step: 9200, Train Loss: 7.205443786683644, Valid Loss: 8.71480185882082\n",
            "Epoch: 22, Step: 9300, Train Loss: 7.159059882876989, Valid Loss: 8.786453851134462\n",
            "Epoch: 22, Step: 9400, Train Loss: 7.1410111723915755, Valid Loss: 8.697260600945686\n",
            "Epoch: 22, Step: 9500, Train Loss: 7.127331249533718, Valid Loss: 8.653147104365061\n",
            "Epoch: 22, Step: 9600, Train Loss: 7.1711926238140515, Valid Loss: 8.669879869362136\n",
            "Epoch: 23, Step: 9700, Train Loss: 7.090915511893053, Valid Loss: 8.751773741035242\n",
            "Epoch: 23, Step: 9800, Train Loss: 7.154132874646586, Valid Loss: 8.920478002342005\n",
            "Epoch: 23, Step: 9900, Train Loss: 7.089102076312305, Valid Loss: 8.70071191477355\n",
            "Epoch: 23, Step: 10000, Train Loss: 7.081906180612589, Valid Loss: 8.747710506099331\n",
            "Epoch: 24, Step: 10100, Train Loss: 7.13727810693063, Valid Loss: 8.79494385164716\n",
            "Epoch: 24, Step: 10200, Train Loss: 7.078228717390844, Valid Loss: 8.789724039578017\n",
            "Epoch: 24, Step: 10300, Train Loss: 7.177147918020747, Valid Loss: 8.622484190914856\n",
            "Epoch: 24, Step: 10400, Train Loss: 7.031697392462075, Valid Loss: 8.699680413652189\n",
            "Epoch: 25, Step: 10500, Train Loss: 6.952015861936218, Valid Loss: 8.730124993337181\n",
            "Epoch: 25, Step: 10600, Train Loss: 6.944099548586934, Valid Loss: 8.704593583555301\n",
            "Epoch: 25, Step: 10700, Train Loss: 7.0360392152876265, Valid Loss: 8.712463299436724\n",
            "Epoch: 25, Step: 10800, Train Loss: 6.973358813971945, Valid Loss: 8.710237706836322\n",
            "Epoch: 26, Step: 10900, Train Loss: 6.9151836817776475, Valid Loss: 8.74914209130319\n",
            "Epoch: 26, Step: 11000, Train Loss: 6.906620756956103, Valid Loss: 8.754980315291766\n",
            "Epoch: 26, Step: 11100, Train Loss: 6.931866440493838, Valid Loss: 8.744441057223177\n",
            "Epoch: 26, Step: 11200, Train Loss: 6.937038707488876, Valid Loss: 8.727663863452747\n",
            "Epoch: 27, Step: 11300, Train Loss: 7.012516975085257, Valid Loss: 8.731150350627312\n",
            "Epoch: 27, Step: 11400, Train Loss: 6.848717895551111, Valid Loss: 8.748939115462314\n",
            "Epoch: 27, Step: 11500, Train Loss: 6.878005934948366, Valid Loss: 8.854516956322525\n",
            "Epoch: 27, Step: 11600, Train Loss: 6.869188800156723, Valid Loss: 8.750343149554357\n",
            "Epoch: 27, Step: 11700, Train Loss: 6.860099096472836, Valid Loss: 8.842516617071817\n",
            "Epoch: 28, Step: 11800, Train Loss: 6.84171999174115, Valid Loss: 8.84232740020529\n",
            "Epoch: 28, Step: 11900, Train Loss: 6.827707553691268, Valid Loss: 8.879848316700993\n",
            "Epoch: 28, Step: 12000, Train Loss: 6.829323879319765, Valid Loss: 8.805568257664609\n",
            "Epoch: 28, Step: 12100, Train Loss: 6.782409211276088, Valid Loss: 8.823448665618614\n",
            "Epoch: 29, Step: 12200, Train Loss: 6.759502629155491, Valid Loss: 8.83877088838497\n",
            "Epoch: 29, Step: 12300, Train Loss: 6.818362776931016, Valid Loss: 8.769359241002572\n",
            "Epoch: 29, Step: 12400, Train Loss: 6.897277850577671, Valid Loss: 8.816806857234603\n",
            "Epoch: 29, Step: 12500, Train Loss: 6.883907557422433, Valid Loss: 8.783807702514464\n",
            "Epoch: 30, Step: 12600, Train Loss: 6.812669248909944, Valid Loss: 9.029972065243209\n",
            "Epoch: 30, Step: 12700, Train Loss: 6.7208272108483476, Valid Loss: 8.947820394324758\n",
            "Epoch: 30, Step: 12800, Train Loss: 6.7176553013211855, Valid Loss: 8.837913199501916\n",
            "Epoch: 30, Step: 12900, Train Loss: 6.713116614557328, Valid Loss: 8.850212266779833\n",
            "Epoch: 31, Step: 13000, Train Loss: 6.793202154542797, Valid Loss: 8.80617034733458\n",
            "Epoch: 31, Step: 13100, Train Loss: 6.6982887428174145, Valid Loss: 8.82462534419229\n",
            "Epoch: 31, Step: 13200, Train Loss: 6.687256581684082, Valid Loss: 8.886034402995453\n",
            "Epoch: 31, Step: 13300, Train Loss: 6.737479178902677, Valid Loss: 8.792965636203762\n",
            "Epoch: 32, Step: 13400, Train Loss: 6.617929311377735, Valid Loss: 8.958666855861377\n",
            "Epoch: 32, Step: 13500, Train Loss: 6.633365021526915, Valid Loss: 8.94502395577817\n",
            "Epoch: 32, Step: 13600, Train Loss: 6.714702040819332, Valid Loss: 9.132099127679883\n",
            "Epoch: 32, Step: 13700, Train Loss: 6.604059755761462, Valid Loss: 8.87469520831269\n",
            "Epoch: 33, Step: 13800, Train Loss: 6.584714909367304, Valid Loss: 9.02761784273767\n",
            "Epoch: 33, Step: 13900, Train Loss: 6.569946431893255, Valid Loss: 8.922805398443803\n",
            "Epoch: 33, Step: 14000, Train Loss: 6.714738371260799, Valid Loss: 8.847750860515651\n",
            "Epoch: 33, Step: 14100, Train Loss: 6.560874019290543, Valid Loss: 8.971584710443311\n",
            "Epoch: 33, Step: 14200, Train Loss: 6.662905988652821, Valid Loss: 8.870841989133124\n",
            "Epoch: 34, Step: 14300, Train Loss: 6.5528015375027096, Valid Loss: 9.127046729131589\n",
            "Epoch: 34, Step: 14400, Train Loss: 6.47578120182641, Valid Loss: 8.952262492962907\n",
            "Epoch: 34, Step: 14500, Train Loss: 6.5569159212427195, Valid Loss: 8.88728816220416\n",
            "Epoch: 34, Step: 14600, Train Loss: 6.52932526852141, Valid Loss: 8.973141722902229\n",
            "Epoch: 35, Step: 14700, Train Loss: 6.502170287678011, Valid Loss: 8.90677266534503\n",
            "Epoch: 35, Step: 14800, Train Loss: 6.47516668388299, Valid Loss: 8.967468001033277\n",
            "Epoch: 35, Step: 14900, Train Loss: 6.495985100395024, Valid Loss: 9.019547102053746\n",
            "Epoch: 35, Step: 15000, Train Loss: 6.500043253581293, Valid Loss: 9.028727238354376\n",
            "Epoch: 36, Step: 15100, Train Loss: 6.426269906087997, Valid Loss: 9.072457950016029\n",
            "Epoch: 36, Step: 15200, Train Loss: 6.424836155090348, Valid Loss: 8.972508180712834\n",
            "Epoch: 36, Step: 15300, Train Loss: 6.451603658483187, Valid Loss: 9.129504932861996\n",
            "Epoch: 36, Step: 15400, Train Loss: 6.4150636675643495, Valid Loss: 8.965529675381687\n",
            "Epoch: 37, Step: 15500, Train Loss: 6.37490127057439, Valid Loss: 9.091705968285938\n",
            "Epoch: 37, Step: 15600, Train Loss: 6.448925579503853, Valid Loss: 9.131153440363999\n",
            "Epoch: 37, Step: 15700, Train Loss: 6.393773242958389, Valid Loss: 9.040786560780019\n",
            "Epoch: 37, Step: 15800, Train Loss: 6.376855946565452, Valid Loss: 9.000844995221597\n",
            "Epoch: 38, Step: 15900, Train Loss: 6.508603051897745, Valid Loss: 9.29016904650426\n",
            "Epoch: 38, Step: 16000, Train Loss: 6.388073054344023, Valid Loss: 8.959904761198532\n",
            "Epoch: 38, Step: 16100, Train Loss: 6.4147987496222125, Valid Loss: 9.172363172090389\n",
            "Epoch: 38, Step: 16200, Train Loss: 6.329658147247763, Valid Loss: 9.099549233153065\n",
            "Epoch: 38, Step: 16300, Train Loss: 6.356491983168952, Valid Loss: 9.065966807647525\n",
            "Epoch: 39, Step: 16400, Train Loss: 6.359113280131238, Valid Loss: 8.953130769124943\n",
            "Epoch: 39, Step: 16500, Train Loss: 6.36569544559395, Valid Loss: 9.164963675371107\n",
            "Epoch: 39, Step: 16600, Train Loss: 6.325487894655866, Valid Loss: 9.120180321490299\n",
            "Epoch: 39, Step: 16700, Train Loss: 6.339313752959832, Valid Loss: 9.04029951301455\n",
            "Epoch: 40, Step: 16800, Train Loss: 6.27719609202701, Valid Loss: 9.063710777346374\n",
            "Epoch: 40, Step: 16900, Train Loss: 6.273137412948892, Valid Loss: 9.215911025961859\n",
            "Epoch: 40, Step: 17000, Train Loss: 6.363915747649057, Valid Loss: 9.212060554354808\n",
            "Epoch: 40, Step: 17100, Train Loss: 6.342907207625755, Valid Loss: 9.07923546026145\n",
            "Epoch: 41, Step: 17200, Train Loss: 6.240786306618249, Valid Loss: 9.182312624731328\n",
            "Epoch: 41, Step: 17300, Train Loss: 6.291044734659382, Valid Loss: 8.980912092424587\n",
            "Epoch: 41, Step: 17400, Train Loss: 6.290547115068687, Valid Loss: 9.178938905526772\n",
            "Epoch: 41, Step: 17500, Train Loss: 6.2745605590973845, Valid Loss: 9.020922161732676\n",
            "Epoch: 42, Step: 17600, Train Loss: 6.1906221249078115, Valid Loss: 9.15339287677723\n",
            "Epoch: 42, Step: 17700, Train Loss: 6.177200237506283, Valid Loss: 9.097173582923684\n",
            "Epoch: 42, Step: 17800, Train Loss: 6.1810434326370025, Valid Loss: 9.12989546944218\n",
            "Epoch: 42, Step: 17900, Train Loss: 6.3115998640778415, Valid Loss: 9.033229538823589\n",
            "Epoch: 43, Step: 18000, Train Loss: 6.137032981540743, Valid Loss: 9.195197203327952\n",
            "Epoch: 43, Step: 18100, Train Loss: 6.204097124336887, Valid Loss: 9.095185363347948\n",
            "Epoch: 43, Step: 18200, Train Loss: 6.185217616258668, Valid Loss: 9.111284265018117\n",
            "Epoch: 43, Step: 18300, Train Loss: 6.164837947990664, Valid Loss: 9.0715734064694\n",
            "Epoch: 44, Step: 18400, Train Loss: 6.259904729258498, Valid Loss: 9.02821068503463\n",
            "Epoch: 44, Step: 18500, Train Loss: 6.12855979211258, Valid Loss: 9.18246681183553\n",
            "Epoch: 44, Step: 18600, Train Loss: 6.1739439911900345, Valid Loss: 9.032840835726534\n",
            "Epoch: 44, Step: 18700, Train Loss: 6.176570154657833, Valid Loss: 9.16555085531247\n",
            "Epoch: 44, Step: 18800, Train Loss: 6.188073367083219, Valid Loss: 9.065683141080834\n",
            "Epoch: 45, Step: 18900, Train Loss: 6.206292081328848, Valid Loss: 9.102611214507544\n",
            "Epoch: 45, Step: 19000, Train Loss: 6.178727507873804, Valid Loss: 9.109055847966626\n",
            "Epoch: 45, Step: 19100, Train Loss: 6.129134122936649, Valid Loss: 9.326592565408328\n",
            "Epoch: 45, Step: 19200, Train Loss: 6.098097320732743, Valid Loss: 9.130665619713998\n",
            "Epoch: 46, Step: 19300, Train Loss: 6.177142537481425, Valid Loss: 9.050790105478121\n",
            "Epoch: 46, Step: 19400, Train Loss: 6.179059731419722, Valid Loss: 9.38518117913702\n",
            "Epoch: 46, Step: 19500, Train Loss: 6.036794956290818, Valid Loss: 9.180561976414243\n",
            "Epoch: 46, Step: 19600, Train Loss: 6.227161245460998, Valid Loss: 9.010660493896658\n",
            "Epoch: 47, Step: 19700, Train Loss: 6.019941906952023, Valid Loss: 9.160836706529674\n",
            "Epoch: 47, Step: 19800, Train Loss: 6.019367299809972, Valid Loss: 9.223945892293777\n",
            "Epoch: 47, Step: 19900, Train Loss: 6.095554848707748, Valid Loss: 9.16372798887105\n",
            "Epoch: 47, Step: 20000, Train Loss: 6.059475765621793, Valid Loss: 9.18431137717015\n",
            "Epoch: 48, Step: 20100, Train Loss: 6.152463315423116, Valid Loss: 9.142904493178348\n",
            "Epoch: 48, Step: 20200, Train Loss: 5.989313584275009, Valid Loss: 9.264125478015735\n",
            "Epoch: 48, Step: 20300, Train Loss: 5.9962281916358435, Valid Loss: 9.232085234691224\n",
            "Epoch: 48, Step: 20400, Train Loss: 6.061160646912045, Valid Loss: 9.255024748075503\n",
            "Epoch: 49, Step: 20500, Train Loss: 6.019673380024216, Valid Loss: 9.105181106881526\n",
            "Epoch: 49, Step: 20600, Train Loss: 5.973494554225356, Valid Loss: 9.194988169774126\n",
            "Epoch: 49, Step: 20700, Train Loss: 6.057753462054837, Valid Loss: 9.293010092082685\n",
            "Epoch: 49, Step: 20800, Train Loss: 6.092639400367759, Valid Loss: 9.490970636309434\n",
            "Epoch: 49, Step: 20900, Train Loss: 6.008431605713667, Valid Loss: 9.24189069321293\n",
            "Epoch: 50, Step: 21000, Train Loss: 6.032738038114097, Valid Loss: 9.440994112045484\n",
            "Epoch: 50, Step: 21100, Train Loss: 6.011951667629421, Valid Loss: 9.129125572548034\n",
            "Epoch: 50, Step: 21200, Train Loss: 5.944226359554215, Valid Loss: 9.337422334816305\n",
            "Epoch: 50, Step: 21300, Train Loss: 6.0279135258966985, Valid Loss: 9.101483923551545\n",
            "Epoch: 51, Step: 21400, Train Loss: 5.901041043540135, Valid Loss: 9.166207482535574\n",
            "Epoch: 51, Step: 21500, Train Loss: 6.136344831663798, Valid Loss: 9.534137621545327\n",
            "Epoch: 51, Step: 21600, Train Loss: 5.958392898007484, Valid Loss: 9.17401164093717\n",
            "Epoch: 51, Step: 21700, Train Loss: 5.957424839256038, Valid Loss: 9.235202214295583\n",
            "Epoch: 52, Step: 21800, Train Loss: 5.866819053803274, Valid Loss: 9.263169049882647\n",
            "Epoch: 52, Step: 21900, Train Loss: 6.253920560200015, Valid Loss: 9.119000344184194\n",
            "Epoch: 52, Step: 22000, Train Loss: 5.92753100413171, Valid Loss: 9.273446241343137\n",
            "Epoch: 52, Step: 22100, Train Loss: 5.913218500059662, Valid Loss: 9.189986446786822\n",
            "Epoch: 53, Step: 22200, Train Loss: 5.865599623158185, Valid Loss: 9.223089155116002\n",
            "Epoch: 53, Step: 22300, Train Loss: 5.901989801908916, Valid Loss: 9.137121387702889\n",
            "Epoch: 53, Step: 22400, Train Loss: 5.899148133572153, Valid Loss: 9.315117210431355\n",
            "Epoch: 53, Step: 22500, Train Loss: 5.891324301347945, Valid Loss: 9.166224622200193\n",
            "Epoch: 54, Step: 22600, Train Loss: 5.831821588457574, Valid Loss: 9.24572808350405\n",
            "Epoch: 54, Step: 22700, Train Loss: 5.828819804511795, Valid Loss: 9.310075331351134\n",
            "Epoch: 54, Step: 22800, Train Loss: 5.821798522297282, Valid Loss: 9.337515492037369\n",
            "Epoch: 54, Step: 22900, Train Loss: 5.829552298896048, Valid Loss: 9.307170576479363\n",
            "Epoch: 55, Step: 23000, Train Loss: 5.8580328895772835, Valid Loss: 9.32749268065993\n",
            "Epoch: 55, Step: 23100, Train Loss: 5.855496931938058, Valid Loss: 9.393639945396467\n",
            "Epoch: 55, Step: 23200, Train Loss: 5.848198593476219, Valid Loss: 9.24787436755678\n",
            "Epoch: 55, Step: 23300, Train Loss: 5.991559854000017, Valid Loss: 9.577484448590114\n",
            "Epoch: 55, Step: 23400, Train Loss: 5.846987018383428, Valid Loss: 9.228979422037533\n",
            "Epoch: 56, Step: 23500, Train Loss: 5.705574173589234, Valid Loss: 9.299147899781097\n",
            "Epoch: 56, Step: 23600, Train Loss: 5.729596982730872, Valid Loss: 9.344347528355469\n",
            "Epoch: 56, Step: 23700, Train Loss: 5.809216725416687, Valid Loss: 9.2244422293488\n",
            "Epoch: 56, Step: 23800, Train Loss: 5.810265979025542, Valid Loss: 9.25554845222035\n",
            "Epoch: 57, Step: 23900, Train Loss: 5.750137885067669, Valid Loss: 9.4557931671487\n",
            "Epoch: 57, Step: 24000, Train Loss: 5.83263930073436, Valid Loss: 9.239756138628675\n",
            "Epoch: 57, Step: 24100, Train Loss: 5.8188514948993255, Valid Loss: 9.264753028890853\n",
            "Epoch: 57, Step: 24200, Train Loss: 5.749801825118338, Valid Loss: 9.366651944873022\n",
            "Epoch: 58, Step: 24300, Train Loss: 5.722720175963233, Valid Loss: 9.428764167557906\n",
            "Epoch: 58, Step: 24400, Train Loss: 5.788541897339009, Valid Loss: 9.19756193720396\n",
            "Epoch: 58, Step: 24500, Train Loss: 5.7662016779746725, Valid Loss: 9.499322101785506\n",
            "Epoch: 58, Step: 24600, Train Loss: 5.78572839711602, Valid Loss: 9.200774057481837\n",
            "Epoch: 59, Step: 24700, Train Loss: 5.682799519744091, Valid Loss: 9.307267032338778\n",
            "Epoch: 59, Step: 24800, Train Loss: 5.684217347992611, Valid Loss: 9.319859091903123\n",
            "Epoch: 59, Step: 24900, Train Loss: 5.829738571144922, Valid Loss: 9.212543420350038\n",
            "Epoch: 59, Step: 25000, Train Loss: 5.7543078956710625, Valid Loss: 9.583685125355691\n",
            "Epoch: 60, Step: 25100, Train Loss: 5.684893468738796, Valid Loss: 9.426335631189481\n",
            "Epoch: 60, Step: 25200, Train Loss: 5.781533525920361, Valid Loss: 9.284319716650169\n",
            "Epoch: 60, Step: 25300, Train Loss: 5.9222679726619685, Valid Loss: 9.736410917397484\n",
            "Epoch: 60, Step: 25400, Train Loss: 5.6748925029592465, Valid Loss: 9.402121273011604\n",
            "Epoch: 61, Step: 25500, Train Loss: 5.739696112523961, Valid Loss: 9.272972805836712\n",
            "Epoch: 61, Step: 25600, Train Loss: 5.613791114880246, Valid Loss: 9.296873273028078\n",
            "Epoch: 61, Step: 25700, Train Loss: 5.711463157989699, Valid Loss: 9.251290395655662\n",
            "Epoch: 61, Step: 25800, Train Loss: 5.658951531940934, Valid Loss: 9.435598647540234\n",
            "Epoch: 61, Step: 25900, Train Loss: 5.769885070597522, Valid Loss: 9.568703476105615\n",
            "Epoch: 62, Step: 26000, Train Loss: 5.559605964376237, Valid Loss: 9.368652290214538\n",
            "Epoch: 62, Step: 26100, Train Loss: 5.595412546195353, Valid Loss: 9.284563505213415\n",
            "Epoch: 62, Step: 26200, Train Loss: 5.636410705942968, Valid Loss: 9.453547816938862\n",
            "Epoch: 62, Step: 26300, Train Loss: 5.79406949000955, Valid Loss: 9.634449805327847\n",
            "Epoch: 63, Step: 26400, Train Loss: 5.558980452597554, Valid Loss: 9.457103556649262\n",
            "Epoch: 63, Step: 26500, Train Loss: 5.538577082881345, Valid Loss: 9.330488963279171\n",
            "Epoch: 63, Step: 26600, Train Loss: 5.55551556717693, Valid Loss: 9.462764312654432\n",
            "Epoch: 63, Step: 26700, Train Loss: 5.699021791372972, Valid Loss: 9.561105148102238\n",
            "Epoch: 64, Step: 26800, Train Loss: 5.623515206978945, Valid Loss: 9.653512535682637\n",
            "Epoch: 64, Step: 26900, Train Loss: 5.600568938590587, Valid Loss: 9.414769735243876\n",
            "Epoch: 64, Step: 27000, Train Loss: 5.556973051606993, Valid Loss: 9.43554426819619\n",
            "Epoch: 64, Step: 27100, Train Loss: 5.642254539909281, Valid Loss: 9.265156966976404\n",
            "Epoch: 65, Step: 27200, Train Loss: 5.568821525196168, Valid Loss: 9.297934961298822\n",
            "Epoch: 65, Step: 27300, Train Loss: 5.575648011874228, Valid Loss: 9.526570095912971\n",
            "Epoch: 65, Step: 27400, Train Loss: 5.582568349260299, Valid Loss: 9.572920760373025\n",
            "Epoch: 65, Step: 27500, Train Loss: 5.536732489051293, Valid Loss: 9.326380194843921\n",
            "Epoch: 66, Step: 27600, Train Loss: 5.563476106671729, Valid Loss: 9.36967546819063\n",
            "Epoch: 66, Step: 27700, Train Loss: 5.527338187096788, Valid Loss: 9.477336793707664\n",
            "Epoch: 66, Step: 27800, Train Loss: 5.530281341178879, Valid Loss: 9.529148622736164\n",
            "Epoch: 66, Step: 27900, Train Loss: 5.5406461709669745, Valid Loss: 9.372339905327607\n",
            "Epoch: 66, Step: 28000, Train Loss: 5.58833585517324, Valid Loss: 9.512351482855257\n",
            "Epoch: 67, Step: 28100, Train Loss: 5.52366854623299, Valid Loss: 9.326916549348013\n",
            "Epoch: 67, Step: 28200, Train Loss: 5.597022872895704, Valid Loss: 9.582968278193857\n",
            "Epoch: 67, Step: 28300, Train Loss: 5.5030714806415935, Valid Loss: 9.421514863680992\n",
            "Epoch: 67, Step: 28400, Train Loss: 5.560523493097367, Valid Loss: 9.5113299031623\n",
            "Epoch: 68, Step: 28500, Train Loss: 5.477054648641261, Valid Loss: 9.370129835719228\n",
            "Epoch: 68, Step: 28600, Train Loss: 5.496720305294853, Valid Loss: 9.45227648398731\n",
            "Epoch: 68, Step: 28700, Train Loss: 5.6040991945531, Valid Loss: 9.260550136402308\n",
            "Epoch: 68, Step: 28800, Train Loss: 5.510941848025114, Valid Loss: 9.34356418873541\n",
            "Epoch: 69, Step: 28900, Train Loss: 5.4400260563649665, Valid Loss: 9.381643531501995\n",
            "Epoch: 69, Step: 29000, Train Loss: 5.45826705253224, Valid Loss: 9.47669213957247\n",
            "Epoch: 69, Step: 29100, Train Loss: 5.433374209775056, Valid Loss: 9.516835501542658\n",
            "Epoch: 69, Step: 29200, Train Loss: 5.493477772134552, Valid Loss: 9.572665850940071\n",
            "Epoch: 70, Step: 29300, Train Loss: 5.409079838551075, Valid Loss: 9.531002762574769\n",
            "Epoch: 70, Step: 29400, Train Loss: 5.509585872103476, Valid Loss: 9.594071405913636\n",
            "Epoch: 70, Step: 29500, Train Loss: 5.488769798945999, Valid Loss: 9.44958430785324\n",
            "Epoch: 70, Step: 29600, Train Loss: 5.493894397717846, Valid Loss: 9.41976406179817\n",
            "Epoch: 71, Step: 29700, Train Loss: 5.487288352670763, Valid Loss: 9.29973932741578\n",
            "Epoch: 71, Step: 29800, Train Loss: 5.446208068963316, Valid Loss: 9.660777900262653\n",
            "Epoch: 71, Step: 29900, Train Loss: 5.415691556753455, Valid Loss: 9.391689821574563\n",
            "Epoch: 71, Step: 30000, Train Loss: 5.412546982335821, Valid Loss: 9.377797887652118\n",
            "Epoch: 72, Step: 30100, Train Loss: 5.484524220351348, Valid Loss: 9.69639210193653\n",
            "Epoch: 72, Step: 30200, Train Loss: 5.391121554242172, Valid Loss: 9.597482509216022\n",
            "Epoch: 72, Step: 30300, Train Loss: 5.5673142131068545, Valid Loss: 9.871997988268088\n",
            "Epoch: 72, Step: 30400, Train Loss: 5.48291810591476, Valid Loss: 9.67659564446372\n",
            "Epoch: 72, Step: 30500, Train Loss: 5.437610730074792, Valid Loss: 9.534539112412903\n",
            "Epoch: 73, Step: 30600, Train Loss: 5.356964378014865, Valid Loss: 9.525634571202607\n",
            "Epoch: 73, Step: 30700, Train Loss: 5.379700402483939, Valid Loss: 9.736692032937109\n",
            "Epoch: 73, Step: 30800, Train Loss: 5.434231033464657, Valid Loss: 9.27571724521265\n",
            "Epoch: 73, Step: 30900, Train Loss: 5.38417346984835, Valid Loss: 9.438766774667764\n",
            "Epoch: 74, Step: 31000, Train Loss: 5.321227962450133, Valid Loss: 9.50330545825679\n",
            "Epoch: 74, Step: 31100, Train Loss: 5.340363384566062, Valid Loss: 9.565341770991882\n",
            "Epoch: 74, Step: 31200, Train Loss: 5.351822624094724, Valid Loss: 9.453500014605586\n",
            "Epoch: 74, Step: 31300, Train Loss: 5.331475831824909, Valid Loss: 9.533534198223682\n",
            "Epoch: 75, Step: 31400, Train Loss: 5.341309817262563, Valid Loss: 9.328602435368673\n",
            "Epoch: 75, Step: 31500, Train Loss: 5.31463019995839, Valid Loss: 9.46099933871972\n",
            "Epoch: 75, Step: 31600, Train Loss: 5.381575364336604, Valid Loss: 9.51660352303237\n",
            "Epoch: 75, Step: 31700, Train Loss: 5.418973912161847, Valid Loss: 9.315643163169561\n",
            "Epoch: 76, Step: 31800, Train Loss: 5.307491788599744, Valid Loss: 9.4989415502534\n",
            "Epoch: 76, Step: 31900, Train Loss: 5.2918095972965995, Valid Loss: 9.50065296780904\n",
            "Epoch: 76, Step: 32000, Train Loss: 5.357705168860008, Valid Loss: 9.335285794559208\n",
            "Epoch: 76, Step: 32100, Train Loss: 5.3353349370299785, Valid Loss: 9.358769344792131\n",
            "Epoch: 77, Step: 32200, Train Loss: 5.27668345897479, Valid Loss: 9.560418931931954\n",
            "Epoch: 77, Step: 32300, Train Loss: 5.335544797329395, Valid Loss: 9.446995168135915\n",
            "Epoch: 77, Step: 32400, Train Loss: 5.39157021653897, Valid Loss: 9.750539588163479\n",
            "Epoch: 77, Step: 32500, Train Loss: 5.4674741478132045, Valid Loss: 9.364462297777779\n",
            "Epoch: 77, Step: 32600, Train Loss: 5.575074698212277, Valid Loss: 9.309486711695284\n",
            "Epoch: 78, Step: 32700, Train Loss: 5.2910788840796235, Valid Loss: 9.551195451201377\n",
            "Epoch: 78, Step: 32800, Train Loss: 5.320933962206311, Valid Loss: 9.665825768748583\n",
            "Epoch: 78, Step: 32900, Train Loss: 5.2849650677436815, Valid Loss: 9.614148062478055\n",
            "Epoch: 78, Step: 33000, Train Loss: 5.282026412504418, Valid Loss: 9.501541562997707\n",
            "Epoch: 79, Step: 33100, Train Loss: 5.247857519049272, Valid Loss: 9.575557571463909\n",
            "Epoch: 79, Step: 33200, Train Loss: 5.263285847401994, Valid Loss: 9.561073676587684\n",
            "Epoch: 79, Step: 33300, Train Loss: 5.342564380908806, Valid Loss: 9.614570922162917\n",
            "Epoch: 79, Step: 33400, Train Loss: 5.292472565700753, Valid Loss: 9.48385395384423\n",
            "Epoch: 80, Step: 33500, Train Loss: 5.296172621248809, Valid Loss: 9.65098822621531\n",
            "Epoch: 80, Step: 33600, Train Loss: 5.311588225428105, Valid Loss: 9.382445309570839\n",
            "Epoch: 80, Step: 33700, Train Loss: 5.264546404141907, Valid Loss: 9.576273222833787\n",
            "Epoch: 80, Step: 33800, Train Loss: 5.273043363289139, Valid Loss: 9.617177166098042\n",
            "Epoch: 81, Step: 33900, Train Loss: 5.186178549782849, Valid Loss: 9.610729871190147\n",
            "Epoch: 81, Step: 34000, Train Loss: 5.19038877321375, Valid Loss: 9.675088096515498\n",
            "Epoch: 81, Step: 34100, Train Loss: 5.260715129489871, Valid Loss: 9.411890124892066\n",
            "Epoch: 81, Step: 34200, Train Loss: 5.243521558051722, Valid Loss: 9.437037303045082\n",
            "Epoch: 82, Step: 34300, Train Loss: 5.239416989814266, Valid Loss: 9.703175213682021\n",
            "Epoch: 82, Step: 34400, Train Loss: 5.242896593077216, Valid Loss: 9.41388213948334\n",
            "Epoch: 82, Step: 34500, Train Loss: 5.23555052044403, Valid Loss: 9.594507102123217\n",
            "Epoch: 82, Step: 34600, Train Loss: 5.208504774986539, Valid Loss: 9.589947875875927\n",
            "Epoch: 83, Step: 34700, Train Loss: 5.1993403909554035, Valid Loss: 9.539481541128954\n",
            "Epoch: 83, Step: 34800, Train Loss: 5.233937622175926, Valid Loss: 9.754079743244905\n",
            "Epoch: 83, Step: 34900, Train Loss: 5.197332092342337, Valid Loss: 9.589871442911507\n",
            "Epoch: 83, Step: 35000, Train Loss: 5.1965284567150904, Valid Loss: 9.602197948714235\n",
            "Epoch: 83, Step: 35100, Train Loss: 5.3821407497231615, Valid Loss: 9.888731127804444\n",
            "Epoch: 84, Step: 35200, Train Loss: 5.161234454843876, Valid Loss: 9.508979272660751\n",
            "Epoch: 84, Step: 35300, Train Loss: 5.206526286853467, Valid Loss: 9.611687279921778\n",
            "Epoch: 84, Step: 35400, Train Loss: 5.192983344302201, Valid Loss: 9.586667054618632\n",
            "Epoch: 84, Step: 35500, Train Loss: 5.2036890756112, Valid Loss: 9.483186253911974\n",
            "Epoch: 85, Step: 35600, Train Loss: 5.1581429654390725, Valid Loss: 9.57915622998594\n",
            "Epoch: 85, Step: 35700, Train Loss: 5.199518477925465, Valid Loss: 9.444806712910538\n",
            "Epoch: 85, Step: 35800, Train Loss: 5.417907348600444, Valid Loss: 9.319455552026914\n",
            "Epoch: 85, Step: 35900, Train Loss: 5.405630571183065, Valid Loss: 9.941745492546644\n",
            "Epoch: 86, Step: 36000, Train Loss: 5.157860922187187, Valid Loss: 9.587198993760984\n",
            "Epoch: 86, Step: 36100, Train Loss: 5.140565092931951, Valid Loss: 9.643329951626395\n",
            "Epoch: 86, Step: 36200, Train Loss: 5.150534808559608, Valid Loss: 9.607891584474144\n",
            "Epoch: 86, Step: 36300, Train Loss: 5.169539019193175, Valid Loss: 9.61636489053815\n",
            "Epoch: 87, Step: 36400, Train Loss: 5.201852699380506, Valid Loss: 9.408553286634179\n",
            "Epoch: 87, Step: 36500, Train Loss: 5.143349394702539, Valid Loss: 9.589503791949962\n",
            "Epoch: 87, Step: 36600, Train Loss: 5.1336568087367285, Valid Loss: 9.62375771078397\n",
            "Epoch: 87, Step: 36700, Train Loss: 5.266400716260203, Valid Loss: 9.745288165050836\n",
            "Epoch: 88, Step: 36800, Train Loss: 5.145220876267943, Valid Loss: 9.668658769188434\n",
            "Epoch: 88, Step: 36900, Train Loss: 5.102408649542936, Valid Loss: 9.581656958105349\n",
            "Epoch: 88, Step: 37000, Train Loss: 5.1112495088416905, Valid Loss: 9.642779216587165\n",
            "Epoch: 88, Step: 37100, Train Loss: 5.122067982964151, Valid Loss: 9.593113117208642\n",
            "Epoch: 88, Step: 37200, Train Loss: 5.126040504284614, Valid Loss: 9.614260283204064\n",
            "Epoch: 89, Step: 37300, Train Loss: 5.090553215853871, Valid Loss: 9.602436822893749\n",
            "Epoch: 89, Step: 37400, Train Loss: 5.174054056465422, Valid Loss: 9.476059367085771\n",
            "Epoch: 89, Step: 37500, Train Loss: 5.11797934797084, Valid Loss: 9.544897883489993\n",
            "Epoch: 89, Step: 37600, Train Loss: 5.141332410626643, Valid Loss: 9.637788510940698\n",
            "Epoch: 90, Step: 37700, Train Loss: 5.1766876580279275, Valid Loss: 9.40947329435814\n",
            "Epoch: 90, Step: 37800, Train Loss: 5.068461277973624, Valid Loss: 9.664780605142196\n",
            "Epoch: 90, Step: 37900, Train Loss: 5.100421527885631, Valid Loss: 9.582528686029509\n",
            "Epoch: 90, Step: 38000, Train Loss: 5.098615913745476, Valid Loss: 9.564224593782509\n",
            "Epoch: 91, Step: 38100, Train Loss: 5.083379524070305, Valid Loss: 9.475653904857458\n",
            "Epoch: 91, Step: 38200, Train Loss: 5.137987115252065, Valid Loss: 9.513817050848962\n",
            "Epoch: 91, Step: 38300, Train Loss: 5.197011522433542, Valid Loss: 9.389390139690152\n",
            "Epoch: 91, Step: 38400, Train Loss: 5.099274960055084, Valid Loss: 9.537011127226117\n",
            "Epoch: 92, Step: 38500, Train Loss: 5.15775973303262, Valid Loss: 9.80104294008488\n",
            "Epoch: 92, Step: 38600, Train Loss: 5.048834072562177, Valid Loss: 9.523844669729542\n",
            "Epoch: 92, Step: 38700, Train Loss: 5.081386655293006, Valid Loss: 9.653276485249766\n",
            "Epoch: 92, Step: 38800, Train Loss: 5.259069007743143, Valid Loss: 9.919166967783234\n",
            "Epoch: 93, Step: 38900, Train Loss: 5.091239232563586, Valid Loss: 9.753289101934719\n",
            "Epoch: 93, Step: 39000, Train Loss: 5.0309007302418465, Valid Loss: 9.522150559915046\n",
            "Epoch: 93, Step: 39100, Train Loss: 5.075549115216782, Valid Loss: 9.674437659163846\n",
            "Epoch: 93, Step: 39200, Train Loss: 5.030289089710466, Valid Loss: 9.628662005077178\n",
            "Epoch: 94, Step: 39300, Train Loss: 5.055262589283269, Valid Loss: 9.636982909267267\n",
            "Epoch: 94, Step: 39400, Train Loss: 5.018324275426008, Valid Loss: 9.604751232219849\n",
            "Epoch: 94, Step: 39500, Train Loss: 5.084824339610389, Valid Loss: 9.834782459841556\n",
            "Epoch: 94, Step: 39600, Train Loss: 5.076110691616357, Valid Loss: 9.486045164967909\n",
            "Epoch: 94, Step: 39700, Train Loss: 5.089884607745866, Valid Loss: 9.652659558324181\n",
            "Epoch: 95, Step: 39800, Train Loss: 5.045534893147098, Valid Loss: 9.539912730387782\n",
            "Epoch: 95, Step: 39900, Train Loss: 5.040711428576643, Valid Loss: 9.699638942707834\n",
            "Epoch: 95, Step: 40000, Train Loss: 5.030767908922449, Valid Loss: 9.66841835531268\n",
            "Epoch: 95, Step: 40100, Train Loss: 5.083934969652218, Valid Loss: 9.777086535702754\n",
            "Epoch: 96, Step: 40200, Train Loss: 5.04795493545999, Valid Loss: 9.786158069920392\n",
            "Epoch: 96, Step: 40300, Train Loss: 5.0527112092227995, Valid Loss: 9.70956796245028\n",
            "Epoch: 96, Step: 40400, Train Loss: 5.067119406571136, Valid Loss: 9.544557980840404\n",
            "Epoch: 96, Step: 40500, Train Loss: 5.034703878326116, Valid Loss: 9.652465072268122\n",
            "Epoch: 97, Step: 40600, Train Loss: 5.071473565781064, Valid Loss: 9.495255792091944\n",
            "Epoch: 97, Step: 40700, Train Loss: 5.054656779906904, Valid Loss: 9.507347543014419\n",
            "Epoch: 97, Step: 40800, Train Loss: 5.021601198374889, Valid Loss: 9.554974383871613\n",
            "Epoch: 97, Step: 40900, Train Loss: 5.095906553539486, Valid Loss: 9.471278296976623\n",
            "Epoch: 98, Step: 41000, Train Loss: 5.016410906901956, Valid Loss: 9.574982668114204\n",
            "Epoch: 98, Step: 41100, Train Loss: 5.093799392780117, Valid Loss: 9.525045016015454\n",
            "Epoch: 98, Step: 41200, Train Loss: 5.022600803094392, Valid Loss: 9.606525440869557\n",
            "Epoch: 98, Step: 41300, Train Loss: 4.987533674693423, Valid Loss: 9.697927637103025\n",
            "Epoch: 99, Step: 41400, Train Loss: 4.946135416436539, Valid Loss: 9.640149768786179\n",
            "Epoch: 99, Step: 41500, Train Loss: 5.044626096204154, Valid Loss: 9.89175267491025\n",
            "Epoch: 99, Step: 41600, Train Loss: 4.987983242556829, Valid Loss: 9.818224421592243\n",
            "Epoch: 99, Step: 41700, Train Loss: 4.9939374801709375, Valid Loss: 9.554772485136088\n",
            "Epoch: 99, Step: 41800, Train Loss: 5.046251896834653, Valid Loss: 9.555606049725785\n",
            "Test RMSE Loss for h = 180, wd = 0.2: 8.854007896971186\n",
            "Start training for h = 180, wd = 0.4\n",
            "Epoch: 0, Step: 100, Train Loss: 9.306956755091415, Valid Loss: 9.262774317280858\n",
            "Epoch: 0, Step: 200, Train Loss: 8.974530187914148, Valid Loss: 8.923656705069948\n",
            "Epoch: 0, Step: 300, Train Loss: 8.830196431389625, Valid Loss: 8.78594476088964\n",
            "Epoch: 0, Step: 400, Train Loss: 8.754595033426197, Valid Loss: 8.717043700163506\n",
            "Epoch: 1, Step: 500, Train Loss: 8.864330782520934, Valid Loss: 8.830692566614518\n",
            "Epoch: 1, Step: 600, Train Loss: 8.791236956048245, Valid Loss: 8.768926681045153\n",
            "Epoch: 1, Step: 700, Train Loss: 8.685240164353067, Valid Loss: 8.678916122511044\n",
            "Epoch: 1, Step: 800, Train Loss: 8.634496803123293, Valid Loss: 8.634942766713765\n",
            "Epoch: 2, Step: 900, Train Loss: 8.833999027853977, Valid Loss: 8.840031453300211\n",
            "Epoch: 2, Step: 1000, Train Loss: 8.580683041856826, Valid Loss: 8.606819150562803\n",
            "Epoch: 2, Step: 1100, Train Loss: 8.594600840866862, Valid Loss: 8.626062543294921\n",
            "Epoch: 2, Step: 1200, Train Loss: 8.559852000125712, Valid Loss: 8.599667604515997\n",
            "Epoch: 3, Step: 1300, Train Loss: 8.756943259125647, Valid Loss: 8.81903102609282\n",
            "Epoch: 3, Step: 1400, Train Loss: 8.476480126040771, Valid Loss: 8.539742358006647\n",
            "Epoch: 3, Step: 1500, Train Loss: 8.526534107591683, Valid Loss: 8.626387762467798\n",
            "Epoch: 3, Step: 1600, Train Loss: 8.431919740756047, Valid Loss: 8.537695734528866\n",
            "Epoch: 4, Step: 1700, Train Loss: 8.63923592797594, Valid Loss: 8.737540324560118\n",
            "Epoch: 4, Step: 1800, Train Loss: 8.397511665921481, Valid Loss: 8.532713243580174\n",
            "Epoch: 4, Step: 1900, Train Loss: 8.420363437283266, Valid Loss: 8.564578000766875\n",
            "Epoch: 4, Step: 2000, Train Loss: 8.43039500743999, Valid Loss: 8.576395506143951\n",
            "Epoch: 5, Step: 2100, Train Loss: 8.375356559416339, Valid Loss: 8.562226436315335\n",
            "Epoch: 5, Step: 2200, Train Loss: 8.495281079457511, Valid Loss: 8.63816989266581\n",
            "Epoch: 5, Step: 2300, Train Loss: 8.518608060608749, Valid Loss: 8.690527056384644\n",
            "Epoch: 5, Step: 2400, Train Loss: 8.368593373901357, Valid Loss: 8.59068899607112\n",
            "Epoch: 5, Step: 2500, Train Loss: 8.28671894128516, Valid Loss: 8.539251436745186\n",
            "Epoch: 6, Step: 2600, Train Loss: 8.318032166031966, Valid Loss: 8.53659064685961\n",
            "Epoch: 6, Step: 2700, Train Loss: 8.212274683817371, Valid Loss: 8.480484693344915\n",
            "Epoch: 6, Step: 2800, Train Loss: 8.299876716082107, Valid Loss: 8.606065229541315\n",
            "Epoch: 6, Step: 2900, Train Loss: 8.224117333108445, Valid Loss: 8.523425749220474\n",
            "Epoch: 7, Step: 3000, Train Loss: 8.220775204481859, Valid Loss: 8.520303476577118\n",
            "Epoch: 7, Step: 3100, Train Loss: 8.176892061514572, Valid Loss: 8.506564188431577\n",
            "Epoch: 7, Step: 3200, Train Loss: 8.32491825322879, Valid Loss: 8.563005075199703\n",
            "Epoch: 7, Step: 3300, Train Loss: 8.13161497366362, Valid Loss: 8.497392871644015\n",
            "Epoch: 8, Step: 3400, Train Loss: 8.106676748788468, Valid Loss: 8.514068798925182\n",
            "Epoch: 8, Step: 3500, Train Loss: 8.127892159847349, Valid Loss: 8.571099631598234\n",
            "Epoch: 8, Step: 3600, Train Loss: 8.090635069626854, Valid Loss: 8.500208677440344\n",
            "Epoch: 8, Step: 3700, Train Loss: 8.14234042605016, Valid Loss: 8.507607627878528\n",
            "Epoch: 9, Step: 3800, Train Loss: 8.041191614817722, Valid Loss: 8.522041469125677\n",
            "Epoch: 9, Step: 3900, Train Loss: 8.066620964103288, Valid Loss: 8.5494491250313\n",
            "Epoch: 9, Step: 4000, Train Loss: 8.09465581226342, Valid Loss: 8.545824527811636\n",
            "Epoch: 9, Step: 4100, Train Loss: 8.041018212546971, Valid Loss: 8.588365429057443\n",
            "Epoch: 10, Step: 4200, Train Loss: 7.993653034653135, Valid Loss: 8.485428749217256\n",
            "Epoch: 10, Step: 4300, Train Loss: 7.975866429906396, Valid Loss: 8.507037738555937\n",
            "Epoch: 10, Step: 4400, Train Loss: 8.006727868237236, Valid Loss: 8.573892707142425\n",
            "Epoch: 10, Step: 4500, Train Loss: 8.141148804848019, Valid Loss: 8.667967978390568\n",
            "Epoch: 11, Step: 4600, Train Loss: 7.942656002491972, Valid Loss: 8.493482253090992\n",
            "Epoch: 11, Step: 4700, Train Loss: 7.918005352722705, Valid Loss: 8.552383012736268\n",
            "Epoch: 11, Step: 4800, Train Loss: 7.883653310166998, Valid Loss: 8.5261777455935\n",
            "Epoch: 11, Step: 4900, Train Loss: 7.9030095973358945, Valid Loss: 8.574474589282218\n",
            "Epoch: 11, Step: 5000, Train Loss: 7.910057109059736, Valid Loss: 8.550299868398222\n",
            "Epoch: 12, Step: 5100, Train Loss: 7.894166734424302, Valid Loss: 8.589642701295434\n",
            "Epoch: 12, Step: 5200, Train Loss: 7.831508105915317, Valid Loss: 8.524271379152308\n",
            "Epoch: 12, Step: 5300, Train Loss: 7.8399170396353295, Valid Loss: 8.57769063976638\n",
            "Epoch: 12, Step: 5400, Train Loss: 7.765148085754409, Valid Loss: 8.516353540356949\n",
            "Epoch: 13, Step: 5500, Train Loss: 7.782740183428884, Valid Loss: 8.540467078865472\n",
            "Epoch: 13, Step: 5600, Train Loss: 7.741080818485612, Valid Loss: 8.531086801745765\n",
            "Epoch: 13, Step: 5700, Train Loss: 7.790527136125275, Valid Loss: 8.573107930829849\n",
            "Epoch: 13, Step: 5800, Train Loss: 7.786354877878974, Valid Loss: 8.630462934216249\n",
            "Epoch: 14, Step: 5900, Train Loss: 7.713479646844725, Valid Loss: 8.580896367323705\n",
            "Epoch: 14, Step: 6000, Train Loss: 7.717419959582806, Valid Loss: 8.554687973662055\n",
            "Epoch: 14, Step: 6100, Train Loss: 7.659525095804154, Valid Loss: 8.591750772439903\n",
            "Epoch: 14, Step: 6200, Train Loss: 7.693078686440402, Valid Loss: 8.575522218993045\n",
            "Epoch: 15, Step: 6300, Train Loss: 7.664208632000406, Valid Loss: 8.604530726771129\n",
            "Epoch: 15, Step: 6400, Train Loss: 7.571030313277551, Valid Loss: 8.584671831167412\n",
            "Epoch: 15, Step: 6500, Train Loss: 7.633414071523917, Valid Loss: 8.554750153099528\n",
            "Epoch: 15, Step: 6600, Train Loss: 7.680680959428867, Valid Loss: 8.654836559773752\n",
            "Epoch: 16, Step: 6700, Train Loss: 7.530208603867403, Valid Loss: 8.574161182724753\n",
            "Epoch: 16, Step: 6800, Train Loss: 7.627491304880559, Valid Loss: 8.55642581942452\n",
            "Epoch: 16, Step: 6900, Train Loss: 7.568168234487543, Valid Loss: 8.648335508255046\n",
            "Epoch: 16, Step: 7000, Train Loss: 7.543215371826973, Valid Loss: 8.598584344029764\n",
            "Epoch: 16, Step: 7100, Train Loss: 7.526435969252523, Valid Loss: 8.62464527540346\n",
            "Epoch: 17, Step: 7200, Train Loss: 7.513774216033526, Valid Loss: 8.609228456708069\n",
            "Epoch: 17, Step: 7300, Train Loss: 7.647735951519843, Valid Loss: 8.623798921145294\n",
            "Epoch: 17, Step: 7400, Train Loss: 7.477661015552306, Valid Loss: 8.62025211694393\n",
            "Epoch: 17, Step: 7500, Train Loss: 7.558046044505078, Valid Loss: 8.568010411163172\n",
            "Epoch: 18, Step: 7600, Train Loss: 7.383825821502937, Valid Loss: 8.615309817533344\n",
            "Epoch: 18, Step: 7700, Train Loss: 7.402698955018779, Valid Loss: 8.605707274826617\n",
            "Epoch: 18, Step: 7800, Train Loss: 7.419997136807491, Valid Loss: 8.633021506499697\n",
            "Epoch: 18, Step: 7900, Train Loss: 7.535362152125593, Valid Loss: 8.812588320340875\n",
            "Epoch: 19, Step: 8000, Train Loss: 7.399376111073391, Valid Loss: 8.631061335063592\n",
            "Epoch: 19, Step: 8100, Train Loss: 7.347510460825293, Valid Loss: 8.716508518986352\n",
            "Epoch: 19, Step: 8200, Train Loss: 7.3489748337772705, Valid Loss: 8.693890718637919\n",
            "Epoch: 19, Step: 8300, Train Loss: 7.421620773330746, Valid Loss: 8.547674677824912\n",
            "Epoch: 20, Step: 8400, Train Loss: 7.289592950200231, Valid Loss: 8.616017136586697\n",
            "Epoch: 20, Step: 8500, Train Loss: 7.603350593523411, Valid Loss: 8.98098398119442\n",
            "Epoch: 20, Step: 8600, Train Loss: 7.254004135611432, Valid Loss: 8.607430638367832\n",
            "Epoch: 20, Step: 8700, Train Loss: 7.294937125333654, Valid Loss: 8.632799255701887\n",
            "Epoch: 21, Step: 8800, Train Loss: 7.240410696270236, Valid Loss: 8.754854686758856\n",
            "Epoch: 21, Step: 8900, Train Loss: 7.258867460681235, Valid Loss: 8.667879892489834\n",
            "Epoch: 21, Step: 9000, Train Loss: 7.262371519234639, Valid Loss: 8.689491757858473\n",
            "Epoch: 21, Step: 9100, Train Loss: 7.236965651883823, Valid Loss: 8.64863939906555\n",
            "Epoch: 22, Step: 9200, Train Loss: 7.192709000207234, Valid Loss: 8.677510052292371\n",
            "Epoch: 22, Step: 9300, Train Loss: 7.210507608315274, Valid Loss: 8.75159666369426\n",
            "Epoch: 22, Step: 9400, Train Loss: 7.176688108778576, Valid Loss: 8.706623547628658\n",
            "Epoch: 22, Step: 9500, Train Loss: 7.2198032712268, Valid Loss: 8.635393273576833\n",
            "Epoch: 22, Step: 9600, Train Loss: 7.1841079568463355, Valid Loss: 8.643263584236177\n",
            "Epoch: 23, Step: 9700, Train Loss: 7.103286987941544, Valid Loss: 8.728486492694921\n",
            "Epoch: 23, Step: 9800, Train Loss: 7.227446599234519, Valid Loss: 8.8143762548645\n",
            "Epoch: 23, Step: 9900, Train Loss: 7.0818971862989315, Valid Loss: 8.707014548595943\n",
            "Epoch: 23, Step: 10000, Train Loss: 7.087309773843998, Valid Loss: 8.676664337692587\n",
            "Epoch: 24, Step: 10100, Train Loss: 7.060313124925933, Valid Loss: 8.728000292962486\n",
            "Epoch: 24, Step: 10200, Train Loss: 7.24523439818613, Valid Loss: 8.984695513256549\n",
            "Epoch: 24, Step: 10300, Train Loss: 7.048715214997328, Valid Loss: 8.66975280121224\n",
            "Epoch: 24, Step: 10400, Train Loss: 7.202594005078519, Valid Loss: 8.925725530177731\n",
            "Epoch: 25, Step: 10500, Train Loss: 7.018801550631224, Valid Loss: 8.74072721944706\n",
            "Epoch: 25, Step: 10600, Train Loss: 7.029850814740315, Valid Loss: 8.749909654332455\n",
            "Epoch: 25, Step: 10700, Train Loss: 7.132711283533401, Valid Loss: 8.667742099612168\n",
            "Epoch: 25, Step: 10800, Train Loss: 7.07026809646262, Valid Loss: 8.750998291581324\n",
            "Epoch: 26, Step: 10900, Train Loss: 6.928113746775292, Valid Loss: 8.72341438795778\n",
            "Epoch: 26, Step: 11000, Train Loss: 7.062337104735073, Valid Loss: 8.716654968348308\n",
            "Epoch: 26, Step: 11100, Train Loss: 6.985175885644645, Valid Loss: 8.943695451955884\n",
            "Epoch: 26, Step: 11200, Train Loss: 6.978577837904641, Valid Loss: 8.763814168736145\n",
            "Epoch: 27, Step: 11300, Train Loss: 6.906044793539051, Valid Loss: 8.76722916306263\n",
            "Epoch: 27, Step: 11400, Train Loss: 6.984676330491995, Valid Loss: 8.778586832074154\n",
            "Epoch: 27, Step: 11500, Train Loss: 6.931250549170366, Valid Loss: 8.719290781161975\n",
            "Epoch: 27, Step: 11600, Train Loss: 6.915743471213774, Valid Loss: 8.719506484771363\n",
            "Epoch: 27, Step: 11700, Train Loss: 7.1047496891692825, Valid Loss: 8.702211887975558\n",
            "Epoch: 28, Step: 11800, Train Loss: 7.048400796342081, Valid Loss: 8.843570343322433\n",
            "Epoch: 28, Step: 11900, Train Loss: 6.822567198755345, Valid Loss: 8.837197472130919\n",
            "Epoch: 28, Step: 12000, Train Loss: 6.842506380027365, Valid Loss: 8.84228625863005\n",
            "Epoch: 28, Step: 12100, Train Loss: 6.812616917649351, Valid Loss: 8.828805452823676\n",
            "Epoch: 29, Step: 12200, Train Loss: 6.81275239763392, Valid Loss: 8.956560948163267\n",
            "Epoch: 29, Step: 12300, Train Loss: 6.961247066809504, Valid Loss: 8.762420673882062\n",
            "Epoch: 29, Step: 12400, Train Loss: 6.891411507106727, Valid Loss: 9.040640579654948\n",
            "Epoch: 29, Step: 12500, Train Loss: 6.798890753797992, Valid Loss: 8.976967005369339\n",
            "Epoch: 30, Step: 12600, Train Loss: 6.765356025307893, Valid Loss: 8.817849232777942\n",
            "Epoch: 30, Step: 12700, Train Loss: 6.823366775918641, Valid Loss: 9.05434048339511\n",
            "Epoch: 30, Step: 12800, Train Loss: 6.7614939524160125, Valid Loss: 8.98094678116695\n",
            "Epoch: 30, Step: 12900, Train Loss: 6.9967068995510076, Valid Loss: 8.82764036751771\n",
            "Epoch: 31, Step: 13000, Train Loss: 6.662775549778205, Valid Loss: 8.92119861309943\n",
            "Epoch: 31, Step: 13100, Train Loss: 6.88328056936242, Valid Loss: 8.830607210478275\n",
            "Epoch: 31, Step: 13200, Train Loss: 6.716184585464352, Valid Loss: 8.910922538906718\n",
            "Epoch: 31, Step: 13300, Train Loss: 6.760335227472757, Valid Loss: 9.042856672932006\n",
            "Epoch: 32, Step: 13400, Train Loss: 6.6563471800459535, Valid Loss: 8.89993890062357\n",
            "Epoch: 32, Step: 13500, Train Loss: 6.614242861492143, Valid Loss: 8.87068062627838\n",
            "Epoch: 32, Step: 13600, Train Loss: 6.714568694628766, Valid Loss: 9.106050350447969\n",
            "Epoch: 32, Step: 13700, Train Loss: 6.634030512928809, Valid Loss: 8.920397079667195\n",
            "Epoch: 33, Step: 13800, Train Loss: 6.663356812554168, Valid Loss: 8.875060300732988\n",
            "Epoch: 33, Step: 13900, Train Loss: 6.56860557314714, Valid Loss: 8.994297143185769\n",
            "Epoch: 33, Step: 14000, Train Loss: 6.571797742652528, Valid Loss: 8.943469329092698\n",
            "Epoch: 33, Step: 14100, Train Loss: 6.557087808922747, Valid Loss: 9.001387654512364\n",
            "Epoch: 33, Step: 14200, Train Loss: 6.630395041209867, Valid Loss: 9.026574190342897\n",
            "Epoch: 34, Step: 14300, Train Loss: 6.569638643211664, Valid Loss: 8.985329834447816\n",
            "Epoch: 34, Step: 14400, Train Loss: 6.556126600514624, Valid Loss: 8.967141150069184\n",
            "Epoch: 34, Step: 14500, Train Loss: 6.63220645039428, Valid Loss: 9.11313012603661\n",
            "Epoch: 34, Step: 14600, Train Loss: 6.6093132507998, Valid Loss: 9.101868279151123\n",
            "Epoch: 35, Step: 14700, Train Loss: 6.50337739609646, Valid Loss: 8.956320195314609\n",
            "Epoch: 35, Step: 14800, Train Loss: 6.562578636339721, Valid Loss: 9.100134857608548\n",
            "Epoch: 35, Step: 14900, Train Loss: 6.502336542435912, Valid Loss: 8.983545905412289\n",
            "Epoch: 35, Step: 15000, Train Loss: 6.59703649847241, Valid Loss: 8.914819498260218\n",
            "Epoch: 36, Step: 15100, Train Loss: 6.539775124169668, Valid Loss: 8.920353188895762\n",
            "Epoch: 36, Step: 15200, Train Loss: 6.452984344970579, Valid Loss: 9.007366972348954\n",
            "Epoch: 36, Step: 15300, Train Loss: 6.499134457117293, Valid Loss: 9.067768750594334\n",
            "Epoch: 36, Step: 15400, Train Loss: 6.445076307619592, Valid Loss: 9.034432234006179\n",
            "Epoch: 37, Step: 15500, Train Loss: 6.431308018913576, Valid Loss: 8.997304395293886\n",
            "Epoch: 37, Step: 15600, Train Loss: 6.445420296590232, Valid Loss: 8.997783167683387\n",
            "Epoch: 37, Step: 15700, Train Loss: 6.4481082856388445, Valid Loss: 9.139605774533074\n",
            "Epoch: 37, Step: 15800, Train Loss: 6.486548443454808, Valid Loss: 9.141498389578\n",
            "Epoch: 38, Step: 15900, Train Loss: 6.428794189583097, Valid Loss: 8.999810839176318\n",
            "Epoch: 38, Step: 16000, Train Loss: 6.382685813678331, Valid Loss: 9.135204365217536\n",
            "Epoch: 38, Step: 16100, Train Loss: 6.35843576496522, Valid Loss: 9.024373274266972\n",
            "Epoch: 38, Step: 16200, Train Loss: 6.411097649324763, Valid Loss: 8.96552957672491\n",
            "Epoch: 38, Step: 16300, Train Loss: 6.420066905820362, Valid Loss: 8.962146754008383\n",
            "Epoch: 39, Step: 16400, Train Loss: 6.312591421637738, Valid Loss: 9.00691669508586\n",
            "Epoch: 39, Step: 16500, Train Loss: 6.306586666355134, Valid Loss: 9.062862539122916\n",
            "Epoch: 39, Step: 16600, Train Loss: 6.347418035656434, Valid Loss: 9.125008590073364\n",
            "Epoch: 39, Step: 16700, Train Loss: 6.589878710916968, Valid Loss: 8.990317083685284\n",
            "Epoch: 40, Step: 16800, Train Loss: 6.349193975835397, Valid Loss: 9.012672650167934\n",
            "Epoch: 40, Step: 16900, Train Loss: 6.360274440498847, Valid Loss: 9.122071195207697\n",
            "Epoch: 40, Step: 17000, Train Loss: 6.378027684527384, Valid Loss: 8.978149490472646\n",
            "Epoch: 40, Step: 17100, Train Loss: 6.321676567273018, Valid Loss: 9.099579495868376\n",
            "Epoch: 41, Step: 17200, Train Loss: 6.263937984318555, Valid Loss: 9.147077320511642\n",
            "Epoch: 41, Step: 17300, Train Loss: 6.233312991062666, Valid Loss: 9.145889599487361\n",
            "Epoch: 41, Step: 17400, Train Loss: 6.307652840159716, Valid Loss: 9.023075176685174\n",
            "Epoch: 41, Step: 17500, Train Loss: 6.25341055112168, Valid Loss: 9.04292178364803\n",
            "Epoch: 42, Step: 17600, Train Loss: 6.209024117078199, Valid Loss: 9.16825682151067\n",
            "Epoch: 42, Step: 17700, Train Loss: 6.248900927982635, Valid Loss: 9.091151486181099\n",
            "Epoch: 42, Step: 17800, Train Loss: 6.24517714021783, Valid Loss: 9.098379352550007\n",
            "Epoch: 42, Step: 17900, Train Loss: 6.221147620383809, Valid Loss: 9.170295166080095\n",
            "Epoch: 43, Step: 18000, Train Loss: 6.1878129541678035, Valid Loss: 9.089448402569403\n",
            "Epoch: 43, Step: 18100, Train Loss: 6.177399069873232, Valid Loss: 9.242007650537639\n",
            "Epoch: 43, Step: 18200, Train Loss: 6.200182058606077, Valid Loss: 9.150312976586859\n",
            "Epoch: 43, Step: 18300, Train Loss: 6.266839775016171, Valid Loss: 9.22767619550102\n",
            "Epoch: 44, Step: 18400, Train Loss: 6.16157927147308, Valid Loss: 9.171410334406803\n",
            "Epoch: 44, Step: 18500, Train Loss: 6.117813705492039, Valid Loss: 9.070022234432736\n",
            "Epoch: 44, Step: 18600, Train Loss: 6.19141324461928, Valid Loss: 9.249547620568782\n",
            "Epoch: 44, Step: 18700, Train Loss: 6.199052748816268, Valid Loss: 9.072359773022505\n",
            "Epoch: 44, Step: 18800, Train Loss: 6.14595261424545, Valid Loss: 9.083876124326752\n",
            "Epoch: 45, Step: 18900, Train Loss: 6.19000926774465, Valid Loss: 9.1038885585929\n",
            "Epoch: 45, Step: 19000, Train Loss: 6.153143139256993, Valid Loss: 9.167143433630121\n",
            "Epoch: 45, Step: 19100, Train Loss: 6.179791493409424, Valid Loss: 9.16408187202163\n",
            "Epoch: 45, Step: 19200, Train Loss: 6.229573830311574, Valid Loss: 9.01216190433795\n",
            "Epoch: 46, Step: 19300, Train Loss: 6.134034047304137, Valid Loss: 9.225537687759003\n",
            "Epoch: 46, Step: 19400, Train Loss: 6.094964705264766, Valid Loss: 9.309739482394493\n",
            "Epoch: 46, Step: 19500, Train Loss: 6.072858521371971, Valid Loss: 9.294890546354079\n",
            "Epoch: 46, Step: 19600, Train Loss: 6.123434401318646, Valid Loss: 9.201089127777664\n",
            "Epoch: 47, Step: 19700, Train Loss: 6.098539144786346, Valid Loss: 9.264363628515534\n",
            "Epoch: 47, Step: 19800, Train Loss: 6.049104896587334, Valid Loss: 9.232382723748012\n",
            "Epoch: 47, Step: 19900, Train Loss: 6.072054231488342, Valid Loss: 9.281274924521\n",
            "Epoch: 47, Step: 20000, Train Loss: 6.214969187121919, Valid Loss: 9.098203470464465\n",
            "Epoch: 48, Step: 20100, Train Loss: 6.054783659563339, Valid Loss: 9.15790843720466\n",
            "Epoch: 48, Step: 20200, Train Loss: 6.030987587828245, Valid Loss: 9.250597980181892\n",
            "Epoch: 48, Step: 20300, Train Loss: 6.053904850834137, Valid Loss: 9.368737008584013\n",
            "Epoch: 48, Step: 20400, Train Loss: 6.233908502886214, Valid Loss: 9.446885686524153\n",
            "Epoch: 49, Step: 20500, Train Loss: 6.011664038494727, Valid Loss: 9.214880691080081\n",
            "Epoch: 49, Step: 20600, Train Loss: 6.11435262360667, Valid Loss: 9.477311870390793\n",
            "Epoch: 49, Step: 20700, Train Loss: 6.025044498240771, Valid Loss: 9.151124102929133\n",
            "Epoch: 49, Step: 20800, Train Loss: 6.037632692275904, Valid Loss: 9.14573106537702\n",
            "Epoch: 49, Step: 20900, Train Loss: 6.06459415820013, Valid Loss: 9.248084833621618\n",
            "Epoch: 50, Step: 21000, Train Loss: 5.9914501294982445, Valid Loss: 9.404501337699418\n",
            "Epoch: 50, Step: 21100, Train Loss: 6.057226120927957, Valid Loss: 9.459268829131084\n",
            "Epoch: 50, Step: 21200, Train Loss: 6.077739197159182, Valid Loss: 9.123984546498255\n",
            "Epoch: 50, Step: 21300, Train Loss: 5.994746404062242, Valid Loss: 9.260656065139436\n",
            "Epoch: 51, Step: 21400, Train Loss: 5.9270077137738015, Valid Loss: 9.331686526296643\n",
            "Epoch: 51, Step: 21500, Train Loss: 5.959477893613599, Valid Loss: 9.209822497960163\n",
            "Epoch: 51, Step: 21600, Train Loss: 6.04932916632855, Valid Loss: 9.452427412646001\n",
            "Epoch: 51, Step: 21700, Train Loss: 5.942972437675786, Valid Loss: 9.292804575433456\n",
            "Epoch: 52, Step: 21800, Train Loss: 5.92014268047578, Valid Loss: 9.33770197960851\n",
            "Epoch: 52, Step: 21900, Train Loss: 6.029103461475798, Valid Loss: 9.149762916141876\n",
            "Epoch: 52, Step: 22000, Train Loss: 5.935787871770527, Valid Loss: 9.250968024760976\n",
            "Epoch: 52, Step: 22100, Train Loss: 5.917013240863981, Valid Loss: 9.22371292685955\n",
            "Epoch: 53, Step: 22200, Train Loss: 5.8419443585114506, Valid Loss: 9.24207742636392\n",
            "Epoch: 53, Step: 22300, Train Loss: 5.883819023764205, Valid Loss: 9.20612550334913\n",
            "Epoch: 53, Step: 22400, Train Loss: 5.92600460774279, Valid Loss: 9.395689043209815\n",
            "Epoch: 53, Step: 22500, Train Loss: 5.886079311222196, Valid Loss: 9.302737211948838\n",
            "Epoch: 54, Step: 22600, Train Loss: 5.8220555643061, Valid Loss: 9.285901568758657\n",
            "Epoch: 54, Step: 22700, Train Loss: 5.853690543989869, Valid Loss: 9.297412675227076\n",
            "Epoch: 54, Step: 22800, Train Loss: 5.866241797677728, Valid Loss: 9.274660495522577\n",
            "Epoch: 54, Step: 22900, Train Loss: 5.875917099699826, Valid Loss: 9.264254298230462\n",
            "Epoch: 55, Step: 23000, Train Loss: 5.842113053102601, Valid Loss: 9.280722487315685\n",
            "Epoch: 55, Step: 23100, Train Loss: 5.790223793509134, Valid Loss: 9.245652005218227\n",
            "Epoch: 55, Step: 23200, Train Loss: 5.8187157679474115, Valid Loss: 9.289596653381894\n",
            "Epoch: 55, Step: 23300, Train Loss: 5.841736541021609, Valid Loss: 9.220702514311318\n",
            "Epoch: 55, Step: 23400, Train Loss: 5.911031005258654, Valid Loss: 9.521586936843804\n",
            "Epoch: 56, Step: 23500, Train Loss: 5.84512685853738, Valid Loss: 9.513920814910648\n",
            "Epoch: 56, Step: 23600, Train Loss: 5.828092460178401, Valid Loss: 9.444995086651621\n",
            "Epoch: 56, Step: 23700, Train Loss: 5.807031115462883, Valid Loss: 9.25838815677785\n",
            "Epoch: 56, Step: 23800, Train Loss: 5.784518364551183, Valid Loss: 9.295385943121955\n",
            "Epoch: 57, Step: 23900, Train Loss: 5.903523196656082, Valid Loss: 9.15499816551851\n",
            "Epoch: 57, Step: 24000, Train Loss: 5.818380648887779, Valid Loss: 9.489653372268736\n",
            "Epoch: 57, Step: 24100, Train Loss: 5.952450980695501, Valid Loss: 9.110106593752075\n",
            "Epoch: 57, Step: 24200, Train Loss: 5.822070046937697, Valid Loss: 9.335421783392734\n",
            "Epoch: 58, Step: 24300, Train Loss: 5.833153682023255, Valid Loss: 9.26383184088555\n",
            "Epoch: 58, Step: 24400, Train Loss: 5.756456522097811, Valid Loss: 9.298052226099612\n",
            "Epoch: 58, Step: 24500, Train Loss: 6.011291434500731, Valid Loss: 9.12988096962595\n",
            "Epoch: 58, Step: 24600, Train Loss: 5.862667479791646, Valid Loss: 9.512567410323864\n",
            "Epoch: 59, Step: 24700, Train Loss: 5.850414907151696, Valid Loss: 9.680925969623841\n",
            "Epoch: 59, Step: 24800, Train Loss: 5.802051812472624, Valid Loss: 9.256325279914828\n",
            "Epoch: 59, Step: 24900, Train Loss: 5.778515212713394, Valid Loss: 9.286337694923036\n",
            "Epoch: 59, Step: 25000, Train Loss: 5.763280791948332, Valid Loss: 9.424994152689372\n",
            "Epoch: 60, Step: 25100, Train Loss: 5.785742629945346, Valid Loss: 9.561556189516095\n",
            "Epoch: 60, Step: 25200, Train Loss: 5.680332589270989, Valid Loss: 9.28009893926823\n",
            "Epoch: 60, Step: 25300, Train Loss: 5.952173445084632, Valid Loss: 9.744567269475224\n",
            "Epoch: 60, Step: 25400, Train Loss: 5.717970589834391, Valid Loss: 9.275595163960592\n",
            "Epoch: 61, Step: 25500, Train Loss: 5.7278617134355185, Valid Loss: 9.445017762955956\n",
            "Epoch: 61, Step: 25600, Train Loss: 5.6474461418410735, Valid Loss: 9.34744436047674\n",
            "Epoch: 61, Step: 25700, Train Loss: 5.681709070026161, Valid Loss: 9.487491737952542\n",
            "Epoch: 61, Step: 25800, Train Loss: 5.650999901364279, Valid Loss: 9.382522204511389\n",
            "Epoch: 61, Step: 25900, Train Loss: 5.692962941004565, Valid Loss: 9.461554024009313\n",
            "Epoch: 62, Step: 26000, Train Loss: 5.616640667000159, Valid Loss: 9.461219178255027\n",
            "Epoch: 62, Step: 26100, Train Loss: 5.694965298666095, Valid Loss: 9.618848258009605\n",
            "Epoch: 62, Step: 26200, Train Loss: 5.7014674741057, Valid Loss: 9.31760183711566\n",
            "Epoch: 62, Step: 26300, Train Loss: 5.688894480995356, Valid Loss: 9.244288161575614\n",
            "Epoch: 63, Step: 26400, Train Loss: 5.626511747153754, Valid Loss: 9.37460784656372\n",
            "Epoch: 63, Step: 26500, Train Loss: 5.657935790826125, Valid Loss: 9.308951581314032\n",
            "Epoch: 63, Step: 26600, Train Loss: 5.670025938446662, Valid Loss: 9.222170037322812\n",
            "Epoch: 63, Step: 26700, Train Loss: 5.646642304728969, Valid Loss: 9.399861204529772\n",
            "Epoch: 64, Step: 26800, Train Loss: 5.572792376530075, Valid Loss: 9.407906999475804\n",
            "Epoch: 64, Step: 26900, Train Loss: 5.604810438505138, Valid Loss: 9.366846254372177\n",
            "Epoch: 64, Step: 27000, Train Loss: 5.635446776050637, Valid Loss: 9.448323381240636\n",
            "Epoch: 64, Step: 27100, Train Loss: 5.677127985433378, Valid Loss: 9.488613561683454\n",
            "Epoch: 65, Step: 27200, Train Loss: 5.674168372363815, Valid Loss: 9.245911520710166\n",
            "Epoch: 65, Step: 27300, Train Loss: 5.553284408011887, Valid Loss: 9.36146163865759\n",
            "Epoch: 65, Step: 27400, Train Loss: 5.6428431906251575, Valid Loss: 9.46466227270221\n",
            "Epoch: 65, Step: 27500, Train Loss: 5.590017105124093, Valid Loss: 9.491200616397672\n",
            "Epoch: 66, Step: 27600, Train Loss: 5.560483916240601, Valid Loss: 9.484421431296996\n",
            "Epoch: 66, Step: 27700, Train Loss: 5.742960412878826, Valid Loss: 9.240358672168488\n",
            "Epoch: 66, Step: 27800, Train Loss: 5.589890844209308, Valid Loss: 9.481166339448635\n",
            "Epoch: 66, Step: 27900, Train Loss: 5.695676955737173, Valid Loss: 9.486573351416009\n",
            "Epoch: 66, Step: 28000, Train Loss: 5.602096738033655, Valid Loss: 9.269521562838918\n",
            "Epoch: 67, Step: 28100, Train Loss: 5.5623115710908735, Valid Loss: 9.64936023515696\n",
            "Epoch: 67, Step: 28200, Train Loss: 5.572292388526626, Valid Loss: 9.460481580549581\n",
            "Epoch: 67, Step: 28300, Train Loss: 5.546728245703788, Valid Loss: 9.516478707406817\n",
            "Epoch: 67, Step: 28400, Train Loss: 5.524206937530812, Valid Loss: 9.401960776418411\n",
            "Epoch: 68, Step: 28500, Train Loss: 5.529997206723362, Valid Loss: 9.478395056462755\n",
            "Epoch: 68, Step: 28600, Train Loss: 5.530017017960144, Valid Loss: 9.415168657540795\n",
            "Epoch: 68, Step: 28700, Train Loss: 5.608565788606126, Valid Loss: 9.328194799076728\n",
            "Epoch: 68, Step: 28800, Train Loss: 5.5303935990645865, Valid Loss: 9.469654135104198\n",
            "Epoch: 69, Step: 28900, Train Loss: 5.504911766513618, Valid Loss: 9.343952627987674\n",
            "Epoch: 69, Step: 29000, Train Loss: 5.4557639372641065, Valid Loss: 9.363187971351067\n",
            "Epoch: 69, Step: 29100, Train Loss: 5.521634007423919, Valid Loss: 9.373460522497421\n",
            "Epoch: 69, Step: 29200, Train Loss: 5.496840683241985, Valid Loss: 9.549048529003922\n",
            "Epoch: 70, Step: 29300, Train Loss: 5.472532244608504, Valid Loss: 9.438062131226436\n",
            "Epoch: 70, Step: 29400, Train Loss: 5.472884492830715, Valid Loss: 9.473123935674698\n",
            "Epoch: 70, Step: 29500, Train Loss: 5.506944033623713, Valid Loss: 9.64069257369792\n",
            "Epoch: 70, Step: 29600, Train Loss: 5.474511209205159, Valid Loss: 9.385991558192499\n",
            "Epoch: 71, Step: 29700, Train Loss: 5.4066977379795516, Valid Loss: 9.421647520187747\n",
            "Epoch: 71, Step: 29800, Train Loss: 5.645073747885579, Valid Loss: 9.292171718683926\n",
            "Epoch: 71, Step: 29900, Train Loss: 5.509866389112252, Valid Loss: 9.618072214887043\n",
            "Epoch: 71, Step: 30000, Train Loss: 5.436714725780114, Valid Loss: 9.47537351286102\n",
            "Epoch: 72, Step: 30100, Train Loss: 5.40707521445119, Valid Loss: 9.477306328410391\n",
            "Epoch: 72, Step: 30200, Train Loss: 5.40327792642425, Valid Loss: 9.496985740431791\n",
            "Epoch: 72, Step: 30300, Train Loss: 5.45010018101134, Valid Loss: 9.645892721878093\n",
            "Epoch: 72, Step: 30400, Train Loss: 5.415687753906976, Valid Loss: 9.368721001438589\n",
            "Epoch: 72, Step: 30500, Train Loss: 5.605627039087831, Valid Loss: 9.793818396554688\n",
            "Epoch: 73, Step: 30600, Train Loss: 5.360509501951926, Valid Loss: 9.520307247453932\n",
            "Epoch: 73, Step: 30700, Train Loss: 5.36873241478585, Valid Loss: 9.46089562195876\n",
            "Epoch: 73, Step: 30800, Train Loss: 5.483436153178798, Valid Loss: 9.36843481486848\n",
            "Epoch: 73, Step: 30900, Train Loss: 5.479890729875239, Valid Loss: 9.632380062306533\n",
            "Epoch: 74, Step: 31000, Train Loss: 5.491486119488489, Valid Loss: 9.757408668350463\n",
            "Epoch: 74, Step: 31100, Train Loss: 5.40016760883592, Valid Loss: 9.445788403858606\n",
            "Epoch: 74, Step: 31200, Train Loss: 5.413894411324053, Valid Loss: 9.525916609834786\n",
            "Epoch: 74, Step: 31300, Train Loss: 5.393271150933226, Valid Loss: 9.499706422298456\n",
            "Epoch: 75, Step: 31400, Train Loss: 5.376568073726344, Valid Loss: 9.359335306858954\n",
            "Epoch: 75, Step: 31500, Train Loss: 5.365206298052829, Valid Loss: 9.476329987100629\n",
            "Epoch: 75, Step: 31600, Train Loss: 5.351174065494454, Valid Loss: 9.46099075097769\n",
            "Epoch: 75, Step: 31700, Train Loss: 5.430312133356779, Valid Loss: 9.405736723208312\n",
            "Epoch: 76, Step: 31800, Train Loss: 5.3284227990576465, Valid Loss: 9.480467219830285\n",
            "Epoch: 76, Step: 31900, Train Loss: 5.340743180922945, Valid Loss: 9.447322945521972\n",
            "Epoch: 76, Step: 32000, Train Loss: 5.412847429125383, Valid Loss: 9.626220282044267\n",
            "Epoch: 76, Step: 32100, Train Loss: 5.4120168718386354, Valid Loss: 9.623454047305051\n",
            "Epoch: 77, Step: 32200, Train Loss: 5.525117397016472, Valid Loss: 9.24807239097521\n",
            "Epoch: 77, Step: 32300, Train Loss: 5.316781777543877, Valid Loss: 9.55873823769524\n",
            "Epoch: 77, Step: 32400, Train Loss: 5.321336892794747, Valid Loss: 9.591485629088947\n",
            "Epoch: 77, Step: 32500, Train Loss: 5.315148673918348, Valid Loss: 9.635067312433009\n",
            "Epoch: 77, Step: 32600, Train Loss: 5.38105054375663, Valid Loss: 9.461148198354119\n",
            "Epoch: 78, Step: 32700, Train Loss: 5.264180080928925, Valid Loss: 9.546534791896038\n",
            "Epoch: 78, Step: 32800, Train Loss: 5.265833892475952, Valid Loss: 9.466247826786983\n",
            "Epoch: 78, Step: 32900, Train Loss: 5.335715459250375, Valid Loss: 9.428093287646885\n",
            "Epoch: 78, Step: 33000, Train Loss: 5.350014442618208, Valid Loss: 9.469157613808438\n",
            "Epoch: 79, Step: 33100, Train Loss: 5.261291882474463, Valid Loss: 9.559821088752205\n",
            "Epoch: 79, Step: 33200, Train Loss: 5.245827483669304, Valid Loss: 9.54380654498122\n",
            "Epoch: 79, Step: 33300, Train Loss: 5.364570156145726, Valid Loss: 9.394925518779417\n",
            "Epoch: 79, Step: 33400, Train Loss: 5.350546635470936, Valid Loss: 9.673124443845108\n",
            "Epoch: 80, Step: 33500, Train Loss: 5.325188801738444, Valid Loss: 9.383061219051095\n",
            "Epoch: 80, Step: 33600, Train Loss: 5.3136125474054, Valid Loss: 9.33373486828312\n",
            "Epoch: 80, Step: 33700, Train Loss: 5.292965960659077, Valid Loss: 9.683410291755694\n",
            "Epoch: 80, Step: 33800, Train Loss: 5.287265141154308, Valid Loss: 9.580059728804883\n",
            "Epoch: 81, Step: 33900, Train Loss: 5.252494652151825, Valid Loss: 9.478630340758787\n",
            "Epoch: 81, Step: 34000, Train Loss: 5.219721148160291, Valid Loss: 9.538484095824066\n",
            "Epoch: 81, Step: 34100, Train Loss: 5.222771405137294, Valid Loss: 9.501838703362447\n",
            "Epoch: 81, Step: 34200, Train Loss: 5.410583183364303, Valid Loss: 9.393971292751385\n",
            "Epoch: 82, Step: 34300, Train Loss: 5.252100644830977, Valid Loss: 9.541346677899247\n",
            "Epoch: 82, Step: 34400, Train Loss: 5.243260889734811, Valid Loss: 9.61379635194564\n",
            "Epoch: 82, Step: 34500, Train Loss: 5.56217906033909, Valid Loss: 9.355766348805556\n",
            "Epoch: 82, Step: 34600, Train Loss: 5.293040543800611, Valid Loss: 9.678892969407135\n",
            "Epoch: 83, Step: 34700, Train Loss: 5.308438356400431, Valid Loss: 9.443847726139172\n",
            "Epoch: 83, Step: 34800, Train Loss: 5.235172885032797, Valid Loss: 9.526320845782394\n",
            "Epoch: 83, Step: 34900, Train Loss: 5.302359105384368, Valid Loss: 9.74712944209447\n",
            "Epoch: 83, Step: 35000, Train Loss: 5.237424448246658, Valid Loss: 9.643898470183006\n",
            "Epoch: 83, Step: 35100, Train Loss: 5.241088240143797, Valid Loss: 9.577966919748441\n",
            "Epoch: 84, Step: 35200, Train Loss: 5.285176196030274, Valid Loss: 9.778997991067994\n",
            "Epoch: 84, Step: 35300, Train Loss: 5.225503371631048, Valid Loss: 9.541859964317947\n",
            "Epoch: 84, Step: 35400, Train Loss: 5.230120404204173, Valid Loss: 9.716210050229588\n",
            "Epoch: 84, Step: 35500, Train Loss: 5.218287924634614, Valid Loss: 9.713240224598474\n",
            "Epoch: 85, Step: 35600, Train Loss: 5.401744459892365, Valid Loss: 9.406149710751361\n",
            "Epoch: 85, Step: 35700, Train Loss: 5.17192603189611, Valid Loss: 9.567392186348169\n",
            "Epoch: 85, Step: 35800, Train Loss: 5.160298904467989, Valid Loss: 9.592631710810466\n",
            "Epoch: 85, Step: 35900, Train Loss: 5.245549837438772, Valid Loss: 9.75668572197968\n",
            "Epoch: 86, Step: 36000, Train Loss: 5.23132899320453, Valid Loss: 9.593454572593549\n",
            "Epoch: 86, Step: 36100, Train Loss: 5.191168021856486, Valid Loss: 9.522395369935031\n",
            "Epoch: 86, Step: 36200, Train Loss: 5.238770958965494, Valid Loss: 9.76008369962776\n",
            "Epoch: 86, Step: 36300, Train Loss: 5.394093394594753, Valid Loss: 9.490222189456768\n",
            "Epoch: 87, Step: 36400, Train Loss: 5.164633063839943, Valid Loss: 9.542336044373322\n",
            "Epoch: 87, Step: 36500, Train Loss: 5.1363541476076655, Valid Loss: 9.58173598858587\n",
            "Epoch: 87, Step: 36600, Train Loss: 5.1657561213981635, Valid Loss: 9.657452576629515\n",
            "Epoch: 87, Step: 36700, Train Loss: 5.144565766666664, Valid Loss: 9.511020945170122\n",
            "Epoch: 88, Step: 36800, Train Loss: 5.163801121977067, Valid Loss: 9.69322511082722\n",
            "Epoch: 88, Step: 36900, Train Loss: 5.123707211865608, Valid Loss: 9.624741988675009\n",
            "Epoch: 88, Step: 37000, Train Loss: 5.36079221344812, Valid Loss: 9.42395890648676\n",
            "Epoch: 88, Step: 37100, Train Loss: 5.160503593029162, Valid Loss: 9.667481968463251\n",
            "Epoch: 88, Step: 37200, Train Loss: 5.161901124024509, Valid Loss: 9.716969898434602\n",
            "Epoch: 89, Step: 37300, Train Loss: 5.11979431896681, Valid Loss: 9.767651227922736\n",
            "Epoch: 89, Step: 37400, Train Loss: 5.178835462617276, Valid Loss: 9.566761162215323\n",
            "Epoch: 89, Step: 37500, Train Loss: 5.181408084203483, Valid Loss: 9.681069431033533\n",
            "Epoch: 89, Step: 37600, Train Loss: 5.177284825451883, Valid Loss: 9.669780424113423\n",
            "Epoch: 90, Step: 37700, Train Loss: 5.177422278976287, Valid Loss: 9.556306706732048\n",
            "Epoch: 90, Step: 37800, Train Loss: 5.1414468665572945, Valid Loss: 9.699280928430571\n",
            "Epoch: 90, Step: 37900, Train Loss: 5.159037366507478, Valid Loss: 9.58419775716768\n",
            "Epoch: 90, Step: 38000, Train Loss: 5.157525848332698, Valid Loss: 9.640480239051557\n",
            "Epoch: 91, Step: 38100, Train Loss: 5.0999779176139475, Valid Loss: 9.669430987589006\n",
            "Epoch: 91, Step: 38200, Train Loss: 5.096793460566739, Valid Loss: 9.726122867729762\n",
            "Epoch: 91, Step: 38300, Train Loss: 5.17964066777361, Valid Loss: 9.430160054207438\n",
            "Epoch: 91, Step: 38400, Train Loss: 5.110302242803579, Valid Loss: 9.642227561155403\n",
            "Epoch: 92, Step: 38500, Train Loss: 5.266247341692142, Valid Loss: 9.903146902336482\n",
            "Epoch: 92, Step: 38600, Train Loss: 5.189104836884927, Valid Loss: 9.533046080663006\n",
            "Epoch: 92, Step: 38700, Train Loss: 5.13317930155878, Valid Loss: 9.716250729557803\n",
            "Epoch: 92, Step: 38800, Train Loss: 5.169956217339917, Valid Loss: 9.502141727626785\n",
            "Epoch: 93, Step: 38900, Train Loss: 5.099918532618036, Valid Loss: 9.772254456284218\n",
            "Epoch: 93, Step: 39000, Train Loss: 5.057708706621794, Valid Loss: 9.627016450274915\n",
            "Epoch: 93, Step: 39100, Train Loss: 5.127399409493122, Valid Loss: 9.67300915912147\n",
            "Epoch: 93, Step: 39200, Train Loss: 5.083563985674283, Valid Loss: 9.64464860814817\n",
            "Epoch: 94, Step: 39300, Train Loss: 5.06748571985134, Valid Loss: 9.615857214952493\n",
            "Epoch: 94, Step: 39400, Train Loss: 5.05947890495239, Valid Loss: 9.623156670407836\n",
            "Epoch: 94, Step: 39500, Train Loss: 5.069190952256572, Valid Loss: 9.618889309681904\n",
            "Epoch: 94, Step: 39600, Train Loss: 5.114602854820544, Valid Loss: 9.616170163184469\n",
            "Epoch: 94, Step: 39700, Train Loss: 5.109829940391314, Valid Loss: 9.701701784619205\n",
            "Epoch: 95, Step: 39800, Train Loss: 5.028066101504594, Valid Loss: 9.683226255920637\n",
            "Epoch: 95, Step: 39900, Train Loss: 5.080912861241223, Valid Loss: 9.576993972973968\n",
            "Epoch: 95, Step: 40000, Train Loss: 5.061005462952438, Valid Loss: 9.748018314203826\n",
            "Epoch: 95, Step: 40100, Train Loss: 5.106902636213355, Valid Loss: 9.55453314446431\n",
            "Epoch: 96, Step: 40200, Train Loss: 5.071365795514683, Valid Loss: 9.5425263682258\n",
            "Epoch: 96, Step: 40300, Train Loss: 5.052666057704544, Valid Loss: 9.472382978486714\n",
            "Epoch: 96, Step: 40400, Train Loss: 5.088314789112921, Valid Loss: 9.814555509945647\n",
            "Epoch: 96, Step: 40500, Train Loss: 5.063292129177042, Valid Loss: 9.675595293590785\n",
            "Epoch: 97, Step: 40600, Train Loss: 5.0538103221933035, Valid Loss: 9.812267952416288\n",
            "Epoch: 97, Step: 40700, Train Loss: 5.095959677351685, Valid Loss: 9.901500200114496\n",
            "Epoch: 97, Step: 40800, Train Loss: 5.0635939542057, Valid Loss: 9.738071349330449\n",
            "Epoch: 97, Step: 40900, Train Loss: 5.047215823285287, Valid Loss: 9.62815620279065\n",
            "Epoch: 98, Step: 41000, Train Loss: 5.0437503952125455, Valid Loss: 9.826808848737182\n",
            "Epoch: 98, Step: 41100, Train Loss: 5.004308324236216, Valid Loss: 9.688285971917074\n",
            "Epoch: 98, Step: 41200, Train Loss: 5.011386244165521, Valid Loss: 9.619361485461754\n",
            "Epoch: 98, Step: 41300, Train Loss: 5.0299809078798905, Valid Loss: 9.710683122389604\n",
            "Epoch: 99, Step: 41400, Train Loss: 4.968414502671482, Valid Loss: 9.707447106524949\n",
            "Epoch: 99, Step: 41500, Train Loss: 4.985716399286942, Valid Loss: 9.652665075988782\n",
            "Epoch: 99, Step: 41600, Train Loss: 5.024004370409422, Valid Loss: 9.577991095218916\n",
            "Epoch: 99, Step: 41700, Train Loss: 5.031603067061986, Valid Loss: 9.68988159472542\n",
            "Epoch: 99, Step: 41800, Train Loss: 5.021580161373442, Valid Loss: 9.733885442938133\n",
            "Test RMSE Loss for h = 180, wd = 0.4: 8.81817506092217\n"
          ]
        }
      ],
      "source": [
        "h_list = [45, 90, 180]\n",
        "wd_list = [0.1, 0.2, 0.4]\n",
        "rmse_dict = []\n",
        "for h in h_list:\n",
        "    for wd in wd_list:\n",
        "        print(f\"Start training for h = {h}, wd = {wd}\")\n",
        "        mlp = MyMLP(\n",
        "            X_subtrain=X_subtrain,\n",
        "            Y_subtrain=Y_subtrain,\n",
        "            X_valid=X_valid,\n",
        "            Y_valid=Y_valid,\n",
        "            H=h,\n",
        "            lr=0.00001,\n",
        "            wd=wd,\n",
        "            mom=0,\n",
        "            loss_type=0,\n",
        "            optimizer_type=0,\n",
        "            use_dropout=False\n",
        "        )\n",
        "        mlp.fit(\n",
        "            max_epoch=100,\n",
        "            verbose=True,\n",
        "            patience_batch_num=5000,\n",
        "            model_path=f'q4_mlp_h_{h}_wd_{wd}.ckpt'\n",
        "        )\n",
        "        best_model = torch.load(f'q4_mlp_h_{h}_wd_{wd}.ckpt')\n",
        "        test_rmse_loss = calculate_test_rmse(best_model, X_test, Y_test)\n",
        "        rmse_record = {\n",
        "            'h': h,\n",
        "            'wd': wd,\n",
        "            'test_rmse_loss': test_rmse_loss\n",
        "        }\n",
        "        rmse_dict.append(rmse_record)\n",
        "        print(f\"Test RMSE Loss for h = {h}, wd = {wd}: {test_rmse_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "imOXBVfRPs3t",
        "outputId": "f17cc7be-b187-479d-da68-e08e50a6cfc3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>h</th>\n",
              "      <th>wd</th>\n",
              "      <th>test_rmse_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>45</td>\n",
              "      <td>0.1</td>\n",
              "      <td>8.854781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>45</td>\n",
              "      <td>0.2</td>\n",
              "      <td>8.876086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>45</td>\n",
              "      <td>0.4</td>\n",
              "      <td>8.828734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>90</td>\n",
              "      <td>0.1</td>\n",
              "      <td>8.855875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>90</td>\n",
              "      <td>0.2</td>\n",
              "      <td>8.848237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>90</td>\n",
              "      <td>0.4</td>\n",
              "      <td>8.843196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>180</td>\n",
              "      <td>0.1</td>\n",
              "      <td>8.835052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>180</td>\n",
              "      <td>0.2</td>\n",
              "      <td>8.854008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>180</td>\n",
              "      <td>0.4</td>\n",
              "      <td>8.818175</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     h   wd  test_rmse_loss\n",
              "0   45  0.1        8.854781\n",
              "1   45  0.2        8.876086\n",
              "2   45  0.4        8.828734\n",
              "3   90  0.1        8.855875\n",
              "4   90  0.2        8.848237\n",
              "5   90  0.4        8.843196\n",
              "6  180  0.1        8.835052\n",
              "7  180  0.2        8.854008\n",
              "8  180  0.4        8.818175"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# display the result in a table\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "h_list = [45, 90, 180]\n",
        "wd_list = [0.1, 0.2, 0.4]\n",
        "rmse_dict = []\n",
        "for h in h_list:\n",
        "    for wd in wd_list:\n",
        "        best_model = torch.load(f'q4_mlp_h_{h}_wd_{wd}.ckpt', map_location=torch.device('cpu'))\n",
        "        test_rmse_loss = calculate_test_rmse(best_model, X_test, Y_test)\n",
        "        rmse_record = {\n",
        "            'h': h,\n",
        "            'wd': wd,\n",
        "            'test_rmse_loss': test_rmse_loss\n",
        "        }\n",
        "        rmse_dict.append(rmse_record)\n",
        "\n",
        "pd.DataFrame(rmse_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ven0dIAdPs3t",
        "outputId": "9c4d15c1-3e13-4003-f076-ee275cc7bae0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best h: 180\n",
            "Best wd: 0.4\n",
            "Best test rmse loss: 8.818175069502011\n"
          ]
        }
      ],
      "source": [
        "# print the best h and wd with lowest test rmse loss\n",
        "best_rmse_record = min(rmse_dict, key=lambda x: x['test_rmse_loss'])\n",
        "print(f\"Best h: {best_rmse_record['h']}\")\n",
        "print(f\"Best wd: {best_rmse_record['wd']}\")\n",
        "print(f\"Best test rmse loss: {best_rmse_record['test_rmse_loss']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8Pfd3ANPs3t"
      },
      "source": [
        "由上可知，我們應該選擇 H = 180, weight decay = 0.4 的組合，因為其 Test RMSE 最小。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZoFWZ8rPs3u"
      },
      "source": [
        "#### Q5 MLP with Dropout (15%)\n",
        "建構一個有Dropout的四層Hidden Layer的MLP。此模型由輸入層開始，第一層由90個Input Features通過線性層轉換為H個Hidden Nodes，通過ReLu Activation Function，之後對Hidden Unit Dropout，機率為0.5。後面各Hidden Lyaer均在ReLu後有Dropout，機率皆為0.5。最後通過一個線性層輸出。所有Hidden Layer的寬度都為H。\n",
        "\n",
        "令H= 90, 使用Adaptive Moment Estimation (Adam)更新參數，設Learning Rate = 0.001，無Weight Decay與Momentum，其他參數使用預設值。畫出模型訓練過程中的Training與Validation RMSE，列出Test RMSE。 並討論訓練過程中Training與Validation RMSE的圖形意義。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "q5_mlp = MyMLP(\n",
        "    X_subtrain=X_subtrain,\n",
        "    Y_subtrain=Y_subtrain,\n",
        "    X_valid=X_valid,\n",
        "    Y_valid=Y_valid,\n",
        "    H=90,\n",
        "    lr=0.001,\n",
        "    wd=0,\n",
        "    mom=0,\n",
        "    loss_type=0,\n",
        "    optimizer_type=1,\n",
        "    use_dropout=True,\n",
        "    dropout_rate=0.5\n",
        ")\n",
        "train_loss_list, valid_loss_list = q5_mlp.fit(\n",
        "    max_epoch=100,\n",
        "    verbose=True,\n",
        "    patience_batch_num=5000,\n",
        "    model_path='q5_mlp.ckpt'\n",
        ")\n",
        "\n",
        "# draw the training loss and validation loss\n",
        "# make figure larger\n",
        "plt.figure(figsize=(10, 8))\n",
        "draw_loss(train_loss_list, valid_loss_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test RMSE Loss: 8.761011974908858\n"
          ]
        }
      ],
      "source": [
        "q5_mlp_best_model = torch.load('q5_mlp.ckpt', map_location=torch.device('cpu'))\n",
        "test_rmse_loss = calculate_test_rmse(q5_mlp_best_model, X_test, Y_test)\n",
        "print(f\"Test RMSE Loss: {test_rmse_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# display png image\n",
        "from IPython.display import Image\n",
        "Image(filename='q5_train_loss_chart.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBNkvNpHPs3u"
      },
      "source": [
        "從上圖同樣可看出，在訓練過程中，Training RMSE 與 Validation RMSE 在前幾千個 Batch 都在下降。  \n",
        "而這邊相較於 Q2 的圖，Validation RMSE 似乎更小，且與 Training RMSE 的差距也比較小，代表模型可能有較好的 Generalization。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AkUEadCPs3u"
      },
      "source": [
        "#### Q6 Explore Number of Hidden Units (10%)\n",
        "使用上題的模型，考慮H = 20, 180, 360。 討論H = 20, 45, 180, 360的Test RMSE。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExydlHGPPs3v",
        "outputId": "490bd4c0-dfd5-4f7f-d7e1-f1403bf9efde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training for h = 20\n",
            "Epoch: 0, Step: 100, Train Loss: 9.5200245485868, Valid Loss: 9.473878970533127\n",
            "Epoch: 0, Step: 200, Train Loss: 9.155681479375327, Valid Loss: 9.103037489278522\n",
            "Epoch: 0, Step: 300, Train Loss: 9.048587743011534, Valid Loss: 9.000576991066888\n",
            "Epoch: 0, Step: 400, Train Loss: 8.981370225019269, Valid Loss: 8.933047292973816\n",
            "Epoch: 1, Step: 500, Train Loss: 8.936449112436254, Valid Loss: 8.889940263354816\n",
            "Epoch: 1, Step: 600, Train Loss: 8.91545739063537, Valid Loss: 8.86611002445239\n",
            "Epoch: 1, Step: 700, Train Loss: 8.885559468197407, Valid Loss: 8.836600156222692\n",
            "Epoch: 1, Step: 800, Train Loss: 8.8519678239549, Valid Loss: 8.809738843945533\n",
            "Epoch: 2, Step: 900, Train Loss: 8.87204431715522, Valid Loss: 8.83389402192675\n",
            "Epoch: 2, Step: 1000, Train Loss: 8.84360485930608, Valid Loss: 8.806929178630424\n",
            "Epoch: 2, Step: 1100, Train Loss: 8.84601911392216, Valid Loss: 8.810785777265478\n",
            "Epoch: 2, Step: 1200, Train Loss: 8.805173136088172, Valid Loss: 8.765194330754882\n",
            "Epoch: 3, Step: 1300, Train Loss: 8.794193437160994, Valid Loss: 8.75301023716919\n",
            "Epoch: 3, Step: 1400, Train Loss: 8.811275688566898, Valid Loss: 8.774111516680822\n",
            "Epoch: 3, Step: 1500, Train Loss: 8.801829304681716, Valid Loss: 8.766129698238437\n",
            "Epoch: 3, Step: 1600, Train Loss: 8.768510268040286, Valid Loss: 8.73431577415595\n",
            "Epoch: 4, Step: 1700, Train Loss: 8.77563869084521, Valid Loss: 8.744167927998678\n",
            "Epoch: 4, Step: 1800, Train Loss: 8.748456150226666, Valid Loss: 8.712063794767104\n",
            "Epoch: 4, Step: 1900, Train Loss: 8.762305852809416, Valid Loss: 8.727002471541855\n",
            "Epoch: 4, Step: 2000, Train Loss: 8.759397274728654, Valid Loss: 8.73048274123173\n",
            "Epoch: 5, Step: 2100, Train Loss: 8.759689255401602, Valid Loss: 8.73144517973584\n",
            "Epoch: 5, Step: 2200, Train Loss: 8.729561511685494, Valid Loss: 8.69958769109774\n",
            "Epoch: 5, Step: 2300, Train Loss: 8.724274881001165, Valid Loss: 8.699597906773674\n",
            "Epoch: 5, Step: 2400, Train Loss: 8.738636620770968, Valid Loss: 8.709099734898247\n",
            "Epoch: 5, Step: 2500, Train Loss: 8.737017051079972, Valid Loss: 8.71510069412835\n",
            "Epoch: 6, Step: 2600, Train Loss: 8.710482952952283, Valid Loss: 8.691119514040118\n",
            "Epoch: 6, Step: 2700, Train Loss: 8.71277579325954, Valid Loss: 8.694021791766284\n",
            "Epoch: 6, Step: 2800, Train Loss: 8.701404311785248, Valid Loss: 8.68130312243957\n",
            "Epoch: 6, Step: 2900, Train Loss: 8.731760633504987, Valid Loss: 8.718377906570064\n",
            "Epoch: 7, Step: 3000, Train Loss: 8.719123346371918, Valid Loss: 8.703064656485267\n",
            "Epoch: 7, Step: 3100, Train Loss: 8.698691092629158, Valid Loss: 8.691744219766246\n",
            "Epoch: 7, Step: 3200, Train Loss: 8.704957047713076, Valid Loss: 8.690730698570395\n",
            "Epoch: 7, Step: 3300, Train Loss: 8.694906540731939, Valid Loss: 8.684715397858708\n",
            "Epoch: 8, Step: 3400, Train Loss: 8.667442977862663, Valid Loss: 8.65979148776409\n",
            "Epoch: 8, Step: 3500, Train Loss: 8.700499147218295, Valid Loss: 8.694128576995418\n",
            "Epoch: 8, Step: 3600, Train Loss: 8.693353285649351, Valid Loss: 8.691488312420063\n",
            "Epoch: 8, Step: 3700, Train Loss: 8.688551514508077, Valid Loss: 8.68017959728723\n",
            "Epoch: 9, Step: 3800, Train Loss: 8.687073167169814, Valid Loss: 8.678409993858196\n",
            "Epoch: 9, Step: 3900, Train Loss: 8.669148843516851, Valid Loss: 8.665680054338862\n",
            "Epoch: 9, Step: 4000, Train Loss: 8.67063438731314, Valid Loss: 8.662751759771385\n",
            "Epoch: 9, Step: 4100, Train Loss: 8.679702341387683, Valid Loss: 8.673516909959607\n",
            "Epoch: 10, Step: 4200, Train Loss: 8.657452549195211, Valid Loss: 8.65395344773802\n",
            "Epoch: 10, Step: 4300, Train Loss: 8.656638058113732, Valid Loss: 8.654279185941279\n",
            "Epoch: 10, Step: 4400, Train Loss: 8.651198421380972, Valid Loss: 8.650583914045962\n",
            "Epoch: 10, Step: 4500, Train Loss: 8.656052864334459, Valid Loss: 8.653486253273277\n",
            "Epoch: 11, Step: 4600, Train Loss: 8.662965262139807, Valid Loss: 8.672777322297016\n",
            "Epoch: 11, Step: 4700, Train Loss: 8.648651237707407, Valid Loss: 8.653272634866214\n",
            "Epoch: 11, Step: 4800, Train Loss: 8.659258763185791, Valid Loss: 8.662230269537396\n",
            "Epoch: 11, Step: 4900, Train Loss: 8.638934211422542, Valid Loss: 8.642963410339966\n",
            "Epoch: 11, Step: 5000, Train Loss: 8.662307036507114, Valid Loss: 8.662693182908205\n",
            "Epoch: 12, Step: 5100, Train Loss: 8.654292974738167, Valid Loss: 8.659097555512936\n",
            "Epoch: 12, Step: 5200, Train Loss: 8.661111812304716, Valid Loss: 8.662786392745172\n",
            "Epoch: 12, Step: 5300, Train Loss: 8.642572297691245, Valid Loss: 8.646642485844307\n",
            "Epoch: 12, Step: 5400, Train Loss: 8.655941468153895, Valid Loss: 8.659349580362202\n",
            "Epoch: 13, Step: 5500, Train Loss: 8.648584212290583, Valid Loss: 8.654352853502523\n",
            "Epoch: 13, Step: 5600, Train Loss: 8.621642531344062, Valid Loss: 8.63317018685957\n",
            "Epoch: 13, Step: 5700, Train Loss: 8.626815683902239, Valid Loss: 8.637837154319081\n",
            "Epoch: 13, Step: 5800, Train Loss: 8.622546225385861, Valid Loss: 8.636847291790115\n",
            "Epoch: 14, Step: 5900, Train Loss: 8.625987350856855, Valid Loss: 8.637998436910898\n",
            "Epoch: 14, Step: 6000, Train Loss: 8.631738956573358, Valid Loss: 8.643327299856661\n",
            "Epoch: 14, Step: 6100, Train Loss: 8.636785046180309, Valid Loss: 8.647524888157589\n",
            "Epoch: 14, Step: 6200, Train Loss: 8.647920423450895, Valid Loss: 8.656909488349502\n",
            "Epoch: 15, Step: 6300, Train Loss: 8.613661023030469, Valid Loss: 8.628427111951389\n",
            "Epoch: 15, Step: 6400, Train Loss: 8.635065999483214, Valid Loss: 8.653916976152011\n",
            "Epoch: 15, Step: 6500, Train Loss: 8.615068289780003, Valid Loss: 8.630452241399297\n",
            "Epoch: 15, Step: 6600, Train Loss: 8.604495191800225, Valid Loss: 8.621237130597464\n",
            "Epoch: 16, Step: 6700, Train Loss: 8.631724858447683, Valid Loss: 8.641992124276479\n",
            "Epoch: 16, Step: 6800, Train Loss: 8.621224498084946, Valid Loss: 8.63948647978803\n",
            "Epoch: 16, Step: 6900, Train Loss: 8.617728818053227, Valid Loss: 8.63429344352237\n",
            "Epoch: 16, Step: 7000, Train Loss: 8.619178133455046, Valid Loss: 8.63971878653768\n",
            "Epoch: 16, Step: 7100, Train Loss: 8.605940415841815, Valid Loss: 8.623552111598173\n",
            "Epoch: 17, Step: 7200, Train Loss: 8.602963707337484, Valid Loss: 8.621887969558653\n",
            "Epoch: 17, Step: 7300, Train Loss: 8.599041221660547, Valid Loss: 8.616355693873924\n",
            "Epoch: 17, Step: 7400, Train Loss: 8.591887352096597, Valid Loss: 8.618677144269347\n",
            "Epoch: 17, Step: 7500, Train Loss: 8.602282878574307, Valid Loss: 8.62291163882801\n",
            "Epoch: 18, Step: 7600, Train Loss: 8.594006240265813, Valid Loss: 8.616242479552787\n",
            "Epoch: 18, Step: 7700, Train Loss: 8.603521416412132, Valid Loss: 8.627633309395144\n",
            "Epoch: 18, Step: 7800, Train Loss: 8.61821755282179, Valid Loss: 8.63979175192552\n",
            "Epoch: 18, Step: 7900, Train Loss: 8.604205471536565, Valid Loss: 8.631548052038777\n",
            "Epoch: 19, Step: 8000, Train Loss: 8.588573910824262, Valid Loss: 8.62100343663482\n",
            "Epoch: 19, Step: 8100, Train Loss: 8.634774589352993, Valid Loss: 8.659720877189175\n",
            "Epoch: 19, Step: 8200, Train Loss: 8.579689514914085, Valid Loss: 8.608540632869124\n",
            "Epoch: 19, Step: 8300, Train Loss: 8.599435056704394, Valid Loss: 8.630351676540183\n",
            "Epoch: 20, Step: 8400, Train Loss: 8.57992406243793, Valid Loss: 8.619314689401959\n",
            "Epoch: 20, Step: 8500, Train Loss: 8.571496933263125, Valid Loss: 8.61565486306281\n",
            "Epoch: 20, Step: 8600, Train Loss: 8.612914405728185, Valid Loss: 8.638752884062551\n",
            "Epoch: 20, Step: 8700, Train Loss: 8.585406226337762, Valid Loss: 8.617160545669664\n",
            "Epoch: 21, Step: 8800, Train Loss: 8.574669202748838, Valid Loss: 8.610921018746629\n",
            "Epoch: 21, Step: 8900, Train Loss: 8.583926463723063, Valid Loss: 8.612187502145668\n",
            "Epoch: 21, Step: 9000, Train Loss: 8.581119699485514, Valid Loss: 8.614463391950387\n",
            "Epoch: 21, Step: 9100, Train Loss: 8.57236909414502, Valid Loss: 8.611716877788526\n",
            "Epoch: 22, Step: 9200, Train Loss: 8.587810013668289, Valid Loss: 8.623559689499993\n",
            "Epoch: 22, Step: 9300, Train Loss: 8.597385649593617, Valid Loss: 8.631724615574232\n",
            "Epoch: 22, Step: 9400, Train Loss: 8.5729003175902, Valid Loss: 8.610238146092444\n",
            "Epoch: 22, Step: 9500, Train Loss: 8.582443679732663, Valid Loss: 8.616931098799354\n",
            "Epoch: 22, Step: 9600, Train Loss: 8.571838012485909, Valid Loss: 8.61035934439842\n",
            "Epoch: 23, Step: 9700, Train Loss: 8.574364420068648, Valid Loss: 8.614491454601726\n",
            "Epoch: 23, Step: 9800, Train Loss: 8.575780345981686, Valid Loss: 8.614823635227019\n",
            "Epoch: 23, Step: 9900, Train Loss: 8.589620419960026, Valid Loss: 8.63102271469698\n",
            "Epoch: 23, Step: 10000, Train Loss: 8.591532729073739, Valid Loss: 8.629237886045896\n",
            "Epoch: 24, Step: 10100, Train Loss: 8.564078668698139, Valid Loss: 8.616131585758506\n",
            "Epoch: 24, Step: 10200, Train Loss: 8.576002308897023, Valid Loss: 8.61256968883156\n",
            "Epoch: 24, Step: 10300, Train Loss: 8.568045370780958, Valid Loss: 8.614336437808383\n",
            "Epoch: 24, Step: 10400, Train Loss: 8.5672313281883, Valid Loss: 8.613282208548895\n",
            "Epoch: 25, Step: 10500, Train Loss: 8.56817205726821, Valid Loss: 8.610545076415757\n",
            "Epoch: 25, Step: 10600, Train Loss: 8.57097105570377, Valid Loss: 8.617910700423359\n",
            "Epoch: 25, Step: 10700, Train Loss: 8.577974661104147, Valid Loss: 8.61734351370197\n",
            "Epoch: 25, Step: 10800, Train Loss: 8.558541743238328, Valid Loss: 8.602014664860683\n",
            "Epoch: 26, Step: 10900, Train Loss: 8.559938935928583, Valid Loss: 8.601039281026432\n",
            "Epoch: 26, Step: 11000, Train Loss: 8.556647386124679, Valid Loss: 8.602600336587514\n",
            "Epoch: 26, Step: 11100, Train Loss: 8.556507674841571, Valid Loss: 8.609013806165859\n",
            "Epoch: 26, Step: 11200, Train Loss: 8.564976759786676, Valid Loss: 8.609091819052361\n",
            "Epoch: 27, Step: 11300, Train Loss: 8.549951377389279, Valid Loss: 8.597580279359732\n",
            "Epoch: 27, Step: 11400, Train Loss: 8.55837961349104, Valid Loss: 8.608516134777588\n",
            "Epoch: 27, Step: 11500, Train Loss: 8.565195841016877, Valid Loss: 8.61280480242744\n",
            "Epoch: 27, Step: 11600, Train Loss: 8.557143882437233, Valid Loss: 8.603510534725885\n",
            "Epoch: 27, Step: 11700, Train Loss: 8.551446734164545, Valid Loss: 8.598009898577548\n",
            "Epoch: 28, Step: 11800, Train Loss: 8.539703344952061, Valid Loss: 8.59130886079342\n",
            "Epoch: 28, Step: 11900, Train Loss: 8.56372144431201, Valid Loss: 8.613271998080807\n",
            "Epoch: 28, Step: 12000, Train Loss: 8.566053032628115, Valid Loss: 8.61703455508508\n",
            "Epoch: 28, Step: 12100, Train Loss: 8.548691144896406, Valid Loss: 8.600461948492166\n",
            "Epoch: 29, Step: 12200, Train Loss: 8.537889812765405, Valid Loss: 8.597295613419805\n",
            "Epoch: 29, Step: 12300, Train Loss: 8.568164224282754, Valid Loss: 8.61730206781286\n",
            "Epoch: 29, Step: 12400, Train Loss: 8.55877416558309, Valid Loss: 8.610831087278235\n",
            "Epoch: 29, Step: 12500, Train Loss: 8.554132977950083, Valid Loss: 8.608018892117451\n",
            "Epoch: 30, Step: 12600, Train Loss: 8.549723102125357, Valid Loss: 8.60803559700134\n",
            "Epoch: 30, Step: 12700, Train Loss: 8.538962314879765, Valid Loss: 8.597317875178852\n",
            "Epoch: 30, Step: 12800, Train Loss: 8.536153707043281, Valid Loss: 8.590433544783163\n",
            "Epoch: 30, Step: 12900, Train Loss: 8.562401205467474, Valid Loss: 8.609064619443535\n",
            "Epoch: 31, Step: 13000, Train Loss: 8.53550453771265, Valid Loss: 8.591362514406528\n",
            "Epoch: 31, Step: 13100, Train Loss: 8.544126595115426, Valid Loss: 8.594906178535796\n",
            "Epoch: 31, Step: 13200, Train Loss: 8.561984344777155, Valid Loss: 8.608818087115344\n",
            "Epoch: 31, Step: 13300, Train Loss: 8.545554255392812, Valid Loss: 8.603358460674\n",
            "Epoch: 32, Step: 13400, Train Loss: 8.546459196968636, Valid Loss: 8.600120167371854\n",
            "Epoch: 32, Step: 13500, Train Loss: 8.55755563269437, Valid Loss: 8.609457661218121\n",
            "Epoch: 32, Step: 13600, Train Loss: 8.541458490159398, Valid Loss: 8.591460243621404\n",
            "Epoch: 32, Step: 13700, Train Loss: 8.529424862329579, Valid Loss: 8.58364153566391\n",
            "Epoch: 33, Step: 13800, Train Loss: 8.538880698812546, Valid Loss: 8.59206523108522\n",
            "Epoch: 33, Step: 13900, Train Loss: 8.559721135919196, Valid Loss: 8.607538307008038\n",
            "Epoch: 33, Step: 14000, Train Loss: 8.542137061306216, Valid Loss: 8.590508262410992\n",
            "Epoch: 33, Step: 14100, Train Loss: 8.537805162096854, Valid Loss: 8.585571591784513\n",
            "Epoch: 33, Step: 14200, Train Loss: 8.542547103332987, Valid Loss: 8.594332533566432\n",
            "Epoch: 34, Step: 14300, Train Loss: 8.54628879495642, Valid Loss: 8.603729657530799\n",
            "Epoch: 34, Step: 14400, Train Loss: 8.567573720219734, Valid Loss: 8.61563284423369\n",
            "Epoch: 34, Step: 14500, Train Loss: 8.559103616173223, Valid Loss: 8.608840292168962\n",
            "Epoch: 34, Step: 14600, Train Loss: 8.537501251640204, Valid Loss: 8.590903709797345\n",
            "Epoch: 35, Step: 14700, Train Loss: 8.559592760670727, Valid Loss: 8.609928247630425\n",
            "Epoch: 35, Step: 14800, Train Loss: 8.526108129105017, Valid Loss: 8.586691735472806\n",
            "Epoch: 35, Step: 14900, Train Loss: 8.541305067395374, Valid Loss: 8.601355574292311\n",
            "Epoch: 35, Step: 15000, Train Loss: 8.555896804158078, Valid Loss: 8.613311047676792\n",
            "Epoch: 36, Step: 15100, Train Loss: 8.538523701388264, Valid Loss: 8.605192794837695\n",
            "Epoch: 36, Step: 15200, Train Loss: 8.549113762502111, Valid Loss: 8.608221295976099\n",
            "Epoch: 36, Step: 15300, Train Loss: 8.519335469858456, Valid Loss: 8.587832109073897\n",
            "Epoch: 36, Step: 15400, Train Loss: 8.529932353781726, Valid Loss: 8.588308198660565\n",
            "Epoch: 37, Step: 15500, Train Loss: 8.529560497846925, Valid Loss: 8.588589923309955\n",
            "Epoch: 37, Step: 15600, Train Loss: 8.548935506413661, Valid Loss: 8.60346372266045\n",
            "Epoch: 37, Step: 15700, Train Loss: 8.535520853573457, Valid Loss: 8.602727128543561\n",
            "Epoch: 37, Step: 15800, Train Loss: 8.53259204259132, Valid Loss: 8.592567909127297\n",
            "Epoch: 38, Step: 15900, Train Loss: 8.54411753480016, Valid Loss: 8.596281652648146\n",
            "Epoch: 38, Step: 16000, Train Loss: 8.530365112287434, Valid Loss: 8.588160779405566\n",
            "Epoch: 38, Step: 16100, Train Loss: 8.535437148512738, Valid Loss: 8.59510832028465\n",
            "Epoch: 38, Step: 16200, Train Loss: 8.526749063199686, Valid Loss: 8.57875552484604\n",
            "Epoch: 38, Step: 16300, Train Loss: 8.542156807241515, Valid Loss: 8.592985735678075\n",
            "Epoch: 39, Step: 16400, Train Loss: 8.527336921016001, Valid Loss: 8.577073054024254\n",
            "Epoch: 39, Step: 16500, Train Loss: 8.551516335639873, Valid Loss: 8.593252426139117\n",
            "Epoch: 39, Step: 16600, Train Loss: 8.541364494932042, Valid Loss: 8.602177593469778\n",
            "Epoch: 39, Step: 16700, Train Loss: 8.52735277449129, Valid Loss: 8.589002796744104\n",
            "Epoch: 40, Step: 16800, Train Loss: 8.522285142726153, Valid Loss: 8.591679953192505\n",
            "Epoch: 40, Step: 16900, Train Loss: 8.50907535625285, Valid Loss: 8.580218711462075\n",
            "Epoch: 40, Step: 17000, Train Loss: 8.540044445094837, Valid Loss: 8.60312478532959\n",
            "Epoch: 40, Step: 17100, Train Loss: 8.539685527569752, Valid Loss: 8.595144789042184\n",
            "Epoch: 41, Step: 17200, Train Loss: 8.526761762443929, Valid Loss: 8.582627814378998\n",
            "Epoch: 41, Step: 17300, Train Loss: 8.523939973751558, Valid Loss: 8.583154413404948\n",
            "Epoch: 41, Step: 17400, Train Loss: 8.512281377712602, Valid Loss: 8.574382583278577\n",
            "Epoch: 41, Step: 17500, Train Loss: 8.523161050954604, Valid Loss: 8.593760131963263\n",
            "Epoch: 42, Step: 17600, Train Loss: 8.518296930773142, Valid Loss: 8.582207692213478\n",
            "Epoch: 42, Step: 17700, Train Loss: 8.51518321659306, Valid Loss: 8.58017053526752\n",
            "Epoch: 42, Step: 17800, Train Loss: 8.527658201549876, Valid Loss: 8.593639679803422\n",
            "Epoch: 42, Step: 17900, Train Loss: 8.525308183188594, Valid Loss: 8.595062672864712\n",
            "Epoch: 43, Step: 18000, Train Loss: 8.515174466504805, Valid Loss: 8.58107514126895\n",
            "Epoch: 43, Step: 18100, Train Loss: 8.531350896922813, Valid Loss: 8.587795459672776\n",
            "Epoch: 43, Step: 18200, Train Loss: 8.517303556094836, Valid Loss: 8.57830217365291\n",
            "Epoch: 43, Step: 18300, Train Loss: 8.52186074074765, Valid Loss: 8.587260743184961\n",
            "Epoch: 44, Step: 18400, Train Loss: 8.530087893907302, Valid Loss: 8.589730981471288\n",
            "Epoch: 44, Step: 18500, Train Loss: 8.517557223993412, Valid Loss: 8.585435882605651\n",
            "Epoch: 44, Step: 18600, Train Loss: 8.522356386216359, Valid Loss: 8.589848882280199\n",
            "Epoch: 44, Step: 18700, Train Loss: 8.508703342046493, Valid Loss: 8.575639450719082\n",
            "Epoch: 44, Step: 18800, Train Loss: 8.52296309536519, Valid Loss: 8.586405962927103\n",
            "Epoch: 45, Step: 18900, Train Loss: 8.523591279515616, Valid Loss: 8.5807690269945\n",
            "Epoch: 45, Step: 19000, Train Loss: 8.521392388040832, Valid Loss: 8.584337872918033\n",
            "Epoch: 45, Step: 19100, Train Loss: 8.52200589240806, Valid Loss: 8.579019218499605\n",
            "Epoch: 45, Step: 19200, Train Loss: 8.509371099927018, Valid Loss: 8.576939786085566\n",
            "Epoch: 46, Step: 19300, Train Loss: 8.523970683921824, Valid Loss: 8.590068177060036\n",
            "Epoch: 46, Step: 19400, Train Loss: 8.519051304279332, Valid Loss: 8.58857083642638\n",
            "Epoch: 46, Step: 19500, Train Loss: 8.527564611236436, Valid Loss: 8.587437287840714\n",
            "Epoch: 46, Step: 19600, Train Loss: 8.52630695272544, Valid Loss: 8.59472609712841\n",
            "Epoch: 47, Step: 19700, Train Loss: 8.529835496522487, Valid Loss: 8.597750064735974\n",
            "Epoch: 47, Step: 19800, Train Loss: 8.4957865578779, Valid Loss: 8.571954640974475\n",
            "Epoch: 47, Step: 19900, Train Loss: 8.526131408031489, Valid Loss: 8.584976452171041\n",
            "Epoch: 47, Step: 20000, Train Loss: 8.543574123707266, Valid Loss: 8.599663389949665\n",
            "Epoch: 48, Step: 20100, Train Loss: 8.529767892177365, Valid Loss: 8.594396178221292\n",
            "Epoch: 48, Step: 20200, Train Loss: 8.52845098439996, Valid Loss: 8.597163190526004\n",
            "Epoch: 48, Step: 20300, Train Loss: 8.519400447459043, Valid Loss: 8.585861024762563\n",
            "Epoch: 48, Step: 20400, Train Loss: 8.51496835868361, Valid Loss: 8.577425574520491\n",
            "Epoch: 49, Step: 20500, Train Loss: 8.502541063254986, Valid Loss: 8.569459758283942\n",
            "Epoch: 49, Step: 20600, Train Loss: 8.52809821484829, Valid Loss: 8.596881232390716\n",
            "Epoch: 49, Step: 20700, Train Loss: 8.52269084397696, Valid Loss: 8.589882347976348\n",
            "Epoch: 49, Step: 20800, Train Loss: 8.506339945204427, Valid Loss: 8.585707318441395\n",
            "Epoch: 49, Step: 20900, Train Loss: 8.532989011468702, Valid Loss: 8.605060941341993\n",
            "Epoch: 50, Step: 21000, Train Loss: 8.510220331052246, Valid Loss: 8.589427350446606\n",
            "Epoch: 50, Step: 21100, Train Loss: 8.511751223674677, Valid Loss: 8.584979452306435\n",
            "Epoch: 50, Step: 21200, Train Loss: 8.514452431505578, Valid Loss: 8.58831439277394\n",
            "Epoch: 50, Step: 21300, Train Loss: 8.528123102794403, Valid Loss: 8.604024740813307\n",
            "Epoch: 51, Step: 21400, Train Loss: 8.493526743739247, Valid Loss: 8.579646254355668\n",
            "Epoch: 51, Step: 21500, Train Loss: 8.515315055993442, Valid Loss: 8.602319004677002\n",
            "Epoch: 51, Step: 21600, Train Loss: 8.522124720510563, Valid Loss: 8.602995548829089\n",
            "Epoch: 51, Step: 21700, Train Loss: 8.510680992842012, Valid Loss: 8.586885307637965\n",
            "Epoch: 52, Step: 21800, Train Loss: 8.514915717368032, Valid Loss: 8.594396207626176\n",
            "Epoch: 52, Step: 21900, Train Loss: 8.516780861907883, Valid Loss: 8.591918410635783\n",
            "Epoch: 52, Step: 22000, Train Loss: 8.493221764631059, Valid Loss: 8.571728568590315\n",
            "Epoch: 52, Step: 22100, Train Loss: 8.497533614183395, Valid Loss: 8.584487813511682\n",
            "Epoch: 53, Step: 22200, Train Loss: 8.508433581162373, Valid Loss: 8.588227284885608\n",
            "Epoch: 53, Step: 22300, Train Loss: 8.524093948437454, Valid Loss: 8.595049549492888\n",
            "Epoch: 53, Step: 22400, Train Loss: 8.49686088356851, Valid Loss: 8.580889495387328\n",
            "Epoch: 53, Step: 22500, Train Loss: 8.52852362805984, Valid Loss: 8.602614143691333\n",
            "Epoch: 54, Step: 22600, Train Loss: 8.497440827983674, Valid Loss: 8.591034792675087\n",
            "Epoch: 54, Step: 22700, Train Loss: 8.52022709710753, Valid Loss: 8.592788918745505\n",
            "Epoch: 54, Step: 22800, Train Loss: 8.49190921359266, Valid Loss: 8.56554178637822\n",
            "Epoch: 54, Step: 22900, Train Loss: 8.492160060201108, Valid Loss: 8.57188876840611\n",
            "Epoch: 55, Step: 23000, Train Loss: 8.508103617363128, Valid Loss: 8.57407975473107\n",
            "Epoch: 55, Step: 23100, Train Loss: 8.487333726600855, Valid Loss: 8.568483138704476\n",
            "Epoch: 55, Step: 23200, Train Loss: 8.506770266125814, Valid Loss: 8.576924538074207\n",
            "Epoch: 55, Step: 23300, Train Loss: 8.503831134338888, Valid Loss: 8.578062623272293\n",
            "Epoch: 55, Step: 23400, Train Loss: 8.527888652037154, Valid Loss: 8.593415192485654\n",
            "Epoch: 56, Step: 23500, Train Loss: 8.500623359177126, Valid Loss: 8.57414165108055\n",
            "Epoch: 56, Step: 23600, Train Loss: 8.486960783367069, Valid Loss: 8.573495040607582\n",
            "Epoch: 56, Step: 23700, Train Loss: 8.511765047948328, Valid Loss: 8.587253670336281\n",
            "Epoch: 56, Step: 23800, Train Loss: 8.510290013715004, Valid Loss: 8.58208575279398\n",
            "Epoch: 57, Step: 23900, Train Loss: 8.51052090040991, Valid Loss: 8.57772890364944\n",
            "Epoch: 57, Step: 24000, Train Loss: 8.497852769528667, Valid Loss: 8.576901727391471\n",
            "Epoch: 57, Step: 24100, Train Loss: 8.49131796699191, Valid Loss: 8.575531624702109\n",
            "Epoch: 57, Step: 24200, Train Loss: 8.509322594701576, Valid Loss: 8.584029473150322\n",
            "Epoch: 58, Step: 24300, Train Loss: 8.513209712934634, Valid Loss: 8.589296500482876\n",
            "Epoch: 58, Step: 24400, Train Loss: 8.516412155244728, Valid Loss: 8.591054898752972\n",
            "Epoch: 58, Step: 24500, Train Loss: 8.482480888291345, Valid Loss: 8.572986643153675\n",
            "Epoch: 58, Step: 24600, Train Loss: 8.49841110126213, Valid Loss: 8.575870788169063\n",
            "Epoch: 59, Step: 24700, Train Loss: 8.5015213582488, Valid Loss: 8.582575418934669\n",
            "Epoch: 59, Step: 24800, Train Loss: 8.492394104772247, Valid Loss: 8.57833788406983\n",
            "Epoch: 59, Step: 24900, Train Loss: 8.501603822062787, Valid Loss: 8.587640397501563\n",
            "Epoch: 59, Step: 25000, Train Loss: 8.51794313908239, Valid Loss: 8.605230870352427\n",
            "Epoch: 60, Step: 25100, Train Loss: 8.506485020059445, Valid Loss: 8.599516686979651\n",
            "Epoch: 60, Step: 25200, Train Loss: 8.476495670493989, Valid Loss: 8.572292452276198\n",
            "Epoch: 60, Step: 25300, Train Loss: 8.50818935230725, Valid Loss: 8.59801711198988\n",
            "Epoch: 60, Step: 25400, Train Loss: 8.521866231005035, Valid Loss: 8.60389459749495\n",
            "Epoch: 61, Step: 25500, Train Loss: 8.506400359732762, Valid Loss: 8.585040595475155\n",
            "Epoch: 61, Step: 25600, Train Loss: 8.47758148891256, Valid Loss: 8.58096293401398\n",
            "Epoch: 61, Step: 25700, Train Loss: 8.49073186827335, Valid Loss: 8.581041543056253\n",
            "Epoch: 61, Step: 25800, Train Loss: 8.491397464768026, Valid Loss: 8.579734301109363\n",
            "Epoch: 61, Step: 25900, Train Loss: 8.506760552069354, Valid Loss: 8.584439842815911\n",
            "Epoch: 62, Step: 26000, Train Loss: 8.478894873632742, Valid Loss: 8.569260801118068\n",
            "Epoch: 62, Step: 26100, Train Loss: 8.506217774088707, Valid Loss: 8.57996286498829\n",
            "Epoch: 62, Step: 26200, Train Loss: 8.529725960359565, Valid Loss: 8.609756510761965\n",
            "Epoch: 62, Step: 26300, Train Loss: 8.494683270019227, Valid Loss: 8.572978089524506\n",
            "Epoch: 63, Step: 26400, Train Loss: 8.499323046433007, Valid Loss: 8.584175250933404\n",
            "Epoch: 63, Step: 26500, Train Loss: 8.492788717535468, Valid Loss: 8.582534168308525\n",
            "Epoch: 63, Step: 26600, Train Loss: 8.48908658628874, Valid Loss: 8.582510420460595\n",
            "Epoch: 63, Step: 26700, Train Loss: 8.495733814233496, Valid Loss: 8.583751202936316\n",
            "Epoch: 64, Step: 26800, Train Loss: 8.484148033871211, Valid Loss: 8.57423422213223\n",
            "Epoch: 64, Step: 26900, Train Loss: 8.464800624839638, Valid Loss: 8.567951901847229\n",
            "Epoch: 64, Step: 27000, Train Loss: 8.48138553546428, Valid Loss: 8.57492934871867\n",
            "Epoch: 64, Step: 27100, Train Loss: 8.485713995949933, Valid Loss: 8.573373387489113\n",
            "Epoch: 65, Step: 27200, Train Loss: 8.482329614494539, Valid Loss: 8.573208430226755\n",
            "Epoch: 65, Step: 27300, Train Loss: 8.501062386447842, Valid Loss: 8.596527348995743\n",
            "Epoch: 65, Step: 27400, Train Loss: 8.498335734791656, Valid Loss: 8.581715769471671\n",
            "Epoch: 65, Step: 27500, Train Loss: 8.502581558549112, Valid Loss: 8.580078182896385\n",
            "Epoch: 66, Step: 27600, Train Loss: 8.486929424246007, Valid Loss: 8.568958944218323\n",
            "Epoch: 66, Step: 27700, Train Loss: 8.487922559826242, Valid Loss: 8.573537521150588\n",
            "Epoch: 66, Step: 27800, Train Loss: 8.478334887402728, Valid Loss: 8.571220132698922\n",
            "Epoch: 66, Step: 27900, Train Loss: 8.475305366813231, Valid Loss: 8.575516914463282\n",
            "Epoch: 66, Step: 28000, Train Loss: 8.514880505359578, Valid Loss: 8.601415594756068\n",
            "Epoch: 67, Step: 28100, Train Loss: 8.48441709086478, Valid Loss: 8.569197198337143\n",
            "Epoch: 67, Step: 28200, Train Loss: 8.48391034480909, Valid Loss: 8.573496064918283\n",
            "Epoch: 67, Step: 28300, Train Loss: 8.492444534019105, Valid Loss: 8.580320148613586\n",
            "Epoch: 67, Step: 28400, Train Loss: 8.487709664219592, Valid Loss: 8.582764154399545\n",
            "Epoch: 68, Step: 28500, Train Loss: 8.5022835269884, Valid Loss: 8.590722850727316\n",
            "Epoch: 68, Step: 28600, Train Loss: 8.487663478901492, Valid Loss: 8.576995972686351\n",
            "Epoch: 68, Step: 28700, Train Loss: 8.495258856321731, Valid Loss: 8.584198157594246\n",
            "Epoch: 68, Step: 28800, Train Loss: 8.483482542162841, Valid Loss: 8.568722485004933\n",
            "Epoch: 69, Step: 28900, Train Loss: 8.491815825578069, Valid Loss: 8.575925815323515\n",
            "Epoch: 69, Step: 29000, Train Loss: 8.49032942240801, Valid Loss: 8.58095818505252\n",
            "Epoch: 69, Step: 29100, Train Loss: 8.486798558308076, Valid Loss: 8.578762727444724\n",
            "Epoch: 69, Step: 29200, Train Loss: 8.480526928601304, Valid Loss: 8.57164246409821\n",
            "Epoch: 70, Step: 29300, Train Loss: 8.489469000201273, Valid Loss: 8.587410399765771\n",
            "Epoch: 70, Step: 29400, Train Loss: 8.461260463829875, Valid Loss: 8.569181420449013\n",
            "Epoch: 70, Step: 29500, Train Loss: 8.476033090040394, Valid Loss: 8.575845351988248\n",
            "Epoch: 70, Step: 29600, Train Loss: 8.481031136820597, Valid Loss: 8.572327693928539\n",
            "Epoch: 71, Step: 29700, Train Loss: 8.496448570334605, Valid Loss: 8.580085158571306\n",
            "Epoch: 71, Step: 29800, Train Loss: 8.48828491348701, Valid Loss: 8.58265528425537\n",
            "Epoch: 71, Step: 29900, Train Loss: 8.486038333327317, Valid Loss: 8.5736420733842\n",
            "Epoch: 71, Step: 30000, Train Loss: 8.494825564896779, Valid Loss: 8.582817557167658\n",
            "Epoch: 72, Step: 30100, Train Loss: 8.496147340644791, Valid Loss: 8.588540916228814\n",
            "Epoch: 72, Step: 30200, Train Loss: 8.46581404928258, Valid Loss: 8.571446736913312\n",
            "Epoch: 72, Step: 30300, Train Loss: 8.492343552336884, Valid Loss: 8.589384472833666\n",
            "Epoch: 72, Step: 30400, Train Loss: 8.485217837123503, Valid Loss: 8.579285510964292\n",
            "Epoch: 72, Step: 30500, Train Loss: 8.482439449601532, Valid Loss: 8.577244755352128\n",
            "Epoch: 73, Step: 30600, Train Loss: 8.4900011277347, Valid Loss: 8.586181521946584\n",
            "Epoch: 73, Step: 30700, Train Loss: 8.479076767427802, Valid Loss: 8.57044524538936\n",
            "Epoch: 73, Step: 30800, Train Loss: 8.467061082724381, Valid Loss: 8.568707971985598\n",
            "Epoch: 73, Step: 30900, Train Loss: 8.464243178922027, Valid Loss: 8.567814055327869\n",
            "Epoch: 74, Step: 31000, Train Loss: 8.472240373778227, Valid Loss: 8.564324310815124\n",
            "Epoch: 74, Step: 31100, Train Loss: 8.469029821303906, Valid Loss: 8.56562091065769\n",
            "Epoch: 74, Step: 31200, Train Loss: 8.480187249220034, Valid Loss: 8.573616244944928\n",
            "Epoch: 74, Step: 31300, Train Loss: 8.475896643226445, Valid Loss: 8.572206694936328\n",
            "Epoch: 75, Step: 31400, Train Loss: 8.482041720792047, Valid Loss: 8.576218714544208\n",
            "Epoch: 75, Step: 31500, Train Loss: 8.485533285963571, Valid Loss: 8.57991802553474\n",
            "Epoch: 75, Step: 31600, Train Loss: 8.477524610704036, Valid Loss: 8.57005050288313\n",
            "Epoch: 75, Step: 31700, Train Loss: 8.48860765857126, Valid Loss: 8.58693935431838\n",
            "Epoch: 76, Step: 31800, Train Loss: 8.489555822151965, Valid Loss: 8.584518439645697\n",
            "Epoch: 76, Step: 31900, Train Loss: 8.48991604202564, Valid Loss: 8.585045820528153\n",
            "Epoch: 76, Step: 32000, Train Loss: 8.473867628759882, Valid Loss: 8.565930477192927\n",
            "Epoch: 76, Step: 32100, Train Loss: 8.467769052944353, Valid Loss: 8.575400157494094\n",
            "Epoch: 77, Step: 32200, Train Loss: 8.48443573806302, Valid Loss: 8.579526512266172\n",
            "Epoch: 77, Step: 32300, Train Loss: 8.48761934556378, Valid Loss: 8.585080671304999\n",
            "Epoch: 77, Step: 32400, Train Loss: 8.467373169983437, Valid Loss: 8.568514293942691\n",
            "Epoch: 77, Step: 32500, Train Loss: 8.481044526682838, Valid Loss: 8.574945484425413\n",
            "Epoch: 77, Step: 32600, Train Loss: 8.466004115152302, Valid Loss: 8.56537923288828\n",
            "Epoch: 78, Step: 32700, Train Loss: 8.483712801138893, Valid Loss: 8.575719275037194\n",
            "Epoch: 78, Step: 32800, Train Loss: 8.492629797982449, Valid Loss: 8.583289822515665\n",
            "Epoch: 78, Step: 32900, Train Loss: 8.48041120976289, Valid Loss: 8.576813990408874\n",
            "Epoch: 78, Step: 33000, Train Loss: 8.475488157336539, Valid Loss: 8.587361130902131\n",
            "Epoch: 79, Step: 33100, Train Loss: 8.484496127091637, Valid Loss: 8.588680712933632\n",
            "Epoch: 79, Step: 33200, Train Loss: 8.483360733371773, Valid Loss: 8.57583564948233\n",
            "Epoch: 79, Step: 33300, Train Loss: 8.484681940779785, Valid Loss: 8.581117736447235\n",
            "Epoch: 79, Step: 33400, Train Loss: 8.484517650883454, Valid Loss: 8.575903154237158\n",
            "Epoch: 80, Step: 33500, Train Loss: 8.477193943424544, Valid Loss: 8.58190589070542\n",
            "Epoch: 80, Step: 33600, Train Loss: 8.468539626450903, Valid Loss: 8.572295184154324\n",
            "Epoch: 80, Step: 33700, Train Loss: 8.494816982526311, Valid Loss: 8.585014492240227\n",
            "Epoch: 80, Step: 33800, Train Loss: 8.455377679817998, Valid Loss: 8.55560594714699\n",
            "Epoch: 81, Step: 33900, Train Loss: 8.47444268609979, Valid Loss: 8.570606769056146\n",
            "Epoch: 81, Step: 34000, Train Loss: 8.485312760132643, Valid Loss: 8.576638611970314\n",
            "Epoch: 81, Step: 34100, Train Loss: 8.480020250849337, Valid Loss: 8.572296331444608\n",
            "Epoch: 81, Step: 34200, Train Loss: 8.477570268312046, Valid Loss: 8.566040742448552\n",
            "Epoch: 82, Step: 34300, Train Loss: 8.480366078739577, Valid Loss: 8.566922606907013\n",
            "Epoch: 82, Step: 34400, Train Loss: 8.491980636972801, Valid Loss: 8.576493698115122\n",
            "Epoch: 82, Step: 34500, Train Loss: 8.458882793560504, Valid Loss: 8.566874795973174\n",
            "Epoch: 82, Step: 34600, Train Loss: 8.459549808883043, Valid Loss: 8.561333633684859\n",
            "Epoch: 83, Step: 34700, Train Loss: 8.459525119994629, Valid Loss: 8.565941540660564\n",
            "Epoch: 83, Step: 34800, Train Loss: 8.468012606403962, Valid Loss: 8.56589458228843\n",
            "Epoch: 83, Step: 34900, Train Loss: 8.472114355455057, Valid Loss: 8.57825150221634\n",
            "Epoch: 83, Step: 35000, Train Loss: 8.489155672841102, Valid Loss: 8.582501370839752\n",
            "Epoch: 83, Step: 35100, Train Loss: 8.476546168100198, Valid Loss: 8.5715562334374\n",
            "Epoch: 84, Step: 35200, Train Loss: 8.465675743129301, Valid Loss: 8.578217222721566\n",
            "Epoch: 84, Step: 35300, Train Loss: 8.473161444237382, Valid Loss: 8.576912130936428\n",
            "Epoch: 84, Step: 35400, Train Loss: 8.478227209839497, Valid Loss: 8.58160786519662\n",
            "Epoch: 84, Step: 35500, Train Loss: 8.471891033183525, Valid Loss: 8.583784420074132\n",
            "Epoch: 85, Step: 35600, Train Loss: 8.467399498220274, Valid Loss: 8.579354272384485\n",
            "Epoch: 85, Step: 35700, Train Loss: 8.48124349901797, Valid Loss: 8.587733056013438\n",
            "Epoch: 85, Step: 35800, Train Loss: 8.462540617198535, Valid Loss: 8.576654869690595\n",
            "Epoch: 85, Step: 35900, Train Loss: 8.48653090801552, Valid Loss: 8.589686425925073\n",
            "Epoch: 86, Step: 36000, Train Loss: 8.478348277935279, Valid Loss: 8.579472043323886\n",
            "Epoch: 86, Step: 36100, Train Loss: 8.461628756043808, Valid Loss: 8.563102136930306\n",
            "Epoch: 86, Step: 36200, Train Loss: 8.47750908635469, Valid Loss: 8.570747907494104\n",
            "Epoch: 86, Step: 36300, Train Loss: 8.478169533916818, Valid Loss: 8.574674724719893\n",
            "Epoch: 87, Step: 36400, Train Loss: 8.451747584207572, Valid Loss: 8.551270287293892\n",
            "Epoch: 87, Step: 36500, Train Loss: 8.442389000371056, Valid Loss: 8.558665880122204\n",
            "Epoch: 87, Step: 36600, Train Loss: 8.470365890936218, Valid Loss: 8.573324890339157\n",
            "Epoch: 87, Step: 36700, Train Loss: 8.470187941498567, Valid Loss: 8.574603256033162\n",
            "Epoch: 88, Step: 36800, Train Loss: 8.449795623161746, Valid Loss: 8.560697527023606\n",
            "Epoch: 88, Step: 36900, Train Loss: 8.478788316194366, Valid Loss: 8.576502189305591\n",
            "Epoch: 88, Step: 37000, Train Loss: 8.458274424825495, Valid Loss: 8.57416802321085\n",
            "Epoch: 88, Step: 37100, Train Loss: 8.466040596768304, Valid Loss: 8.579643907738998\n",
            "Epoch: 88, Step: 37200, Train Loss: 8.460639258780377, Valid Loss: 8.565144762349886\n",
            "Epoch: 89, Step: 37300, Train Loss: 8.462709300978757, Valid Loss: 8.573912035484048\n",
            "Epoch: 89, Step: 37400, Train Loss: 8.461659140951086, Valid Loss: 8.559102632004901\n",
            "Epoch: 89, Step: 37500, Train Loss: 8.468820663800425, Valid Loss: 8.565466255963422\n",
            "Epoch: 89, Step: 37600, Train Loss: 8.462357329940676, Valid Loss: 8.560811333229967\n",
            "Epoch: 90, Step: 37700, Train Loss: 8.447511480862929, Valid Loss: 8.557498531024025\n",
            "Epoch: 90, Step: 37800, Train Loss: 8.465878206322529, Valid Loss: 8.573293718192451\n",
            "Epoch: 90, Step: 37900, Train Loss: 8.481812575966195, Valid Loss: 8.578136935548056\n",
            "Epoch: 90, Step: 38000, Train Loss: 8.471890838737902, Valid Loss: 8.572764118278878\n",
            "Epoch: 91, Step: 38100, Train Loss: 8.464181144810185, Valid Loss: 8.569646079372385\n",
            "Epoch: 91, Step: 38200, Train Loss: 8.469644590125219, Valid Loss: 8.576595307008757\n",
            "Epoch: 91, Step: 38300, Train Loss: 8.485267897577543, Valid Loss: 8.581140931047763\n",
            "Epoch: 91, Step: 38400, Train Loss: 8.464738819277907, Valid Loss: 8.567940978621989\n",
            "Epoch: 92, Step: 38500, Train Loss: 8.465811074671384, Valid Loss: 8.569998551508546\n",
            "Epoch: 92, Step: 38600, Train Loss: 8.461799786112804, Valid Loss: 8.55945618293988\n",
            "Epoch: 92, Step: 38700, Train Loss: 8.44357910761751, Valid Loss: 8.5489586577138\n",
            "Epoch: 92, Step: 38800, Train Loss: 8.467015152885576, Valid Loss: 8.568362975163456\n",
            "Epoch: 93, Step: 38900, Train Loss: 8.466176672870558, Valid Loss: 8.57158752246452\n",
            "Epoch: 93, Step: 39000, Train Loss: 8.46663207385481, Valid Loss: 8.568283809843113\n",
            "Epoch: 93, Step: 39100, Train Loss: 8.476144426252512, Valid Loss: 8.571749540551565\n",
            "Epoch: 93, Step: 39200, Train Loss: 8.445975550006779, Valid Loss: 8.561166090298231\n",
            "Epoch: 94, Step: 39300, Train Loss: 8.465664356661076, Valid Loss: 8.56787166595979\n",
            "Epoch: 94, Step: 39400, Train Loss: 8.458835585833434, Valid Loss: 8.561556192452091\n",
            "Epoch: 94, Step: 39500, Train Loss: 8.463600950118293, Valid Loss: 8.57585158457168\n",
            "Epoch: 94, Step: 39600, Train Loss: 8.460350594626336, Valid Loss: 8.567547013019057\n",
            "Epoch: 94, Step: 39700, Train Loss: 8.454580973380864, Valid Loss: 8.561032801054916\n",
            "Epoch: 95, Step: 39800, Train Loss: 8.449107307539842, Valid Loss: 8.5586140368091\n",
            "Epoch: 95, Step: 39900, Train Loss: 8.454015321442544, Valid Loss: 8.560274530173869\n",
            "Epoch: 95, Step: 40000, Train Loss: 8.443632921131417, Valid Loss: 8.551399411543295\n",
            "Epoch: 95, Step: 40100, Train Loss: 8.457196105251791, Valid Loss: 8.55574995205369\n",
            "Epoch: 96, Step: 40200, Train Loss: 8.466502473677028, Valid Loss: 8.567231325041488\n",
            "Epoch: 96, Step: 40300, Train Loss: 8.486060391451998, Valid Loss: 8.586107728153623\n",
            "Epoch: 96, Step: 40400, Train Loss: 8.467194765779277, Valid Loss: 8.576099572885957\n",
            "Epoch: 96, Step: 40500, Train Loss: 8.468913315962105, Valid Loss: 8.57085868819988\n",
            "Epoch: 97, Step: 40600, Train Loss: 8.462692326022504, Valid Loss: 8.571661799970837\n",
            "Epoch: 97, Step: 40700, Train Loss: 8.475305919269097, Valid Loss: 8.57349871535236\n",
            "Epoch: 97, Step: 40800, Train Loss: 8.489308143637631, Valid Loss: 8.5835525009716\n",
            "Epoch: 97, Step: 40900, Train Loss: 8.485070978423096, Valid Loss: 8.577435289959453\n",
            "Epoch: 98, Step: 41000, Train Loss: 8.443637646674501, Valid Loss: 8.54911682821459\n",
            "Epoch: 98, Step: 41100, Train Loss: 8.459144309443984, Valid Loss: 8.571640596845176\n",
            "Epoch: 98, Step: 41200, Train Loss: 8.464278334258669, Valid Loss: 8.57415481627821\n",
            "Epoch: 98, Step: 41300, Train Loss: 8.460519655340438, Valid Loss: 8.569473832589145\n",
            "Epoch: 99, Step: 41400, Train Loss: 8.456065130008326, Valid Loss: 8.56367309813237\n",
            "Epoch: 99, Step: 41500, Train Loss: 8.441202537184939, Valid Loss: 8.551718378082558\n",
            "Epoch: 99, Step: 41600, Train Loss: 8.481331255752169, Valid Loss: 8.573750183533344\n",
            "Epoch: 99, Step: 41700, Train Loss: 8.46683017965248, Valid Loss: 8.567123503818989\n",
            "Epoch: 99, Step: 41800, Train Loss: 8.456290229706296, Valid Loss: 8.5667748245599\n",
            "Test RMSE Loss for h = 20: 8.779735188766702\n",
            "Start training for h = 45\n",
            "Epoch: 0, Step: 100, Train Loss: 9.49367666788485, Valid Loss: 9.447747338113539\n",
            "Epoch: 0, Step: 200, Train Loss: 9.19108824800436, Valid Loss: 9.133762290104091\n",
            "Epoch: 0, Step: 300, Train Loss: 9.047188572895609, Valid Loss: 8.985598905075825\n",
            "Epoch: 0, Step: 400, Train Loss: 8.975694283677418, Valid Loss: 8.918477374867233\n",
            "Epoch: 1, Step: 500, Train Loss: 8.935391535839036, Valid Loss: 8.885267208112653\n",
            "Epoch: 1, Step: 600, Train Loss: 8.909658128701876, Valid Loss: 8.856698065907372\n",
            "Epoch: 1, Step: 700, Train Loss: 8.896390581300729, Valid Loss: 8.840883345922943\n",
            "Epoch: 1, Step: 800, Train Loss: 8.868972323252605, Valid Loss: 8.807101544651335\n",
            "Epoch: 2, Step: 900, Train Loss: 8.861714440532587, Valid Loss: 8.808756485626272\n",
            "Epoch: 2, Step: 1000, Train Loss: 8.852309410833143, Valid Loss: 8.805410857506468\n",
            "Epoch: 2, Step: 1100, Train Loss: 8.820442307868158, Valid Loss: 8.775154305615102\n",
            "Epoch: 2, Step: 1200, Train Loss: 8.83800808336628, Valid Loss: 8.78924872231066\n",
            "Epoch: 3, Step: 1300, Train Loss: 8.796995605602113, Valid Loss: 8.75304365407959\n",
            "Epoch: 3, Step: 1400, Train Loss: 8.777107939906458, Valid Loss: 8.734014536436767\n",
            "Epoch: 3, Step: 1500, Train Loss: 8.781089384358605, Valid Loss: 8.73943054461119\n",
            "Epoch: 3, Step: 1600, Train Loss: 8.76744967113228, Valid Loss: 8.730065240213605\n",
            "Epoch: 4, Step: 1700, Train Loss: 8.765073680997261, Valid Loss: 8.73029889020964\n",
            "Epoch: 4, Step: 1800, Train Loss: 8.754821678461575, Valid Loss: 8.714711008091664\n",
            "Epoch: 4, Step: 1900, Train Loss: 8.746113363337942, Valid Loss: 8.709733710219222\n",
            "Epoch: 4, Step: 2000, Train Loss: 8.760568704547955, Valid Loss: 8.72889088409594\n",
            "Epoch: 5, Step: 2100, Train Loss: 8.751809726726243, Valid Loss: 8.720897872829216\n",
            "Epoch: 5, Step: 2200, Train Loss: 8.762495411829653, Valid Loss: 8.729500296071146\n",
            "Epoch: 5, Step: 2300, Train Loss: 8.717271922700725, Valid Loss: 8.68724650820778\n",
            "Epoch: 5, Step: 2400, Train Loss: 8.712215184029725, Valid Loss: 8.680218333755894\n",
            "Epoch: 5, Step: 2500, Train Loss: 8.72198817457136, Valid Loss: 8.691220534009092\n",
            "Epoch: 6, Step: 2600, Train Loss: 8.7300215646678, Valid Loss: 8.700469717262338\n",
            "Epoch: 6, Step: 2700, Train Loss: 8.714056274314052, Valid Loss: 8.6861874924762\n",
            "Epoch: 6, Step: 2800, Train Loss: 8.726503383634007, Valid Loss: 8.697894671665873\n",
            "Epoch: 6, Step: 2900, Train Loss: 8.708303030208747, Valid Loss: 8.690403990565775\n",
            "Epoch: 7, Step: 3000, Train Loss: 8.71995048667461, Valid Loss: 8.698320563764044\n",
            "Epoch: 7, Step: 3100, Train Loss: 8.695334328100657, Valid Loss: 8.675068848830007\n",
            "Epoch: 7, Step: 3200, Train Loss: 8.696920360007027, Valid Loss: 8.675108168897031\n",
            "Epoch: 7, Step: 3300, Train Loss: 8.6857557522386, Valid Loss: 8.665144070453492\n",
            "Epoch: 8, Step: 3400, Train Loss: 8.679222353649, Valid Loss: 8.664643855348485\n",
            "Epoch: 8, Step: 3500, Train Loss: 8.707857859291128, Valid Loss: 8.690849301054516\n",
            "Epoch: 8, Step: 3600, Train Loss: 8.661059406241392, Valid Loss: 8.651327424309548\n",
            "Epoch: 8, Step: 3700, Train Loss: 8.675038945863568, Valid Loss: 8.664170331797473\n",
            "Epoch: 9, Step: 3800, Train Loss: 8.700538504917672, Valid Loss: 8.69633913437823\n",
            "Epoch: 9, Step: 3900, Train Loss: 8.667632432903861, Valid Loss: 8.658725786864254\n",
            "Epoch: 9, Step: 4000, Train Loss: 8.679630573181335, Valid Loss: 8.669662338320357\n",
            "Epoch: 9, Step: 4100, Train Loss: 8.691810614928, Valid Loss: 8.677539379346381\n",
            "Epoch: 10, Step: 4200, Train Loss: 8.66629556671632, Valid Loss: 8.653383167079062\n",
            "Epoch: 10, Step: 4300, Train Loss: 8.684362843380482, Valid Loss: 8.674301883741045\n",
            "Epoch: 10, Step: 4400, Train Loss: 8.659618036584813, Valid Loss: 8.657519783201591\n",
            "Epoch: 10, Step: 4500, Train Loss: 8.6706011948293, Valid Loss: 8.66623069258444\n",
            "Epoch: 11, Step: 4600, Train Loss: 8.648552441383526, Valid Loss: 8.650320675841483\n",
            "Epoch: 11, Step: 4700, Train Loss: 8.644709700993321, Valid Loss: 8.64688968846861\n",
            "Epoch: 11, Step: 4800, Train Loss: 8.677815631555973, Valid Loss: 8.682140480017603\n",
            "Epoch: 11, Step: 4900, Train Loss: 8.652823435955918, Valid Loss: 8.65361198814765\n",
            "Epoch: 11, Step: 5000, Train Loss: 8.677153938108427, Valid Loss: 8.671783664633468\n",
            "Epoch: 12, Step: 5100, Train Loss: 8.622877501249953, Valid Loss: 8.625564749548724\n",
            "Epoch: 12, Step: 5200, Train Loss: 8.644863978572511, Valid Loss: 8.650225895272209\n",
            "Epoch: 12, Step: 5300, Train Loss: 8.663884878801943, Valid Loss: 8.663747374587503\n",
            "Epoch: 12, Step: 5400, Train Loss: 8.638451180485166, Valid Loss: 8.648555526200534\n",
            "Epoch: 13, Step: 5500, Train Loss: 8.630486077717599, Valid Loss: 8.646104630040542\n",
            "Epoch: 13, Step: 5600, Train Loss: 8.631994922764662, Valid Loss: 8.645498420714988\n",
            "Epoch: 13, Step: 5700, Train Loss: 8.63326097332092, Valid Loss: 8.645017029964464\n",
            "Epoch: 13, Step: 5800, Train Loss: 8.658013656290448, Valid Loss: 8.659666044464402\n",
            "Epoch: 14, Step: 5900, Train Loss: 8.629700457546784, Valid Loss: 8.648076253779957\n",
            "Epoch: 14, Step: 6000, Train Loss: 8.642085268687703, Valid Loss: 8.656224382769116\n",
            "Epoch: 14, Step: 6100, Train Loss: 8.62328249596425, Valid Loss: 8.64027874488542\n",
            "Epoch: 14, Step: 6200, Train Loss: 8.630981782849721, Valid Loss: 8.643595424172593\n",
            "Epoch: 15, Step: 6300, Train Loss: 8.609648711495115, Valid Loss: 8.628826344713739\n",
            "Epoch: 15, Step: 6400, Train Loss: 8.651845013301564, Valid Loss: 8.66371888079662\n",
            "Epoch: 15, Step: 6500, Train Loss: 8.611745153147547, Valid Loss: 8.633566190210255\n",
            "Epoch: 15, Step: 6600, Train Loss: 8.612016656956108, Valid Loss: 8.632211790404623\n",
            "Epoch: 16, Step: 6700, Train Loss: 8.624301562489702, Valid Loss: 8.641356649493845\n",
            "Epoch: 16, Step: 6800, Train Loss: 8.606864740255878, Valid Loss: 8.63266565022783\n",
            "Epoch: 16, Step: 6900, Train Loss: 8.611963319869526, Valid Loss: 8.6299999562972\n",
            "Epoch: 16, Step: 7000, Train Loss: 8.60477248137584, Valid Loss: 8.625853233668101\n",
            "Epoch: 16, Step: 7100, Train Loss: 8.605603737991494, Valid Loss: 8.627782202213757\n",
            "Epoch: 17, Step: 7200, Train Loss: 8.612140196131898, Valid Loss: 8.630519584939622\n",
            "Epoch: 17, Step: 7300, Train Loss: 8.605014738278811, Valid Loss: 8.632329728237094\n",
            "Epoch: 17, Step: 7400, Train Loss: 8.598348315298587, Valid Loss: 8.621044046380606\n",
            "Epoch: 17, Step: 7500, Train Loss: 8.619743076333647, Valid Loss: 8.630964003015075\n",
            "Epoch: 18, Step: 7600, Train Loss: 8.61865921026691, Valid Loss: 8.643937669263192\n",
            "Epoch: 18, Step: 7700, Train Loss: 8.604897643566888, Valid Loss: 8.63017288137169\n",
            "Epoch: 18, Step: 7800, Train Loss: 8.590943537369007, Valid Loss: 8.620352257271946\n",
            "Epoch: 18, Step: 7900, Train Loss: 8.607981100178204, Valid Loss: 8.639032471252426\n",
            "Epoch: 19, Step: 8000, Train Loss: 8.616379395828202, Valid Loss: 8.640049504890918\n",
            "Epoch: 19, Step: 8100, Train Loss: 8.605998098195558, Valid Loss: 8.635958504236381\n",
            "Epoch: 19, Step: 8200, Train Loss: 8.59621131529772, Valid Loss: 8.633243549107371\n",
            "Epoch: 19, Step: 8300, Train Loss: 8.59916646672674, Valid Loss: 8.639091781412132\n",
            "Epoch: 20, Step: 8400, Train Loss: 8.613507817075211, Valid Loss: 8.649180196853193\n",
            "Epoch: 20, Step: 8500, Train Loss: 8.592516131418567, Valid Loss: 8.625284796612187\n",
            "Epoch: 20, Step: 8600, Train Loss: 8.560779261458592, Valid Loss: 8.602143304047743\n",
            "Epoch: 20, Step: 8700, Train Loss: 8.585501752962045, Valid Loss: 8.616516332470576\n",
            "Epoch: 21, Step: 8800, Train Loss: 8.581373870431923, Valid Loss: 8.619891636334634\n",
            "Epoch: 21, Step: 8900, Train Loss: 8.573898923388825, Valid Loss: 8.621750746003503\n",
            "Epoch: 21, Step: 9000, Train Loss: 8.577636863210474, Valid Loss: 8.6144891199217\n",
            "Epoch: 21, Step: 9100, Train Loss: 8.591509474017677, Valid Loss: 8.622911912366007\n",
            "Epoch: 22, Step: 9200, Train Loss: 8.570977977826264, Valid Loss: 8.615318140910771\n",
            "Epoch: 22, Step: 9300, Train Loss: 8.5665944616712, Valid Loss: 8.60816563856983\n",
            "Epoch: 22, Step: 9400, Train Loss: 8.61580134882104, Valid Loss: 8.652057772925897\n",
            "Epoch: 22, Step: 9500, Train Loss: 8.582257276903885, Valid Loss: 8.615942891933079\n",
            "Epoch: 22, Step: 9600, Train Loss: 8.57620878450587, Valid Loss: 8.614656459871306\n",
            "Epoch: 23, Step: 9700, Train Loss: 8.582096755871351, Valid Loss: 8.625564769081144\n",
            "Epoch: 23, Step: 9800, Train Loss: 8.580925795059324, Valid Loss: 8.619727816064117\n",
            "Epoch: 23, Step: 9900, Train Loss: 8.587568077687083, Valid Loss: 8.622801920083983\n",
            "Epoch: 23, Step: 10000, Train Loss: 8.603184634648931, Valid Loss: 8.637708732193962\n",
            "Epoch: 24, Step: 10100, Train Loss: 8.570486251337183, Valid Loss: 8.62017626701709\n",
            "Epoch: 24, Step: 10200, Train Loss: 8.55786625559351, Valid Loss: 8.604746556238817\n",
            "Epoch: 24, Step: 10300, Train Loss: 8.576624486199794, Valid Loss: 8.623679357436027\n",
            "Epoch: 24, Step: 10400, Train Loss: 8.576268985147742, Valid Loss: 8.619823892148835\n",
            "Epoch: 25, Step: 10500, Train Loss: 8.584565396291008, Valid Loss: 8.628170896094606\n",
            "Epoch: 25, Step: 10600, Train Loss: 8.546346120687021, Valid Loss: 8.594471983675778\n",
            "Epoch: 25, Step: 10700, Train Loss: 8.551945570845868, Valid Loss: 8.604204649633571\n",
            "Epoch: 25, Step: 10800, Train Loss: 8.566499095146998, Valid Loss: 8.614389566620943\n",
            "Epoch: 26, Step: 10900, Train Loss: 8.58585132264126, Valid Loss: 8.626383890534333\n",
            "Epoch: 26, Step: 11000, Train Loss: 8.578039122232438, Valid Loss: 8.619860481111616\n",
            "Epoch: 26, Step: 11100, Train Loss: 8.557031728056055, Valid Loss: 8.60303853485638\n",
            "Epoch: 26, Step: 11200, Train Loss: 8.551069910227254, Valid Loss: 8.605607003235688\n",
            "Epoch: 27, Step: 11300, Train Loss: 8.569591993790649, Valid Loss: 8.62727990628177\n",
            "Epoch: 27, Step: 11400, Train Loss: 8.547025474445888, Valid Loss: 8.605233058257719\n",
            "Epoch: 27, Step: 11500, Train Loss: 8.55489043400095, Valid Loss: 8.603941243220968\n",
            "Epoch: 27, Step: 11600, Train Loss: 8.569182529632652, Valid Loss: 8.619013104794664\n",
            "Epoch: 27, Step: 11700, Train Loss: 8.557339307409192, Valid Loss: 8.603217886596958\n",
            "Epoch: 28, Step: 11800, Train Loss: 8.539667542182722, Valid Loss: 8.589364451111956\n",
            "Epoch: 28, Step: 11900, Train Loss: 8.553220815294438, Valid Loss: 8.609313982259344\n",
            "Epoch: 28, Step: 12000, Train Loss: 8.539755464383761, Valid Loss: 8.597072955914138\n",
            "Epoch: 28, Step: 12100, Train Loss: 8.575367648850879, Valid Loss: 8.629775511889795\n",
            "Epoch: 29, Step: 12200, Train Loss: 8.56017304956651, Valid Loss: 8.611278216257519\n",
            "Epoch: 29, Step: 12300, Train Loss: 8.544218090807345, Valid Loss: 8.604950853040645\n",
            "Epoch: 29, Step: 12400, Train Loss: 8.563319160329788, Valid Loss: 8.61615769982346\n",
            "Epoch: 29, Step: 12500, Train Loss: 8.567404081753837, Valid Loss: 8.624129720088925\n",
            "Epoch: 30, Step: 12600, Train Loss: 8.558096849885054, Valid Loss: 8.611745851768346\n",
            "Epoch: 30, Step: 12700, Train Loss: 8.549781332894748, Valid Loss: 8.606607691902786\n",
            "Epoch: 30, Step: 12800, Train Loss: 8.544166383103823, Valid Loss: 8.605368017165748\n",
            "Epoch: 30, Step: 12900, Train Loss: 8.542529142597076, Valid Loss: 8.603685431493883\n",
            "Epoch: 31, Step: 13000, Train Loss: 8.548220271707057, Valid Loss: 8.60686468914073\n",
            "Epoch: 31, Step: 13100, Train Loss: 8.56811694847772, Valid Loss: 8.61145155885447\n",
            "Epoch: 31, Step: 13200, Train Loss: 8.564529001922146, Valid Loss: 8.61986493500587\n",
            "Epoch: 31, Step: 13300, Train Loss: 8.529912549646838, Valid Loss: 8.597499533681487\n",
            "Epoch: 32, Step: 13400, Train Loss: 8.537231394543058, Valid Loss: 8.60225499021053\n",
            "Epoch: 32, Step: 13500, Train Loss: 8.533774061629378, Valid Loss: 8.594765294693985\n",
            "Epoch: 32, Step: 13600, Train Loss: 8.55643323030354, Valid Loss: 8.611438445785089\n",
            "Epoch: 32, Step: 13700, Train Loss: 8.550987575375068, Valid Loss: 8.613340405051852\n",
            "Epoch: 33, Step: 13800, Train Loss: 8.555152234349702, Valid Loss: 8.616000725823524\n",
            "Epoch: 33, Step: 13900, Train Loss: 8.556196119281244, Valid Loss: 8.618191643097592\n",
            "Epoch: 33, Step: 14000, Train Loss: 8.550801597027537, Valid Loss: 8.615031944112578\n",
            "Epoch: 33, Step: 14100, Train Loss: 8.535870712668023, Valid Loss: 8.59936100105724\n",
            "Epoch: 33, Step: 14200, Train Loss: 8.55324710525089, Valid Loss: 8.6177927900863\n",
            "Epoch: 34, Step: 14300, Train Loss: 8.55188456637645, Valid Loss: 8.615375372773995\n",
            "Epoch: 34, Step: 14400, Train Loss: 8.535082820720742, Valid Loss: 8.605345634289401\n",
            "Epoch: 34, Step: 14500, Train Loss: 8.553461293516966, Valid Loss: 8.619492053264604\n",
            "Epoch: 34, Step: 14600, Train Loss: 8.52852828708607, Valid Loss: 8.58673435429399\n",
            "Epoch: 35, Step: 14700, Train Loss: 8.54721220678257, Valid Loss: 8.605172746266536\n",
            "Epoch: 35, Step: 14800, Train Loss: 8.533031751015008, Valid Loss: 8.596984765829104\n",
            "Epoch: 35, Step: 14900, Train Loss: 8.524726582722051, Valid Loss: 8.594755880629723\n",
            "Epoch: 35, Step: 15000, Train Loss: 8.523686877392521, Valid Loss: 8.593626250380861\n",
            "Epoch: 36, Step: 15100, Train Loss: 8.53092031718923, Valid Loss: 8.603927046616006\n",
            "Epoch: 36, Step: 15200, Train Loss: 8.553715973372595, Valid Loss: 8.620553814555352\n",
            "Epoch: 36, Step: 15300, Train Loss: 8.529040793240886, Valid Loss: 8.606469781813907\n",
            "Epoch: 36, Step: 15400, Train Loss: 8.530061017953456, Valid Loss: 8.598083284049597\n",
            "Epoch: 37, Step: 15500, Train Loss: 8.534754417999965, Valid Loss: 8.602529278613709\n",
            "Epoch: 37, Step: 15600, Train Loss: 8.52421497998802, Valid Loss: 8.594265791079458\n",
            "Epoch: 37, Step: 15700, Train Loss: 8.537545511378363, Valid Loss: 8.607302462338893\n",
            "Epoch: 37, Step: 15800, Train Loss: 8.533679068122181, Valid Loss: 8.60848002112666\n",
            "Epoch: 38, Step: 15900, Train Loss: 8.53165221523349, Valid Loss: 8.60470450388842\n",
            "Epoch: 38, Step: 16000, Train Loss: 8.517659212208716, Valid Loss: 8.593333496006887\n",
            "Epoch: 38, Step: 16100, Train Loss: 8.522742879651435, Valid Loss: 8.594526259539174\n",
            "Epoch: 38, Step: 16200, Train Loss: 8.528139925562693, Valid Loss: 8.602434933594727\n",
            "Epoch: 38, Step: 16300, Train Loss: 8.563257762243039, Valid Loss: 8.626246877656326\n",
            "Epoch: 39, Step: 16400, Train Loss: 8.531729086843471, Valid Loss: 8.601924934069546\n",
            "Epoch: 39, Step: 16500, Train Loss: 8.508882256174914, Valid Loss: 8.589227985383538\n",
            "Epoch: 39, Step: 16600, Train Loss: 8.534578233040945, Valid Loss: 8.59725762034181\n",
            "Epoch: 39, Step: 16700, Train Loss: 8.51650944761331, Valid Loss: 8.587684279465362\n",
            "Epoch: 40, Step: 16800, Train Loss: 8.513028853543654, Valid Loss: 8.579465329800302\n",
            "Epoch: 40, Step: 16900, Train Loss: 8.545549396176483, Valid Loss: 8.611775997026895\n",
            "Epoch: 40, Step: 17000, Train Loss: 8.52237818862176, Valid Loss: 8.598851985005886\n",
            "Epoch: 40, Step: 17100, Train Loss: 8.519768091541653, Valid Loss: 8.583622351905014\n",
            "Epoch: 41, Step: 17200, Train Loss: 8.529650619568883, Valid Loss: 8.60130763890789\n",
            "Epoch: 41, Step: 17300, Train Loss: 8.515724758440067, Valid Loss: 8.596022432951116\n",
            "Epoch: 41, Step: 17400, Train Loss: 8.51050801883628, Valid Loss: 8.596582332118729\n",
            "Epoch: 41, Step: 17500, Train Loss: 8.531473246801639, Valid Loss: 8.603623823569961\n",
            "Epoch: 42, Step: 17600, Train Loss: 8.5287862646535, Valid Loss: 8.60471515527725\n",
            "Epoch: 42, Step: 17700, Train Loss: 8.510989858513927, Valid Loss: 8.587830703917408\n",
            "Epoch: 42, Step: 17800, Train Loss: 8.523783732320936, Valid Loss: 8.598312212521543\n",
            "Epoch: 42, Step: 17900, Train Loss: 8.554185934042835, Valid Loss: 8.619269275241741\n",
            "Epoch: 43, Step: 18000, Train Loss: 8.522340637117045, Valid Loss: 8.59607093189277\n",
            "Epoch: 43, Step: 18100, Train Loss: 8.533567290861578, Valid Loss: 8.60953594159963\n",
            "Epoch: 43, Step: 18200, Train Loss: 8.54524906654589, Valid Loss: 8.610956031380528\n",
            "Epoch: 43, Step: 18300, Train Loss: 8.548261603660222, Valid Loss: 8.618595850967278\n",
            "Epoch: 44, Step: 18400, Train Loss: 8.509589374144696, Valid Loss: 8.593872917098686\n",
            "Epoch: 44, Step: 18500, Train Loss: 8.501673633974196, Valid Loss: 8.593565075174562\n",
            "Epoch: 44, Step: 18600, Train Loss: 8.533494798449096, Valid Loss: 8.609771947676618\n",
            "Epoch: 44, Step: 18700, Train Loss: 8.516009954127234, Valid Loss: 8.59708415322945\n",
            "Epoch: 44, Step: 18800, Train Loss: 8.52548029316174, Valid Loss: 8.60114135898324\n",
            "Epoch: 45, Step: 18900, Train Loss: 8.519898466171885, Valid Loss: 8.60319208575243\n",
            "Epoch: 45, Step: 19000, Train Loss: 8.548612673443973, Valid Loss: 8.625747614998868\n",
            "Epoch: 45, Step: 19100, Train Loss: 8.52388110663215, Valid Loss: 8.600966491284334\n",
            "Epoch: 45, Step: 19200, Train Loss: 8.505402645028784, Valid Loss: 8.590293551058476\n",
            "Epoch: 46, Step: 19300, Train Loss: 8.511793126106948, Valid Loss: 8.597103218762525\n",
            "Epoch: 46, Step: 19400, Train Loss: 8.514183554962548, Valid Loss: 8.598806903619947\n",
            "Epoch: 46, Step: 19500, Train Loss: 8.500548329156498, Valid Loss: 8.57916675065208\n",
            "Epoch: 46, Step: 19600, Train Loss: 8.515785846364414, Valid Loss: 8.595737774220416\n",
            "Epoch: 47, Step: 19700, Train Loss: 8.508792200211222, Valid Loss: 8.59683034220128\n",
            "Epoch: 47, Step: 19800, Train Loss: 8.524449630877344, Valid Loss: 8.608395292162756\n",
            "Epoch: 47, Step: 19900, Train Loss: 8.498343008490155, Valid Loss: 8.585652520775913\n",
            "Epoch: 47, Step: 20000, Train Loss: 8.509822475210251, Valid Loss: 8.591232733271756\n",
            "Epoch: 48, Step: 20100, Train Loss: 8.515041941221588, Valid Loss: 8.600435608015085\n",
            "Epoch: 48, Step: 20200, Train Loss: 8.517620185266221, Valid Loss: 8.589607918063656\n",
            "Epoch: 48, Step: 20300, Train Loss: 8.5147346971741, Valid Loss: 8.598370691592487\n",
            "Epoch: 48, Step: 20400, Train Loss: 8.51290184752061, Valid Loss: 8.598050303406115\n",
            "Epoch: 49, Step: 20500, Train Loss: 8.489830921287272, Valid Loss: 8.574056334706125\n",
            "Epoch: 49, Step: 20600, Train Loss: 8.520275863222578, Valid Loss: 8.594155035193062\n",
            "Epoch: 49, Step: 20700, Train Loss: 8.514806527075493, Valid Loss: 8.59742102123004\n",
            "Epoch: 49, Step: 20800, Train Loss: 8.537980664646831, Valid Loss: 8.620313496091466\n",
            "Epoch: 49, Step: 20900, Train Loss: 8.530503923552427, Valid Loss: 8.611588909841918\n",
            "Epoch: 50, Step: 21000, Train Loss: 8.515483831430872, Valid Loss: 8.60505936768294\n",
            "Epoch: 50, Step: 21100, Train Loss: 8.51086790838565, Valid Loss: 8.605266194920864\n",
            "Epoch: 50, Step: 21200, Train Loss: 8.493545863250759, Valid Loss: 8.572977411522517\n",
            "Epoch: 50, Step: 21300, Train Loss: 8.50120540580173, Valid Loss: 8.585963788452268\n",
            "Epoch: 51, Step: 21400, Train Loss: 8.496566852282966, Valid Loss: 8.586286969983306\n",
            "Epoch: 51, Step: 21500, Train Loss: 8.50806513074562, Valid Loss: 8.592701329275966\n",
            "Epoch: 51, Step: 21600, Train Loss: 8.49029041673607, Valid Loss: 8.574495952414336\n",
            "Epoch: 51, Step: 21700, Train Loss: 8.508316517506072, Valid Loss: 8.588386291733775\n",
            "Epoch: 52, Step: 21800, Train Loss: 8.50068425888735, Valid Loss: 8.588887110717776\n",
            "Epoch: 52, Step: 21900, Train Loss: 8.505587003078334, Valid Loss: 8.586525057233317\n",
            "Epoch: 52, Step: 22000, Train Loss: 8.50319380421263, Valid Loss: 8.591624374920762\n",
            "Epoch: 52, Step: 22100, Train Loss: 8.491824541869285, Valid Loss: 8.576147113842557\n",
            "Epoch: 53, Step: 22200, Train Loss: 8.517514764211313, Valid Loss: 8.594494147382395\n",
            "Epoch: 53, Step: 22300, Train Loss: 8.49377457886108, Valid Loss: 8.586504884137241\n",
            "Epoch: 53, Step: 22400, Train Loss: 8.504541124344112, Valid Loss: 8.58774702434943\n",
            "Epoch: 53, Step: 22500, Train Loss: 8.514096386117142, Valid Loss: 8.598248834436331\n",
            "Epoch: 54, Step: 22600, Train Loss: 8.508158316751077, Valid Loss: 8.592104992278387\n",
            "Epoch: 54, Step: 22700, Train Loss: 8.497531106684859, Valid Loss: 8.584184696227759\n",
            "Epoch: 54, Step: 22800, Train Loss: 8.49026425374016, Valid Loss: 8.585839177263363\n",
            "Epoch: 54, Step: 22900, Train Loss: 8.496809334079714, Valid Loss: 8.581396312927769\n",
            "Epoch: 55, Step: 23000, Train Loss: 8.493519358196533, Valid Loss: 8.585609820544171\n",
            "Epoch: 55, Step: 23100, Train Loss: 8.519685118012749, Valid Loss: 8.605579288484584\n",
            "Epoch: 55, Step: 23200, Train Loss: 8.477016164372653, Valid Loss: 8.577381757925911\n",
            "Epoch: 55, Step: 23300, Train Loss: 8.50650805780383, Valid Loss: 8.59012135300041\n",
            "Epoch: 55, Step: 23400, Train Loss: 8.50685618339837, Valid Loss: 8.594072225869237\n",
            "Epoch: 56, Step: 23500, Train Loss: 8.50797498857436, Valid Loss: 8.586609849940274\n",
            "Epoch: 56, Step: 23600, Train Loss: 8.48225840060986, Valid Loss: 8.567608177356385\n",
            "Epoch: 56, Step: 23700, Train Loss: 8.503474020958548, Valid Loss: 8.581658322945657\n",
            "Epoch: 56, Step: 23800, Train Loss: 8.510411893763203, Valid Loss: 8.594530018405692\n",
            "Epoch: 57, Step: 23900, Train Loss: 8.507306518472385, Valid Loss: 8.588051202773398\n",
            "Epoch: 57, Step: 24000, Train Loss: 8.497686695484909, Valid Loss: 8.580126138706078\n",
            "Epoch: 57, Step: 24100, Train Loss: 8.499218113140229, Valid Loss: 8.573065480092724\n",
            "Epoch: 57, Step: 24200, Train Loss: 8.50022420160196, Valid Loss: 8.57898297086072\n",
            "Epoch: 58, Step: 24300, Train Loss: 8.49678032046985, Valid Loss: 8.575465995722224\n",
            "Epoch: 58, Step: 24400, Train Loss: 8.50678836480139, Valid Loss: 8.58036226647457\n",
            "Epoch: 58, Step: 24500, Train Loss: 8.50474472361709, Valid Loss: 8.588376120327151\n",
            "Epoch: 58, Step: 24600, Train Loss: 8.476599816215014, Valid Loss: 8.571114246221608\n",
            "Epoch: 59, Step: 24700, Train Loss: 8.521379176085224, Valid Loss: 8.606738584542637\n",
            "Epoch: 59, Step: 24800, Train Loss: 8.510313539651047, Valid Loss: 8.594649326033048\n",
            "Epoch: 59, Step: 24900, Train Loss: 8.531119916240026, Valid Loss: 8.610211475974664\n",
            "Epoch: 59, Step: 25000, Train Loss: 8.487127336105571, Valid Loss: 8.578805196632349\n",
            "Epoch: 60, Step: 25100, Train Loss: 8.505639104600593, Valid Loss: 8.585346007975229\n",
            "Epoch: 60, Step: 25200, Train Loss: 8.473935366488323, Valid Loss: 8.56622069918109\n",
            "Epoch: 60, Step: 25300, Train Loss: 8.518992073495168, Valid Loss: 8.59565946128124\n",
            "Epoch: 60, Step: 25400, Train Loss: 8.503129347529118, Valid Loss: 8.589542649169177\n",
            "Epoch: 61, Step: 25500, Train Loss: 8.502143608767042, Valid Loss: 8.596589791707101\n",
            "Epoch: 61, Step: 25600, Train Loss: 8.502618214569155, Valid Loss: 8.594300077625006\n",
            "Epoch: 61, Step: 25700, Train Loss: 8.506740583686375, Valid Loss: 8.595762156829444\n",
            "Epoch: 61, Step: 25800, Train Loss: 8.506880806988415, Valid Loss: 8.587315972024923\n",
            "Epoch: 61, Step: 25900, Train Loss: 8.4816915217312, Valid Loss: 8.576659373031918\n",
            "Epoch: 62, Step: 26000, Train Loss: 8.500166282932673, Valid Loss: 8.598087979464008\n",
            "Epoch: 62, Step: 26100, Train Loss: 8.497706410056495, Valid Loss: 8.595733623877946\n",
            "Epoch: 62, Step: 26200, Train Loss: 8.481667045167255, Valid Loss: 8.582870841826026\n",
            "Epoch: 62, Step: 26300, Train Loss: 8.500910632398067, Valid Loss: 8.584962364031451\n",
            "Epoch: 63, Step: 26400, Train Loss: 8.478864094030522, Valid Loss: 8.572896836991406\n",
            "Epoch: 63, Step: 26500, Train Loss: 8.49226453073066, Valid Loss: 8.587328420536778\n",
            "Epoch: 63, Step: 26600, Train Loss: 8.506036640864759, Valid Loss: 8.597650912835032\n",
            "Epoch: 63, Step: 26700, Train Loss: 8.492060129096476, Valid Loss: 8.584769057604651\n",
            "Epoch: 64, Step: 26800, Train Loss: 8.503996738937104, Valid Loss: 8.595593189709868\n",
            "Epoch: 64, Step: 26900, Train Loss: 8.481041751649352, Valid Loss: 8.583774559708134\n",
            "Epoch: 64, Step: 27000, Train Loss: 8.483389311835348, Valid Loss: 8.57924894533518\n",
            "Epoch: 64, Step: 27100, Train Loss: 8.477741997187794, Valid Loss: 8.57267270561514\n",
            "Epoch: 65, Step: 27200, Train Loss: 8.489476597643202, Valid Loss: 8.586005515614268\n",
            "Epoch: 65, Step: 27300, Train Loss: 8.504506020708813, Valid Loss: 8.60517897718733\n",
            "Epoch: 65, Step: 27400, Train Loss: 8.485081547383787, Valid Loss: 8.586576472047927\n",
            "Epoch: 65, Step: 27500, Train Loss: 8.495571668284272, Valid Loss: 8.590075414302808\n",
            "Epoch: 66, Step: 27600, Train Loss: 8.4795496845694, Valid Loss: 8.58746774405084\n",
            "Epoch: 66, Step: 27700, Train Loss: 8.487393567326448, Valid Loss: 8.587568641890432\n",
            "Epoch: 66, Step: 27800, Train Loss: 8.479315213167494, Valid Loss: 8.571314971178587\n",
            "Epoch: 66, Step: 27900, Train Loss: 8.492898902841912, Valid Loss: 8.578911147746918\n",
            "Epoch: 66, Step: 28000, Train Loss: 8.513140827084888, Valid Loss: 8.590684642269302\n",
            "Epoch: 67, Step: 28100, Train Loss: 8.515917417598784, Valid Loss: 8.60310694976028\n",
            "Epoch: 67, Step: 28200, Train Loss: 8.499464668974106, Valid Loss: 8.587517159445003\n",
            "Epoch: 67, Step: 28300, Train Loss: 8.492756167634473, Valid Loss: 8.58635026222023\n",
            "Epoch: 67, Step: 28400, Train Loss: 8.482034459873987, Valid Loss: 8.579814124148646\n",
            "Epoch: 68, Step: 28500, Train Loss: 8.486346568335609, Valid Loss: 8.578411108863078\n",
            "Epoch: 68, Step: 28600, Train Loss: 8.474916747293282, Valid Loss: 8.577786678474945\n",
            "Epoch: 68, Step: 28700, Train Loss: 8.473969223551888, Valid Loss: 8.569698448195997\n",
            "Epoch: 68, Step: 28800, Train Loss: 8.500562553998606, Valid Loss: 8.58621802371396\n",
            "Epoch: 69, Step: 28900, Train Loss: 8.49508263285615, Valid Loss: 8.583200087351683\n",
            "Epoch: 69, Step: 29000, Train Loss: 8.497343807725187, Valid Loss: 8.584964179324983\n",
            "Epoch: 69, Step: 29100, Train Loss: 8.47559500970726, Valid Loss: 8.572738255258683\n",
            "Epoch: 69, Step: 29200, Train Loss: 8.469622163795407, Valid Loss: 8.5750860676347\n",
            "Epoch: 70, Step: 29300, Train Loss: 8.469751098219056, Valid Loss: 8.574764826631972\n",
            "Epoch: 70, Step: 29400, Train Loss: 8.484684473035301, Valid Loss: 8.576094538833283\n",
            "Epoch: 70, Step: 29500, Train Loss: 8.498492350183895, Valid Loss: 8.580647729800123\n",
            "Epoch: 70, Step: 29600, Train Loss: 8.481898188503363, Valid Loss: 8.569828439945823\n",
            "Epoch: 71, Step: 29700, Train Loss: 8.505223352047087, Valid Loss: 8.59708179177541\n",
            "Epoch: 71, Step: 29800, Train Loss: 8.498604499832487, Valid Loss: 8.575455192580138\n",
            "Epoch: 71, Step: 29900, Train Loss: 8.488815494972306, Valid Loss: 8.584142199681324\n",
            "Epoch: 71, Step: 30000, Train Loss: 8.492732498557796, Valid Loss: 8.578226237581566\n",
            "Epoch: 72, Step: 30100, Train Loss: 8.47903271937746, Valid Loss: 8.573760168418028\n",
            "Epoch: 72, Step: 30200, Train Loss: 8.48749370914161, Valid Loss: 8.579433387102235\n",
            "Epoch: 72, Step: 30300, Train Loss: 8.494025023825694, Valid Loss: 8.579268524265533\n",
            "Epoch: 72, Step: 30400, Train Loss: 8.499062056527201, Valid Loss: 8.585264018845765\n",
            "Epoch: 72, Step: 30500, Train Loss: 8.479702145963264, Valid Loss: 8.570561249147467\n",
            "Epoch: 73, Step: 30600, Train Loss: 8.465712573045819, Valid Loss: 8.566804388116507\n",
            "Epoch: 73, Step: 30700, Train Loss: 8.478968197965955, Valid Loss: 8.575388818896343\n",
            "Epoch: 73, Step: 30800, Train Loss: 8.494344241854897, Valid Loss: 8.579369177329065\n",
            "Epoch: 73, Step: 30900, Train Loss: 8.480979551262019, Valid Loss: 8.572113504992501\n",
            "Epoch: 74, Step: 31000, Train Loss: 8.502008154559228, Valid Loss: 8.589370664078354\n",
            "Epoch: 74, Step: 31100, Train Loss: 8.486057139925446, Valid Loss: 8.575783603000241\n",
            "Epoch: 74, Step: 31200, Train Loss: 8.477806043536622, Valid Loss: 8.57794825785511\n",
            "Epoch: 74, Step: 31300, Train Loss: 8.482425308020934, Valid Loss: 8.572082126984208\n",
            "Epoch: 75, Step: 31400, Train Loss: 8.48745046210712, Valid Loss: 8.575064787018464\n",
            "Epoch: 75, Step: 31500, Train Loss: 8.495217900562462, Valid Loss: 8.588586414406356\n",
            "Epoch: 75, Step: 31600, Train Loss: 8.489259871079586, Valid Loss: 8.588792949316899\n",
            "Epoch: 75, Step: 31700, Train Loss: 8.482265742440047, Valid Loss: 8.57889591305877\n",
            "Epoch: 76, Step: 31800, Train Loss: 8.480480085549681, Valid Loss: 8.575783546518561\n",
            "Epoch: 76, Step: 31900, Train Loss: 8.470559863808237, Valid Loss: 8.57357650117455\n",
            "Epoch: 76, Step: 32000, Train Loss: 8.504817193382953, Valid Loss: 8.598640930469859\n",
            "Epoch: 76, Step: 32100, Train Loss: 8.475975611207806, Valid Loss: 8.567644200274826\n",
            "Epoch: 77, Step: 32200, Train Loss: 8.490312454443227, Valid Loss: 8.586165932245118\n",
            "Epoch: 77, Step: 32300, Train Loss: 8.485241634159323, Valid Loss: 8.584324531981451\n",
            "Epoch: 77, Step: 32400, Train Loss: 8.505025896497644, Valid Loss: 8.595460848392838\n",
            "Epoch: 77, Step: 32500, Train Loss: 8.481160735706057, Valid Loss: 8.573208076496098\n",
            "Epoch: 77, Step: 32600, Train Loss: 8.459552391535155, Valid Loss: 8.56470141398564\n",
            "Epoch: 78, Step: 32700, Train Loss: 8.458981846081308, Valid Loss: 8.568217884474416\n",
            "Epoch: 78, Step: 32800, Train Loss: 8.480480320634735, Valid Loss: 8.579106861741286\n",
            "Epoch: 78, Step: 32900, Train Loss: 8.478682160071552, Valid Loss: 8.578726567097869\n",
            "Epoch: 78, Step: 33000, Train Loss: 8.463867820729858, Valid Loss: 8.564376530214192\n",
            "Epoch: 79, Step: 33100, Train Loss: 8.475237237216032, Valid Loss: 8.574770750542864\n",
            "Epoch: 79, Step: 33200, Train Loss: 8.500590741280384, Valid Loss: 8.597877606706357\n",
            "Epoch: 79, Step: 33300, Train Loss: 8.468604125358821, Valid Loss: 8.570624699287947\n",
            "Epoch: 79, Step: 33400, Train Loss: 8.463234637917642, Valid Loss: 8.576014224420595\n",
            "Epoch: 80, Step: 33500, Train Loss: 8.471910661672865, Valid Loss: 8.582177747378074\n",
            "Epoch: 80, Step: 33600, Train Loss: 8.49617384631382, Valid Loss: 8.5872819984826\n",
            "Epoch: 80, Step: 33700, Train Loss: 8.470426294637189, Valid Loss: 8.570745671470883\n",
            "Epoch: 80, Step: 33800, Train Loss: 8.492718648813824, Valid Loss: 8.593806844950315\n",
            "Epoch: 81, Step: 33900, Train Loss: 8.466148719812526, Valid Loss: 8.572195890143451\n",
            "Epoch: 81, Step: 34000, Train Loss: 8.466050585294141, Valid Loss: 8.576054750068097\n",
            "Epoch: 81, Step: 34100, Train Loss: 8.48249909589407, Valid Loss: 8.581916013339331\n",
            "Epoch: 81, Step: 34200, Train Loss: 8.486111733255077, Valid Loss: 8.583579518899171\n",
            "Epoch: 82, Step: 34300, Train Loss: 8.46986339443281, Valid Loss: 8.569406839967455\n",
            "Epoch: 82, Step: 34400, Train Loss: 8.485424800490504, Valid Loss: 8.59290798585726\n",
            "Epoch: 82, Step: 34500, Train Loss: 8.485296003023718, Valid Loss: 8.590701831928955\n",
            "Epoch: 82, Step: 34600, Train Loss: 8.460944295321983, Valid Loss: 8.577676785178758\n",
            "Epoch: 83, Step: 34700, Train Loss: 8.470874167945986, Valid Loss: 8.57160658819968\n",
            "Epoch: 83, Step: 34800, Train Loss: 8.476044450984805, Valid Loss: 8.582326886493302\n",
            "Epoch: 83, Step: 34900, Train Loss: 8.48905711903824, Valid Loss: 8.590185072984841\n",
            "Epoch: 83, Step: 35000, Train Loss: 8.473698279308348, Valid Loss: 8.577356662598543\n",
            "Epoch: 83, Step: 35100, Train Loss: 8.49171016959528, Valid Loss: 8.588896808287926\n",
            "Epoch: 84, Step: 35200, Train Loss: 8.46593011911102, Valid Loss: 8.574379547505732\n",
            "Epoch: 84, Step: 35300, Train Loss: 8.473813953958748, Valid Loss: 8.571806987148904\n",
            "Epoch: 84, Step: 35400, Train Loss: 8.482403062790258, Valid Loss: 8.587152216989177\n",
            "Epoch: 84, Step: 35500, Train Loss: 8.46353011638875, Valid Loss: 8.567984956562805\n",
            "Epoch: 85, Step: 35600, Train Loss: 8.474655451677751, Valid Loss: 8.583623990831223\n",
            "Epoch: 85, Step: 35700, Train Loss: 8.446309976577078, Valid Loss: 8.567866435349245\n",
            "Epoch: 85, Step: 35800, Train Loss: 8.451586210153357, Valid Loss: 8.562634019010277\n",
            "Epoch: 85, Step: 35900, Train Loss: 8.473677820455977, Valid Loss: 8.587202141858242\n",
            "Epoch: 86, Step: 36000, Train Loss: 8.47225787866824, Valid Loss: 8.579998870399136\n",
            "Epoch: 86, Step: 36100, Train Loss: 8.48508178785679, Valid Loss: 8.583417694668853\n",
            "Epoch: 86, Step: 36200, Train Loss: 8.461370648480436, Valid Loss: 8.569157901019786\n",
            "Epoch: 86, Step: 36300, Train Loss: 8.467090891829145, Valid Loss: 8.579163488279667\n",
            "Epoch: 87, Step: 36400, Train Loss: 8.464724393805568, Valid Loss: 8.578477044275465\n",
            "Epoch: 87, Step: 36500, Train Loss: 8.503301567937605, Valid Loss: 8.598568583327118\n",
            "Epoch: 87, Step: 36600, Train Loss: 8.466071502039998, Valid Loss: 8.575848146582954\n",
            "Epoch: 87, Step: 36700, Train Loss: 8.455340753964087, Valid Loss: 8.572602281504881\n",
            "Epoch: 88, Step: 36800, Train Loss: 8.476742161795082, Valid Loss: 8.581015916047656\n",
            "Epoch: 88, Step: 36900, Train Loss: 8.466863991338574, Valid Loss: 8.577134653774884\n",
            "Epoch: 88, Step: 37000, Train Loss: 8.490131123548661, Valid Loss: 8.589810817037575\n",
            "Epoch: 88, Step: 37100, Train Loss: 8.471179554124713, Valid Loss: 8.579737300623222\n",
            "Epoch: 88, Step: 37200, Train Loss: 8.48160522263773, Valid Loss: 8.592007924837697\n",
            "Epoch: 89, Step: 37300, Train Loss: 8.469551584316083, Valid Loss: 8.585776808442594\n",
            "Epoch: 89, Step: 37400, Train Loss: 8.461700540650646, Valid Loss: 8.580763524455651\n",
            "Epoch: 89, Step: 37500, Train Loss: 8.459740331366891, Valid Loss: 8.570397955389723\n",
            "Epoch: 89, Step: 37600, Train Loss: 8.48850303249558, Valid Loss: 8.592636047115127\n",
            "Epoch: 90, Step: 37700, Train Loss: 8.475586366175758, Valid Loss: 8.588631176727382\n",
            "Epoch: 90, Step: 37800, Train Loss: 8.44968195060055, Valid Loss: 8.564951493039995\n",
            "Epoch: 90, Step: 37900, Train Loss: 8.471603743193361, Valid Loss: 8.57790455195563\n",
            "Epoch: 90, Step: 38000, Train Loss: 8.476445318894914, Valid Loss: 8.578089471906079\n",
            "Epoch: 91, Step: 38100, Train Loss: 8.459787675662575, Valid Loss: 8.574782866057665\n",
            "Epoch: 91, Step: 38200, Train Loss: 8.47128603048976, Valid Loss: 8.576095624224402\n",
            "Epoch: 91, Step: 38300, Train Loss: 8.486184227031123, Valid Loss: 8.579790678048825\n",
            "Epoch: 91, Step: 38400, Train Loss: 8.45981459895284, Valid Loss: 8.567594203225449\n",
            "Epoch: 92, Step: 38500, Train Loss: 8.486632651749774, Valid Loss: 8.597227343339327\n",
            "Epoch: 92, Step: 38600, Train Loss: 8.469055625511182, Valid Loss: 8.581454060608259\n",
            "Epoch: 92, Step: 38700, Train Loss: 8.469725362712136, Valid Loss: 8.581682647396283\n",
            "Epoch: 92, Step: 38800, Train Loss: 8.47499738487022, Valid Loss: 8.588390342637785\n",
            "Epoch: 93, Step: 38900, Train Loss: 8.472262299903909, Valid Loss: 8.570231341144476\n",
            "Epoch: 93, Step: 39000, Train Loss: 8.437279275362648, Valid Loss: 8.552047951208284\n",
            "Epoch: 93, Step: 39100, Train Loss: 8.482564903235755, Valid Loss: 8.589561778025434\n",
            "Epoch: 93, Step: 39200, Train Loss: 8.463301792770123, Valid Loss: 8.575128520648573\n",
            "Epoch: 94, Step: 39300, Train Loss: 8.47367305312317, Valid Loss: 8.58227950737019\n",
            "Epoch: 94, Step: 39400, Train Loss: 8.45925482676314, Valid Loss: 8.572153847626284\n",
            "Epoch: 94, Step: 39500, Train Loss: 8.459162464098586, Valid Loss: 8.57447974954699\n",
            "Epoch: 94, Step: 39600, Train Loss: 8.471307612153, Valid Loss: 8.57794874151088\n",
            "Epoch: 94, Step: 39700, Train Loss: 8.454803879421785, Valid Loss: 8.570604562482613\n",
            "Epoch: 95, Step: 39800, Train Loss: 8.461823865253072, Valid Loss: 8.569139596616823\n",
            "Epoch: 95, Step: 39900, Train Loss: 8.468413454965296, Valid Loss: 8.57173828065888\n",
            "Epoch: 95, Step: 40000, Train Loss: 8.442064002706406, Valid Loss: 8.565432031024944\n",
            "Epoch: 95, Step: 40100, Train Loss: 8.477352198407477, Valid Loss: 8.578798713343952\n",
            "Epoch: 96, Step: 40200, Train Loss: 8.459412226266625, Valid Loss: 8.569870115428465\n",
            "Epoch: 96, Step: 40300, Train Loss: 8.46577182059584, Valid Loss: 8.568460030273014\n",
            "Epoch: 96, Step: 40400, Train Loss: 8.469466006388515, Valid Loss: 8.581280040888867\n",
            "Epoch: 96, Step: 40500, Train Loss: 8.460730581754458, Valid Loss: 8.570377125100295\n",
            "Epoch: 97, Step: 40600, Train Loss: 8.451025532159662, Valid Loss: 8.57455434339399\n",
            "Epoch: 97, Step: 40700, Train Loss: 8.460534814812968, Valid Loss: 8.565626191812216\n",
            "Epoch: 97, Step: 40800, Train Loss: 8.460171510462766, Valid Loss: 8.561423192164213\n",
            "Epoch: 97, Step: 40900, Train Loss: 8.464011041445243, Valid Loss: 8.575815350556946\n",
            "Epoch: 98, Step: 41000, Train Loss: 8.47063522741211, Valid Loss: 8.581919331106683\n",
            "Epoch: 98, Step: 41100, Train Loss: 8.481292234355672, Valid Loss: 8.592948100970547\n",
            "Epoch: 98, Step: 41200, Train Loss: 8.462387119668124, Valid Loss: 8.573701899758383\n",
            "Epoch: 98, Step: 41300, Train Loss: 8.457909011249967, Valid Loss: 8.57422334129683\n",
            "Epoch: 99, Step: 41400, Train Loss: 8.467595838757807, Valid Loss: 8.587828587599258\n",
            "Epoch: 99, Step: 41500, Train Loss: 8.462866132734355, Valid Loss: 8.580178706193012\n",
            "Epoch: 99, Step: 41600, Train Loss: 8.453544062711085, Valid Loss: 8.566721051369926\n",
            "Epoch: 99, Step: 41700, Train Loss: 8.456835884263946, Valid Loss: 8.573705864260301\n",
            "Epoch: 99, Step: 41800, Train Loss: 8.46094204164635, Valid Loss: 8.576396537474889\n",
            "Test RMSE Loss for h = 45: 8.761881446680928\n",
            "Start training for h = 180\n",
            "Epoch: 0, Step: 100, Train Loss: 9.461094019162418, Valid Loss: 9.414275034876347\n",
            "Epoch: 0, Step: 200, Train Loss: 9.169066303379028, Valid Loss: 9.106699601030378\n",
            "Epoch: 0, Step: 300, Train Loss: 9.067480665300659, Valid Loss: 9.008828328593772\n",
            "Epoch: 0, Step: 400, Train Loss: 8.994261075651409, Valid Loss: 8.937358420685833\n",
            "Epoch: 1, Step: 500, Train Loss: 8.945595243494685, Valid Loss: 8.893679127178386\n",
            "Epoch: 1, Step: 600, Train Loss: 8.92421069406119, Valid Loss: 8.875529964212268\n",
            "Epoch: 1, Step: 700, Train Loss: 8.917350529180053, Valid Loss: 8.873065864228947\n",
            "Epoch: 1, Step: 800, Train Loss: 8.884977921327316, Valid Loss: 8.848385209742503\n",
            "Epoch: 2, Step: 900, Train Loss: 8.845152285414386, Valid Loss: 8.80563215254374\n",
            "Epoch: 2, Step: 1000, Train Loss: 8.865360639663093, Valid Loss: 8.831276641731797\n",
            "Epoch: 2, Step: 1100, Train Loss: 8.814171517086152, Valid Loss: 8.772750315475733\n",
            "Epoch: 2, Step: 1200, Train Loss: 8.84743585096156, Valid Loss: 8.80464880981373\n",
            "Epoch: 3, Step: 1300, Train Loss: 8.803311894389022, Valid Loss: 8.765560202345682\n",
            "Epoch: 3, Step: 1400, Train Loss: 8.791444833214053, Valid Loss: 8.750847902194407\n",
            "Epoch: 3, Step: 1500, Train Loss: 8.792475012258379, Valid Loss: 8.756160966075786\n",
            "Epoch: 3, Step: 1600, Train Loss: 8.80206860982949, Valid Loss: 8.768288151707157\n",
            "Epoch: 4, Step: 1700, Train Loss: 8.768283339024524, Valid Loss: 8.734537681764133\n",
            "Epoch: 4, Step: 1800, Train Loss: 8.763292208007561, Valid Loss: 8.729786718199193\n",
            "Epoch: 4, Step: 1900, Train Loss: 8.758107055370745, Valid Loss: 8.724168207603133\n",
            "Epoch: 4, Step: 2000, Train Loss: 8.758364105357755, Valid Loss: 8.725385399193089\n",
            "Epoch: 5, Step: 2100, Train Loss: 8.745530341804484, Valid Loss: 8.715017035580768\n",
            "Epoch: 5, Step: 2200, Train Loss: 8.743564494757857, Valid Loss: 8.718136451060936\n",
            "Epoch: 5, Step: 2300, Train Loss: 8.739084297994056, Valid Loss: 8.712901570317733\n",
            "Epoch: 5, Step: 2400, Train Loss: 8.735015216620935, Valid Loss: 8.71118897432688\n",
            "Epoch: 5, Step: 2500, Train Loss: 8.7237165476808, Valid Loss: 8.703119082694712\n",
            "Epoch: 6, Step: 2600, Train Loss: 8.713603959322958, Valid Loss: 8.69334209886171\n",
            "Epoch: 6, Step: 2700, Train Loss: 8.738934890760124, Valid Loss: 8.719001637497957\n",
            "Epoch: 6, Step: 2800, Train Loss: 8.705694044330716, Valid Loss: 8.689534124537271\n",
            "Epoch: 6, Step: 2900, Train Loss: 8.700351330091395, Valid Loss: 8.681503609101618\n",
            "Epoch: 7, Step: 3000, Train Loss: 8.68924672842719, Valid Loss: 8.669849102673522\n",
            "Epoch: 7, Step: 3100, Train Loss: 8.703498439131474, Valid Loss: 8.68650747855773\n",
            "Epoch: 7, Step: 3200, Train Loss: 8.708159256854055, Valid Loss: 8.696060970691489\n",
            "Epoch: 7, Step: 3300, Train Loss: 8.685449164578316, Valid Loss: 8.677210262179923\n",
            "Epoch: 8, Step: 3400, Train Loss: 8.68021742253492, Valid Loss: 8.668079421382641\n",
            "Epoch: 8, Step: 3500, Train Loss: 8.683658847576702, Valid Loss: 8.675815312197162\n",
            "Epoch: 8, Step: 3600, Train Loss: 8.706740798731882, Valid Loss: 8.694901979840088\n",
            "Epoch: 8, Step: 3700, Train Loss: 8.677608635714178, Valid Loss: 8.669555101125882\n",
            "Epoch: 9, Step: 3800, Train Loss: 8.677498602700034, Valid Loss: 8.673173394208801\n",
            "Epoch: 9, Step: 3900, Train Loss: 8.673219956199333, Valid Loss: 8.677978574343285\n",
            "Epoch: 9, Step: 4000, Train Loss: 8.667750918625142, Valid Loss: 8.665417845851247\n",
            "Epoch: 9, Step: 4100, Train Loss: 8.692057799593119, Valid Loss: 8.687083383559521\n",
            "Epoch: 10, Step: 4200, Train Loss: 8.660332281266765, Valid Loss: 8.658522282152875\n",
            "Epoch: 10, Step: 4300, Train Loss: 8.673134466947936, Valid Loss: 8.672769029781334\n",
            "Epoch: 10, Step: 4400, Train Loss: 8.669537870276368, Valid Loss: 8.664449174157577\n",
            "Epoch: 10, Step: 4500, Train Loss: 8.664568882201337, Valid Loss: 8.660117628921231\n",
            "Epoch: 11, Step: 4600, Train Loss: 8.663405548313634, Valid Loss: 8.656359218156682\n",
            "Epoch: 11, Step: 4700, Train Loss: 8.646830618451066, Valid Loss: 8.641439207976308\n",
            "Epoch: 11, Step: 4800, Train Loss: 8.638729055439208, Valid Loss: 8.644254602577137\n",
            "Epoch: 11, Step: 4900, Train Loss: 8.676652806773046, Valid Loss: 8.675540932758658\n",
            "Epoch: 11, Step: 5000, Train Loss: 8.641419761840682, Valid Loss: 8.648701879486943\n",
            "Epoch: 12, Step: 5100, Train Loss: 8.67594342289948, Valid Loss: 8.671277969708797\n",
            "Epoch: 12, Step: 5200, Train Loss: 8.64755345499012, Valid Loss: 8.656793629663214\n",
            "Epoch: 12, Step: 5300, Train Loss: 8.642575455136328, Valid Loss: 8.643131054241028\n",
            "Epoch: 12, Step: 5400, Train Loss: 8.664955860502506, Valid Loss: 8.666651861637146\n",
            "Epoch: 13, Step: 5500, Train Loss: 8.632299109503439, Valid Loss: 8.63568609214193\n",
            "Epoch: 13, Step: 5600, Train Loss: 8.642447524079188, Valid Loss: 8.650223869691697\n",
            "Epoch: 13, Step: 5700, Train Loss: 8.636823158068182, Valid Loss: 8.639473341016892\n",
            "Epoch: 13, Step: 5800, Train Loss: 8.621155837473763, Valid Loss: 8.622526371485442\n",
            "Epoch: 14, Step: 5900, Train Loss: 8.642735677579708, Valid Loss: 8.65810179761625\n",
            "Epoch: 14, Step: 6000, Train Loss: 8.623931131268947, Valid Loss: 8.637315954368075\n",
            "Epoch: 14, Step: 6100, Train Loss: 8.620580213843363, Valid Loss: 8.637495850023912\n",
            "Epoch: 14, Step: 6200, Train Loss: 8.63813701426285, Valid Loss: 8.652148395542527\n",
            "Epoch: 15, Step: 6300, Train Loss: 8.617865338582869, Valid Loss: 8.630501293550982\n",
            "Epoch: 15, Step: 6400, Train Loss: 8.619916606313424, Valid Loss: 8.628127061388726\n",
            "Epoch: 15, Step: 6500, Train Loss: 8.623158749001892, Valid Loss: 8.641264029811177\n",
            "Epoch: 15, Step: 6600, Train Loss: 8.608299080043544, Valid Loss: 8.626027296565645\n",
            "Epoch: 16, Step: 6700, Train Loss: 8.620526733762507, Valid Loss: 8.633939612974782\n",
            "Epoch: 16, Step: 6800, Train Loss: 8.604938905598987, Valid Loss: 8.624212765818022\n",
            "Epoch: 16, Step: 6900, Train Loss: 8.597286736094981, Valid Loss: 8.619011252690992\n",
            "Epoch: 16, Step: 7000, Train Loss: 8.612612441682801, Valid Loss: 8.629608365062621\n",
            "Epoch: 16, Step: 7100, Train Loss: 8.63460129754258, Valid Loss: 8.646625056668572\n",
            "Epoch: 17, Step: 7200, Train Loss: 8.613631322096515, Valid Loss: 8.639919562965042\n",
            "Epoch: 17, Step: 7300, Train Loss: 8.626420943527956, Valid Loss: 8.642308366212744\n",
            "Epoch: 17, Step: 7400, Train Loss: 8.635945854412125, Valid Loss: 8.656311962025196\n",
            "Epoch: 17, Step: 7500, Train Loss: 8.595642708446968, Valid Loss: 8.612664874018657\n",
            "Epoch: 18, Step: 7600, Train Loss: 8.624043007671622, Valid Loss: 8.634971521305141\n",
            "Epoch: 18, Step: 7700, Train Loss: 8.629940542645711, Valid Loss: 8.640502009775142\n",
            "Epoch: 18, Step: 7800, Train Loss: 8.616411782879029, Valid Loss: 8.635951666360457\n",
            "Epoch: 18, Step: 7900, Train Loss: 8.623113515460043, Valid Loss: 8.647386970632597\n",
            "Epoch: 19, Step: 8000, Train Loss: 8.59390273780445, Valid Loss: 8.619904607057316\n",
            "Epoch: 19, Step: 8100, Train Loss: 8.604088632873413, Valid Loss: 8.62615543850082\n",
            "Epoch: 19, Step: 8200, Train Loss: 8.612204055660388, Valid Loss: 8.636672447209014\n",
            "Epoch: 19, Step: 8300, Train Loss: 8.612377128944742, Valid Loss: 8.633902844635832\n",
            "Epoch: 20, Step: 8400, Train Loss: 8.58911331379375, Valid Loss: 8.620183349498973\n",
            "Epoch: 20, Step: 8500, Train Loss: 8.631011738292978, Valid Loss: 8.657752150952055\n",
            "Epoch: 20, Step: 8600, Train Loss: 8.596729543095696, Valid Loss: 8.625339855229141\n",
            "Epoch: 20, Step: 8700, Train Loss: 8.577347025514483, Valid Loss: 8.606817535628421\n",
            "Epoch: 21, Step: 8800, Train Loss: 8.590862558683048, Valid Loss: 8.617922474262313\n",
            "Epoch: 21, Step: 8900, Train Loss: 8.604787745665856, Valid Loss: 8.634786017143043\n",
            "Epoch: 21, Step: 9000, Train Loss: 8.599358699449631, Valid Loss: 8.630044123129975\n",
            "Epoch: 21, Step: 9100, Train Loss: 8.605479657409518, Valid Loss: 8.63318355476922\n",
            "Epoch: 22, Step: 9200, Train Loss: 8.581271529381196, Valid Loss: 8.61349420016912\n",
            "Epoch: 22, Step: 9300, Train Loss: 8.565305549454106, Valid Loss: 8.605400147479848\n",
            "Epoch: 22, Step: 9400, Train Loss: 8.593176577448183, Valid Loss: 8.62542954020248\n",
            "Epoch: 22, Step: 9500, Train Loss: 8.580963970977047, Valid Loss: 8.618917987272416\n",
            "Epoch: 22, Step: 9600, Train Loss: 8.58288940186335, Valid Loss: 8.622741262047123\n",
            "Epoch: 23, Step: 9700, Train Loss: 8.572105462228313, Valid Loss: 8.612244595866901\n",
            "Epoch: 23, Step: 9800, Train Loss: 8.582403946507576, Valid Loss: 8.615955663281197\n",
            "Epoch: 23, Step: 9900, Train Loss: 8.584204151125492, Valid Loss: 8.614851004886283\n",
            "Epoch: 23, Step: 10000, Train Loss: 8.581561825135342, Valid Loss: 8.616595338167548\n",
            "Epoch: 24, Step: 10100, Train Loss: 8.583462887921192, Valid Loss: 8.614891648523175\n",
            "Epoch: 24, Step: 10200, Train Loss: 8.58393697802667, Valid Loss: 8.616792063482569\n",
            "Epoch: 24, Step: 10300, Train Loss: 8.584108949163628, Valid Loss: 8.613188934886358\n",
            "Epoch: 24, Step: 10400, Train Loss: 8.572575623142368, Valid Loss: 8.600171618450718\n",
            "Epoch: 25, Step: 10500, Train Loss: 8.586437198148683, Valid Loss: 8.607005724828339\n",
            "Epoch: 25, Step: 10600, Train Loss: 8.577479572427233, Valid Loss: 8.603810508737736\n",
            "Epoch: 25, Step: 10700, Train Loss: 8.583559847736732, Valid Loss: 8.608023266510214\n",
            "Epoch: 25, Step: 10800, Train Loss: 8.584210952451865, Valid Loss: 8.6202167291895\n",
            "Epoch: 26, Step: 10900, Train Loss: 8.586673170523644, Valid Loss: 8.620109929849267\n",
            "Epoch: 26, Step: 11000, Train Loss: 8.58797567128262, Valid Loss: 8.617927576742941\n",
            "Epoch: 26, Step: 11100, Train Loss: 8.565423151813055, Valid Loss: 8.600383654083128\n",
            "Epoch: 26, Step: 11200, Train Loss: 8.562176345917639, Valid Loss: 8.594528080163775\n",
            "Epoch: 27, Step: 11300, Train Loss: 8.577959574967542, Valid Loss: 8.611039050080738\n",
            "Epoch: 27, Step: 11400, Train Loss: 8.559257094162106, Valid Loss: 8.603810134235697\n",
            "Epoch: 27, Step: 11500, Train Loss: 8.561719974666635, Valid Loss: 8.59837988616312\n",
            "Epoch: 27, Step: 11600, Train Loss: 8.555401697632224, Valid Loss: 8.597637925685962\n",
            "Epoch: 27, Step: 11700, Train Loss: 8.581854409338632, Valid Loss: 8.612653273938085\n",
            "Epoch: 28, Step: 11800, Train Loss: 8.55642644518336, Valid Loss: 8.594445105357204\n",
            "Epoch: 28, Step: 11900, Train Loss: 8.5593252269545, Valid Loss: 8.59240411937995\n",
            "Epoch: 28, Step: 12000, Train Loss: 8.585969574823725, Valid Loss: 8.62157311350084\n",
            "Epoch: 28, Step: 12100, Train Loss: 8.566771148762639, Valid Loss: 8.60305574879068\n",
            "Epoch: 29, Step: 12200, Train Loss: 8.560518720277043, Valid Loss: 8.596313907569046\n",
            "Epoch: 29, Step: 12300, Train Loss: 8.54502404782744, Valid Loss: 8.585819414635363\n",
            "Epoch: 29, Step: 12400, Train Loss: 8.557960113050266, Valid Loss: 8.599451777371742\n",
            "Epoch: 29, Step: 12500, Train Loss: 8.544795175119619, Valid Loss: 8.591668555190427\n",
            "Epoch: 30, Step: 12600, Train Loss: 8.55808408476322, Valid Loss: 8.612829845817462\n",
            "Epoch: 30, Step: 12700, Train Loss: 8.565617385326501, Valid Loss: 8.612198995258577\n",
            "Epoch: 30, Step: 12800, Train Loss: 8.548169599121671, Valid Loss: 8.592026104538538\n",
            "Epoch: 30, Step: 12900, Train Loss: 8.561245749452654, Valid Loss: 8.600010138264272\n",
            "Epoch: 31, Step: 13000, Train Loss: 8.545898718762007, Valid Loss: 8.590966747396703\n",
            "Epoch: 31, Step: 13100, Train Loss: 8.552043307167194, Valid Loss: 8.595631939861475\n",
            "Epoch: 31, Step: 13200, Train Loss: 8.551999039555433, Valid Loss: 8.594650132193145\n",
            "Epoch: 31, Step: 13300, Train Loss: 8.552151006672904, Valid Loss: 8.588897131948737\n",
            "Epoch: 32, Step: 13400, Train Loss: 8.55082771162285, Valid Loss: 8.593619305302642\n",
            "Epoch: 32, Step: 13500, Train Loss: 8.555108536691662, Valid Loss: 8.597728169065121\n",
            "Epoch: 32, Step: 13600, Train Loss: 8.551690625659779, Valid Loss: 8.596381533246094\n",
            "Epoch: 32, Step: 13700, Train Loss: 8.544343263956796, Valid Loss: 8.589934833693476\n",
            "Epoch: 33, Step: 13800, Train Loss: 8.567629100976221, Valid Loss: 8.613655919000408\n",
            "Epoch: 33, Step: 13900, Train Loss: 8.558777836215135, Valid Loss: 8.614920369813166\n",
            "Epoch: 33, Step: 14000, Train Loss: 8.578166318525282, Valid Loss: 8.620371923630538\n",
            "Epoch: 33, Step: 14100, Train Loss: 8.553963388064135, Valid Loss: 8.60269078005888\n",
            "Epoch: 33, Step: 14200, Train Loss: 8.56364440493393, Valid Loss: 8.608353147547135\n",
            "Epoch: 34, Step: 14300, Train Loss: 8.552128429516118, Valid Loss: 8.599127488930144\n",
            "Epoch: 34, Step: 14400, Train Loss: 8.56990124443314, Valid Loss: 8.622928485821642\n",
            "Epoch: 34, Step: 14500, Train Loss: 8.555965885634125, Valid Loss: 8.604026042970847\n",
            "Epoch: 34, Step: 14600, Train Loss: 8.56444619046609, Valid Loss: 8.61362975334193\n",
            "Epoch: 35, Step: 14700, Train Loss: 8.529357636611389, Valid Loss: 8.58234046855851\n",
            "Epoch: 35, Step: 14800, Train Loss: 8.536895763735785, Valid Loss: 8.587528030793559\n",
            "Epoch: 35, Step: 14900, Train Loss: 8.576304359553253, Valid Loss: 8.619930145324664\n",
            "Epoch: 35, Step: 15000, Train Loss: 8.544372469884799, Valid Loss: 8.597649766478643\n",
            "Epoch: 36, Step: 15100, Train Loss: 8.530622825290516, Valid Loss: 8.583521606411876\n",
            "Epoch: 36, Step: 15200, Train Loss: 8.537613089218684, Valid Loss: 8.594424933698516\n",
            "Epoch: 36, Step: 15300, Train Loss: 8.536753042173208, Valid Loss: 8.58924386375252\n",
            "Epoch: 36, Step: 15400, Train Loss: 8.538977877398688, Valid Loss: 8.593109392814169\n",
            "Epoch: 37, Step: 15500, Train Loss: 8.537633058298773, Valid Loss: 8.5980357713082\n",
            "Epoch: 37, Step: 15600, Train Loss: 8.542195313108492, Valid Loss: 8.597900356840164\n",
            "Epoch: 37, Step: 15700, Train Loss: 8.550755661635757, Valid Loss: 8.600100797531793\n",
            "Epoch: 37, Step: 15800, Train Loss: 8.537436036586657, Valid Loss: 8.590987270368323\n",
            "Epoch: 38, Step: 15900, Train Loss: 8.54348196762517, Valid Loss: 8.597792780606794\n",
            "Epoch: 38, Step: 16000, Train Loss: 8.537453182233874, Valid Loss: 8.591488136285024\n",
            "Epoch: 38, Step: 16100, Train Loss: 8.538028776029527, Valid Loss: 8.594960924554794\n",
            "Epoch: 38, Step: 16200, Train Loss: 8.534385433622242, Valid Loss: 8.58915585563495\n",
            "Epoch: 38, Step: 16300, Train Loss: 8.535144873291873, Valid Loss: 8.59092962844319\n",
            "Epoch: 39, Step: 16400, Train Loss: 8.523823086544505, Valid Loss: 8.582627593539977\n",
            "Epoch: 39, Step: 16500, Train Loss: 8.524344341222355, Valid Loss: 8.586777348148562\n",
            "Epoch: 39, Step: 16600, Train Loss: 8.542738078650194, Valid Loss: 8.588942081451574\n",
            "Epoch: 39, Step: 16700, Train Loss: 8.52644829337637, Valid Loss: 8.581970390697373\n",
            "Epoch: 40, Step: 16800, Train Loss: 8.54082635825279, Valid Loss: 8.592675786059756\n",
            "Epoch: 40, Step: 16900, Train Loss: 8.522192004931686, Valid Loss: 8.582814406600045\n",
            "Epoch: 40, Step: 17000, Train Loss: 8.552242849634636, Valid Loss: 8.603816818972234\n",
            "Epoch: 40, Step: 17100, Train Loss: 8.537649612373128, Valid Loss: 8.597709371909314\n",
            "Epoch: 41, Step: 17200, Train Loss: 8.517507818969042, Valid Loss: 8.575431417761\n",
            "Epoch: 41, Step: 17300, Train Loss: 8.545197448599568, Valid Loss: 8.597177947013755\n",
            "Epoch: 41, Step: 17400, Train Loss: 8.559057354896195, Valid Loss: 8.606720093352756\n",
            "Epoch: 41, Step: 17500, Train Loss: 8.527396179176801, Valid Loss: 8.579778586787633\n",
            "Epoch: 42, Step: 17600, Train Loss: 8.530260047341804, Valid Loss: 8.591552101059277\n",
            "Epoch: 42, Step: 17700, Train Loss: 8.52430940533179, Valid Loss: 8.58706896490158\n",
            "Epoch: 42, Step: 17800, Train Loss: 8.528893687455549, Valid Loss: 8.585010846955544\n",
            "Epoch: 42, Step: 17900, Train Loss: 8.527347582202857, Valid Loss: 8.585045849965063\n",
            "Epoch: 43, Step: 18000, Train Loss: 8.52532148676667, Valid Loss: 8.588154222257597\n",
            "Epoch: 43, Step: 18100, Train Loss: 8.528481810297766, Valid Loss: 8.58809582320395\n",
            "Epoch: 43, Step: 18200, Train Loss: 8.524934953782363, Valid Loss: 8.585553619180164\n",
            "Epoch: 43, Step: 18300, Train Loss: 8.536245106999955, Valid Loss: 8.592543090991372\n",
            "Epoch: 44, Step: 18400, Train Loss: 8.520427568963704, Valid Loss: 8.581578050787783\n",
            "Epoch: 44, Step: 18500, Train Loss: 8.513277575478794, Valid Loss: 8.574886732623911\n",
            "Epoch: 44, Step: 18600, Train Loss: 8.520969211669973, Valid Loss: 8.576545332673854\n",
            "Epoch: 44, Step: 18700, Train Loss: 8.518969533711566, Valid Loss: 8.574525101288241\n",
            "Epoch: 44, Step: 18800, Train Loss: 8.531258606334864, Valid Loss: 8.586173388619395\n",
            "Epoch: 45, Step: 18900, Train Loss: 8.530761041256063, Valid Loss: 8.587773011377546\n",
            "Epoch: 45, Step: 19000, Train Loss: 8.521358467867424, Valid Loss: 8.582361090681104\n",
            "Epoch: 45, Step: 19100, Train Loss: 8.520885413915142, Valid Loss: 8.57076134082133\n",
            "Epoch: 45, Step: 19200, Train Loss: 8.517251557245316, Valid Loss: 8.57014876263712\n",
            "Epoch: 46, Step: 19300, Train Loss: 8.530297297080558, Valid Loss: 8.58300338462933\n",
            "Epoch: 46, Step: 19400, Train Loss: 8.500911035926721, Valid Loss: 8.56529228269098\n",
            "Epoch: 46, Step: 19500, Train Loss: 8.533259190838999, Valid Loss: 8.593315505326265\n",
            "Epoch: 46, Step: 19600, Train Loss: 8.510168213859018, Valid Loss: 8.573236325773422\n",
            "Epoch: 47, Step: 19700, Train Loss: 8.508291888720409, Valid Loss: 8.573489715172517\n",
            "Epoch: 47, Step: 19800, Train Loss: 8.517495368836814, Valid Loss: 8.575654691192351\n",
            "Epoch: 47, Step: 19900, Train Loss: 8.523602412629867, Valid Loss: 8.584492180269287\n",
            "Epoch: 47, Step: 20000, Train Loss: 8.505693667705373, Valid Loss: 8.576628976659835\n",
            "Epoch: 48, Step: 20100, Train Loss: 8.520866781949739, Valid Loss: 8.58988073721286\n",
            "Epoch: 48, Step: 20200, Train Loss: 8.512053000206969, Valid Loss: 8.582232115716119\n",
            "Epoch: 48, Step: 20300, Train Loss: 8.506224799246368, Valid Loss: 8.578660142622969\n",
            "Epoch: 48, Step: 20400, Train Loss: 8.516777760581483, Valid Loss: 8.5748506836435\n",
            "Epoch: 49, Step: 20500, Train Loss: 8.513986202779755, Valid Loss: 8.576588493013553\n",
            "Epoch: 49, Step: 20600, Train Loss: 8.519343821532008, Valid Loss: 8.584010749038894\n",
            "Epoch: 49, Step: 20700, Train Loss: 8.519580208923465, Valid Loss: 8.584484977571286\n",
            "Epoch: 49, Step: 20800, Train Loss: 8.50596878189688, Valid Loss: 8.574847898547686\n",
            "Epoch: 49, Step: 20900, Train Loss: 8.53340047474522, Valid Loss: 8.592359266419802\n",
            "Epoch: 50, Step: 21000, Train Loss: 8.508186499220093, Valid Loss: 8.581923848863273\n",
            "Epoch: 50, Step: 21100, Train Loss: 8.519831887607289, Valid Loss: 8.579380788046938\n",
            "Epoch: 50, Step: 21200, Train Loss: 8.514290295203184, Valid Loss: 8.568858154125978\n",
            "Epoch: 50, Step: 21300, Train Loss: 8.532312851518551, Valid Loss: 8.600046067090526\n",
            "Epoch: 51, Step: 21400, Train Loss: 8.512364616929915, Valid Loss: 8.57745097409698\n",
            "Epoch: 51, Step: 21500, Train Loss: 8.538433188528115, Valid Loss: 8.606388321818\n",
            "Epoch: 51, Step: 21600, Train Loss: 8.521311365704122, Valid Loss: 8.591024746972604\n",
            "Epoch: 51, Step: 21700, Train Loss: 8.500940376757603, Valid Loss: 8.57411933897179\n",
            "Epoch: 52, Step: 21800, Train Loss: 8.500235009383793, Valid Loss: 8.572303981680394\n",
            "Epoch: 52, Step: 21900, Train Loss: 8.50461288901419, Valid Loss: 8.578507478288277\n",
            "Epoch: 52, Step: 22000, Train Loss: 8.525524886201538, Valid Loss: 8.588282242677671\n",
            "Epoch: 52, Step: 22100, Train Loss: 8.501580718458621, Valid Loss: 8.56824801573916\n",
            "Epoch: 53, Step: 22200, Train Loss: 8.510036599381737, Valid Loss: 8.577955544605143\n",
            "Epoch: 53, Step: 22300, Train Loss: 8.516922950240659, Valid Loss: 8.58406761309688\n",
            "Epoch: 53, Step: 22400, Train Loss: 8.517584709718694, Valid Loss: 8.579535805580692\n",
            "Epoch: 53, Step: 22500, Train Loss: 8.504537522750377, Valid Loss: 8.575777561914107\n",
            "Epoch: 54, Step: 22600, Train Loss: 8.515162545793503, Valid Loss: 8.580523708996708\n",
            "Epoch: 54, Step: 22700, Train Loss: 8.513617151634861, Valid Loss: 8.579987056792339\n",
            "Epoch: 54, Step: 22800, Train Loss: 8.489376534620547, Valid Loss: 8.564109786464972\n",
            "Epoch: 54, Step: 22900, Train Loss: 8.50188442781586, Valid Loss: 8.56929710954185\n",
            "Epoch: 55, Step: 23000, Train Loss: 8.505769279569034, Valid Loss: 8.570018881413226\n",
            "Epoch: 55, Step: 23100, Train Loss: 8.50706246548032, Valid Loss: 8.576238536129473\n",
            "Epoch: 55, Step: 23200, Train Loss: 8.52063008477807, Valid Loss: 8.591351348854472\n",
            "Epoch: 55, Step: 23300, Train Loss: 8.503059875594388, Valid Loss: 8.573149818822959\n",
            "Epoch: 55, Step: 23400, Train Loss: 8.50435195441612, Valid Loss: 8.5719165085532\n",
            "Epoch: 56, Step: 23500, Train Loss: 8.506821842174709, Valid Loss: 8.573707549295877\n",
            "Epoch: 56, Step: 23600, Train Loss: 8.514452021198544, Valid Loss: 8.577634349687319\n",
            "Epoch: 56, Step: 23700, Train Loss: 8.501539935452625, Valid Loss: 8.573758790430944\n",
            "Epoch: 56, Step: 23800, Train Loss: 8.496757605272828, Valid Loss: 8.565418417230866\n",
            "Epoch: 57, Step: 23900, Train Loss: 8.49498595834921, Valid Loss: 8.56833467786681\n",
            "Epoch: 57, Step: 24000, Train Loss: 8.491392106923433, Valid Loss: 8.567887023485838\n",
            "Epoch: 57, Step: 24100, Train Loss: 8.500988227082594, Valid Loss: 8.574733428853497\n",
            "Epoch: 57, Step: 24200, Train Loss: 8.49090567478585, Valid Loss: 8.563190560374954\n",
            "Epoch: 58, Step: 24300, Train Loss: 8.517989886819809, Valid Loss: 8.587823011110697\n",
            "Epoch: 58, Step: 24400, Train Loss: 8.499006193736966, Valid Loss: 8.568224034122014\n",
            "Epoch: 58, Step: 24500, Train Loss: 8.51191896521882, Valid Loss: 8.573439599915378\n",
            "Epoch: 58, Step: 24600, Train Loss: 8.490966163581483, Valid Loss: 8.564199173288515\n",
            "Epoch: 59, Step: 24700, Train Loss: 8.49562522324909, Valid Loss: 8.574651461051111\n",
            "Epoch: 59, Step: 24800, Train Loss: 8.507395841205978, Valid Loss: 8.57145911756472\n",
            "Epoch: 59, Step: 24900, Train Loss: 8.493715175942457, Valid Loss: 8.55958509758923\n",
            "Epoch: 59, Step: 25000, Train Loss: 8.481624095621813, Valid Loss: 8.550942181613813\n",
            "Epoch: 60, Step: 25100, Train Loss: 8.505525491965834, Valid Loss: 8.57317075291687\n",
            "Epoch: 60, Step: 25200, Train Loss: 8.516071827766421, Valid Loss: 8.581318710879179\n",
            "Epoch: 60, Step: 25300, Train Loss: 8.496488024731239, Valid Loss: 8.555157353483953\n",
            "Epoch: 60, Step: 25400, Train Loss: 8.493129929774996, Valid Loss: 8.55892027911622\n",
            "Epoch: 61, Step: 25500, Train Loss: 8.518249160921304, Valid Loss: 8.579461787708777\n",
            "Epoch: 61, Step: 25600, Train Loss: 8.515323137642111, Valid Loss: 8.571474404782169\n",
            "Epoch: 61, Step: 25700, Train Loss: 8.482480646641182, Valid Loss: 8.562731330893794\n",
            "Epoch: 61, Step: 25800, Train Loss: 8.503678051104176, Valid Loss: 8.57601210273081\n",
            "Epoch: 61, Step: 25900, Train Loss: 8.493982408394032, Valid Loss: 8.56256582182118\n",
            "Epoch: 62, Step: 26000, Train Loss: 8.500918260352572, Valid Loss: 8.565220418537391\n",
            "Epoch: 62, Step: 26100, Train Loss: 8.507669214361217, Valid Loss: 8.56736114528289\n",
            "Epoch: 62, Step: 26200, Train Loss: 8.504536382841517, Valid Loss: 8.567299266316388\n",
            "Epoch: 62, Step: 26300, Train Loss: 8.493992359929408, Valid Loss: 8.56366438518934\n",
            "Epoch: 63, Step: 26400, Train Loss: 8.48973905140564, Valid Loss: 8.559267885723873\n",
            "Epoch: 63, Step: 26500, Train Loss: 8.497451083057525, Valid Loss: 8.565416809242922\n",
            "Epoch: 63, Step: 26600, Train Loss: 8.499093617841949, Valid Loss: 8.565204038304142\n",
            "Epoch: 63, Step: 26700, Train Loss: 8.496638528706546, Valid Loss: 8.567334998014225\n",
            "Epoch: 64, Step: 26800, Train Loss: 8.50336374786174, Valid Loss: 8.572873018228895\n",
            "Epoch: 64, Step: 26900, Train Loss: 8.497483655178405, Valid Loss: 8.567567100474864\n",
            "Epoch: 64, Step: 27000, Train Loss: 8.497321645609743, Valid Loss: 8.572336710068674\n",
            "Epoch: 64, Step: 27100, Train Loss: 8.479121568359576, Valid Loss: 8.553281238210715\n",
            "Epoch: 65, Step: 27200, Train Loss: 8.498213354713785, Valid Loss: 8.568661279466365\n",
            "Epoch: 65, Step: 27300, Train Loss: 8.50293400856978, Valid Loss: 8.570722586339903\n",
            "Epoch: 65, Step: 27400, Train Loss: 8.490616644583232, Valid Loss: 8.560692843081783\n",
            "Epoch: 65, Step: 27500, Train Loss: 8.494730649297336, Valid Loss: 8.568440456139156\n",
            "Epoch: 66, Step: 27600, Train Loss: 8.491689763507528, Valid Loss: 8.559678546948176\n",
            "Epoch: 66, Step: 27700, Train Loss: 8.509212242578725, Valid Loss: 8.571810753521296\n",
            "Epoch: 66, Step: 27800, Train Loss: 8.48707299330961, Valid Loss: 8.563838954677768\n",
            "Epoch: 66, Step: 27900, Train Loss: 8.4966004189508, Valid Loss: 8.567978570779239\n",
            "Epoch: 66, Step: 28000, Train Loss: 8.492638431882439, Valid Loss: 8.55393489649053\n",
            "Epoch: 67, Step: 28100, Train Loss: 8.493666150163264, Valid Loss: 8.555189084040663\n",
            "Epoch: 67, Step: 28200, Train Loss: 8.48432076919946, Valid Loss: 8.558569946738835\n",
            "Epoch: 67, Step: 28300, Train Loss: 8.484211051668131, Valid Loss: 8.562495088307308\n",
            "Epoch: 67, Step: 28400, Train Loss: 8.492198876530878, Valid Loss: 8.571550584938905\n",
            "Epoch: 68, Step: 28500, Train Loss: 8.491878921217166, Valid Loss: 8.564435120512384\n",
            "Epoch: 68, Step: 28600, Train Loss: 8.486475728430666, Valid Loss: 8.554598098490715\n",
            "Epoch: 68, Step: 28700, Train Loss: 8.491338445345415, Valid Loss: 8.568576820364857\n",
            "Epoch: 68, Step: 28800, Train Loss: 8.508238945376748, Valid Loss: 8.583878577534566\n",
            "Epoch: 69, Step: 28900, Train Loss: 8.470651996949293, Valid Loss: 8.557692032637537\n",
            "Epoch: 69, Step: 29000, Train Loss: 8.494269409255406, Valid Loss: 8.573178987012554\n",
            "Epoch: 69, Step: 29100, Train Loss: 8.501563332295206, Valid Loss: 8.575640239020089\n",
            "Epoch: 69, Step: 29200, Train Loss: 8.480108382819026, Valid Loss: 8.560599702464911\n",
            "Epoch: 70, Step: 29300, Train Loss: 8.485387264696291, Valid Loss: 8.58337809197085\n",
            "Epoch: 70, Step: 29400, Train Loss: 8.473469340475978, Valid Loss: 8.564103087948512\n",
            "Epoch: 70, Step: 29500, Train Loss: 8.493221272299166, Valid Loss: 8.570617563566369\n",
            "Epoch: 70, Step: 29600, Train Loss: 8.493819853795928, Valid Loss: 8.587928380138115\n",
            "Epoch: 71, Step: 29700, Train Loss: 8.48080337624987, Valid Loss: 8.56742720506636\n",
            "Epoch: 71, Step: 29800, Train Loss: 8.492399494222179, Valid Loss: 8.57502498856327\n",
            "Epoch: 71, Step: 29900, Train Loss: 8.473678872284315, Valid Loss: 8.557323150487585\n",
            "Epoch: 71, Step: 30000, Train Loss: 8.499044023718792, Valid Loss: 8.580857543265154\n",
            "Epoch: 72, Step: 30100, Train Loss: 8.484191852926248, Valid Loss: 8.56601803062635\n",
            "Epoch: 72, Step: 30200, Train Loss: 8.504400703705775, Valid Loss: 8.583957618659674\n",
            "Epoch: 72, Step: 30300, Train Loss: 8.481918119366668, Valid Loss: 8.567624329309409\n",
            "Epoch: 72, Step: 30400, Train Loss: 8.488241003640006, Valid Loss: 8.570918477488794\n",
            "Epoch: 72, Step: 30500, Train Loss: 8.482735413812941, Valid Loss: 8.558996595617556\n",
            "Epoch: 73, Step: 30600, Train Loss: 8.481845874937058, Valid Loss: 8.561853425339391\n",
            "Epoch: 73, Step: 30700, Train Loss: 8.499301324193631, Valid Loss: 8.576879289870622\n",
            "Epoch: 73, Step: 30800, Train Loss: 8.49605297852495, Valid Loss: 8.5742391983291\n",
            "Epoch: 73, Step: 30900, Train Loss: 8.50774635300103, Valid Loss: 8.572351126046328\n",
            "Epoch: 74, Step: 31000, Train Loss: 8.495377353149653, Valid Loss: 8.564619128868353\n",
            "Epoch: 74, Step: 31100, Train Loss: 8.493015434567932, Valid Loss: 8.563580998211929\n",
            "Epoch: 74, Step: 31200, Train Loss: 8.476149268388543, Valid Loss: 8.561753889703642\n",
            "Epoch: 74, Step: 31300, Train Loss: 8.484224579376031, Valid Loss: 8.556589581647371\n",
            "Epoch: 75, Step: 31400, Train Loss: 8.50231277886777, Valid Loss: 8.578014341675338\n",
            "Epoch: 75, Step: 31500, Train Loss: 8.474903869308038, Valid Loss: 8.556914278685468\n",
            "Epoch: 75, Step: 31600, Train Loss: 8.496249073876502, Valid Loss: 8.567818347011302\n",
            "Epoch: 75, Step: 31700, Train Loss: 8.49243128171243, Valid Loss: 8.564947416297283\n",
            "Epoch: 76, Step: 31800, Train Loss: 8.480374500246542, Valid Loss: 8.556112903670384\n",
            "Epoch: 76, Step: 31900, Train Loss: 8.490951183553959, Valid Loss: 8.571720126722639\n",
            "Epoch: 76, Step: 32000, Train Loss: 8.482969422432644, Valid Loss: 8.562909227321175\n",
            "Epoch: 76, Step: 32100, Train Loss: 8.466488412711328, Valid Loss: 8.54631984956714\n",
            "Epoch: 77, Step: 32200, Train Loss: 8.471197584098427, Valid Loss: 8.555013311523595\n",
            "Epoch: 77, Step: 32300, Train Loss: 8.484790504972059, Valid Loss: 8.56395269454658\n",
            "Epoch: 77, Step: 32400, Train Loss: 8.486230131981623, Valid Loss: 8.57484578639329\n",
            "Epoch: 77, Step: 32500, Train Loss: 8.473508715744579, Valid Loss: 8.565700152219835\n",
            "Epoch: 77, Step: 32600, Train Loss: 8.482863750328676, Valid Loss: 8.571344150526144\n",
            "Epoch: 78, Step: 32700, Train Loss: 8.47983221191088, Valid Loss: 8.569228554960352\n",
            "Epoch: 78, Step: 32800, Train Loss: 8.483651688579398, Valid Loss: 8.568539562635454\n",
            "Epoch: 78, Step: 32900, Train Loss: 8.498208548278548, Valid Loss: 8.578680569836267\n",
            "Epoch: 78, Step: 33000, Train Loss: 8.495230822949267, Valid Loss: 8.579776382572936\n",
            "Epoch: 79, Step: 33100, Train Loss: 8.467420707048896, Valid Loss: 8.563012077079808\n",
            "Epoch: 79, Step: 33200, Train Loss: 8.484303583749002, Valid Loss: 8.567215327212942\n",
            "Epoch: 79, Step: 33300, Train Loss: 8.510427273407936, Valid Loss: 8.585184469704625\n",
            "Epoch: 79, Step: 33400, Train Loss: 8.500923394739427, Valid Loss: 8.586004887697039\n",
            "Epoch: 80, Step: 33500, Train Loss: 8.463930704490927, Valid Loss: 8.558217587114452\n",
            "Epoch: 80, Step: 33600, Train Loss: 8.494391884444601, Valid Loss: 8.580919812856648\n",
            "Epoch: 80, Step: 33700, Train Loss: 8.465448501761472, Valid Loss: 8.552371670709677\n",
            "Epoch: 80, Step: 33800, Train Loss: 8.485998903969499, Valid Loss: 8.569208267417643\n",
            "Epoch: 81, Step: 33900, Train Loss: 8.481195862026693, Valid Loss: 8.574846530559375\n",
            "Epoch: 81, Step: 34000, Train Loss: 8.473212081014802, Valid Loss: 8.565959129086142\n",
            "Epoch: 81, Step: 34100, Train Loss: 8.494449412756572, Valid Loss: 8.571334121058001\n",
            "Epoch: 81, Step: 34200, Train Loss: 8.484390912791254, Valid Loss: 8.57203035007125\n",
            "Epoch: 82, Step: 34300, Train Loss: 8.474807873281508, Valid Loss: 8.560019772867335\n",
            "Epoch: 82, Step: 34400, Train Loss: 8.499571490545625, Valid Loss: 8.575010442003745\n",
            "Epoch: 82, Step: 34500, Train Loss: 8.456655600336562, Valid Loss: 8.541823003091157\n",
            "Epoch: 82, Step: 34600, Train Loss: 8.479262719744433, Valid Loss: 8.57106750784118\n",
            "Epoch: 83, Step: 34700, Train Loss: 8.460040342079335, Valid Loss: 8.555925805432818\n",
            "Epoch: 83, Step: 34800, Train Loss: 8.480835795719921, Valid Loss: 8.571750827957073\n",
            "Epoch: 83, Step: 34900, Train Loss: 8.475152446142099, Valid Loss: 8.564065232871206\n",
            "Epoch: 83, Step: 35000, Train Loss: 8.4981821703836, Valid Loss: 8.572078407409997\n",
            "Epoch: 83, Step: 35100, Train Loss: 8.464530364335266, Valid Loss: 8.559757336694949\n",
            "Epoch: 84, Step: 35200, Train Loss: 8.481865814475455, Valid Loss: 8.571752791004414\n",
            "Epoch: 84, Step: 35300, Train Loss: 8.480352332413968, Valid Loss: 8.577904141950913\n",
            "Epoch: 84, Step: 35400, Train Loss: 8.472458038631485, Valid Loss: 8.56931534971131\n",
            "Epoch: 84, Step: 35500, Train Loss: 8.477491092713866, Valid Loss: 8.564459884856264\n",
            "Epoch: 85, Step: 35600, Train Loss: 8.482792024543995, Valid Loss: 8.574347588317332\n",
            "Epoch: 85, Step: 35700, Train Loss: 8.483048907143711, Valid Loss: 8.5787764452608\n",
            "Epoch: 85, Step: 35800, Train Loss: 8.493004220582373, Valid Loss: 8.584320201937816\n",
            "Epoch: 85, Step: 35900, Train Loss: 8.465731629887944, Valid Loss: 8.560196315953133\n",
            "Epoch: 86, Step: 36000, Train Loss: 8.496790309761655, Valid Loss: 8.59049725019432\n",
            "Epoch: 86, Step: 36100, Train Loss: 8.47456152148457, Valid Loss: 8.570833718744483\n",
            "Epoch: 86, Step: 36200, Train Loss: 8.520101258764644, Valid Loss: 8.604598924220541\n",
            "Epoch: 86, Step: 36300, Train Loss: 8.47442892243456, Valid Loss: 8.565381834201135\n",
            "Epoch: 87, Step: 36400, Train Loss: 8.455613500467315, Valid Loss: 8.559436209311636\n",
            "Epoch: 87, Step: 36500, Train Loss: 8.467770125402813, Valid Loss: 8.565044753531987\n",
            "Epoch: 87, Step: 36600, Train Loss: 8.459628921988733, Valid Loss: 8.557152757411897\n",
            "Epoch: 87, Step: 36700, Train Loss: 8.47072077778164, Valid Loss: 8.569450368028345\n",
            "Epoch: 88, Step: 36800, Train Loss: 8.472328137873047, Valid Loss: 8.566545004129644\n",
            "Epoch: 88, Step: 36900, Train Loss: 8.45491110279379, Valid Loss: 8.560524982101153\n",
            "Epoch: 88, Step: 37000, Train Loss: 8.46698886932955, Valid Loss: 8.559292251661244\n",
            "Epoch: 88, Step: 37100, Train Loss: 8.493392775365622, Valid Loss: 8.583845296972221\n",
            "Epoch: 88, Step: 37200, Train Loss: 8.477190472642874, Valid Loss: 8.571926348164\n",
            "Epoch: 89, Step: 37300, Train Loss: 8.490387380345625, Valid Loss: 8.576194445808982\n",
            "Epoch: 89, Step: 37400, Train Loss: 8.470542491045538, Valid Loss: 8.56425872382175\n",
            "Epoch: 89, Step: 37500, Train Loss: 8.459788886877798, Valid Loss: 8.551630513247927\n",
            "Epoch: 89, Step: 37600, Train Loss: 8.511156824098652, Valid Loss: 8.59150692744228\n",
            "Epoch: 90, Step: 37700, Train Loss: 8.469849161682745, Valid Loss: 8.561138396494764\n",
            "Epoch: 90, Step: 37800, Train Loss: 8.47536225494553, Valid Loss: 8.56560396816093\n",
            "Epoch: 90, Step: 37900, Train Loss: 8.493620648779073, Valid Loss: 8.571687671152777\n",
            "Epoch: 90, Step: 38000, Train Loss: 8.48317755253972, Valid Loss: 8.570065881078346\n",
            "Epoch: 91, Step: 38100, Train Loss: 8.453835873614187, Valid Loss: 8.554035202612319\n",
            "Epoch: 91, Step: 38200, Train Loss: 8.473519210207114, Valid Loss: 8.571441456888055\n",
            "Epoch: 91, Step: 38300, Train Loss: 8.483852008063561, Valid Loss: 8.584530791632139\n",
            "Epoch: 91, Step: 38400, Train Loss: 8.465645617533188, Valid Loss: 8.558132975729704\n",
            "Epoch: 92, Step: 38500, Train Loss: 8.472474398573036, Valid Loss: 8.571686919341753\n",
            "Epoch: 92, Step: 38600, Train Loss: 8.459623241144893, Valid Loss: 8.56600453331444\n",
            "Epoch: 92, Step: 38700, Train Loss: 8.47574758751403, Valid Loss: 8.570428078989938\n",
            "Epoch: 92, Step: 38800, Train Loss: 8.49628303287948, Valid Loss: 8.588721301415957\n",
            "Epoch: 93, Step: 38900, Train Loss: 8.474113181455113, Valid Loss: 8.568253403421306\n",
            "Epoch: 93, Step: 39000, Train Loss: 8.467262289934153, Valid Loss: 8.571097975534766\n",
            "Epoch: 93, Step: 39100, Train Loss: 8.459839740058454, Valid Loss: 8.558948413154537\n",
            "Epoch: 93, Step: 39200, Train Loss: 8.475058725769648, Valid Loss: 8.572875477244054\n",
            "Epoch: 94, Step: 39300, Train Loss: 8.475168423461554, Valid Loss: 8.565780521543175\n",
            "Epoch: 94, Step: 39400, Train Loss: 8.470270529694444, Valid Loss: 8.563899975818298\n",
            "Epoch: 94, Step: 39500, Train Loss: 8.469414101182663, Valid Loss: 8.562442062998846\n",
            "Epoch: 94, Step: 39600, Train Loss: 8.460531643638976, Valid Loss: 8.566853551500166\n",
            "Epoch: 94, Step: 39700, Train Loss: 8.452405602838432, Valid Loss: 8.560294132820767\n",
            "Epoch: 95, Step: 39800, Train Loss: 8.463089861258059, Valid Loss: 8.573834473771496\n",
            "Epoch: 95, Step: 39900, Train Loss: 8.47626692233389, Valid Loss: 8.580147968844111\n",
            "Epoch: 95, Step: 40000, Train Loss: 8.479516748013868, Valid Loss: 8.572316393031056\n",
            "Epoch: 95, Step: 40100, Train Loss: 8.456818156809941, Valid Loss: 8.556987425928487\n",
            "Epoch: 96, Step: 40200, Train Loss: 8.477603803654143, Valid Loss: 8.576947756291343\n",
            "Epoch: 96, Step: 40300, Train Loss: 8.462611452297779, Valid Loss: 8.568933364635845\n",
            "Epoch: 96, Step: 40400, Train Loss: 8.466172306221868, Valid Loss: 8.572347332887551\n",
            "Epoch: 96, Step: 40500, Train Loss: 8.452950953685837, Valid Loss: 8.559106507309176\n",
            "Epoch: 97, Step: 40600, Train Loss: 8.464677096748737, Valid Loss: 8.563880838824893\n",
            "Epoch: 97, Step: 40700, Train Loss: 8.46880463029694, Valid Loss: 8.569684701103592\n",
            "Epoch: 97, Step: 40800, Train Loss: 8.473999302712983, Valid Loss: 8.581223315425632\n",
            "Epoch: 97, Step: 40900, Train Loss: 8.462573887482703, Valid Loss: 8.561979807315058\n",
            "Epoch: 98, Step: 41000, Train Loss: 8.459174151425591, Valid Loss: 8.567964553009011\n",
            "Epoch: 98, Step: 41100, Train Loss: 8.472062620867991, Valid Loss: 8.573363431630987\n",
            "Epoch: 98, Step: 41200, Train Loss: 8.473537368029463, Valid Loss: 8.570526712828732\n",
            "Epoch: 98, Step: 41300, Train Loss: 8.489356291059467, Valid Loss: 8.573047628661\n",
            "Epoch: 99, Step: 41400, Train Loss: 8.475601610788528, Valid Loss: 8.569725445907498\n",
            "Epoch: 99, Step: 41500, Train Loss: 8.461835896225102, Valid Loss: 8.560183316263139\n",
            "Epoch: 99, Step: 41600, Train Loss: 8.461938211474925, Valid Loss: 8.566212716536137\n",
            "Epoch: 99, Step: 41700, Train Loss: 8.463130435004183, Valid Loss: 8.563649278307185\n",
            "Epoch: 99, Step: 41800, Train Loss: 8.496163766512874, Valid Loss: 8.58878096635743\n",
            "Test RMSE Loss for h = 180: 8.769820182275133\n",
            "Start training for h = 360\n",
            "Epoch: 0, Step: 100, Train Loss: 9.471916961901485, Valid Loss: 9.41972499062126\n",
            "Epoch: 0, Step: 200, Train Loss: 9.187558726285332, Valid Loss: 9.12589910047867\n",
            "Epoch: 0, Step: 300, Train Loss: 9.05413479590234, Valid Loss: 8.987934582309222\n",
            "Epoch: 0, Step: 400, Train Loss: 9.007910203088924, Valid Loss: 8.94985526509204\n",
            "Epoch: 1, Step: 500, Train Loss: 8.981815479230917, Valid Loss: 8.922308561034352\n",
            "Epoch: 1, Step: 600, Train Loss: 8.971219672853175, Valid Loss: 8.918864580505685\n",
            "Epoch: 1, Step: 700, Train Loss: 8.924415220839828, Valid Loss: 8.875294207793887\n",
            "Epoch: 1, Step: 800, Train Loss: 8.884536368129101, Valid Loss: 8.837409730713022\n",
            "Epoch: 2, Step: 900, Train Loss: 8.88168478288796, Valid Loss: 8.83462222663704\n",
            "Epoch: 2, Step: 1000, Train Loss: 8.869603244244002, Valid Loss: 8.823788701054093\n",
            "Epoch: 2, Step: 1100, Train Loss: 8.850781805398654, Valid Loss: 8.806710154471888\n",
            "Epoch: 2, Step: 1200, Train Loss: 8.842961183283165, Valid Loss: 8.80072492182951\n",
            "Epoch: 3, Step: 1300, Train Loss: 8.827331121501343, Valid Loss: 8.791261450624788\n",
            "Epoch: 3, Step: 1400, Train Loss: 8.81700493674245, Valid Loss: 8.775242685042102\n",
            "Epoch: 3, Step: 1500, Train Loss: 8.77543675693077, Valid Loss: 8.734638915668587\n",
            "Epoch: 3, Step: 1600, Train Loss: 8.775493928850612, Valid Loss: 8.741303060110866\n",
            "Epoch: 4, Step: 1700, Train Loss: 8.799010866622702, Valid Loss: 8.763150898271052\n",
            "Epoch: 4, Step: 1800, Train Loss: 8.747125402399762, Valid Loss: 8.715671281034615\n",
            "Epoch: 4, Step: 1900, Train Loss: 8.770202150383271, Valid Loss: 8.738409822147776\n",
            "Epoch: 4, Step: 2000, Train Loss: 8.750052079291809, Valid Loss: 8.714442602029022\n",
            "Epoch: 5, Step: 2100, Train Loss: 8.72619363926023, Valid Loss: 8.696852188901183\n",
            "Epoch: 5, Step: 2200, Train Loss: 8.746011522094484, Valid Loss: 8.723135822584531\n",
            "Epoch: 5, Step: 2300, Train Loss: 8.757532211893167, Valid Loss: 8.738950788068042\n",
            "Epoch: 5, Step: 2400, Train Loss: 8.728991767723809, Valid Loss: 8.713870931692519\n",
            "Epoch: 5, Step: 2500, Train Loss: 8.698652651934484, Valid Loss: 8.682647133416317\n",
            "Epoch: 6, Step: 2600, Train Loss: 8.73324709903403, Valid Loss: 8.710411372123541\n",
            "Epoch: 6, Step: 2700, Train Loss: 8.742002173561824, Valid Loss: 8.720953267180805\n",
            "Epoch: 6, Step: 2800, Train Loss: 8.72003664398601, Valid Loss: 8.705476626588544\n",
            "Epoch: 6, Step: 2900, Train Loss: 8.71408681339537, Valid Loss: 8.694706270967725\n",
            "Epoch: 7, Step: 3000, Train Loss: 8.71027323437206, Valid Loss: 8.687173131707178\n",
            "Epoch: 7, Step: 3100, Train Loss: 8.706817809019554, Valid Loss: 8.690137269277136\n",
            "Epoch: 7, Step: 3200, Train Loss: 8.713559070694389, Valid Loss: 8.704192324051634\n",
            "Epoch: 7, Step: 3300, Train Loss: 8.688154130863127, Valid Loss: 8.671449899188934\n",
            "Epoch: 8, Step: 3400, Train Loss: 8.695809379938975, Valid Loss: 8.682693382743253\n",
            "Epoch: 8, Step: 3500, Train Loss: 8.68396086137548, Valid Loss: 8.672624869867569\n",
            "Epoch: 8, Step: 3600, Train Loss: 8.68531057469808, Valid Loss: 8.668622621376201\n",
            "Epoch: 8, Step: 3700, Train Loss: 8.6776709936348, Valid Loss: 8.66683091024067\n",
            "Epoch: 9, Step: 3800, Train Loss: 8.67818252803792, Valid Loss: 8.673432418122852\n",
            "Epoch: 9, Step: 3900, Train Loss: 8.676275993659315, Valid Loss: 8.676665468755267\n",
            "Epoch: 9, Step: 4000, Train Loss: 8.669416120417075, Valid Loss: 8.667997915973782\n",
            "Epoch: 9, Step: 4100, Train Loss: 8.652876862452363, Valid Loss: 8.648059889241715\n",
            "Epoch: 10, Step: 4200, Train Loss: 8.645163093600736, Valid Loss: 8.64532726133104\n",
            "Epoch: 10, Step: 4300, Train Loss: 8.664105306232527, Valid Loss: 8.663551880795476\n",
            "Epoch: 10, Step: 4400, Train Loss: 8.670375665061025, Valid Loss: 8.669756578474727\n",
            "Epoch: 10, Step: 4500, Train Loss: 8.658974263985701, Valid Loss: 8.66137725910313\n",
            "Epoch: 11, Step: 4600, Train Loss: 8.630376506469187, Valid Loss: 8.63903128406998\n",
            "Epoch: 11, Step: 4700, Train Loss: 8.642992634403587, Valid Loss: 8.654079321834995\n",
            "Epoch: 11, Step: 4800, Train Loss: 8.63896341555408, Valid Loss: 8.650468484323538\n",
            "Epoch: 11, Step: 4900, Train Loss: 8.648066628798597, Valid Loss: 8.653437917931043\n",
            "Epoch: 11, Step: 5000, Train Loss: 8.658646006845863, Valid Loss: 8.666725569827488\n",
            "Epoch: 12, Step: 5100, Train Loss: 8.627904344610885, Valid Loss: 8.642498921162787\n",
            "Epoch: 12, Step: 5200, Train Loss: 8.62686733373164, Valid Loss: 8.645486068140919\n",
            "Epoch: 12, Step: 5300, Train Loss: 8.633914626242369, Valid Loss: 8.651564325360235\n",
            "Epoch: 12, Step: 5400, Train Loss: 8.655565990993408, Valid Loss: 8.664159187152979\n",
            "Epoch: 13, Step: 5500, Train Loss: 8.64932401175049, Valid Loss: 8.662618793732342\n",
            "Epoch: 13, Step: 5600, Train Loss: 8.631695507564967, Valid Loss: 8.646778552038981\n",
            "Epoch: 13, Step: 5700, Train Loss: 8.625101028653422, Valid Loss: 8.647439392263875\n",
            "Epoch: 13, Step: 5800, Train Loss: 8.619620443424663, Valid Loss: 8.638434083801716\n",
            "Epoch: 14, Step: 5900, Train Loss: 8.637876913661376, Valid Loss: 8.657955513225724\n",
            "Epoch: 14, Step: 6000, Train Loss: 8.64313810686142, Valid Loss: 8.663026383828957\n",
            "Epoch: 14, Step: 6100, Train Loss: 8.642993781229901, Valid Loss: 8.658651161728772\n",
            "Epoch: 14, Step: 6200, Train Loss: 8.608206990319335, Valid Loss: 8.631961344651666\n",
            "Epoch: 15, Step: 6300, Train Loss: 8.622226328474063, Valid Loss: 8.647493128672437\n",
            "Epoch: 15, Step: 6400, Train Loss: 8.619171982771208, Valid Loss: 8.644132912298172\n",
            "Epoch: 15, Step: 6500, Train Loss: 8.623459335156552, Valid Loss: 8.644715374875082\n",
            "Epoch: 15, Step: 6600, Train Loss: 8.605908181888742, Valid Loss: 8.635375889982091\n",
            "Epoch: 16, Step: 6700, Train Loss: 8.611448925249523, Valid Loss: 8.640327748563035\n",
            "Epoch: 16, Step: 6800, Train Loss: 8.599939357392628, Valid Loss: 8.62784523886767\n",
            "Epoch: 16, Step: 6900, Train Loss: 8.59555639808634, Valid Loss: 8.624002639625754\n",
            "Epoch: 16, Step: 7000, Train Loss: 8.630242622967446, Valid Loss: 8.654304493799025\n",
            "Epoch: 16, Step: 7100, Train Loss: 8.612112723360852, Valid Loss: 8.643709111049656\n",
            "Epoch: 17, Step: 7200, Train Loss: 8.611147721485732, Valid Loss: 8.642883432220295\n",
            "Epoch: 17, Step: 7300, Train Loss: 8.632188754286611, Valid Loss: 8.66032248224383\n",
            "Epoch: 17, Step: 7400, Train Loss: 8.605426311076362, Valid Loss: 8.647732112220845\n",
            "Epoch: 17, Step: 7500, Train Loss: 8.584708299319662, Valid Loss: 8.619658897842788\n",
            "Epoch: 18, Step: 7600, Train Loss: 8.58012477361418, Valid Loss: 8.624365180293118\n",
            "Epoch: 18, Step: 7700, Train Loss: 8.609706569306304, Valid Loss: 8.648296085945377\n",
            "Epoch: 18, Step: 7800, Train Loss: 8.588100223005092, Valid Loss: 8.624816322683643\n",
            "Epoch: 18, Step: 7900, Train Loss: 8.615291861736054, Valid Loss: 8.649549354219296\n",
            "Epoch: 19, Step: 8000, Train Loss: 8.581527682522355, Valid Loss: 8.621859948079893\n",
            "Epoch: 19, Step: 8100, Train Loss: 8.61008037258861, Valid Loss: 8.641314863039574\n",
            "Epoch: 19, Step: 8200, Train Loss: 8.585585532993546, Valid Loss: 8.624563669638533\n",
            "Epoch: 19, Step: 8300, Train Loss: 8.607574925824714, Valid Loss: 8.639304173287801\n",
            "Epoch: 20, Step: 8400, Train Loss: 8.596884506155488, Valid Loss: 8.629490219206808\n",
            "Epoch: 20, Step: 8500, Train Loss: 8.583720056538588, Valid Loss: 8.622278336022937\n",
            "Epoch: 20, Step: 8600, Train Loss: 8.592369498535481, Valid Loss: 8.627535902058238\n",
            "Epoch: 20, Step: 8700, Train Loss: 8.583082129452636, Valid Loss: 8.620467303611733\n",
            "Epoch: 21, Step: 8800, Train Loss: 8.590821509862907, Valid Loss: 8.625793393013602\n",
            "Epoch: 21, Step: 8900, Train Loss: 8.595814655820108, Valid Loss: 8.635968087983468\n",
            "Epoch: 21, Step: 9000, Train Loss: 8.581724294830677, Valid Loss: 8.613800488820777\n",
            "Epoch: 21, Step: 9100, Train Loss: 8.582256027624839, Valid Loss: 8.621853712116032\n",
            "Epoch: 22, Step: 9200, Train Loss: 8.586150122242966, Valid Loss: 8.630447400101277\n",
            "Epoch: 22, Step: 9300, Train Loss: 8.586525117530758, Valid Loss: 8.636074166916424\n",
            "Epoch: 22, Step: 9400, Train Loss: 8.569263913347669, Valid Loss: 8.60936772180227\n",
            "Epoch: 22, Step: 9500, Train Loss: 8.579912966951374, Valid Loss: 8.625446997557829\n",
            "Epoch: 22, Step: 9600, Train Loss: 8.597302877822282, Valid Loss: 8.635446804488426\n",
            "Epoch: 23, Step: 9700, Train Loss: 8.58642963908148, Valid Loss: 8.624567940408214\n",
            "Epoch: 23, Step: 9800, Train Loss: 8.580005844163818, Valid Loss: 8.626040192144037\n",
            "Epoch: 23, Step: 9900, Train Loss: 8.584088467782598, Valid Loss: 8.628962093898263\n",
            "Epoch: 23, Step: 10000, Train Loss: 8.584423557490911, Valid Loss: 8.627723041315386\n",
            "Epoch: 24, Step: 10100, Train Loss: 8.571260905892258, Valid Loss: 8.622513070095591\n",
            "Epoch: 24, Step: 10200, Train Loss: 8.566032033586579, Valid Loss: 8.617370721393781\n",
            "Epoch: 24, Step: 10300, Train Loss: 8.573330583491119, Valid Loss: 8.616722232143495\n",
            "Epoch: 24, Step: 10400, Train Loss: 8.571777208007262, Valid Loss: 8.615875586004659\n",
            "Epoch: 25, Step: 10500, Train Loss: 8.558094903688472, Valid Loss: 8.613251655349686\n",
            "Epoch: 25, Step: 10600, Train Loss: 8.595363722339044, Valid Loss: 8.64748181397759\n",
            "Epoch: 25, Step: 10700, Train Loss: 8.556604501188149, Valid Loss: 8.616368784794902\n",
            "Epoch: 25, Step: 10800, Train Loss: 8.578692359453767, Valid Loss: 8.62806023616051\n",
            "Epoch: 26, Step: 10900, Train Loss: 8.575469472335243, Valid Loss: 8.623485873663656\n",
            "Epoch: 26, Step: 11000, Train Loss: 8.544398009585432, Valid Loss: 8.602014917029058\n",
            "Epoch: 26, Step: 11100, Train Loss: 8.598044019560913, Valid Loss: 8.647727465679123\n",
            "Epoch: 26, Step: 11200, Train Loss: 8.585704194921098, Valid Loss: 8.635341644588282\n",
            "Epoch: 27, Step: 11300, Train Loss: 8.568478779026332, Valid Loss: 8.61827671804521\n",
            "Epoch: 27, Step: 11400, Train Loss: 8.584382335865634, Valid Loss: 8.634656793554537\n",
            "Epoch: 27, Step: 11500, Train Loss: 8.569948976748963, Valid Loss: 8.624043767600897\n",
            "Epoch: 27, Step: 11600, Train Loss: 8.547574418547315, Valid Loss: 8.60460188080173\n",
            "Epoch: 27, Step: 11700, Train Loss: 8.556680955565755, Valid Loss: 8.611849294537068\n",
            "Epoch: 28, Step: 11800, Train Loss: 8.58430122250557, Valid Loss: 8.629912539514757\n",
            "Epoch: 28, Step: 11900, Train Loss: 8.580830449010628, Valid Loss: 8.636915992391422\n",
            "Epoch: 28, Step: 12000, Train Loss: 8.5609488095289, Valid Loss: 8.617875388584904\n",
            "Epoch: 28, Step: 12100, Train Loss: 8.569478719509808, Valid Loss: 8.621270977662288\n",
            "Epoch: 29, Step: 12200, Train Loss: 8.578420103719669, Valid Loss: 8.629438837993856\n",
            "Epoch: 29, Step: 12300, Train Loss: 8.55767199407966, Valid Loss: 8.61661628158258\n",
            "Epoch: 29, Step: 12400, Train Loss: 8.562527371519431, Valid Loss: 8.609944156311695\n",
            "Epoch: 29, Step: 12500, Train Loss: 8.572134568108925, Valid Loss: 8.620254603975678\n",
            "Epoch: 30, Step: 12600, Train Loss: 8.55035565207456, Valid Loss: 8.60775897873734\n",
            "Epoch: 30, Step: 12700, Train Loss: 8.563393966084092, Valid Loss: 8.617012987028966\n",
            "Epoch: 30, Step: 12800, Train Loss: 8.555084118376838, Valid Loss: 8.6105228782043\n",
            "Epoch: 30, Step: 12900, Train Loss: 8.544374098793021, Valid Loss: 8.604315327559224\n",
            "Epoch: 31, Step: 13000, Train Loss: 8.554055186673853, Valid Loss: 8.620425494075176\n",
            "Epoch: 31, Step: 13100, Train Loss: 8.567297902476346, Valid Loss: 8.627637842269621\n",
            "Epoch: 31, Step: 13200, Train Loss: 8.558479741372386, Valid Loss: 8.618353940885356\n",
            "Epoch: 31, Step: 13300, Train Loss: 8.55062998414192, Valid Loss: 8.61610519786032\n",
            "Epoch: 32, Step: 13400, Train Loss: 8.555819077600455, Valid Loss: 8.624266976607109\n",
            "Epoch: 32, Step: 13500, Train Loss: 8.546143619836222, Valid Loss: 8.610959809977729\n",
            "Epoch: 32, Step: 13600, Train Loss: 8.56636654850952, Valid Loss: 8.623400488667006\n",
            "Epoch: 32, Step: 13700, Train Loss: 8.55523821549483, Valid Loss: 8.623391354964589\n",
            "Epoch: 33, Step: 13800, Train Loss: 8.53570503098574, Valid Loss: 8.601232846148951\n",
            "Epoch: 33, Step: 13900, Train Loss: 8.561829065437067, Valid Loss: 8.63155324405908\n",
            "Epoch: 33, Step: 14000, Train Loss: 8.539582207940205, Valid Loss: 8.616025994572775\n",
            "Epoch: 33, Step: 14100, Train Loss: 8.54473978904429, Valid Loss: 8.603897676703214\n",
            "Epoch: 33, Step: 14200, Train Loss: 8.561286999936202, Valid Loss: 8.617061084331056\n",
            "Epoch: 34, Step: 14300, Train Loss: 8.543037636505343, Valid Loss: 8.60916240945205\n",
            "Epoch: 34, Step: 14400, Train Loss: 8.543925450459545, Valid Loss: 8.607073369265407\n",
            "Epoch: 34, Step: 14500, Train Loss: 8.535166910329803, Valid Loss: 8.593582232115482\n",
            "Epoch: 34, Step: 14600, Train Loss: 8.556787624273351, Valid Loss: 8.618598438661527\n",
            "Epoch: 35, Step: 14700, Train Loss: 8.561101835160835, Valid Loss: 8.623341456449477\n",
            "Epoch: 35, Step: 14800, Train Loss: 8.527772465419083, Valid Loss: 8.593404473175529\n",
            "Epoch: 35, Step: 14900, Train Loss: 8.530997280149087, Valid Loss: 8.590853056305825\n",
            "Epoch: 35, Step: 15000, Train Loss: 8.536146509423023, Valid Loss: 8.597805931634106\n",
            "Epoch: 36, Step: 15100, Train Loss: 8.556360640915754, Valid Loss: 8.611492529119577\n",
            "Epoch: 36, Step: 15200, Train Loss: 8.533447134788313, Valid Loss: 8.608714514962882\n",
            "Epoch: 36, Step: 15300, Train Loss: 8.530133980646548, Valid Loss: 8.594490239027005\n",
            "Epoch: 36, Step: 15400, Train Loss: 8.527662722205166, Valid Loss: 8.593609537092087\n",
            "Epoch: 37, Step: 15500, Train Loss: 8.518365283498746, Valid Loss: 8.596279447765342\n",
            "Epoch: 37, Step: 15600, Train Loss: 8.53274214915334, Valid Loss: 8.594631428769356\n",
            "Epoch: 37, Step: 15700, Train Loss: 8.526132820869432, Valid Loss: 8.592336744263152\n",
            "Epoch: 37, Step: 15800, Train Loss: 8.535762957149869, Valid Loss: 8.59431428523151\n",
            "Epoch: 38, Step: 15900, Train Loss: 8.554754493025177, Valid Loss: 8.609436245409478\n",
            "Epoch: 38, Step: 16000, Train Loss: 8.520955754654642, Valid Loss: 8.585554350153478\n",
            "Epoch: 38, Step: 16100, Train Loss: 8.532612473991406, Valid Loss: 8.601852878961937\n",
            "Epoch: 38, Step: 16200, Train Loss: 8.535842752363815, Valid Loss: 8.59512237709295\n",
            "Epoch: 38, Step: 16300, Train Loss: 8.531892083126904, Valid Loss: 8.596899869677497\n",
            "Epoch: 39, Step: 16400, Train Loss: 8.530505181233398, Valid Loss: 8.602148638674073\n",
            "Epoch: 39, Step: 16500, Train Loss: 8.551846818008187, Valid Loss: 8.613790374317741\n",
            "Epoch: 39, Step: 16600, Train Loss: 8.527322038358834, Valid Loss: 8.602539614444488\n",
            "Epoch: 39, Step: 16700, Train Loss: 8.511588494888542, Valid Loss: 8.586789701790748\n",
            "Epoch: 40, Step: 16800, Train Loss: 8.526980261345534, Valid Loss: 8.598191956395407\n",
            "Epoch: 40, Step: 16900, Train Loss: 8.50363214238176, Valid Loss: 8.57654658743789\n",
            "Epoch: 40, Step: 17000, Train Loss: 8.537167109605853, Valid Loss: 8.603097104196847\n",
            "Epoch: 40, Step: 17100, Train Loss: 8.545488256779993, Valid Loss: 8.60438659106379\n",
            "Epoch: 41, Step: 17200, Train Loss: 8.529565813610038, Valid Loss: 8.59977215497648\n",
            "Epoch: 41, Step: 17300, Train Loss: 8.525614176150935, Valid Loss: 8.596562140977104\n",
            "Epoch: 41, Step: 17400, Train Loss: 8.544775378047609, Valid Loss: 8.618423658816814\n",
            "Epoch: 41, Step: 17500, Train Loss: 8.509138944294014, Valid Loss: 8.592567352766553\n",
            "Epoch: 42, Step: 17600, Train Loss: 8.538537647839448, Valid Loss: 8.60791210291019\n",
            "Epoch: 42, Step: 17700, Train Loss: 8.524520520294876, Valid Loss: 8.596075278066646\n",
            "Epoch: 42, Step: 17800, Train Loss: 8.526532075977965, Valid Loss: 8.602786630250652\n",
            "Epoch: 42, Step: 17900, Train Loss: 8.534603963537801, Valid Loss: 8.599513202126145\n",
            "Epoch: 43, Step: 18000, Train Loss: 8.524704515401167, Valid Loss: 8.599038117415772\n",
            "Epoch: 43, Step: 18100, Train Loss: 8.521997849222364, Valid Loss: 8.59475456481617\n",
            "Epoch: 43, Step: 18200, Train Loss: 8.50740314373964, Valid Loss: 8.588267299254298\n",
            "Epoch: 43, Step: 18300, Train Loss: 8.527768040858959, Valid Loss: 8.603235758654622\n",
            "Epoch: 44, Step: 18400, Train Loss: 8.552625335903494, Valid Loss: 8.620730959933313\n",
            "Epoch: 44, Step: 18500, Train Loss: 8.556410024519394, Valid Loss: 8.62924075120778\n",
            "Epoch: 44, Step: 18600, Train Loss: 8.527885501519114, Valid Loss: 8.597115430186076\n",
            "Epoch: 44, Step: 18700, Train Loss: 8.51719828304762, Valid Loss: 8.59014876949426\n",
            "Epoch: 44, Step: 18800, Train Loss: 8.519042458457976, Valid Loss: 8.59378899494369\n",
            "Epoch: 45, Step: 18900, Train Loss: 8.537385287110922, Valid Loss: 8.605083065538311\n",
            "Epoch: 45, Step: 19000, Train Loss: 8.53265789277219, Valid Loss: 8.602521672393973\n",
            "Epoch: 45, Step: 19100, Train Loss: 8.515931326333844, Valid Loss: 8.58906552464035\n",
            "Epoch: 45, Step: 19200, Train Loss: 8.506865548012305, Valid Loss: 8.590691810351753\n",
            "Epoch: 46, Step: 19300, Train Loss: 8.53245500809679, Valid Loss: 8.608230279414713\n",
            "Epoch: 46, Step: 19400, Train Loss: 8.51323414141292, Valid Loss: 8.5893454836382\n",
            "Epoch: 46, Step: 19500, Train Loss: 8.530048869239607, Valid Loss: 8.600962720539522\n",
            "Epoch: 46, Step: 19600, Train Loss: 8.528763165545408, Valid Loss: 8.597933360103413\n",
            "Epoch: 47, Step: 19700, Train Loss: 8.52316680667773, Valid Loss: 8.591955755721239\n",
            "Epoch: 47, Step: 19800, Train Loss: 8.517883511603294, Valid Loss: 8.596725092083116\n",
            "Epoch: 47, Step: 19900, Train Loss: 8.507816922836634, Valid Loss: 8.590069236169116\n",
            "Epoch: 47, Step: 20000, Train Loss: 8.52044168482036, Valid Loss: 8.59971569842444\n",
            "Epoch: 48, Step: 20100, Train Loss: 8.505207224572283, Valid Loss: 8.589667419934198\n",
            "Epoch: 48, Step: 20200, Train Loss: 8.522604689050386, Valid Loss: 8.602137888627455\n",
            "Epoch: 48, Step: 20300, Train Loss: 8.52395196426742, Valid Loss: 8.597881905428114\n",
            "Epoch: 48, Step: 20400, Train Loss: 8.527273370165274, Valid Loss: 8.600620861374113\n",
            "Epoch: 49, Step: 20500, Train Loss: 8.505172955099946, Valid Loss: 8.580342108342307\n",
            "Epoch: 49, Step: 20600, Train Loss: 8.50021823874841, Valid Loss: 8.576815743585847\n",
            "Epoch: 49, Step: 20700, Train Loss: 8.509636801043403, Valid Loss: 8.588501677994989\n",
            "Epoch: 49, Step: 20800, Train Loss: 8.516520436123082, Valid Loss: 8.593678948490885\n",
            "Epoch: 49, Step: 20900, Train Loss: 8.526262149987268, Valid Loss: 8.601324792646995\n",
            "Epoch: 50, Step: 21000, Train Loss: 8.510943067103973, Valid Loss: 8.589454183171435\n",
            "Epoch: 50, Step: 21100, Train Loss: 8.537248601174422, Valid Loss: 8.606464060797613\n",
            "Epoch: 50, Step: 21200, Train Loss: 8.491211456050358, Valid Loss: 8.573893967207198\n",
            "Epoch: 50, Step: 21300, Train Loss: 8.527496075135732, Valid Loss: 8.593440961417228\n",
            "Epoch: 51, Step: 21400, Train Loss: 8.50667103534106, Valid Loss: 8.584271114055259\n",
            "Epoch: 51, Step: 21500, Train Loss: 8.521859826940025, Valid Loss: 8.601285049589793\n",
            "Epoch: 51, Step: 21600, Train Loss: 8.517799059069903, Valid Loss: 8.592880975202911\n",
            "Epoch: 51, Step: 21700, Train Loss: 8.517199152024618, Valid Loss: 8.594714240056259\n",
            "Epoch: 52, Step: 21800, Train Loss: 8.506162842426406, Valid Loss: 8.580410372869387\n",
            "Epoch: 52, Step: 21900, Train Loss: 8.502720655523023, Valid Loss: 8.584486542735739\n",
            "Epoch: 52, Step: 22000, Train Loss: 8.521855023956755, Valid Loss: 8.602655701960872\n",
            "Epoch: 52, Step: 22100, Train Loss: 8.510288613919348, Valid Loss: 8.596989559830202\n",
            "Epoch: 53, Step: 22200, Train Loss: 8.50001644402472, Valid Loss: 8.577330599816579\n",
            "Epoch: 53, Step: 22300, Train Loss: 8.50616555452188, Valid Loss: 8.586579133160278\n",
            "Epoch: 53, Step: 22400, Train Loss: 8.493246949284107, Valid Loss: 8.574625218122637\n",
            "Epoch: 53, Step: 22500, Train Loss: 8.51250806020848, Valid Loss: 8.59600236544175\n",
            "Epoch: 54, Step: 22600, Train Loss: 8.516745341102776, Valid Loss: 8.598823437815724\n",
            "Epoch: 54, Step: 22700, Train Loss: 8.491981109261637, Valid Loss: 8.582922354594874\n",
            "Epoch: 54, Step: 22800, Train Loss: 8.51207424896968, Valid Loss: 8.600841404512938\n",
            "Epoch: 54, Step: 22900, Train Loss: 8.518880009859638, Valid Loss: 8.610439294429712\n",
            "Epoch: 55, Step: 23000, Train Loss: 8.499991815139596, Valid Loss: 8.593727142080363\n",
            "Epoch: 55, Step: 23100, Train Loss: 8.491200401168179, Valid Loss: 8.577318893022609\n",
            "Epoch: 55, Step: 23200, Train Loss: 8.496693690210432, Valid Loss: 8.57697464770116\n",
            "Epoch: 55, Step: 23300, Train Loss: 8.50083873380107, Valid Loss: 8.580495716877172\n",
            "Epoch: 55, Step: 23400, Train Loss: 8.492887903882293, Valid Loss: 8.578571309087618\n",
            "Epoch: 56, Step: 23500, Train Loss: 8.508383913872827, Valid Loss: 8.59424935593512\n",
            "Epoch: 56, Step: 23600, Train Loss: 8.500676110195085, Valid Loss: 8.588932048013229\n",
            "Epoch: 56, Step: 23700, Train Loss: 8.509798970739203, Valid Loss: 8.59890025736388\n",
            "Epoch: 56, Step: 23800, Train Loss: 8.515304787809448, Valid Loss: 8.598306988182442\n",
            "Epoch: 57, Step: 23900, Train Loss: 8.487304980835713, Valid Loss: 8.583966602957283\n",
            "Epoch: 57, Step: 24000, Train Loss: 8.515993972375421, Valid Loss: 8.599108239307245\n",
            "Epoch: 57, Step: 24100, Train Loss: 8.503360717591281, Valid Loss: 8.590375707921305\n",
            "Epoch: 57, Step: 24200, Train Loss: 8.49913211314851, Valid Loss: 8.592682364266238\n",
            "Epoch: 58, Step: 24300, Train Loss: 8.48697428298098, Valid Loss: 8.580543915789594\n",
            "Epoch: 58, Step: 24400, Train Loss: 8.517951804463653, Valid Loss: 8.595631917810984\n",
            "Epoch: 58, Step: 24500, Train Loss: 8.493206830459098, Valid Loss: 8.581299082635264\n",
            "Epoch: 58, Step: 24600, Train Loss: 8.502461940200742, Valid Loss: 8.595079768011136\n",
            "Epoch: 59, Step: 24700, Train Loss: 8.495431735922356, Valid Loss: 8.589184309909902\n",
            "Epoch: 59, Step: 24800, Train Loss: 8.498950271110045, Valid Loss: 8.58771652989276\n",
            "Epoch: 59, Step: 24900, Train Loss: 8.501234692769058, Valid Loss: 8.588939416167408\n",
            "Epoch: 59, Step: 25000, Train Loss: 8.517676824300372, Valid Loss: 8.603943708042923\n",
            "Epoch: 60, Step: 25100, Train Loss: 8.514643910819633, Valid Loss: 8.602406324850866\n",
            "Epoch: 60, Step: 25200, Train Loss: 8.501691613314435, Valid Loss: 8.593385321014063\n",
            "Epoch: 60, Step: 25300, Train Loss: 8.502738422947566, Valid Loss: 8.60152912664913\n",
            "Epoch: 60, Step: 25400, Train Loss: 8.519966980447004, Valid Loss: 8.608185127265644\n",
            "Epoch: 61, Step: 25500, Train Loss: 8.516599980268008, Valid Loss: 8.595329840364979\n",
            "Epoch: 61, Step: 25600, Train Loss: 8.501682768421784, Valid Loss: 8.590321849567683\n",
            "Epoch: 61, Step: 25700, Train Loss: 8.503976026913222, Valid Loss: 8.590904636427666\n",
            "Epoch: 61, Step: 25800, Train Loss: 8.509131594576367, Valid Loss: 8.598288780123461\n",
            "Epoch: 61, Step: 25900, Train Loss: 8.508199220945185, Valid Loss: 8.603332572128028\n",
            "Epoch: 62, Step: 26000, Train Loss: 8.500121254320506, Valid Loss: 8.591119374180593\n",
            "Epoch: 62, Step: 26100, Train Loss: 8.485693350980746, Valid Loss: 8.582305447153491\n",
            "Epoch: 62, Step: 26200, Train Loss: 8.500214273053455, Valid Loss: 8.585370991642092\n",
            "Epoch: 62, Step: 26300, Train Loss: 8.512115542291749, Valid Loss: 8.596545903756741\n",
            "Epoch: 63, Step: 26400, Train Loss: 8.504222762054086, Valid Loss: 8.59648990862924\n",
            "Epoch: 63, Step: 26500, Train Loss: 8.503249549507313, Valid Loss: 8.58893598586946\n",
            "Epoch: 63, Step: 26600, Train Loss: 8.50441091984594, Valid Loss: 8.597852683875473\n",
            "Epoch: 63, Step: 26700, Train Loss: 8.485597544297892, Valid Loss: 8.58478366858258\n",
            "Epoch: 64, Step: 26800, Train Loss: 8.495955822335196, Valid Loss: 8.587301048991954\n",
            "Epoch: 64, Step: 26900, Train Loss: 8.48836522117983, Valid Loss: 8.573794868565228\n",
            "Epoch: 64, Step: 27000, Train Loss: 8.512131344946303, Valid Loss: 8.59697208878215\n",
            "Epoch: 64, Step: 27100, Train Loss: 8.496359838927424, Valid Loss: 8.59118859978731\n",
            "Epoch: 65, Step: 27200, Train Loss: 8.485093793848897, Valid Loss: 8.578721986285414\n",
            "Epoch: 65, Step: 27300, Train Loss: 8.47930746394988, Valid Loss: 8.576326519644883\n",
            "Epoch: 65, Step: 27400, Train Loss: 8.49452392202691, Valid Loss: 8.585708007702554\n",
            "Epoch: 65, Step: 27500, Train Loss: 8.48219483009201, Valid Loss: 8.579951795055209\n",
            "Epoch: 66, Step: 27600, Train Loss: 8.504209533879878, Valid Loss: 8.586490136286375\n",
            "Epoch: 66, Step: 27700, Train Loss: 8.4962626686205, Valid Loss: 8.587370714932195\n",
            "Epoch: 66, Step: 27800, Train Loss: 8.486893828681287, Valid Loss: 8.589584471727367\n",
            "Epoch: 66, Step: 27900, Train Loss: 8.501725761956024, Valid Loss: 8.591414711637862\n",
            "Epoch: 66, Step: 28000, Train Loss: 8.505523713386289, Valid Loss: 8.600468322400452\n",
            "Epoch: 67, Step: 28100, Train Loss: 8.487413113095393, Valid Loss: 8.585579769834418\n",
            "Epoch: 67, Step: 28200, Train Loss: 8.49269076630972, Valid Loss: 8.59615506696071\n",
            "Epoch: 67, Step: 28300, Train Loss: 8.515303986238218, Valid Loss: 8.607024335298348\n",
            "Epoch: 67, Step: 28400, Train Loss: 8.487049621036135, Valid Loss: 8.594136614947587\n",
            "Epoch: 68, Step: 28500, Train Loss: 8.487166460254958, Valid Loss: 8.586781875612106\n",
            "Epoch: 68, Step: 28600, Train Loss: 8.503720197491518, Valid Loss: 8.59913706229526\n",
            "Epoch: 68, Step: 28700, Train Loss: 8.506417433971887, Valid Loss: 8.59663778034598\n",
            "Epoch: 68, Step: 28800, Train Loss: 8.500519070930054, Valid Loss: 8.600215404641151\n",
            "Epoch: 69, Step: 28900, Train Loss: 8.484059818649733, Valid Loss: 8.582716993619526\n",
            "Epoch: 69, Step: 29000, Train Loss: 8.49250072101276, Valid Loss: 8.581764229019258\n",
            "Epoch: 69, Step: 29100, Train Loss: 8.486634127410545, Valid Loss: 8.588990654700714\n",
            "Epoch: 69, Step: 29200, Train Loss: 8.481465371052135, Valid Loss: 8.583710043831635\n",
            "Epoch: 70, Step: 29300, Train Loss: 8.497877971021657, Valid Loss: 8.595637790589976\n",
            "Epoch: 70, Step: 29400, Train Loss: 8.493929786330177, Valid Loss: 8.58985153012025\n",
            "Epoch: 70, Step: 29500, Train Loss: 8.49129468659359, Valid Loss: 8.587413298502453\n",
            "Epoch: 70, Step: 29600, Train Loss: 8.470079441812826, Valid Loss: 8.587934115954557\n",
            "Epoch: 71, Step: 29700, Train Loss: 8.468393465239075, Valid Loss: 8.57396970584816\n",
            "Epoch: 71, Step: 29800, Train Loss: 8.476289557174558, Valid Loss: 8.581869530105937\n",
            "Epoch: 71, Step: 29900, Train Loss: 8.494444264815202, Valid Loss: 8.591396452202892\n",
            "Epoch: 71, Step: 30000, Train Loss: 8.487512104489554, Valid Loss: 8.587978994478444\n",
            "Epoch: 72, Step: 30100, Train Loss: 8.50144997979246, Valid Loss: 8.59136076419801\n",
            "Epoch: 72, Step: 30200, Train Loss: 8.467388071788806, Valid Loss: 8.570816214036649\n",
            "Epoch: 72, Step: 30300, Train Loss: 8.478473524950285, Valid Loss: 8.575890293773892\n",
            "Epoch: 72, Step: 30400, Train Loss: 8.486670802681841, Valid Loss: 8.586569307887908\n",
            "Epoch: 72, Step: 30500, Train Loss: 8.481001629165727, Valid Loss: 8.577367357769052\n",
            "Epoch: 73, Step: 30600, Train Loss: 8.477303016720708, Valid Loss: 8.584236510229049\n",
            "Epoch: 73, Step: 30700, Train Loss: 8.484164746637026, Valid Loss: 8.585025732267102\n",
            "Epoch: 73, Step: 30800, Train Loss: 8.509440146635214, Valid Loss: 8.598404932378376\n",
            "Epoch: 73, Step: 30900, Train Loss: 8.481742426500984, Valid Loss: 8.587514645765564\n",
            "Epoch: 74, Step: 31000, Train Loss: 8.48760634652571, Valid Loss: 8.58715440704822\n",
            "Epoch: 74, Step: 31100, Train Loss: 8.474340739130668, Valid Loss: 8.57820402202304\n",
            "Epoch: 74, Step: 31200, Train Loss: 8.494116363611322, Valid Loss: 8.599307301837754\n",
            "Epoch: 74, Step: 31300, Train Loss: 8.491748049164672, Valid Loss: 8.58523687620923\n",
            "Epoch: 75, Step: 31400, Train Loss: 8.475230959707263, Valid Loss: 8.58141305981966\n",
            "Epoch: 75, Step: 31500, Train Loss: 8.476292621692082, Valid Loss: 8.580235956451503\n",
            "Epoch: 75, Step: 31600, Train Loss: 8.498843108311565, Valid Loss: 8.591252691808194\n",
            "Epoch: 75, Step: 31700, Train Loss: 8.50291588407773, Valid Loss: 8.596218624625283\n",
            "Epoch: 76, Step: 31800, Train Loss: 8.462271619807733, Valid Loss: 8.570133188003085\n",
            "Epoch: 76, Step: 31900, Train Loss: 8.489089852090869, Valid Loss: 8.585128059499725\n",
            "Epoch: 76, Step: 32000, Train Loss: 8.476336707170319, Valid Loss: 8.58686166256817\n",
            "Epoch: 76, Step: 32100, Train Loss: 8.482468526607757, Valid Loss: 8.593089267035584\n",
            "Epoch: 77, Step: 32200, Train Loss: 8.503464904945666, Valid Loss: 8.602651461929845\n",
            "Epoch: 77, Step: 32300, Train Loss: 8.49152714683385, Valid Loss: 8.594752283582144\n",
            "Epoch: 77, Step: 32400, Train Loss: 8.472663502246366, Valid Loss: 8.571547078887727\n",
            "Epoch: 77, Step: 32500, Train Loss: 8.48992479196296, Valid Loss: 8.587944003423926\n",
            "Epoch: 77, Step: 32600, Train Loss: 8.481211336035791, Valid Loss: 8.586463618051432\n",
            "Epoch: 78, Step: 32700, Train Loss: 8.478211597354209, Valid Loss: 8.579148629618212\n",
            "Epoch: 78, Step: 32800, Train Loss: 8.487200448706732, Valid Loss: 8.583143762249787\n",
            "Epoch: 78, Step: 32900, Train Loss: 8.494141798500417, Valid Loss: 8.587158105377346\n",
            "Epoch: 78, Step: 33000, Train Loss: 8.480023300221845, Valid Loss: 8.586167676154286\n",
            "Epoch: 79, Step: 33100, Train Loss: 8.460460405581951, Valid Loss: 8.568255679421048\n",
            "Epoch: 79, Step: 33200, Train Loss: 8.489492845958859, Valid Loss: 8.58655530329902\n",
            "Epoch: 79, Step: 33300, Train Loss: 8.487195923041636, Valid Loss: 8.589006940531236\n",
            "Epoch: 79, Step: 33400, Train Loss: 8.481752906085996, Valid Loss: 8.581714989090836\n",
            "Epoch: 80, Step: 33500, Train Loss: 8.483383512023101, Valid Loss: 8.59081664777521\n",
            "Epoch: 80, Step: 33600, Train Loss: 8.490859625353362, Valid Loss: 8.58808849110911\n",
            "Epoch: 80, Step: 33700, Train Loss: 8.472478800362977, Valid Loss: 8.57390487302323\n",
            "Epoch: 80, Step: 33800, Train Loss: 8.486830106777456, Valid Loss: 8.589761049530782\n",
            "Epoch: 81, Step: 33900, Train Loss: 8.479876329484588, Valid Loss: 8.581330748433412\n",
            "Epoch: 81, Step: 34000, Train Loss: 8.470534250649049, Valid Loss: 8.5822391583489\n",
            "Epoch: 81, Step: 34100, Train Loss: 8.498403284377078, Valid Loss: 8.595524871806676\n",
            "Epoch: 81, Step: 34200, Train Loss: 8.456613938807282, Valid Loss: 8.581438651284756\n",
            "Epoch: 82, Step: 34300, Train Loss: 8.48333281955898, Valid Loss: 8.594557991770058\n",
            "Epoch: 82, Step: 34400, Train Loss: 8.479682652235136, Valid Loss: 8.58923202608306\n",
            "Epoch: 82, Step: 34500, Train Loss: 8.477181180966364, Valid Loss: 8.59067037472761\n",
            "Epoch: 82, Step: 34600, Train Loss: 8.484528085672208, Valid Loss: 8.594227706192767\n",
            "Epoch: 83, Step: 34700, Train Loss: 8.473052068803756, Valid Loss: 8.58490585914873\n",
            "Epoch: 83, Step: 34800, Train Loss: 8.473651126781892, Valid Loss: 8.583722320924286\n",
            "Epoch: 83, Step: 34900, Train Loss: 8.458650057785132, Valid Loss: 8.574388085000088\n",
            "Epoch: 83, Step: 35000, Train Loss: 8.486497044819004, Valid Loss: 8.584482146536425\n",
            "Epoch: 83, Step: 35100, Train Loss: 8.478729169536063, Valid Loss: 8.581820977901053\n",
            "Epoch: 84, Step: 35200, Train Loss: 8.48256040571724, Valid Loss: 8.589761044627323\n",
            "Epoch: 84, Step: 35300, Train Loss: 8.477890479071974, Valid Loss: 8.58708568601547\n",
            "Epoch: 84, Step: 35400, Train Loss: 8.471281606258577, Valid Loss: 8.580512750233913\n",
            "Epoch: 84, Step: 35500, Train Loss: 8.463554097711155, Valid Loss: 8.575709712366178\n",
            "Epoch: 85, Step: 35600, Train Loss: 8.450750537631059, Valid Loss: 8.567425678573429\n",
            "Epoch: 85, Step: 35700, Train Loss: 8.474267540432194, Valid Loss: 8.590934913650695\n",
            "Epoch: 85, Step: 35800, Train Loss: 8.481425821942075, Valid Loss: 8.587018109756094\n",
            "Epoch: 85, Step: 35900, Train Loss: 8.473028909163826, Valid Loss: 8.577622024597813\n",
            "Epoch: 86, Step: 36000, Train Loss: 8.474713275765174, Valid Loss: 8.591538254117111\n",
            "Epoch: 86, Step: 36100, Train Loss: 8.481361809273258, Valid Loss: 8.593884341583772\n",
            "Epoch: 86, Step: 36200, Train Loss: 8.483946722949309, Valid Loss: 8.59119048240225\n",
            "Epoch: 86, Step: 36300, Train Loss: 8.49080954743506, Valid Loss: 8.597376765190228\n",
            "Epoch: 87, Step: 36400, Train Loss: 8.485457296062236, Valid Loss: 8.602390632353833\n",
            "Epoch: 87, Step: 36500, Train Loss: 8.49443930914738, Valid Loss: 8.598639911603733\n",
            "Epoch: 87, Step: 36600, Train Loss: 8.482092150620698, Valid Loss: 8.591751929388012\n",
            "Epoch: 87, Step: 36700, Train Loss: 8.493881950787669, Valid Loss: 8.592947316708154\n",
            "Epoch: 88, Step: 36800, Train Loss: 8.479875645150697, Valid Loss: 8.592292180170125\n",
            "Epoch: 88, Step: 36900, Train Loss: 8.463694945828898, Valid Loss: 8.586408310148427\n",
            "Epoch: 88, Step: 37000, Train Loss: 8.476009940202134, Valid Loss: 8.584423901589489\n",
            "Epoch: 88, Step: 37100, Train Loss: 8.466882737719049, Valid Loss: 8.577654732694887\n",
            "Epoch: 88, Step: 37200, Train Loss: 8.477931298010214, Valid Loss: 8.587860523593948\n",
            "Epoch: 89, Step: 37300, Train Loss: 8.476681211259496, Valid Loss: 8.590656393984647\n",
            "Epoch: 89, Step: 37400, Train Loss: 8.466384291301937, Valid Loss: 8.580904007437939\n",
            "Epoch: 89, Step: 37500, Train Loss: 8.461639395765237, Valid Loss: 8.577494925375921\n",
            "Epoch: 89, Step: 37600, Train Loss: 8.478715964786861, Valid Loss: 8.595292536891012\n",
            "Epoch: 90, Step: 37700, Train Loss: 8.491179988895022, Valid Loss: 8.603111688943434\n",
            "Epoch: 90, Step: 37800, Train Loss: 8.47662526170498, Valid Loss: 8.589570047895885\n",
            "Epoch: 90, Step: 37900, Train Loss: 8.487299227535505, Valid Loss: 8.596680898604037\n",
            "Epoch: 90, Step: 38000, Train Loss: 8.491291974157525, Valid Loss: 8.59827186525769\n",
            "Epoch: 91, Step: 38100, Train Loss: 8.452941091966318, Valid Loss: 8.569222664082215\n",
            "Epoch: 91, Step: 38200, Train Loss: 8.469180015585026, Valid Loss: 8.590935055831556\n",
            "Epoch: 91, Step: 38300, Train Loss: 8.468806933272347, Valid Loss: 8.584145310508276\n",
            "Epoch: 91, Step: 38400, Train Loss: 8.47190514926097, Valid Loss: 8.584546977981908\n",
            "Epoch: 92, Step: 38500, Train Loss: 8.455539361746277, Valid Loss: 8.58218061843541\n",
            "Epoch: 92, Step: 38600, Train Loss: 8.468993010510447, Valid Loss: 8.587294223872258\n",
            "Epoch: 92, Step: 38700, Train Loss: 8.474977025236937, Valid Loss: 8.59521881641984\n",
            "Epoch: 92, Step: 38800, Train Loss: 8.50140355919809, Valid Loss: 8.609126271603765\n",
            "Epoch: 93, Step: 38900, Train Loss: 8.477657590535062, Valid Loss: 8.589455463019704\n",
            "Epoch: 93, Step: 39000, Train Loss: 8.457231839644981, Valid Loss: 8.575094589690098\n",
            "Epoch: 93, Step: 39100, Train Loss: 8.474698061583155, Valid Loss: 8.590955301872004\n",
            "Epoch: 93, Step: 39200, Train Loss: 8.46780675857897, Valid Loss: 8.585522476715647\n",
            "Epoch: 94, Step: 39300, Train Loss: 8.466307432212457, Valid Loss: 8.587358997301353\n",
            "Epoch: 94, Step: 39400, Train Loss: 8.45210968009441, Valid Loss: 8.583931541572207\n",
            "Epoch: 94, Step: 39500, Train Loss: 8.46356195452391, Valid Loss: 8.57764053923256\n",
            "Epoch: 94, Step: 39600, Train Loss: 8.477438206259356, Valid Loss: 8.58354769701226\n",
            "Epoch: 94, Step: 39700, Train Loss: 8.476891816412513, Valid Loss: 8.5927732528004\n",
            "Epoch: 95, Step: 39800, Train Loss: 8.449389447070708, Valid Loss: 8.577933804636848\n",
            "Epoch: 95, Step: 39900, Train Loss: 8.47903693093213, Valid Loss: 8.603971924941495\n",
            "Epoch: 95, Step: 40000, Train Loss: 8.462487119134634, Valid Loss: 8.576425208425281\n",
            "Epoch: 95, Step: 40100, Train Loss: 8.46566030374359, Valid Loss: 8.582483251938976\n",
            "Epoch: 96, Step: 40200, Train Loss: 8.477030128930183, Valid Loss: 8.605434409241003\n",
            "Epoch: 96, Step: 40300, Train Loss: 8.449722847377265, Valid Loss: 8.576630776528798\n",
            "Epoch: 96, Step: 40400, Train Loss: 8.486872702153361, Valid Loss: 8.601493397440104\n",
            "Epoch: 96, Step: 40500, Train Loss: 8.469901385519881, Valid Loss: 8.591393890633196\n",
            "Epoch: 97, Step: 40600, Train Loss: 8.470461804428707, Valid Loss: 8.592487486440245\n",
            "Epoch: 97, Step: 40700, Train Loss: 8.464320699774202, Valid Loss: 8.590799127401857\n",
            "Epoch: 97, Step: 40800, Train Loss: 8.469126698988218, Valid Loss: 8.593502828593685\n",
            "Epoch: 97, Step: 40900, Train Loss: 8.486913108711667, Valid Loss: 8.595675577673983\n",
            "Epoch: 98, Step: 41000, Train Loss: 8.474120190980242, Valid Loss: 8.591758241125481\n",
            "Epoch: 98, Step: 41100, Train Loss: 8.497550570207222, Valid Loss: 8.61065369420799\n",
            "Epoch: 98, Step: 41200, Train Loss: 8.470673922214111, Valid Loss: 8.591123198272573\n",
            "Epoch: 98, Step: 41300, Train Loss: 8.500059462504426, Valid Loss: 8.615114644861075\n",
            "Epoch: 99, Step: 41400, Train Loss: 8.44917276540392, Valid Loss: 8.579645326509404\n",
            "Epoch: 99, Step: 41500, Train Loss: 8.474097754956832, Valid Loss: 8.595582862676782\n",
            "Epoch: 99, Step: 41600, Train Loss: 8.450805403348504, Valid Loss: 8.583568581212257\n",
            "Epoch: 99, Step: 41700, Train Loss: 8.44880926134296, Valid Loss: 8.579294883090915\n",
            "Epoch: 99, Step: 41800, Train Loss: 8.468009055868999, Valid Loss: 8.586982303001138\n",
            "Test RMSE Loss for h = 360: 8.763776141523703\n"
          ]
        }
      ],
      "source": [
        "q6_h_list = [20, 45, 180, 360]\n",
        "test_rmse_list = []\n",
        "for h in q6_h_list:\n",
        "    print(f\"Start training for h = {h}\")\n",
        "    mlp = MyMLP(\n",
        "        X_subtrain=X_subtrain,\n",
        "        Y_subtrain=Y_subtrain,\n",
        "        X_valid=X_valid,\n",
        "        Y_valid=Y_valid,\n",
        "        H=90,\n",
        "        lr=0.001,\n",
        "        wd=0,\n",
        "        mom=0,\n",
        "        loss_type=0,\n",
        "        optimizer_type=1,\n",
        "        use_dropout=True,\n",
        "        dropout_rate=0.5\n",
        "    )\n",
        "    mlp.fit(\n",
        "        max_epoch=100,\n",
        "        verbose=True,\n",
        "        patience_batch_num=5000,\n",
        "        model_path=f'q6_mlp_h_{h}.ckpt'\n",
        "    )\n",
        "    best_model = torch.load(f'q6_mlp_h_{h}.ckpt')\n",
        "    test_rmse_loss = calculate_test_rmse(best_model, X_test, Y_test)\n",
        "    test_rmse_list.append(test_rmse_loss)\n",
        "    print(f\"Test RMSE Loss for h = {h}: {test_rmse_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83EFf--5Ps3v",
        "outputId": "a3637e7d-0fde-4e4d-9fec-0695543eb4c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test RMSE Loss for h = 20: 8.779735188766702\n",
            "Test RMSE Loss for h = 45: 8.761881446680928\n",
            "Test RMSE Loss for h = 180: 8.769820182275133\n",
            "Test RMSE Loss for h = 360: 8.763776141523703\n"
          ]
        }
      ],
      "source": [
        "# print rmse according to different h\n",
        "for h, rmse in zip(q6_h_list, test_rmse_list):\n",
        "    print(f\"Test RMSE Loss for h = {h}: {rmse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiSF6tyUPs3v"
      },
      "source": [
        "從上面的結果並沒有看到在 H 不同的情況下，Test RMSE 有特別的走向，只能看出 45 個 Hidden Nodes 的 Test RMSE 最小，而 20 個 Hidden Nodes 的 Test RMSE 最大，但四個的差距都不大。  \n",
        "而如果要預測的話，可能使用 45 個 Hidden Nodes 的模型比較好，因為其 Test RMSE 最小。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U7nS1n3Ps3w"
      },
      "source": [
        "\n",
        "#### Q7 L2 + L1 Loss (15%)\n",
        "我們前面的小題皆是使用SSE，也就是L2 Loss。一個改善模型訓練的方式是使用多種類似的Loss，以線性組合的方式建構Loss Function。請使用Q5中的MLP with Dropout模型 (H = 90)，並以L2 + L1 Loss訓練模型。這個Loss的定義如下:\n",
        "\n",
        "$$\n",
        "loss(\\mathbf{y}, \\hat{\\mathbf{y}}) = z \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + (1 - z) \\sum_{i = 1}^n | y_i - \\hat{y}_i |,\n",
        "$$\n",
        "其中z為實數且$0 <=z <= 1$。\n",
        "\n",
        "使用z = 0.5。並以Adam訓練模型。畫出Training and Validation RMSE，並報告Test RMSE。注意這裡繪圖時應使用RMSE而不是這個特殊的Loss。\n",
        "\n",
        "另外，使用z = 0.0, 0.1, 0.9, 1.0訓練模型(不須提供訓練過程的Loss圖形)，統整各個z值下的Test RMSE並討論。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "q7_mlp = MyMLP(\n",
        "        X_subtrain=X_subtrain,\n",
        "        Y_subtrain=Y_subtrain,\n",
        "        X_valid=X_valid,\n",
        "        Y_valid=Y_valid,\n",
        "        H=90,\n",
        "        lr=0.001,\n",
        "        wd=0,\n",
        "        mom=0,\n",
        "        loss_type=1,\n",
        "        optimizer_type=1,\n",
        "        use_dropout=True,\n",
        "        dropout_rate=0.5,\n",
        "        loss_z=0.5\n",
        "    )\n",
        "train_loss_list, valid_loss_list = q7_mlp.fit(\n",
        "    max_epoch=100,\n",
        "    verbose=True,\n",
        "    patience_batch_num=5000,\n",
        "    model_path='q7_mlp.ckpt'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "o6O_MHLbAQeh",
        "outputId": "39bddcd2-7e5d-476f-a3df-db64d508b71e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAHgCAYAAADt8bqrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXzU1b3/8deZyUYSshAI+47sBFS0KALudbcuuFRrta3e9nq1tr2tdrN2sdVfvbderdalbtVqa13rCm6AGygg+76TBMhC9j0z5/fHmcmEMAkJ5MuE+H4+HjwmmfnOd84M0bz5nM85X2OtRUREREQOL1+sByAiIiLyZaQQJiIiIhIDCmEiIiIiMaAQJiIiIhIDCmEiIiIiMaAQJiIiIhIDcbEeQEf17t3bDhs2LNbDEBERETmgJUuWFFlr+0R77IgLYcOGDWPx4sWxHoaIiIjIARljtrf2mKYjRURERGJAIUxEREQkBhTCRERERGLgiOsJExERkc7V0NBAbm4utbW1sR7KESspKYlBgwYRHx/f7ucohImIiHzJ5ebm0rNnT4YNG4YxJtbDOeJYaykuLiY3N5fhw4e3+3majhQREfmSq62tJSsrSwHsIBljyMrK6nAlUSFMREREFMAO0cF8fgphIiIiElOlpaU8+OCDB/Xcc845h9LS0nYff8cdd3DPPfcc1Gt1NoUwERERiam2QlhjY2Obz33zzTfJyMjwYlieUwgTERGRmLrtttvYvHkzU6ZM4cc//jHz5s1jxowZXHDBBYwfPx6Ar33taxx77LFMmDCBRx55pOm5w4YNo6ioiG3btjFu3Diuv/56JkyYwJlnnklNTU2br7ts2TKmTZtGTk4OF110ESUlJQDcd999jB8/npycHK644goA5s+fz5QpU5gyZQpHH300FRUVh/y+tTpSREREmvz6tdWsyS/v1HOOH5DGr86f0Orjd911F6tWrWLZsmUAzJs3j6VLl7Jq1aqm1YaPP/44vXr1oqamhuOOO45LLrmErKysfc6zceNGnnvuOR599FEuu+wyXnzxRa6++upWX/eaa67h/vvvZ9asWdx+++38+te/5t577+Wuu+5i69atJCYmNk113nPPPTzwwANMnz6dyspKkpKSDvVjUSVMREREup7jjz9+n+0e7rvvPiZPnsy0adPYuXMnGzdu3O85w4cPZ8qUKQAce+yxbNu2rdXzl5WVUVpayqxZswD45je/yYIFCwDIycnhqquu4plnniEuztWrpk+fzg9/+EPuu+8+SktLm+4/FKqEiYiISJO2KlaHU0pKStPX8+bN49133+XTTz8lOTmZk08+Oep2EImJiU1f+/3+A05HtuaNN95gwYIFvPbaa9x5552sXLmS2267jXPPPZc333yT6dOnM2fOHMaOHXtQ5w9TJUxERERiqmfPnm32WJWVlZGZmUlycjLr1q1j4cKFh/ya6enpZGZm8uGHHwLw9NNPM2vWLILBIDt37uSUU07h7rvvpqysjMrKSjZv3sykSZO49dZbOe6441i3bt0hj0GVMBEREYmprKwspk+fzsSJEzn77LM599xz93n8rLPO4qGHHmLcuHGMGTOGadOmdcrrPvXUU3z3u9+lurqaESNG8MQTTxAIBLj66qspKyvDWsvNN99MRkYGv/zlL/nggw/w+XxMmDCBs88++5Bf31hrO+FtHD5Tp061ixcvjvUwREREuo21a9cybty4WA/jiBftczTGLLHWTo12vKYjWwgELWXVDdQ3BmM9FBEREenGFMJaWLe7nMm/mcsH6wtiPRQRERHpxhTCWvCFrv10pE3TioiIyJFFIayFcAgLaDZSREREPKQQ1oI/9IkEVQkTERERDymEtWBClTCFMBEREfGSQlgLPoUwERGRLi81NRWA/Px8Lr300qjHnHzyyUTb1qq1+w83hbAW/OEQpp4wERGRLm/AgAG88MILsR7GQVEIayGUwVQJExEROUxuu+02Hnjggabv77jjDu655x4qKys57bTTOOaYY5g0aRKvvvrqfs/dtm0bEydOBKCmpoYrrriCcePGcdFFF7Xr2pHPPfcckyZNYuLEidx6660ABAIBrr32WiZOnMikSZP405/+BLiLiI8fP56cnByuuOKKQ37fumxRCz5feIuKGA9EREQkFt66DXav7Nxz9psEZ9/V6sOXX345t9xyCzfeeCMAzz//PHPmzCEpKYmXX36ZtLQ0ioqKmDZtGhdccEFT/3ZLf/nLX0hOTmbt2rWsWLGCY445ps1h5efnc+utt7JkyRIyMzM588wzeeWVVxg8eDB5eXmsWrUKgNLSUgDuuusutm7dSmJiYtN9h0KVsBZCGYyAUpiIiMhhcfTRR1NQUEB+fj7Lly8nMzOTwYMHY63lZz/7GTk5OZx++unk5eWxZ8+eVs+zYMECrr76agBycnLIyclp83U///xzTj75ZPr06UNcXBxXXXUVCxYsYMSIEWzZsoWbbrqJt99+m7S0tKZzXnXVVTzzzDPExR16HUuVsBb8aswXEZEvszYqVl6aPXs2L7zwArt37+byyy8H4O9//zuFhYUsWbKE+Ph4hg0bRm1tredjyczMZPny5cyZM4eHHnqI559/nscff5w33niDBQsW8Nprr3HnnXeycuXKQwpjqoS1ENmiIsYDERER+RK5/PLL+cc//sELL7zA7NmzASgrKyM7O5v4+Hg++OADtm/f3uY5Zs6cybPPPgvAqlWrWLFiRZvHH3/88cyfP5+ioiICgQDPPfccs2bNoqioiGAwyCWXXMLvfvc7li5dSjAYZOfOnZxyyincfffdlJWVUVlZeUjvWZWwFsLTkUGlMBERkcNmwoQJVFRUMHDgQPr37w/AVVddxfnnn8+kSZOYOnUqY8eObfMc3/ve97juuusYN24c48aN49hjj23z+P79+3PXXXdxyimnYK3l3HPP5cILL2T58uVcd911BENbJfzhD38gEAhw9dVXU1ZWhrWWm2++mYyMjEN6z+ZIu0bi1KlTrZd7e5RW1zPlN+/wq/PHc9304Z69joiISFexdu1axo0bF+thHPGifY7GmCXW2qnRjtd0ZAuajhQREZHDQSGsBV9tCRf6PqJH9a5YD0VERES6MYWwFuIq8/i/hAfJKl8d66GIiIhIN6YQ1oLPF1qrcIT1yomIiByKI61HvKs5mM9PIawF07RjfiDGIxERETk8kpKSKC4uVhA7SNZaiouLSUpK6tDztEVFCz7jd1/oCt4iIvIlMWjQIHJzcyksLIz1UI5YSUlJDBo0qEPPUQhrwedTCBMRkS+X+Ph4hg/XtkyHm6YjWzB+F8I0HSkiIiJeUghrwZjQR2JVCRMRERHvKIS1FAphViFMREREPKQQ1lIohBmFMBEREfGQQlhLocZ8G1RPmIiIiHhHIawl9YSJiIjIYaAQ1pJCmIiIiBwGCmEthUOY9gkTERERDymEtaRKmIiIiBwGCmEtKYSJiIjIYaAQ1lJTCNPqSBEREfGOQlhL4WtHqhImIiIiHlIIa6mpEmZjOw4RERHp1hTCWlJPmIiIiBwGCmEtqSdMREREDgNPQ5gx5vvGmFXGmNXGmFvaOO44Y0yjMeZSL8fTLsb1hOnakSIiIuIlz0KYMWYicD1wPDAZOM8YMyrKcX7gbmCuV2PpEGPcrUKYiIiIeMjLStg4YJG1ttpa2wjMBy6OctxNwItAgYdjaT9jCGIUwkRERMRTXoawVcAMY0yWMSYZOAcY3PwAY8xA4CLgLx6Oo8MsRtORIiIi4qk4r05srV1rjAlPM1YBy4CW3e73Ardaa4MmPA0YhTHmBuAGgCFDhngz4GYC+FQJExEREU952phvrX3MWnustXYmUAJsaHHIVOAfxphtwKXAg8aYr0U5zyPW2qnW2ql9+vTxcsju9RTCRERExGOeVcIAjDHZ1toCY8wQXD/YtOaPW2uHNzv2SeB1a+0rXo6pPTQdKSIiIl7zNIQBLxpjsoAG4EZrbakx5rsA1tqHPH7tgxbEh0EhTERERLzjaQiz1s6Icl/U8GWtvdbLsXRE0Gg6UkRERLylHfOjsBhdO1JEREQ8pRAWhcWHb7+FnCIiIiKdRyEsCk1HioiIiNcUwqKw2jFfREREPKYQFoXFh1FPmIiIiHhIISwKawzGqidMREREvKMQFkUQP6BKmIiIiHhHISwK7ZgvIiIiXlMIi8Ia7ZgvIiIi3lIIi0KVMBEREfGaQlgU1vi1OlJEREQ8pRAWhcVoOlJEREQ8pRAWhTU+bVEhIiIinlIIi8JVwjQdKSIiIt5RCIvC9YRpOlJERES8oxAWhSphIiIi4jWFsChcT5gqYSIiIuIdhbAotFmriIiIeE0hLAqLKmEiIiLiLYWwaIxPPWEiIiLiKYWwKKwx+DQdKSIiIh5SCItCW1SIiIiI1xTCotJ0pIiIiHhLISwKTUeKiIiI1xTCotEWFSIiIuIxhbAoLH58VtORIiIi4h2FsGiMUSVMREREPKUQFoU1PvWEiYiIiKcUwqLRZq0iIiLiMYWwKKzx4dM+YSIiIuIhhbBoVAkTERERjymERRHuCbNaISkiIiIeUQiLwjSFsFiPRERERLorhbAoXCXMElQKExEREY8ohEUTCmEBhTARERHxiEJYNMaHz2g6UkRERLyjEBaFNX5NR4qIiIinFMKiCTXmB4IKYSIiIuINhbBomhrzYz0QERER6a4UwqIxRvuEiYiIiKcUwqIJ9YRpOlJERES8ohAWTagnTBlMREREvKIQFo3PXTtS05EiIiLiFYWwKIzx4SeozVpFRETEMwphUUT2CYv1SERERKS7UgiLwoS3qFAKExEREY8ohEUTaszXbKSIiIh4RSEsGp/bJ0w9YSIiIuIVhbBojB+/0bUjRURExDsKYVEY4wfABoMxHomIiIh0Vwph0Rj3sQQCgRgPRERERLorhbAojM99LMGgQpiIiIh4QyEsGl94OlIhTERERLyhEBaNCVfC1JgvIiIi3lAIiyI8HWkDjTEeiYiIiHRXCmHRhFZHBq2mI0VERMQbCmFRmNB0pLaoEBEREa8ohEURWR2pECYiIiLeUAiLoqkSpp4wERER8YhCWDS+cE+YKmEiIiLiDYWwaEKVMDQdKSIiIh5RCIvC19QTpulIERER8YZCWDThC3hbbdYqIiIi3lAIiyKyWav2CRMRERFvKIRFYcLXjlRjvoiIiHhEISyKpkqYLuAtIiIiHvE0hBljvm+MWWWMWW2MuSXK41cZY1YYY1YaYz4xxkz2cjztFr5skaYjRURExCOehTBjzETgeuB4YDJwnjFmVIvDtgKzrLWTgN8Cj3g1no4Ir45E05EiIiLiES8rYeOARdbaamttIzAfuLj5AdbaT6y1JaFvFwKDPBxPu+myRSIiIuI1L0PYKmCGMSbLGJMMnAMMbuP4bwNveTiedmu6bJHVdKSIiIh4I86rE1tr1xpj7gbmAlXAMiBqqjHGnIILYSe18vgNwA0AQ4YM8WS8+7yeL/SxqDFfREREPOJpY7619jFr7bHW2plACbCh5THGmBzgr8CF1triVs7ziLV2qrV2ap8+fbwcshtTeHWkesJERETEI55VwgCMMdnW2gJjzBBcP9i0Fo8PAV4CvmGt3S+gxYoJr45UT5iIiIh4xNMQBrxojMkCGoAbrbWlxpjvAlhrHwJuB7KAB40xAI3W2qkej+mAjM8A2idMREREvONpCLPWzohy30PNvv4O8B0vx3AwfP5wT5gqYSIiIuIN7ZgfRdPqSIUwERER8YhCWBSRxnxNR4qIiIg3FMKi8IUu4K0tKkRERMQrCmFRGL8LYdbaGI9EREREuiuFsCgiPWGqhImIiIg3FMKi8IUqYagnTERERDyiEBaFaeoJ0+pIERER8YZCWBS+0OrIoC5bJCIiIh5RCIsifNkiFMJERETEIwphUTRtUaGeMBEREfGIQlgUTZu1qidMREREPKIQFoXfr+lIERER8ZZCWBTh1ZGqhImIiIhXFMKiCW3WqssWiYiIiFcUwqIJhzBNR4qIiIhHFMKiCYWwoCphIiIi4hGFsGhCIay+oTHGAxEREZHuSiEsmlAIq1MIExEREY8ohEUTWh2pSpiIiIh4RSEsmvB0ZKNCmIiIiHhDISyaUAhraGiI8UBERESku1IIi8aEpyO1OlJERES8oRAWjTEANGg6UkRERDyiEBZNs54wa22MByMiIiLdkUJYNKEQZmyQ2gbtmi8iIiKdTyEsmtAWFT6CVNSpOV9EREQ6n0JYNKFKmA9LZa36wkRERKTzKYRF0zyE1SmEiYiISOdTCIumKYQFVQkTERERTyiERWPCPWGWcoUwERER8YBCWDShfcJ8JqjpSBEREfGEQlg0xmAxoelIrY4UERGRzqcQ1hrjU2O+iIiIeEYhrBXG5yfBZ6lQCBMREREPKIS1JiGFdH+9VkeKiIiIJxTCWpOYRqavRtORIiIi4gmFsNYkpZPuq1YlTERERDyhENaapHTSqFZPmIiIiHhCIaw1SemkUkV9YzDWIxEREZFuSCGsNUkZpASrqFMIExEREQ8ohLUmKY3kYCV1jYFYj0RERES6IYWw1iSlk2RraKzXjvkiIiLS+RTCWpOUDkBcY0WMByIiIiLdkUJYa0IhLLGxMsYDERERke5IIaw1CmEiIiLiIYWw1oRCWA9bSTBoYzwYERER6W4UwlqTmAZAGlXUB7RNhYiIiHQuhbDWhCphaaaaugaFMBEREelcCmGtCYcwqrVXmIiIiHQ6hbDWJKZhMa4Spl3zRUREpJMphLXG56MxPpU0qlQJExERkU6nENaGxvg00kw1teoJExERkU6mENaGQEJP9YSJiIiIJxTC2hBM6EkqNVodKSIiIp1OIawtcUkkmno15ouIiEinUwhrg4lLJIFGTUeKiIhIp1MIa4OJTyCRBlXCREREpNMphLXBF9+DBBrUEyYiIiKdTiGsDb64RBKMpiNFRESk8ymEtcEXn+gqYZqOFBERkU6mENYGf0IP9YSJiIiIJxTC2uCLC1XCGjQdKSIiIp1LIawNJi6RBBOgrqEx1kMRERGRbkYhrC1xiQA01tfGeCAiIiLS3SiEtSUUwgINCmEiIiLSuRTC2uJPACDQUBfjgYiIiEh3oxDWlrgkAGxDTYwHIiIiIt2NpyHMGPN9Y8wqY8xqY8wtUR43xpj7jDGbjDErjDHHeDmeDgtNRwYb6mM8EBEREeluPAthxpiJwPXA8cBk4DxjzKgWh50NHBX6cwPwF6/Gc1BC05HBRk1HioiISOfyshI2Dlhkra221jYC84GLWxxzIfA36ywEMowx/T0cU8eEpyMb1ZgvIiIincvLELYKmGGMyTLGJAPnAINbHDMQ2Nns+9zQffswxtxgjFlsjFlcWFjo2YD3E+cqYagSJiIiIp2sXSEs1NuVFurheswYs9QYc2Zbz7HWrgXuBuYCbwPLgIPaet5a+4i1dqq1dmqfPn0O5hQHx+96wmhUT5iIiIh0rvZWwr5lrS0HzgQygW8Adx3oSdbax6y1x1prZwIlwIYWh+Sxb3VsUOi+riEuHMI0HSkiIiKdq70hzIRuzwGettaubnZf608yJjt0OwTXD/Zsi0P+DVwTqrBNA8qstbvaOSbvhUKYCagSJiIiIp0rrp3HLTHGzAWGAz81xvQEgu143ovGmCygAbjRWltqjPkugLX2IeBNXLDbBFQD13X0DXjKrxAmIiIi3mhvCPs2MAXYYq2tNsb0oh2ByVo7I8p9DzX72gI3tnMMh1+4MT+gxnwRERHpXO2djjwBWB+qZF0N/AIo825YXURoiwoTqKeitiHGgxEREZHupL0h7C9AtTFmMvAjYDPwN89G1VWENmtNpJ68Ul26SERERDpPe0NYY2jq8ELgz9baB4Ce3g2riwg15ifQSO5ehTARERHpPO3tCaswxvwUtzXFDGOMD4j3blhdhD8cwhpUCRMREZFO1d5K2OVAHW6/sN24/bz+6Nmougp/HNb4SfY3kltSHevRiIiISDfSrhAWCl5/B9KNMecBtdba7t8TBpi4RLISUSVMREREOlV7L1t0GfAZMBu4DFhkjLnUy4F1GXGJZCRacksUwkRERKTztLcn7OfAcdbaAgBjTB/gXeAFrwbWZfgTyUgIkqcQJiIiIp2ovT1hvnAACynuwHOPbHEJpMUHKa6qp7q+MdajERERkW6ivZWwt40xc4DnQt9fjrvkUPfnT6RnXACArUVVTBiQHuMBiYiISHfQrhBmrf2xMeYSYHrorkestS97N6wuJC6J3klgDMxdvUchTERERDpFeythWGtfBF70cCxdU1wCiTTwleG9eH1FPrecfhTGmFiPSkRERI5wbfZ1GWMqjDHlUf5UGGPKD9cgY8qfCIF6zp88gM2FVazdVRHrEYmIiEg30GYlzFrb/S9NdCBxiVBfxcljsgFYuqOE8QPSYjwoEREROdJ9OVY4Hoq4RGisJb2Hu0pTTX0gxgMSERGR7kAh7ED8CRCop0e8H4AqbVMhIiIinUAh7EDikqCxDr/PkBjnUyVMREREOoVC2IHEuUoYQHKCn2qFMBEREekECmEH4nc9YQDJCXEKYSIiItIpFMIOJC4JGl0lrEeCn5oG9YSJiIjIoVMIO5DkXlBfAbXlpCT4qapTJUxEREQOnULYgfSd6G73rHaVME1HioiISCdQCDuQfuEQtsr1hGk6UkRERDqBQtiBpA2EpAzYvZIeWh0pIiIinUQh7ECMgX6TXCUsXtORIiIi0jkUwtqj70TYs4bUBENVnaYjRURE5NC1eQFvCek3CRprGBTIpaYh1oMRERGR7kCVsPYYPgOA0RWLaAhYGgLBGA9IREREjnQKYe2RMQSyJzCy5CMANeeLiIjIIVMIa6/RX6Vf2RekUaXmfBERETlkCmHtNeZsfDbAdN8qqurVnC8iIiKHRiGsvbJGAZBtSlUJExERkUOmENZe8ckAJFOnnjARERE5ZAph7RWXiDU+kk0t1ZqOFBERkUOkENZexhCMSyaZOk1HioiIyCFTCOsAG59MD2qpUggTERGRQ6QQ1gEmIZlkU0eNpiNFRETkECmEdURCqhrzRUREpFMohHWALyGFHgphIiIi0gkUwjrAJCTT01en1ZEiIiJyyBTCOiIhhRRTp8Z8EREROWQKYR0Rn0yav4F31uyhqk7VMBERETl4CmEdkZBMr4QGCivq+OuHW2M9GhERETmCKYR1RHwK8YFaZo7uw0tf5MZ6NCIiInIEUwjriIQUqK9iQv+e5JXUEAjaWI9IREREjlAKYR2RkAxYhqX7aAxadpXVxHpEIiIicoRSCOuI+BQAhvZ03+7cqxAmIiIiB0chrCMSkgEYlOqmIXeWVMdyNCIiInIEUwjriHgXwvomBfAZ2LlXIUxEREQOjkJYRyS46cj4QC3903sohImIiMhBUwjriFAljPpKBvfqwc4S9YSJiIjIwVEI64hQJYyGagZnJqsSJiIiIgdNIawjwiGsvoohvZIpqKijorYhtmMSERGRI5JCWEeEpyMbqpk6rBcAH28qjuGARERE5EilENYRTZWwaqYOyyQ1MY75GwpiOyYRERE5IimEdURTJayKeL+P6aOymLe+EGt1+SIRERHpGIWwjohLBOOD+ioATh6Tza6yWjbsqYzxwERERORIoxDWEcZAQirUu1WRJ4zIAmDJ9pJYjkpERESOQAphHZWQCpW7ARialUx6j3hW5pXGeFAiIiJypFEI66gxZ8O6N6BiN8YYcgals3xnWaxHJSIiIkcYhbCOOvG/INgICx8EIGdQOuv3VFDbEIjxwERERORIohDWUb1GwOizYPXLAOQMyiAQtKzOL4/xwERERORIohB2MAYeA6U7oK6CnEHpACzZvjfGgxIREZEjiULYwcge724L19M/vQeTB2fw3Gc7CQS1X5iIiIi0j0LYwcge524L1gBww4wRbC2q4p01e2I4KBERETmSKIQdjIxhENcDCtYC8NUJfRmY0YMXl+bGdlwiIiJyxPA0hBljfmCMWW2MWWWMec4Yk9Ti8SHGmA+MMV8YY1YYY87xcjydxueD7LFNlbA4v4+cQelsKaxk2c5Szrv/QyrrGmM8SBEREenKPAthxpiBwM3AVGvtRMAPXNHisF8Az1trjw499qBX4+l02eObKmEAQ7NS2Lm3hvfW7mFVXjlbC6tiODgRERHp6ryejowDehhj4oBkIL/F4xZIC32dHuXxrqvvRKjcA2V5AAzLSqY+EGTBhkIACipqYzk6ERER6eI8C2HW2jzgHmAHsAsos9bObXHYHcDVxphc4E3gJq/G0+lGnOxuN78PuEoYwPJct3v+nvK6GAxKREREjhReTkdmAhcCw4EBQIox5uoWh10JPGmtHQScAzxtjNlvTMaYG4wxi40xiwsLC70acsdkj4OeA2DTuwAM6528z8OqhImIiEhbvJyOPB3Yaq0ttNY2AC8BJ7Y45tvA8wDW2k+BJKB3yxNZax+x1k611k7t06ePh0PuAGNg1Gmw5QMINNK3ZxKJcZGPU5UwERERaYuXIWwHMM0Yk2yMMcBpwNoox5wGYIwZhwthXaTU1Q6jTofaMshbgs9nGJrlqmEJfh+FqoSJiIhIG7zsCVsEvAAsBVaGXusRY8xvjDEXhA77EXC9MWY58BxwrbX2yNl2fvhMwMDWBYDrCzMGjh6SQUGFKmEiIiLSujgvT26t/RXwqxZ3397s8TXAdC/H4KnkXm6V5LYFMOvHnDupPxk94gFYsLGQ5TtLGZjZg96piTEeqIiIiHQ12jH/UA07CXZ+Bo11fO3ogfxx9mT6piVRVFnP7Ic/5f73NsZ6hCIiItIFKYQdquEzoLEWProXijbBto/53hfnkxKspL4xyKbCyliPUERERLogT6cjvxSGngj+BJj3e8j9DAZPI6V2D8PNLpbbUWwrqo71CEVERKQLUiXsUPXIhP/4EEaeBsWboHQbAH1NCQl+H/llNdQ2BGI7RhEREelyFMI6Q/ZYGHgMlO5wU5LASf0a+c6M4VgLO/eqGiYiIiL7UgjrLL1GgA1C3hIArpmQyBnj+wKwrVghTERERPalENZZMoe722CDu63YzfDe7nqS26QAc9gAACAASURBVIqqYjQoERER6aoUwjpLrxH7fl+xi4zkBDKS49larBAmIiIi+1II6yyp2RDvKl8kpUPFbsDtor9uV3kMByYiIiJdkUJYZzEGeoWmJAdPg/I8eOxMfpT5EUt3lLJgQ+SSmD99aSVPfbItNuMUERGRLkEhrDP1Gg6+eLdSsrYUdi5iun81Q7OS+fVrq6ltCBAIWl5cmssry/JiPVoRERGJIYWwzjT563DCf0LawKa7/GU7+M2FE9lcWMXv3lhDbkk19Y1B1u2qIBA8cq5VLiIiIp1LO+Z3prHnuD8b343cV7KNWaP7cMPMETyyYAvZPZMAqGkIsLWoilHZqTEarIiIiMSSKmFeyBgcuh0CNSVQW871M9zqyacXbm86bHV+WSxGJyIiIl2AQpgX+oyBr/8LTr3dfV+6nT49EzkqO5XCijp6pSSQ4PexRqsmRUREvrQUwrwy+kzoPcp9XbINgGkjsgAY07cno/ulsiZfIUxEROTLSiHMSxlD3W2Jm4I8YaQLYecnLOHMtFzW7qqI1chEREQkxhTCvNQjExLToNSFsGkjskhJ8HPxrv/hotInKaqso6y6IcaDFBERkVhQCPOSMZA5FPZuAaBXSgIL/3saSXXF9K1eD1hW7yrjnjnrKamqj+1YRURE5LBSCPPa0Omw+QPIWwpAz5p8ABLqSujPXh5dsIU/f7CJRz/cEstRioiIyGGmEOa1k3/qriv5z2/Au3dA8camh6bEbWde6HJGz362g5r6QIwGKSIiIoebQpjXemTApU9Aah/46E+w5Mmmh05K2Ym10C8tidLqBl7VpYxERES+NBTCDoehJ8C1b4IvDrbMg/hk6DOWHL9r2P/micMY0TuFt1btju04RURE5LBRCDtcEpKhXw7YoNu6ov9khjW6PrATRmZx6thsPt1cTFVdY4wHKiIiIoeDQtjhNPgr7jZzKGSPo2d9AT89Po7Jb13MOQOrqA8E+WhTUWzHKCIiIoeFQtjhNCQUwjKGQp+xAPxH4juY/CVMLp9Hz8Q4HvtwK5sKKmM4SBERETkcFMIOp8HTwPjdtSX7jHH3rfwXAP7tH3HLGaNZkVfKlY8uxFobw4GKiIiI1+JiPYAvlbT+8L1PoNcI8PkhrgfUlrrHdizi218fSLzfcPurqymoqKNvWlJsxysiIiKeUSXscMseC3EJLoT1PsrdlzkcGmsgfykjeqcCsLlQU5IiIiLdmUJYLIX6wjjhRsDA+rcY0ScFH0G2FFbFdGgiIiLiLYWwWMoe525HngrjL4DP/0r/ud/jncSfRA1hlXWN/Pn9jdpZX0REpBtQCIulY6+F2U9C1kg45efQUI1Z8zIjTT579uTvd/hTn2zjnrkbePmL1nfW31RQwcY9Fd6NWURERDqFQlgsJfeCCRe5r/uMgTN/B2PPAyBQuGmfQxsDQV75dA23xL3AnC82t3rK219YzB0vLfFsyCIiItI5FMK6khNuhNN/DUBK5VZ+/vJKckuqAXhnzR6+Uf03bol7icydc9lTXhv1FN8q+AP/UfSHwzZkEREROTgKYV1N5lCCJo5hZjd/X7SD5z7bAcBniz7kqrj3AJhuVvF2lOtM7q2qZ0xwC4MadxzWIYuIiEjHKYR1Nf54TK9hXDemgalDM5m/oZCK2gYG7XgFa+Jg+Cxmxq9h4eYi+PRBeP6apqduLSilvymmN6XUNqh5X0REpCtTCOuCTNYoUiu3c8rYbFbllfPPz3eSw0Zqe0+E8RfQ1xaxe9sa9i54iMCa17ANbmpy986txJkgaaaa4rKyGL8LERERaYtCWFeUNQqKNvLNjTcz1uzgT3NWk+PbRsqIr8DwkwG4sO41etVsx0+QrRtXAVC+K9LMX17Y+gpKERERiT2FsK4oayQE6kjN/5hrUhdxUloBidRjBk2FrJHU9P8K18bNbTp81Qq3GrK+eFvTfdXF+29xISIiIl2HQlhXNPFSOP0O6DuJy/vm8+CsUH/XoOPAGJLO+R0ABSYLgMKtKwHwl0Ua8mtLdx3OEYuIiEgH6QLeXVFSGpz0A6guxr/oYUjtAyl9IGMIAGbw8WydeDMmcyhVn91FevV2du6tJqUmnwZ/AvG2nkD5nhi/CREREWmLKmFd2ZATIFAP6153m7oa0/TQ8Et/y7DTvkNj5lGMNPm8v66AARRQlj6OoDVQGQlh1tpYjF5ERETaoBDWlQ3+irtNSIWZP4l6SGK/MYw0+by5Ip9BphCTNZIy05O46kJoqME+fhYr75zBi/988vCNW0RERA5IIawrS+kNk7/uLmeU2ifqIUn9x5FmqinZsZL+7CVl4FjK/L1IqCuGPWswOz4lp3ElozY/ue8TC9ZBfbX370FERESiUgjr6i76C0y9rvXHB04F4Du+N/AZS9KwaVTFZ5FSXwQFawD4KDCB4fUbsMFQg3/1Xnh4Bnz8f16PXkRERFqhEHak6z+ZOl8PLvJ/RBAfDDyW2qTepAdKqMtfTY1N4IO4GaRRRcWip+Ff18KGORCop3bzR+1/nUAjBLULv4iISGdRCDvS+eMozDyGeBNgT49RkJhKY3Jfetu91G7/nE12AP0mzAQg5d2fwOqXaZzzSwBM/hK25ubz8eeLD/w6f7sQ5v7Sy3ciIiLypaIQ1g0EBp8IQEWfowGwI08lwTSSXriYDXYQ044/gUqbhD9QRxBDXE0hFbYHicEa4v52HmNev5iGxmZVruX/hM8f2/dF9qyEgtWH6y2JiIh0ewph3UD/Y85ytzmnAzAg53T22AwAcuOGMXFwJmsYSa2N556G2QA8EfgqAIPrN9PblLF1y/rICT+5Dxb+JfJ9Qw3UlkFVcfsG9MgpMO/uQ3xXIiIi3ZtCWDeQMGQq3DCfnsdcCsDg3qnMNdMBqMscjTGGF3p9m5sabmLjqOu4teF6CqbcTIlNbTrH7g3u0kc01mML12Mrml32qLLA3VYVHngwgUbYtQzyv+iU9yYiItJdacf87mLAlKYvjTEs7XcpffN2ExwyDYBxU09ldX45f7w0h82FOfRNS+S+ZRcSbyw/8T9LbZ679BFFGzDBBqhvgNpyt3t/KITZ6mJMeOPXQAPcfwxM+x6ccGNkHBW7wAahXBcQFxERaYsqYd1U9tBx3NDwI4YP7A/AddOHc8/syRhjGJWdSs+keBYPuIr1I79Fgb8vSXvXAdCQvyJykorQ9SdDu++bYAMs/wf8cRSs/BeU7YQlT7qpyj2hfrFw+CrPd6spa8si5yvZBhVRLqe05Kn9e9BERES6OYWwburYIZkATBqY0eoxT1x7HPdeMYXS1KPoX7uZxkCQok1Lmh4PloYCVeXuyJPWvwnVRfDmj933RRvg0VPh4ZmwdyuU5br7q4tg0cPwp4lQUwrWwlPnw5v/ve8grIX5d2vPMhER+dJRCOumzhjfl7dvmcH4AWmtHpORnEDPpHh8/ScynHw+XbmO+vyVTb1i1cU7Afa9GHheKKQ1VMHwmWD8ULzJVb3m373vNOSaV6CuHDa/73rESne4oNbc3i3uOaXb3fSniIjIl4RCWDdljGFsv9YDWHPDvnI+PmOZ9soMhpYuYn4wB4iEsNrSXZGDy/MgOct9fdz1MPESGHe+6wtb8U/YsTBybG5o/7GNc2Hta+7rsp37vvi2ZhvGhnb4FxER+TJQY74QN/wk3pjxEnkf/JVqEskdehEzc1cQKNkJWz8kULaLvTaVXqbSPWHseXDizZA1EsZf4O4r3ACf/hk2zKHRl0BcsB5saO+xjXMhMRQIa0vdVhd7VsGIWbDtQ4hPhoZq2L0SQgsJPPXxfeDz77ugQERE5DBTJUwAOPPkk2k47TdM/eYfufGi09hje5G95WV46jxSc+ezPjgkcnDmUOg9CoyJ3Nf7KEjtBzbAisDwyP0jTobqYteUPy4U2ObfBX+7AArXu0rYmLMhKT3S3N9egUZY/ET79y8D14P26QOw9G8dey0REZFOphAmAMT7fdx4yihOOqo3fdMS2W0z8QfrAPAFG8ijN+W2BwAPLQ9Q29DiOpLGuMoWsD4wgCqT4u6fdRt87S/w/eWRytO6N9ztF0+7FZjDZkDfSbB7BTTWRR9goAEemgHLno3c99nD8Pot8NrNkftKtsNjZ0Lx5ujnKc9zCw32boVgsN2fzyEr3Qkb39n//rrKSJ9dV7B3iwuqIiLiOYUw2U9yQhxFvt4AlCUNBKDQplNs3ZTi23mJfLZ1L794ZSUfbyqKPHG4u0blLptFoS/UN9Z3Akz5uquepblzNTXvf/64ux02A/pNdGHkD4OhwG2XwSd/hpducF/nLnYh7eP/c9tffPJn+OAP0KMXrHsd1r7ujtv0LuxcBG/80IWJXSvcNS/n3e0WD4T71AJ1kS04OlvuEnjuSmisj9z39m3w90th/dv7Hvvh/8BjX3VXJYi1/GVw39GwdT5seg+2zD/wcwKNCm0iIgdJIUyiKkgcRrlN5sqyG6knni22P3txIWyn7cND8zfzzMId3PvuhsiTRp5K0J/IOjuYXcEsgmmDWLy7MfJ4z/5gmv3INVRBz/788sMa7io/E874DWBhcSicrX7JNfvvXgWb33P3Fa5z22HM/TkkpsK35kC/SfDv/3JVsN2hfc62zIM1r7oq2Sf3w7zfu4CW+3nk9fdu2fdNL34c3vvtoX946990f/Y2q8bVV7nbl/8Dakoi929+D4INTXux7adwQ8dCzuLH939f7bXlg8hrvnM7fHBn28c31MD/jHF7x4mISIcphElUH/W6mJPq7mWNHcaJtffxYmAmxTaNaptIMWl8stn1YX2+rYQXl+Ty5MdbsT3788lFC5kbnMo9DZfyxrDbuOzhTympClWE/HHQc4D7ut8kAIJDp/PKsnze3O6D6d93Ky1X/APqqyMVsSVPuADVbxIkpEL1Xrjm3/DDtdBnNMx+yk0t/vu/XOVryImQPd5Vw/K/gK/eCSnZrn8sb4kLg7B/WPk4dM3MYCB0vpth52cd//BKtrnb4k2R+6qLIS7JLUzIWxq6b68bL0QuDdVc4Xp44DjY8Pb+j0VTvgte/wEsuKfjYwbY9nHoPLluO5EDVQoL17v94HYtO7jXExH5klMIk6h6p6dQjtsvrIh0stOSeSd4LP8MnAy4hvzJgzPwGfjRv5Zzx2tr+L/3NlJQnwAYljQOZ07tBIIW1uxqtv9X+iB3+5XvApCbeTwVdY3sKqshGLRwzDfdLvufPuAqZYnp8MUzbqps3AVw4Z/h8qdd/1l4YUDWSJh+E2xd4FZYDjgaTrvdBZ+EVDj6G3DMN1yY2bkIJlwEvni30ezmD1zoKtkGJVvdaxZtdF8vfQpWv7zvB1O82VXZ6qvc86KFp5KtkWPDyvNhtLtoetMChG0fAqEqV/NKWPVe99zwlh27lu97/mAAFvwRylpcGircW7ZhjjsG3N5rT50Pz14BOxbtP9bm5wxvL7JnjdvfrWJP21W48PjCG/RK9xUMwsoXusa0uUg3oi0qJKrsnokAjOidwpaiKkZlp/KvTScDcNKo3ny0qYjrZwxn/vpCiirrSOsRz73vbuTrX4msolyd78LXmvxypo/qzavL8jjR34c+/kTIuQJ69uetHQOBzTQELAUVdfQbNgNS+8LCB9xJLrwf1vzbBZcJF7tVmdFMuBje/52b2uufA6PPcvdlj3fXv5z6Ldgw14W3WT9xTfKLHnLbaow5x/WlheUvdVUr2DdIFayDB6cBFgYd56prm96BK5+DUadHjmtZCWuodRWjvhNh5+eRELZlvtvs1gb2DWHv/NL1ZB33Hfd9YagiWLHbfR2f7N6r8cOMH0aeFw5h1UXu68HHw/aPXTj1J0BjDVzzavTPb/cKqK8AjAuq4PrmakuhR2b053SlEFZf7f7OfIf535W5i93n1d23O9n0Drz4bbj0cbc3oIh0ClXCJKqvTujHFccNZvbUwQAMzOhBj3g/AGdP6sc7P5jJuZP688fZk3niuuO5fsYIAD5p1qi/tcj1Qa3OL6OgvJYf/2sF91edTvDc/2F7aR2MOo2PtkauLZlXWu1+iY4+K9I3NfJUuPQxuHFh6wEMXDVswDHu6345rko2+wmYFbq8Uvog+N5HbmoyKR16DYdgI/Qe7Spkc37mpikTUt0UZri3rHlf16oX3XlPv8P1lq1/A5Iy4B9XuWpVWZ4LJNWhLTMK1sJ9x7grCQCkDXALFcIhLP8LGHICYPatqO1a7qYCw2GocL27ff938PRFrk8O9p3uBBe8eo1w4WxdaKHCzs/AF+c21t36Yevbeax+xd0On+mqYGHha31aC/Pucgsj6ioj7w8iCy2KNrrp0Oq90V/DK8GAW1Cw4P+1/znPXwNzfn7or/3ZIzD3F/suwjjcGmq9r1CteN7dRrv2q8RW9V63QEaOSAphEtXxw3tx1yU5TBzomvGzUhPITI4HYFBmMkf17Ylptk/Y8N5uS4ptxdX7nWvNrnIe+3gr9YEgH9UM4zXfqZz6P/PZVFDJ4m0lnDjSraTMLQn9IhlzjrvNGAqJPTsw6BugzzgXrA6k92gXVq54Fq78JySkwNhzof9kF47CU4Al2yL/g1vzKgydDif9wC0imHUb/OenboXmc1e6IPBUaC+0pHTIW+xC3NKn3H1pA6DveFfNaqxzt/1zIKV3pBIWDLgwA27aE1zYCjS6qVMbjCxcKNrgQtFLN7jpovwvYMQpbt+1T/7sQmPu566XbvIVruK27nWoq4DPHnXbfoCbKl30MEya7Sp8zYWvG/rFMzDvD65h/9nL3X17QpWwqkL3fub8zI3tuSv3DQXW7n+5qs5UvMmNc8mTkWnYaAKNsPRp9743f+ACak2J+ywOdoVn0Qb3dxKufsbCS9fDc1fsf3/+F5FVw4eirtItNAH3d91cY537mW9v72TJdq2mPZDize3/jAINcP+x8ImuvXukUgiTNk0amE5SvI9hvVPISE4AXFWspZTEOPqnuym8tKTILHdaUhybCip55tPtAOSX1bBudwWBoOXBDzZR0xBg9lTXJ7Z8Zxm/eGUl1YOmQ1wPN5XYEVOudBUzfztm2U/6IXznHbfJ7OgzXZP/V3/v+sl2rXDN8/EprlpWtsOt0CxaD+MvdM+f/n045acuQF38sKtc+eMjlbMRp0ReK1wZSxvopiSDDW66saEasse56ddwJaxkGzTWuq8D9e5zCNS7qw6U5wLG/dLHuLC29GlXGVvw/1wFa9BUuOghNxX58vdcCBt0vAtimcNdSHrjv92F1MO/WD/4vXufp/4C0gfu+zlV7Ha/FN661U3ZzvwJbP/IVecq8qH3GHfcujfcGIfPgp0L9+2lW/Ui3DclEto62+6VobHucnvP5beyUGDTO27xxtK/uc+qcrfrc3rzvw/uklnWRgJz84ppS9V7vduTrrHeLVrZsXD/asj7v4NXbzz00LNxjvtZxewfwkq2uS1Nwpcla0vJdvcPlTWvHNp42qNwA7zw7db3HeyqCte7ULX6pXYevw5q9rp/VMgRydMQZoz5gTFmtTFmlTHmOWNMUpRjLjPGrAkd92y080jsZCQnsOAnp3Dx0QPJTAlXwvYPYQAj+rhq2Oi+kerVqWOzCVrITkvi2ycNp7YhyIrcUgBeWZaHMXDKmGwyk+N5euE2nlm4gw+3VcHsJ/h02Hd57KNIBWVTQSXV9Z1Udk/JgoHHRr5PSoO4RJhylZu+qy2FMWe5x9a/Bc9eBgk9IyGsueEz4fsr4Jv/jtwX7hFLTI/cF56OBFj2d3ebPR5SsyOVsPAUX2jxAyNDYW7RX9zt8ddHzl9bCsWhEDDvD5A2CI76qqsezn7SvZ/GWhfIjHGLFXYtd6tPwQXBgnVuLMd9BzKHuXNAZE+3slxXafPHuXA37nx3//u/c7dHneFu3/2Ve6+X/c1N0e5Y6PZEW/kCfP6YO6ajV0Rorr4aFj7kpt5a2rXc9bwlZcBr34e/nu42x412HMCq0C+4ij2RfrbWNvcNW/Oq2xrl8bMiY6jYBfWhqdniTa7nMLwVSdiW+W4bj8WPte99dlTeYheQGmsjvYPgglf+F+5npLXtT9ord0noH0XjoKpo38fCU9HtCbH5S101dufnBz62NcEALP8n1JS2fdzSp2DVC5Fxbfv4yAgqm98HbGSl8oGEV1fnLY2EcGvdP3y6Qq+mHJBnIcwYMxC4GZhqrZ0I+IErWhxzFPBTYLq1dgJwi1fjkYOX3TOJOL+PrJREeqcmkhTqDWtpZB+3mnJwr2QS49yP1tXThvLbr03kpe+dyHHDXIP3ku2u3ytoYXz/NDKSExiY2YOGgPsX+xc7SmHM2TyyPpk/zllHQyBIVV0j5973IY996OG0FrjpwsuecpdgOvY6d9+cn7uy/3VvusAUTcZg15PWe7Sbnhx5KqQPgdN/5R5PTHPhqM84F3TCPVt9xu5bCSsMhbChJ7rb0aEguHWBq2Sd/mu44H439Rr2le9B9gTXdJ8S2iS3Zz8487euWT18rokXwyV/hUmXubC26T23CCA+BWY2650D9ws3PsVN1eUthnP/1z3Wb5L7bNb+GzKGuI14wW1pMe486JHhQt+OT90ebS9+G3Z84o5p2cPWESv+AW/fCp8/uv9ju1e68Z5/b+R9fHxv5PHSnS5shkPY9tAvuEBdJBi2VclqrIM3fuTOs+PTyEXni5rtkbfyBXh2ttsHLlx5Kt0B/7zaVTLDU8vtVbw5srFwW5pvqLtrmftF/O+bXWWyqTexlYBkrdu2pXqv+xx2fuYC5sKH9p1O3rXcbabcs9/+lbDyfHfbnirn7lWh2xUHPra18b7xQ3j5Blj+XNvHhj+XEleBZ+4vXEDv6rYucLe57Qyq4Z/phiooCP0sF6yFF74FD57oqqSHW/4X8O6vNe3cTl5PR8YBPYwxcUAykN/i8euBB6y1JQDW2ijr/aWr+K9TR/G/l01u9fERob6wrJQEslLc1OWQrGS+MW0omSkJ9E93FbTahiA9E92UYbgfrPkU5xc7XEjbU15HbUOQ1fnlrNlVTl1jkHW7Kzr/jbV01Bnw3+th2EmuUR8LZ9/l+rfaYgyceSec8jM3rfeDlaHKWryrgoFbeJAz232dMdRtOBuuhFnrwkL6YBj8FXdM/xzXe3bstXD+/0FCMhxzjdsfDVwF6PQ74D8/2X/hwrHXwq3bIq8NMOlSuORRGHuOm97cODc0rRoKb+HpyPTB0LOvm7Lr2d+tNA2/x6NCVb5Tb3fBMCxcJRt8vAsolXsgpY+roiRntR10woLB6P+CXxOqMn58n6uKgZu6ee37Lnz0y3Fbj5z6Czj6Ktcf9uR57hfSGz9y23TkfxE6WbNfDvmhPdvaqoStftmFjwsfcO9l41x3f3gqMn1wZK+0ta+5VbfgFjvUlbvFF7mfd+yX0lu3ujG33IakubI8N6Xcf3JoQcky9x6XPuWmIcOaqqst7F7pLvv1wZ3wz2/AP74eCbuLHnbHBIMuNPWf7P4uWwthFfkHXpCxZ1Xk9mB+QW+c6/5eYd+qX0uVhbAnNEVdss29h8J1ULrdTa83V7y5/ZW53CWu2lm0yS3GaVkVPFSBRhfwjd+F4pZV1Wh2r3A/fxDpywtPzyckw6s3Rf57CduxsPWfic7w3m/ho/8NbflT03XCWFne/n//XYBnIcxamwfcA+wAdgFl1tq5LQ4bDYw2xnxsjFlojDnLq/HIoRvdtyczR/dp9fGR2a4SlpWaSK/UBOL9ht4piU2PD2gWtGZPHczAjB6cM8ltnDooMxmAM8b3ZWVeGY2BIAUVbtpn8ba9rMh1qyi3FLXjf0ydxRjoP8X1QoVDyIGMPjMyZQgQn+Sa3bOaBaScUEE43POW2tdVSx47w/XLZI+HsefB0JNcpeyUn7oAFro2J+D+xxuX5Kpv8fvN8jd7/ehTx4w63V29YOx5MO0/I/cnpcPkr7tAFd7UdvRZ+279cMJNLhhOvMT9j75HL1c1C/fBhQNkXA+48TO4cZELSe2phK17Df40wf2yC6ve6/ZUGzodqgrg9/3hrdvc5aiWPOma6/s1C8in/AImXwnbP3Gr+vIWu+dV7Iq8p/B0b7ha1HLj3oZatxr00VPh9R+63rcxZ7up56YQtsFNUYcrjYOnwZhzXdVl5+fuuL4T3edUuQf+fRP8+bjIJbTAhcgnz3M9TMEgLHnKrWDNX+qmGee2soKzZLs71+4V7r32y3FVkW2hSkptqQv/PTJb/4UbDo6f/9UF5KpC954htE9frftFWlfeLIS1Mh0JB56S3L3KTfXXlOz7vMY6eOKcA1dt8pa6n9n+k93nFc2m9+D9Zle9KN3u/jSEgkh4L7yw138Az1y8f1CJ5rXvu7aEv13gKtnhqtWhmneXqz7uXu4+60mz3bRta72NYcGgC1yjz3LV6bm/gEdOdgHUnwAXP+rC8Sf3R55TVwl/v8wtovFC6Y7QlCquGnvPaFdNPxjl+e46wJ3VS/r3S+GvZxx4Kvsw82yfMGNMJnAhMBwoBf5ljLnaWvtMi9c/CjgZGAQsMMZMstaWtjjXDcANAEOGDEG6prH90kiI8zGiTwpZKYn0S2/A54usoMxKSSDB76M+EGTqsExuPz/SeH/ticOYMjiDoLW8s2YPa3aVU1Tplv1/vm1v0/YYW4sqCQbtPuf11FXPu//xm0N4vSv+vu/lmrLHuh6s8N5kKaFgm/s5nHiz27C29yi47o3Wz+nzw4k3RXrMOip9EFz/vpsebfneLgr1n30R+k81vFq1+fizfxr5fuAxLtyEw+CAY9y/5o86A5J7uT9Zo9wCAmvd6+1Z7apaR53hNuGNSwh9BqEpuFdvhP9c6Cp0G952CwfO/K2rkq18IdIjN+UqN1UcrsIBpPZxm/ruWe0anKubbcsx+Ur3r/R+EyMVA9i/ErbwQddnN+QEV7k87juhKuAZrlH9z8e7XzjZYyMBe8zZcOw34eFZ8K9vuuB14k2RhnIrSgAAIABJREFUUPrF0+6KEfN+78LxmHNc6ALXx3bGr11lasenbswZQ10V7ozfuKlfcFd0WP+W2/i3oQq+/S4MPs5NlS5+zH1Oyb1D+9KNd9PgBWtdVWXNq5B1lDseIr10gXo3dV5V6ILqwKkuuK78l1s1DC74VBe716yvitxfnu8CQOVu93knpLgq2gX3u4UqEJrGXegqr/+fvfMOb6u82/D9alnee9ux49ixnb0HCSGDDMIKO+wWWkahFEr5SumAltIFtMxCWYVCA2GkEBJmQsjeiTPtxI4d772nbEnn++M9kuWVnRiS976uXJGPjo6O9Mo+j57fSp0v3buyvZ2h75IMGSLWtK799rpTsU+2YIkeKS/wIKs/1z4FNy2Rr33xTVJw+YRJB7j2cFfXLOfrzkkQ6ZfK53Xa5Xsz6nrZ0Dh8sHzcoVUy7G8wyqbH5XvkF4uGEkAc2Y07VpxOKXhNVulcI2T19e73pEAcdxtMuqv3x1YdlDmJ0SNlOH73+7IopqlCfnkbeL7MYV37tPw/Ik0e11bf2fbmVLNTz3U1+8qG0rYG+fxjb5U5qiDXed3fpTs+4rq+vygeXi/b9Cy9F27/Wq5Dd2ry5Oc2bvyR/0ZXHuj8krDsAdm+6DvC6QxHXgjkaZpWqWlaB7AEOK/bPkXAUk3TOjRNywMOIkVZFzRNe0XTtHGapo0LD+/biVH0L+H+Xmz61SzmDInk7umD+PX8rtWNBoMgOkheqAeE+HS5Lz7Eh0tHxjAqPgiAr/fLZGKLycC2w7XsLqpHCBnKLGuQDpnDqbFsdwnlDZ3J2h0OJx/vLMbh7NsCX5lZzqoDxxj5tvj2/UfiWPEJkblSnlz8NAxdIG+Hp8n/5z8lhcaR+qF5MvM3MgR3osSMPrKLFjpIOmP6YPY+ufFDuPS5zp+9/OC6d+Rr8TyWraHTSVn/nLwgfHQ7vDBOXrhtjVIw+EVKEeHK68r+Wl7oY8bIi8lVr0lnyuwLc/4ow6sB0fQgfmJn64ik6dIZGn0TIGQVpwv/aCkiavOlC2Vvl+eTNB1u+0K6kPqYLdIvkyIlJEm+9xc8LEWo0SIv6t7BMqewuVJe3FPmSGfT7Csv4D9eKd3Hbx6XFxejBX6wTIqbZQ/I53D15Lrgl/L/HH1uqtMhW5LkrYZ1/5DvgUtQjblZitGSHTLkPGKhzP2LSJdi85kR8PHd8PqFsjoWpAiLGy8LNi75R2eYec4fpYgp2CjdMoNZinXXlwVPN6yhRH6OvEPk8+xaLPO18jxy1T69D5bojYdHXCv/98wLc/XDK9jQt8MF0hGJSJevu6VartWWf8kQ7JePyCpfuw1u/h/8aIX8zNXmd158o4bLLxZfPCz/vT5XrpHZR1bMttbCvy+CL34lndaVv4cnk+GJKJnrB/LYd62T6+/pMB5aBf8YJj+r3TlSmLbqoP57USHD2AlTpFgac4v8HdjySt+P3fqaXJvkWTD+djlJBKTLGDlM3p7/lPx9/N+dUvBtfqVzH1svqR22RhlOfGV6Z6h5/XPwyb19n0dHq8xDrD0sxXHCFPk3o61ehsmbyuT7nvstfPQjKcJX/kE6i+9c3TNcmbtauoyuoqPi7Z2FTN15/2YZQXjz4p4VyJnL4MWJ+hcQPZ1hzC3yi9nRCnHOIKdThBUAk4QQPkI2lJoFdPfFP0a6YAghwpDhyROcPqz4LhDia0EIwaSkUOYNi+pxv6uNRXw3EeZiQIgPvhYjaw7K3JMFo2Kobm4nt6qZyUkybymztIGv9pXx4/9s495FO5n2t1V8vkd+u12ZWcH9izNYukuGO5ptdndY08Xjy/bzzNdH+GN/pokeAb8u7xrG/C4w9edw98YjCzWQ30C7d6pPmy+rLV243KK81fIP/YHPpSt1wwfy4v75/8kqzIr9UiANvVK202ipkX+8B83o/KZr8pLC5UcrpMDtC5dAMZjh2rfhztXywnzzEjj/QRlKhE6R+exIeDIJnhkmLxzn/bTnMf0jpZC64T3pGKbOg0Gz4KEceWyQouSSZ+TFKG6CrCyd9guY/6R0Zy59VgrKgo2yf1viVBh3O6BJsaM5pJM47EoZdnaF6Q59o7s4equIwXM6zysivfPzk3i+bJty3r0yVOrskIL01k+ly7r1VcjfKN2o6JHyvUi5EM77mXRj4idAzCjpUBVslvuYLH2IsGKZRxg7Vjq5rrw7V/Pf6kPyvOMnSfGaNEOKoYNfdh6jaIucPmEwdeZ8dae9RYaMI4ZCuN4W5fAamUPlHyOF3/Z/S5E9aKZsxhyUIN3K8n2yGGboFVL0LnxXtlppKJKieerPpQDc9Z587/d8IB2lkTdI0Rg9Ug639w2X72PUMPl+ezphez6A+kIZrty1WL6+/A3y4v9kcqfDW7xD3u+qZPRMwLc1dFZfX/a8HLdWXyjFhaNDiqPMT6X4y1oundWR13XmfPpFyPQJkOfo2jbjESmmDyyXbXZcbmNVL38DP3tIOlclO/WeenbY8JwUUY3l8ueMd+X76iJrucxDXPu0zPdLmg4DJsn7Jt8j1/6zh2R/wT0fyEbJvhHS4c1f17VlyYrfy3DvRz+W5xecKEW364vJ3o/gyRSZ31V5UAr/qOHS0azYJwtNXA2F1z4l1yh/A2R+Itdu+q9kVMJV2HG6WsccB6ctHKlp2mYhxIfADsAO7AReEUL8AdimadpS4EtgjhBiP+AAHtI0rY+W3oqzgfhgHw74NBLobe71fiEEyRF+7C6WOWC3npdIQqgvT311gBsmDmDDoWruXbST1g4HRoPgl/PSWLQln0VbCrhoeDT7S+Tjluwo5orRcfzh0/2szCrn24dm4OdlorrJxuHqFqICjiIszjRHEzr9gcVH/jsVuATKR7dL18RWLy+Kg+fIf1/8Sn7rd9plGC35Qtli4OOfyD5Ig2Z2PZ5fRN+Vqi5cYcBIfXSVVQ/duo7lHwnVjVJE7V4se6wNnidDF95BcvuxIIR0DD0ZfaP858JzvJRvGNz+pUzwdhVCzPyNDIPFjIFXZ0h31OwtXY69S6QbkbFIhnCGXyNdk5S5XZ9z5m9kocRgj+1DFsBvLukMDcaMkeHdD2+TY6xcF22QotUlXGNGS3dHGGDyTzrPG6QAzPpMuhOttdJJ9I+Gb74Gox5yylom3bXtb0pBec2bnW7liOtk/lJVthTnhVukyHbapbCY/rBcL1uTHCtm9tFFvCbX0tWMec1TsmfeDe9JkeMdBGkeYengRClAD62SbuWU+2XFs0+IFAp73pcCfPhVsOqPMjfLYJaPMZilk+sbJoXkS+dB8uzOLxvhafKLhN0mhV3uanm/vU1Wb4JsmRKcIIXd2qdlY+iP75bCYP0zMk2haKvcL3KYFBKeYfWgATJU3FgqXeO1T3Vdb2GUwtmTlDlScHmmKbg+76v1iRIT7pDCviJThooPfCadp4Qp0iWafI8UUxnvyN8JVzFGxjvSXSrZIcPXt30hP7+unoA735FrlDhVCtbdi2X19OR7ZXpBXYF8vzc8J78gTL5Xiqul98nP06S7ZJgyMF6Kz7y1ct2iR+mC6qAUcy3V8m+F2RsQcMmz8NpM+foyl0pndcKPO78QbH5ZirW5f5aCddBMKSSnPwKvTJO/R7N+S39xWmdHapr2KPBot82/87hfA36u/1OcA9w/ezALJxw5ry8l0p9deiJ+ZICVe2Ykc9OkBAKsJv7PspuWdgc/m5XCreclEuJrobC2hU8zSnA6NfaXSot9fU4VZfVtbMitoqqpnTfW5XHfrBQyCmW6YWWTDYdTw3imcsvOdYIHyouyrUnmWnkFyguhiyGXyzwskOG7qOEypObqaea577ESGCedpb7Cqf7RslggeZbMrYoceupE57HgGXa2Bsi8M02DkEGQOEVuT75QCpmvfytDYHP/JC+0ocnyoumJl3/P/CEhOgUYyNDUZc/L0KfBJF2v3ogeJcWD5pAFIuDhhFVKseQKIwbEyhYtINt+pF8mL4b562WuVcrsruHi4dfIyQu73pVuT1O5FMwxo6TT8b87ZSirNl9vUOxBxFB5kTb7SKEQN146VdG9VG0HJ8j/W2vk8Q3GTufU4gN3b5ACymiGyOEy5yvxfCl+vAI6RWfoILhjdVfRH6E7ltvekK07Gorg/AfkZ3b5g/KxG1+UId/ggVLobH1NCrDRN0uR+tqFUkjFjZNuZElG1/fJdf55a2QxR/ql0iG2t8n19w7pmbow5hYZRnR9AQH5/P7RMgTsFyVFiMEs16ClWh4neoT80mPylmI1b7X8wvTZQ9Ix9g6SIUSjRTpYq5+Un6GrXpMhWJ9QeSyTtxROJi85TcSFK1TqdMovCQMmy/W4+g0ppvd+KL8cGC3y78R/r5bh2NAUGHKZbEb95nyZVB8zRrqeFj/dbR4rf2cy9ZDj7veli2b2la5pzgr5WXeFwkdeL1/bng+kOBt5fc/PzhlEDfBWnFFig7x77bjvyeBIWWVpMghC9C79LucsNcqfxjY7985MxmyU30rHDghm0eYCsiuayCprYGRcILuK6nl1bS6FNa1YzQZeXZPLD6ckyh5kyHyy6iYbEUdwxKqabAR5mzEZ1WCJk0YImWQMUvTYGjsTdUFeTF0J5a6q0Yv1sIjF5+iuV1/c8a38A9wbfpHSufEJ7bzg9jdCyHN2vTep8+GKV+SF2uUmwsmFrgfPhfv3ygrKvsK5MbpDJgydoSWXCCveLgWYySoFQUCMvDC6XKSpD0jRkbFIViZO7CYM/aOkkNz4TxmqNJilyA4dJC+qBz6T1Z7hqTLk6xcpW0K0VMkwo8EgL9TtzZ0X1t6IGiFDksOvliHH7riKC0Be6F0ibPove+4bkdb1Z1ce5xcPd24bOF2K3Cv19h6ttVKE3rxE5p999gspHOb9RQqdT+6RYc/EqTJsnNAtZTooUf6/9VUpbmc/Ll//kQiKhwUvdt0mhDz23o+kuDea5XtdmSU/X9e9IwXRrsXyc+cXLvMWU+bKQobRN0lBs/klmS848U4Zkt70knS/HDaY96LM+4uf0PX3ujsGg3y97vcxVeZ0BsZJF2zkDfKzYPKWTm1YinQJw9OgoRSufEWK5Ndny/dyhl4glHSBrPAdeIEU/yUZsrl0faF07gbP6/wdHzxXir0Vuj90rI73aUKJMMV3jhS9436Ev1ePKsiXbhyLySjcAgxgbIJsAvvtgQqKalu5YeIATEYDb244DMCDs1N54rNMvt5fzs7CWvfjyhv6FmFtHQ5mPPktv5ibyq3nJZ7CV9eVXYV1LNlRxGOXDe0yi/OspreKToNRNnvNWt5ZNeflJ0dLncxw4iOFeUdcpw88/46979aAztsGo8z7OdUYDEfOpwuIlaI4MK7zfMze0oVyzUJduEg6QTGjpFCOGSVDXNGjZI7Yng/kft3FBcBlL8Bbl0iRfcW/OgXmVa/LC2f3arc710hR46qQG9nLrMzu+IbB/cfYGHbEtVI0eoYDj0TYYHmOceNlbl97S1eRDFIoznhEvoc//Ey2wki/TH6uvZJlSLquwKNtSjdcvwfF2+U+njmWx4tLhLnc0/BUGXaf+RuP99Tjc2a2ysrwtnrpOrZUy1Dw6Jvl/WmXyLDiisek2zz8ahlaPVoRT19Mf1h+hkbdKEVizGiZpxeWIj8Hty7r6mTelyHfH5fLm3yh/Cye/6B0lH1CpeAs3wffPCELF1x4+cvzzFkhP+euHMN+QokwxXeOFL3fWG8CKSqw57aEUB9CfC28u0Umi6ZHBRDobWZ7fi0Wo4GbJyfw5obDvL0pn6zSRkbEBbK7qJ7yhjaG05nL89SXB8ivaeH560dzuLqZRpud7IpT1xy2vqWDHYW1zEjtdHWW7ynlrY35/HxOap95cucMc56Qf0Q9L77dc61OJa58NEVPhID5f5OhKk9u/EAmWIcMlI5msoeLcP4vpIAyGPQZoptlKMtVWeqJX7jMKao8CAM8QmcB0b1Xup7K/MTeCE6Enx2lL5cnJossDAEZYrO39hTzJq9OIRWWAvft6nmcoCOkZpitUnw1lkon9GS+LKRdKqtsXSJz6gMw+KKjt7hx/f75R8lQp4u4cTK5vrlCurKuz8uJYvLqnHbhOn7BBhmOBPl58aS7I5g6X4aXu7+eyKHwcL4UXt33z1khQ7P9/CVMxVkU3zlig7zxtRiJDDiCre2BEILpqeEcrpYNF9OjA7hkeAwWk4HhcYFYzUYuHRnjDkU+PE+GEso8WlsAfLKrmC/3ldHhcHJYbwpbVt/LrMITIKeikfnPreWH/95KVlmDe3txnRwPU9fSfkqe53uNl9+RL0qKM8uwqzpnl7qISIef7oDrF/fcP3VeZ5jU5YgMmNh7fyeQlYmeAuz7isHQNbR5pP26VxEfjSA9Lyyuj9y9Y8U/Eq5/tzOsHzNa9kU7UQxGmcfpHSId5VPNpJ9Ih9Q/8tj2F6JvQdldgIF08vyjpYPXzygnTPGdQwjBby8ZQkLoMfxh0/nTFcMZMyCYioY2IgO8EELwxIJhbufsyjGxvL4ul99fPpQJA0MQAioa2qhoaOOFVTlcMiKGwhopiHIqmsirkoKupO7UiLA3Nxx2C67cymbSomSIp1TfVtvSQULoKXkqheL0YjTJf0cifoK8yKVedGbO6WwlOEHmjcWN7+8z6cmcx2UvOy+/U3/sgOhjCzmfKP6R8OApaLZ7ClAiTPGd5GgVlN2xmo3cNCmhy7ZrxsW7bw+O9Gf3o3Pxtshv5WF+Xmw9XMt7WwupaLTx+d7OmWJ7i+s7nbCGUyPCSuvaSAj1Ib+6hcPVnaOXXCKv1sMJ+2JvKbuL6vm/eWk9jnM6sdkdPLcymzsvGESA9RwPjSpODpMXPLC/30M933si0mWlZsyoo+97pjF7n3wja4UKRyrOHVwCDCAqwMrG3GrqWju4MD2CykYbXiYDPhYj+0oayNNFWE1zO20djuN6nmabnU25XdvdlTW0kRTmS7i/l1vgec7H9AxHfryzhDfW56Gd4cG3W/NqeXHVIVYfqDz6zgrF0TCc5LgvhQzL3bNFiZ2zGCXCFOckrnyzS0fEuB2nkfFBDIkOYF9JPXnVzXiZ5K+HZ15YTXM7L317iH16U9jeeHtTPgtf2cS67CqeWXGQfSX1lNW3ERXoTWKojzt3rbzRhmu6Um1zh/vxpQ1ttHU4yalo4vIX13Ow/OSKAwprWo5J0OXpDl1Vk+2knk+hUJwiTF69FyoozhqUCFOck0TqlZe3TE5gcKQ/d10wiNumJDIsVlZOVjbaGJcoW1+U6iIss7SBC/62ir9+kcUfPt3f5XgF1S3sL2mgrcPBtsOyDcaP/rOVZ1Zk8/bGfKqb24kKsJIY6ku+LnZK9HwwgLrWThFWrj/f0l0l7CqsY/nu0hN+nXlVzVzw5Co+2lF81H1dDl1lo40mm51m20m0hlAoFArFUVE5YYpzkqvGxhHu78VIfWD4wxdJNywq0JsPtxdhszs5b1AY63OqKWtopbXdwU/f3YnVYuTy0TG8s6mA3368l/U5VaTHBLiF0oJRMWQU1hIZ4EV5gw0hYMOhav3YXpiMgg+2F9HSbu8qwvRwpGeIcrU+P3Pr4SMMAO7Gkh1FTEwKdTfE3ZxbjVOT268eG3fEx+Z7OGH3LtqBj8XIP28ce8zPrVAoFIrjQzlhinOSMQOCuf/CwT22j4oP4rP7zuenM5NZOF4m9h+uauHOd7ZzqLKJv187kp/NGozJIHh7Uz5tHQ4+21PKD85LZP7wKD7dXUpVUzv3zkxh2U+nsmBULAU1MvwYGWAlIVT2OsqvbnE7bOH+XtS2SCesqqndHaLcrY9u2lFQS7vdSWl9K6uyKvp8TXuL6/n5+7t44Zsc97YdBdKV25hbTcVRigzyPJywzNIGd4WoQqFQKE4PyglTKLoxINSHB+fILsqB3mZeXJWDQ9P465UjOD9FNg28ZXIipfWtPLtwNAYBJqOBnQW1fLZHVlmOGRDE0JhABkd29qiJCrRid0iFlVfVTEldKwFWE7FB3m4nrLS+1fNUMBoEbR1O9pbUs2RHEYs2F7Djt7PJLG1k9IAgrObOYoN3NuUDcnKApmkIIdhRUEdSuC+5lc0s213KbVN7H3vicGruFh0ldW1UNKq8MIVCoTjdKCdMoTgCA8N8sZqNvHD9GK4d39ny4neXDuGlm8ZiMRncsyVHxQcxMMwXb7ORVF18JUd09tCJCrCSHOGHr8XI2uxKDpY3EhfsQ7CP2d2iolx3q6xmecxZabK54pa8GnYV1uPU4PV1eVz/6iZeX5fnPnZ9awcfZxQT5udFaX0bmaWN1Ld0kFPRxJWjY0kK92VdTlWfr7OkrpV2hxOzUZBd0YimySKEoyX07yqsU7ljCoVCcYIoJ0yhOAIv3zQWo0EQ7n/07v1CCH5zcTpFta1uYeYSYV4mA4HeZoQQzEqPZNmuUhptdu6/MIWC6hYOljcBnUUAo+OD2ZhbzcSkUPKqmvkmq8Ldaf/l1YcA+GpfGamR/lQ22fCxGGnrcPLswmHc+fZ2nv8m2z1fc8yAYIrr2li2uwSHU8No6Nk2wNW7bERcENvzZQizw6HRZLPj30fPsLqWdq58aQO/mJPK3dMH9bqPQqFQKPpGOWEKxRGICrQekwBzMSs9ssvA7/hgbyxGA1GBVveA7ouGRdFos2MQcO24eIJ8LO5wZFlDGxajgRFxcmbboHBfLhgczpa8GjocGj4WIx0ODZNBsKuonvsXZ/Do0n18ta+cQG8zF6ZHMj4xmM/3lrEis5w5QyIZkxDMxIEhNLbZu4xMcrE+p4pH/rcHo0EwYWDXWYE1zZ39y5psdhzOTmcsq6wRh1Mjr6rpmN+f00WzzY7TeWb7qikUCsXJokSYQnEaMRkNJIX7Eu0xeHx6agQ+FiPTUyOICfIm2MdMc7uDA2WN5FU2ExnoRXKEHwYBqVH+XJDaObz2xolyksC9M5MBKYza7U6W7ynlvEGhGA2Ct2+fyJZfz2LPY3N55ZZxWM1Gt7jakldDTXM7jy/bT7PNjqZp/PbjvQC8dus40qMDupx/tS7CDlc1M+1vq/jhm1vdQszVv8yVS9ZfaJrG9Ke+5YnPMo/5MZWNNupbOo6+o0KhUJxGVDhSoTjN/PWqEZiMnSFAb4uRd388yS3MgnwtAMx9Zg0AExJDWDA6lmGxgUQHehPsY8FqNuBtNnLfrBQiA6zcel4iG3KqGRobwGd7SilvsDElOQyQI5w8E/YBYoK8iQ3yZlNuNdVN7by+Lo9R8UGE+lnIrWrm6WtGMiM1gg2HuuaN1TS1U9/awe1vbaXZZmfNwUpeXJXDfbNSyCqTIqyoTlZRtnU42JxXw7SUMLfrdyaobm6nstHGWxsOc+PEASSFH32W3V3vbCc2yJvnrh99Bs5QoVAoekeJMIXiNOPqRdbXtgBr569hWpQ/E5NCMBsNblfKajayYFQsQoC/1cyPzk8C4P27JgOgaXJA+FRdhPXFvGFR/Ht9Hhv1vmVb8mqoaWkn0NvMxSNkV+4IPfQaE2ilpL6NSr1nWEFNC2/fPpGXVx9i8dZC7puVwgFdhJXUtWF3OHls6T7e21rIdePi+dOVw3vNPTsdlOrzN+1Ojb9/fZAXbhhz1MccrmruElpVKBSK/kCJMIWinxkQInuHPX75UG6enNjrPn+5akSfj79vVgrjE0NIDPM94vPcNyuFTzJKqGqyEexj5pusCioa27hlcqLbOQvzkyJsRFwQJfVlLNpcwJ7iev585XAmJYWyObeG1QcraW13cLCsEV+LkeZ2B5/tLeO9rYUMjvRj8bZCZqVHMHFgKBoaQT6WE3hXjp0Sva3HhIEhfLmvjLqWdlYfrORvXxzgmYWjGJ8oQ7Et7Xbe2pDPzZMTqG5ux8fLeKTDKrqxPb+WkXGB7qIThUJx8qjfJoWinxk9IJgtj8zqU4AdjRBfi9vJOhKB3maevHoEC0bF8MMpAymua6XDoXGDnmfm2ic60MqkpBC8TAb2FNdjNRu4Ru+2nxTui6bBxtwqGm12pg2W+Wp/+HQf4f5evH37RAByq5p54P0MfvTWNsrq23jw/V0n3cqioLr35rGl+uSBe2Yk0+HQuPPt7fzsvQyK61q7TBt4c8Nh/vpFFv/bUQTIUKvi2Fi+u5SrXtrAR/p7p1AoTg1KhCkU3wEiAqxH3+kUMCMtgmcWjmZSUigAk5NCGeSRQyWEYM3/zeCWyYmE6rlq6dEBbvcjKVy6bR/vLAFg9pBIQHb6v3xkDJEBVoJ8zBTWtJBV2sD2glpeXn2Ij3YUsU1vfeGisKaFC/++mpyKow8oX5ddxbQnV7GrsK7HfSX1bXiZDExLCSMlwo/NeTXMSovAYjJQrQstm93Bv9cfBmBHgTxGc7uD7fk1XPevjScsEAtrWlh1oO8pBmcLi7bIRsDNNkc/n4lCcXahRJhCcQ4yMj6Q81PC+Oms5B73mY0GDAZBsC7ChscGuu8bqIc8v9hbhtVsYO7QKFw5+JeMjAEgPtiHnIomSurb0LTOTv55lV1bWby/rZCciia+2FtGfUsHjW19Vyt+qwsdTyFX2WjjzfV5FNe1Eq23ALl3ZjJTk8P4x8JRRAdaqdQ7/y/NKHHfdo1yAvgko4TNeTXsK+nZuuNY+Oe3Odz59nbsDucJPf77QJPNzvocmUdos5+9r1Oh6A+UCFMozkG8TEbevn0i5w3qO5k/RBdhwzxEmI/FREyglXaHk3EJIfh6mYgOsBIX7M1IvbdZXLA3Ows6HSu7s3NUkwtN0/gkQ7ppG3Orue6VjTz4/i5qmtt5+qsDPcY3bcyVImBfcb1725sb8njs0/2sPlBJdKAcWH75qFje+dFEAqxmwvy8qGqSwuuDbUUkhfmSHOFHvkdY0zWfM/sY3LjeyKloot3upKi2f9t0nE5WZpa7bzccQSgfjfqWDqY/uYqdBbVH31mhOEdQIkyhUPSKKxzpahzrwtXpUnTeAAAgAElEQVQCYvIgGdK8b1YKv7l4iLstRXyID+26M+QqOgj1tZDnIX52FtZRUNNCVICVDYeqySprJKOwjiU7inj+mxxm/32Nu/qytrmd/aXSqdpbUk9tczttHQ7WZst2Gk02OzFB3j3OP9zPi8pGGwXVLWw5XMNVY+OID+66n+u4ORUn1nA2t1IKy9x+bFhb3WQ76nD2o9Fks7sbBndnZ0EdPhYjwT5mGlpPXIRlVzRyuLrFPZFBoVAoEaZQKPogNtgbf6uJ5G59t1x5YZOSZNXhwgkDmDcsyn1/nIfQ+dMVw3lw9mCmJId16ay/KqsCo0Hwf/NScY2nrGi0sTa7iiAfM002Oyt0B2ZzXjWaBlOSQ8mpaGLes2u4+uUN7PFwxWKCeubUhflbqGqysWRnEULAgtGxxAX7dNmnXQ+vHSxv5JqXN/Da2lwA7A4nKzPL3bMzKxttFNZ0LQyob+lwN7M9VNHMN1nltLT3zC1zNcg9Hr7cV8Y9i3bw0reHjji/0+nUuOn1Ldz5zvbjOn53HlicwU2vb+71vr3F9QyJDiDYx0L9SYiwYr2Aoqz+5ASjQnE2oUSYQqHolbunJ7Psp1N7tCSYmRbBpKQQRsT17H8GnSIsyMfM1JQwfjorhcQwX4prW7HZZWL3joJa0qL8mZUeidko3DM212ZXMi4hmNggb7L1jvw7CuqwmAzcODEBpwblDTb2FjegabirQl3hSE/C/azUtnSw+mAlI+KCiA3ydp+by6FzsSm3hq2Ha/n71wepaGzj090l3P7WNr49UMnbm/KZ9rdVzHtmDftKOoXfIQ9RuWRnMbe9uY3f6NMHXGzPr+Haf23komfX8NH2oj7dpu68teEwy3eX8tcvsjhQ3neo9Kv9ZWSWNrCvuIGObnlpe4vr+Xp/eR+P7KTZZmf1gUr2FjdQUN3C8yuzaeuQ6+RwauwraWBYbCD+3mYa2k68wrVE7+dWdpKunUJxNqFEmEKh6BU/LxMJoT17j01PjeC9Oya7B4R3J153mzwfmxTmi1ODX364m+W7S8koqGPMgGACvc18cs9UXr91HABODYbGBDI40o8D+lDzzNIGUiL8GDMgGJCzN4dEBxDkY+bheWkkhvowJqGnIAzzl+HU3UX1DI2RjW9dTtjAMF8sJnn+YX4WHE4Ni8lAu93J8ytzWHNQhjoXby3kD5/uY2R8IAHeZm54dTO/WrKHZpvdHYoM8/MiUw9rLtlRzI//s4031uWhaRp//iyLcH8vfL1MPPjBLuY/u5aa5nZeXZNLVZON19bm8vq6vB7nfriqmfP0cO+qrMpe3+eqJhtPfnkAg4B2h9N9Pi7++kUWD324q9fHerI2u8odPn7wgwye/vqgW7zlVjbR2uFgWGwgAVbTSYUjXXl+yglTKDpRzVoVCsUpJVZ3mxJDO90mV1XlxxklfLGvjLYOp1s4DYkJQNM0Qn0tVDe3Myw2kDa7g/U51dgdTg6UNTI1JYyoQCuv3jKO8YnBtLQ7qGluJz7Eh28fmtHreYTrjWcdTo20KH+g06WL8PcizNdCSX0b0waHs2RHMRemRxDobWbxtkJ8LbKR6xf7ygD4/WXDMBrgqS8P8u6WAtKi/ClvaMNkEFwwOJyPdhRx3qBQfCwmdhbUsiKznNyqJrbl1/LEFcNYOH4AKzPLuePt7Vz6/DqK61pZtKWAvKpmDAKmJoeRqp9jW4eDkvo2rh0fT11LB6sOVHD39EGADJM2tztwODWu+Od6Khtt/PriITy+bD/7S+uJC/bmshfW8dDcNDIK6mi02alpbncXWXjS1uHg4Y92c7C8iQCrCYdTY+thma+VUVjHpSNj2Ks7f8NjA/n2QAXFJ1GAUOIKRyonTKFwo5wwhUJxSvGxmLhydCwXeeSJJUf4ER1oZf7wKNo6pOsyOj7Yfb8Qwi1ChsUGkBrpT7vDSUZhHRWNNreImj0kkiAfCzFB3l2qNnsjXB/BBJAW5XLCdBEW4EWInxQmFw2Lxtts5LrxA7htykDa7U5qWzrcY6BGDwgiNcqf5Ah/Xr55LEnhvqzMqiCvqpkBIT6kRslQ6qUjY3jt1nF8+9AMIv2tvLOpgFlpESwcPwCjQTBnaBSzh0RSXNfKeYNCyatqJjXSHz8vE098lkmHw8mzK7LdlaADw3yZmRbB9vxavtpXRluHg4c+3M3Uv37DQx/soqSujf/+aBK3TE7AYjSQWdrI2uwqDlU285fPM2nUe59llTXw6prcHvlc2/Nr+TijhP2lDVyYHsk4fbKAQUgRBtJFtJoNDAr3JcDbfFLVkcV6OLK8oQ3nCYyMqmlu56kvD5xUXppC8V1DOWEKheKU8/frRnX52dfLxIaHZwIw5x9rqGluJyG0a17WpKRQyhraiAqwMjhSiq6lu2QbC5eIOh5cI5gAUvXjhfhaeGR+GhemR7K3WIYQxwwIYu/v57pnXU4bHM6ag5X87tIh3PL6Fn40NanLcWelRfDWhnzMRsHsIZFMT41gZWaFW3T6eZn4+7Uj+WB7EY8vGNZlhuYfFwxjQmIIP5iSyIZD1aRF+fPprhL+uDyT+9/LYPmeUnd+XEKoL0lhfrywKoc73t5OenQAmaUNCAErsyq4bcpAxiZIIZsS6UdmaQP1LVKgHPaoRP33+sN8vb+c/Jpm/rhguHv7lrwaDAI+vPs8BoX7sTSjmJ0FtcwbFsXHGSW0251sz69lRFwQJqOBAKuZhlY7mqb1OaBd0zRaOxz4WHpeWkrrWzEbBR0OjZqW9i7rcyysyCznhVU5rMup4p0fTcTPS12+ANbnVDE40r/Llw7F9wflhCkUijOCEAIhBM/fMJrnbxjd40J+74xkvrx/GkLIRH0h5LgcwO2EHQ+ui1J0oJVAH7P7HO6YNoikcD9C/SxYzQZCfC1dhNJjlw7hr1cNZ3CkP5semdVjJNTMtEjaHU7sTo2fz05lcKQ/i++c3GVG5nnJYfzjulE9hEJkgJUfT0vCbDRwweBwIgOs3Dw5gQEhPizfI1+rq13GwFBfhscFsuWRWfzpiuFkljYQ4mvhv7dP5Oqxcdw/O8V93CHRAewtruebAxXEh0i3L8TXgsVk4Jss2eh20eYCnli+3933a1t+DenRAe7cvBsnJrD5kQu5YHAE7XYn2/JlE9sJukMW6G2m3eF0O5kgw6O/WrKHl1cforGtgyU7ipn4xEpqu1WDNtvs1LV0MDRGupcnkhdWpFenZhTW8akuzs917A4nP/j3Fv61+lB/n4qb0vpWlu1W63OsKBGmUCjOKGlRAb02iTUYhDvZ32o2ctGwKKqb2wn2MZ/Qt3yr2Yi/l6lPAXfL5EQeu3RoDzGYFO7HdeMH9PoYgHGJwaRE+PHQ3FQGdHPzTgQvk5HfXTKEUF+L200L8jG7hWNEgJUbJg7gtVvG8fJNYzkvOYynrhlJgNXsPsZlo2JotjmobLTx0xkpRPh7MSExhIGhvjicmrvFxKtr8/jpuzsprGlhZ0Gde7g5yPff22Jk9ACZq/fPVYdwODXGD5T7BHhLQekZksyuaOLdLQX85fMs/vRZJtvya2i02XuMcnIl5buKK05EhBXUtBAdaMVoEEfNTWu3O9nmMTf0bKWmuZ0Oh3bECtozzZvrD3Pvop00neSs2HMF5ecqFIrvJM9cN5pQ3/0EeJv6DH8djbtnDCK9j1DmqPggRsX33mbjSJiNBr7++QUndD59ceGQSLakXcje4no+31tGYi9VqRfqczp74/yUcD6+ZwpLdhRx8YhoxiUG42c18egn+zhQ3sjsIZHcPX0QZfVtzHt2Dbe8sYWWdkcXEeYiJsibqclhrMupwiBkuBZwi76G1g4i9VmnB/WLf1ywN9sO1xLgLfdZmVnBlWPi3Md0tacYkxDEG+uh9ASS8wtrW0kM9UUApUcRcbe/tZW12VWs++WMHr3hziYq9YkQJ9ps+HTgmoxRXNvqzvNU9I1ywhQKxXcSi8nA4wuG8dDctBM+xk+mJzMjLeIUntXpw2gQDIsNJNTXQkqE39Ef0I0hMQH85pIh+HqZSAr3I8Lf6h7OPikpFKvZSGKYL49eOpSWdjtpUf7uNhjd+fXF6Qghj+mvi69AXWB5JsbnVDRhELJf26HKJrL0Vh2rD1bSbndS1WSjrqWdnQV1CAETEkMwGgRl3cZStXU4jtiUFuSw9PgQb6KDvHuMtfLkYHmje5qCp2N2tOMfjXa7093n7nS5PA6nxlsbDh9zAUSVPqC+tL6Nl1cf4vpXNgGwObeaf36bQ1bZic1EPRkOV+sirK7lKHsqQDlhCoVC8Z3BaBC8f9dkgrzNR9/5GJgzNJLsisYufdSunzCA6yf0HW4FSI8O4I8LhhEV0DmJwOVydQlHljeRGOrL6PhgnBo0tzs4PyWMtdlVbDtcw1+/PECH3Umb3cHEgSFE6HNGD1d1XqAb2zqY8pdv+N2lQ7l6bKd75klbh4OKRhvxwT40tzvIPMLA9WdWHHTfLteHtv/yw93UtrTzxwXDePqrgzxycTqB3mZa2u20dTh7beHRnbvf2c6B8kauHhvHsyuz+ey+80mPPv6CkSOxPqeKR5fuw2gQ3DQp4aj7V+mvD+DZFdm0djjYU1TPdboY21lQx6u3jHPv88XeMqqbbUxLCSc+5NQ7hE6n5p7NWlzbSml9K+F+Xj0aPis6Ue+MQqFQfIcYFO5H6HFWDvbFiLgg/nXzOLxMxuN+7I0TE5iV3hkCDbDqOWGtnS5QdkUjyRF+7ma4ADdPSsAg4JusCvYU1bG/tIHcymYuHxULyCILV3NbgL3FDTS02VmbLZvSappGWX0bHQ4nz6/MZm9xPUW18sIeH+JDdICV0vo2t7PV1uHgq31lOJ0aFY1tfLWvnOvGxQNQroctt+u9257/JofF2wr5YFshIMc1XfLcWrfDtS67itzKnqG9PUX1rMyqoKi2lWdWZKNpsP8IQvBEcRVRZJc38puP9/DzxRlH3N81oB6gVZ9y8HFGMSArgrfn17rfp6omG3e9s51f/28vf1i2/5Sds9Op8eb6POpbOyhraMOmjwLbW9zA9Ce/ZdGWglP2XGcjSoQpFAqF4qh0D0e2250crm4hJdKPuGBv9/1jEoJJiwpg8dZCnJoUb2ajcBcdDIkOJK+62T1nc09xZ08ygMeXZTLpzyuZ+8wanv76IA8sznA7Z/Eh3kQFWmntcNDQKhvRXvevjdzx9naW7ynlw+1F2J0ad1yQhLfZSHmDFGtFtS04NXh7Uz4Ai7YUUFTbwtf7yympb+OTnSVUNLZx8xubmfOPNUz+80rmP7vWLWBeXnMIfy8Tr94yjlsnS5GZ7zFLdG9xPbe/ufWkwpSaprkLGrIrmvhqXznbdBF1sI/E+8pGG14mA16mzkv5st0lGATcMHEANc3tHNInKbhEo7+Xqccc1JNhX0kDj326n093lbhDkSAbHdvsTjbnnViBRFuHgxdX5ZxUb7rvA0qEKRQKheKouHLDaprbqW/p4LM9pTicGoMj/RFCMCQ6gBBfC2F+XoxPDKbRZkcIWHznZP79gwnuFh7p0f5oGmSVSWHhEl95Vc0s2lzAG+vzGDMgiNK6Ni4eEU12RRPPrswGdCdMnxNa2tDK2xvz2V1cj6/FyKqsCt7bUsjEgSEMCvcjMsCLsoY2qpvbu7TVmDAwhNzKZu57dycgJzu8vPoQK/ZXoGkwf3g0EQFW9pc2cKhSisWv9pVx9bg4Zg+J5PeXDyM60JsCD8GxKquClVkV/G+ndKHqWzpoabfT2NbBq2tysXeb6+lC0zR+98leXlubS25VM/nVLVjNBnYWyCbFZQ1tbDxUzZx/rGFnQW2Px1c12Qj392JwpD+pkf74WIyUN9hIDPPl/BRZgbxsdwlrsyvZr7uPM9Mj3NMLPClvaDumqlVHt0a7OZVyHQtrW9xiOSnM1y3Wd+mNf4+XjbnVPPnlAf78WaZ7W5PNzie603e8lNS1sre4vkuj4D98up//7Sw6oeOdKlROmEKhUCiOisVkINTXwrMrs3n+m2xc1zLX5IIHZg+mXK96HJcYwlsb80mN9O+RNzVED11mljYwZkAwe4vr3SOrfvfJXkbEBbL4zskYhUAIqG1uZ8OhagK9zYT7eREVKPPUSuvbOFjeSEKIDyPigli6qwS7U+PBOYMB2dqjosFGkZ6cnxzhR1FtC88tHM1P/rudHQV1zB4SyRWjY/nJf3fwty+ziA3y5tmFoyisaWXak6vYeKiKuBAfOhwaMz0KPBJCfbo0xC3QnaV/r89jxf5y1mZXMj4xhBlpEfzl8ywGR/lzweBwQLbnKKhpYcLAEJbsKOY/G/MZFO7rbs9yzdh4t2Mn+7VJ8bU9v5bRAzqnTIBMzA/z8+JvV4/AIAQ/fz+D3UX1pEX5MzDMlzA/C8+skAJ2ZFwgsUHepEcH8ElGCY1tHW5hrWkaP/j3Vlra7az4+QV9zoUtqm1hwYsb+PnswdwwUeYVHqqQYrSwpgU0+TkZmxBMrl4lWVTbSnWT7bhD7C6h+O6WQq4aE8e4xBDe2ZTPXz7PYnhsIEnhx1e8cs+iHewsqGNcQjAf3DUZgEVb8rlJJHDF6OM61ClFiTCFQqFQHBMf3zOFT3fLbvpjE4IZFO5HTJB0piYM7Gx34Wp94ero70lskDcBVhP7S2SH/8PVLdw9fRAvfXsIu1PjkfnpXUTAW7dNIK+qGW+zESEE0boIK9NFWHKEPzPSwlm6q4RgHzNzh8qwZ1SAlYzCOnc+2VPXjCTYx0xUoJUlP5nC4apmQv0s+HmZGBkfxK7COi4bGYMQgvgQb2KDvNlwqJrY6hYsJkOXdh4Job58qc8VBSnCDAJyK5upaLAxJVkWJ7jCftvza7lgcDjvbing0U/20e5w8vE9U/j9p/swGQSHKpv59kAFUQFWZqSFu0UYwA7dAdtbXN/jvaxqshEX7OOeMJEc7qeLsACEEMweEsWag5UU17Wyq6ieC9Mj3etVWt/mFmE7CmrdeXr/21HMtePjezyX3eHk/vcyqGqysf5QlVuEudpjFNa00uHQSAjxYYCe9J8U7ktuZTO7i+uZkXp8Vcolda0YDQIfs5ElO4sZlxjChkNypFdRbetxiTCnUyOrtJEgHzPb8mvJrWomzNeLtg6nu91Kf6HCkQqFQqE4JuJDfPjJ9GTuv3Aw56eEuy/o3YkKtPKXK4dzx7SkHvcJIRgSE8BX+8t5dOleAM5PDmN4bCCz0iKYlNS1bYbZaGBwpL+7mi/c3wuDkM5LXlUzgyP9mJYSjtkouHpsHFazLEKIDPCivKGNwhrpqAwK9yXBo/9aYpgv/lYzQgh+e3E6FpOBS0fGuM9x8qBQNuZWsya7kvGJwe7jgnTCaprbadTzlQprWrhoeDQPX5TG8vum8vQ1IzEIKHEVBuTXYHc4efqrg6RF+2MyCH7yznYa2uw8MFs6d6sOVDI2MZiUCCmoTPoUhx26E7ZHF2E1ze3k66FQGY7srOxMjpTCxNWg+I8LhrHulzPcUw+GxAQQGyRFR15VM6sPVuJ0avx3UwF+XiaGRAfw+LL93LNoB816fts3WeW8t6WADYeq2ZZfS5CPuUt1ao5eyFBY28K+4nrSowOI1We03jQxASEgo6AOm91xTMUMX+4rY/nuUkrr5Aiz8QND2HSomg5HZwPe3sKpbR0OrvjnejbqQk3TNFrbZbFCSX0rrR0OFupNmNfnVLkHyUcGKhGmUCgUirOMhRMGdBE9nvzm4iF4mQx8nFHC7VMHMikplHfvmMSLN4456nHNRgPRgd58tb8cu1MjJVJWky6/73wenJPq3i8ywIrN7mR/aQNBPma369Mb4xJD2PvY3C5u1/kpYdS1dHCwvIkpyV0nPCTogjC/ugWb3UFpQxvJ4X7cdcEgEkJ9iQiwMk0PP85MiyCjoI5vD1RS1WTjnhnJzEyLoKS+jbEJwdw4sbNdyNgBwcQGySIH1+Mb2qQYyq1qpslm59Gl+7j8xfU0tnVQ09x1BueUQWHEBnkzRncgjQY5KuzSUVJcDon2dwvnv3yexa1vbGHhK5v4X0YxV42J5ZmFoxiXGMzy3aVsy6+loqGNn72bwe8/3c/6Q1UYDYLrxsWTV91Ms81Oh8NJfnUzVrOBupYOSurbGBEXyPjEEEbFB3HxiGhGxgXx6tpcLn5uHfOfW8utb2zhD5/uZ+ZT3/KDf29xV6aCLJy48+3tPLp0H8V1rcQEWZmUFEJuVTMr9pfT4hJVvYiw3UX17CyoY32O7BH3v53FTHhiBXUt7W63bkZqOPEh3qzL7hRhUf3shKlwpEKhUCjOKMNiA/n8Z+dTWNPqzhE7noHc84ZF8fq6PAC3c+QKyblwhZm2H64hLrh3x84Ti6mrJ3HpiBjC/LzILG3gmrFdw3Mucbn1cA1WswFNwx2Cc/Gri9KZPSQSPy8T32RV8Jcvsgj2MTMjNQKLycBX+8u5bcpAgnwsDAzzJa+qmbEJwRgMgo/uPo9AbzPjn1ihvxYvyhts7C2uZ0teNXUtHby6Ng+n1nVQ/cj4INY/PLPHa7t6TBzNNjvTUyMwGw2YDMId4t1yuIY5QyL55UVp+FhMPH3tKMY8/jXZ5Y18vLOYRt0Re3dzAenR/oxLDOFfa3LJKpPhvQ6Hxvkp4e72GiPjg4gP8eHje6YA8K+bx3LXO9sprGnhjmlJfLi9iE25cnj9twcq+eWHu3lm4Wg0TeNvn2fhZTJQ1WTDZncwIzWCyUlSAP/9a9n/LcBqolifwHC4qhm7UyM5wo/dRbIAwJWfty67ikabnU25Ne6QdHKEH1OTw1i2u9TdxFmJMIVCoVCcc/hbzQyJObGmtNeOi+f1dXkIgXsqQHdcIky6M8c/nspgEExJDuvhggEkhvngZTLw+0/3u8VX9zmiqVH+pEb5u3ObciqaeGhuKhaTgRmpEXz1wDT3ZIRR8UGU1be5BWmyvj3Yx0xtSwfzhkbx1sZ8PskoobzBhkHAK2vk0O6wY0h497YYueuCQe6fowKtFNW2cvPkBK6fMICEEB8MevhTVrha2KOP0LpidCxLd5XQ0GZnXEKI+xz3lzbgrYdop6dKEWYQdOkZB3IdPrrrPOxODYvJwCPz0933PbPiIM+syOby0bEMjvSn0Wbn4hHRLN9dSmObnZggbzm1wctEdkUTl42MoaSulZK6Vsrq27jypQ20250s+cl57ipblwjL0EXZxkNVtDs0gnzMhPhapOu6pZDVB2RfuoiAU9OT70RR4UiFQqFQfK9IjfJnZFwgiaG+eFt6b0Q7ONLPncR/qmcY+lhMfPOL6dw0aYD7ot/dCXMRE+TNl/efz5Zfz+KeGcke5+fvnon6i7mpvHXbhB5ViS4hOTwuiHEJwSzeKhufPjgnlfhgH64dF8fUXkTi0XCFJKelhDMwzNctwDzP7at95bTbncwdGsVofcbquMRgYgKtBHqbWZpRzOPL9pMW5c88vRgiJcIfH0tPb8dgED2cRpBjxRJDfXhieSb79Jy3y/W8PHmecmD7Ixen89ilQ3jmulHEBHlTXNfKgx9k0NbhwGo28qO3trFdz50rqm2hvrWDXL0/2sbcag5VNJEc7ocQwi3I12ZXEuxj7pLr1x8oJ0yhUCgU3zuev37MEZujBvlY2PDwTMobbIT6HX0s0fESG+TNQ3PTWJpRgs3uJPwIjlRyxJFFYGyQrMbsTmSAlayyRmKDvFkwOpZt+bV4mQz8+PykLoLueIkL9mZ3kYFxiT2rV0GKMFcl4rjEYLLLw9lRUMv4xBCEEFw2Mob/bs7H32rmlZvHEe7vRbCPuct4rGPBYjLw0Nw07lm0gzc3HAZg4sBQogKslDW0EaP3hPMcsxUT5M2nu0soqGnhkflpjIoP5tp/bQQgyMdMVVM7m3LluU9JDmV9TjU+FiOX6eIuIcQHX4uR5nbHaRnddLwoEaZQKBSK7x3dw3+9IYRw9xU7HQR6m/nNxUPILGvo4SadCiL1UFlskDdpUf48tnQfI+ICe3WVjof7ZqZw9Zi4Pl2gFL3KMinMlzA/L348LYlpg8PdztzjC4bxy4vSsDuc7ia8i++cfEQh2hez0iOwmg1sOFRNVICVQB8z6dH+lDW0ER3Uc+1ig6xoGhgELBgdS4S/lRsnDuC/mwuYNzSK97YWsnx3qft1bjhUjdVsdLcuMRgE6dEBbMuv7ff2FKBEmEKhUCgUJ0xvPbVOFQPD/PC1GIkKtGIxGXjssqGnxL1JDPMlMaz3ylXoLHJwVYtazUZGxnd1uboXUnQvjDhWrGYjU5PDWJFZwWA9bDwkJoBVByp7dQdd7S8mDgwlwl+KqEfmpzM8NpDUKH/e21rIl/vKGBTuy8SkUHb+djYBVnMXkTw0Roqw/k7KByXCFAqFQqH4TvLDKYlcMiLa7XzdNCnhjDxvenQAsUHezB0WefSdTwEz0yJZkVlBqu7A/XDKQIbHBrpdNk8S9crUBaM7c8d8vUwsnDCA2uZ2AGx2Jzfr71VvxxgaI6c89HePMFAiTKFQKBSK7yRWs7Ff8pb8vEy9tro4XcxKj8BnudHtvIX5eTFvWHSv+yaF+7Hsp1MZ0m0cFsicMH8vEyajOKJDOTRWPjZGiTCFQqFQKBTnMpEBVnb+bjaWPmZWdsc1r7Q7QghumzqQxDCfXqs0XQyJDuC560czK+34RimdDpQIUygUCoVC0a94mU5NqwjXGKgj4arw/C6g+oQpFAqFQqFQ9ANKhCkUCoVCoVD0A0qEKRQKhUKhUPQDSoQpFAqFQqFQ9ANKhCkUCoVCoVD0A0qEKRQKhUKhUPQDSoQpFAqFQqFQ9ANKhCkUCoVCoVD0A0qEKRQKhUKhUPQDSoQpFAqFQqFQ9ANKhCkUCoVCoVD0A0qEKRQKhUKhUPQDSoQpFAqFQqFQ9ANKhCkUCoVCoVD0A0qEKRQKhUKhUPQDSoQpFAqFQqFQ9ANKhCkUCoVCoVD0A0qEKRQKhbZxR9YAAAfNSURBVEKhUPQDQtO0/j6H40IIUQnkn4GnCgOqzsDzKPoPtcbnBmqdzw3UOp/9fF/XOEHTtPDe7vjeibAzhRBim6Zp4/r7PBSnD7XG5wZqnc8N1Dqf/ZyNa6zCkQqFQqFQKBT9gBJhCoVCoVAoFP2AEmF980p/n4DitKPW+NxArfO5gVrns5+zbo1VTphCoVAoFApFP6CcMIVCoVAoFIp+QImwbggh5gkhDgghcoQQD/f3+ShOHCHEG0KICiHEXo9tIUKIr4UQ2fr/wfp2IYR4Tl/33UKIMf135opjRQgRL4RYJYTYL4TYJ4T4mb5drfNZhBDCKoTYIoTYpa/z7/XtA4UQm/X1XCyEsOjbvfSfc/T7E/vz/BXHjhDCKITYKYRYpv98Vq+xEmEeCCGMwIvARcAQ4HohxJD+PSvFSfAmMK/btoeBlZqmpQAr9Z9BrnmK/u8O4KUzdI6Kk8MOPKhp2hBgEnCP/jur1vnswgbM1DRtJDAKmCeEmAT8FfiHpmnJQC1wu77/7UCtvv0f+n6K7wc/AzI9fj6r11iJsK5MAHI0TcvVNK0deA+4vJ/PSXGCaJq2Bqjptvly4C399lvAAo/t/9Ekm4AgIUT0mTlTxYmiaVqppmk79NuNyD/esah1PqvQ16tJ/9Gs/9OAmcCH+vbu6+xa/w+BWUIIcYZOV3GCCCHigIuB1/SfBWf5GisR1pVYoNDj5yJ9m+LsIVLTtFL9dhkQqd9Wa/89Rw9HjAY2o9b5rEMPU2UAFcDXwCGgTtM0u76L51q611m/vx4IPbNnrDgBngH+D3DqP4dylq+xEmGKcxZNlgar8uCzACGEH/ARcL+maQ2e96l1PjvQNM2hadooIA4ZtUjr51NSnEKEEJcAFZqmbe/vczmTKBHWlWIg3uPnOH2b4uyh3BV+0v+v0Lertf+eIoQwIwXYfzVNW6JvVut8lqJpWh2wCpiMDCeb9Ls819K9zvr9gUD1GT5VxfExBbhMCHEYmQo0E3iWs3yNlQjrylYgRa/GsAALgaX9fE6KU8tS4Fb99q3AJx7bb9Gr5yYB9R7hLMV3FD0H5HUgU9O0v3vcpdb5LEIIES6ECNJvewOzkfl/q4Cr9d26r7Nr/a8GvtFUU8zvNJqm/UrTtDhN0xKR195vNE27kbN8jVWz1m4IIeYj49JG4A1N057o51NSnCBCiHeB6UAYUA48CnwMvA8MAPKBazVNq9Ev5i8gqylbgB9qmratP85bcewIIaYCa4E9dOaRPILMC1PrfJYghBiBTMI2Is2D9zVN+4MQIgnpmoQAO4GbNE2zCSGswNvIHMEaYKGmabn9c/aK40UIMR34haZpl5zta6xEmEKhUCgUCkU/oMKRCoVCoVAoFP2AEmEKhUKhUCgU/YASYQqFQqFQKBT9gBJhCoVCoVAoFP2AEmEKhUKhUCgU/YASYQqFQqFQKBT9gBJhCoWi3xBCJAoh9h7nY34ghIg5hn1eOMbjfaj3IkII8ZmrKegxPnaaEGKHEMIuhLi62323CiGy9X+3emwfK4TYI4TIEUI85xo6LIR4Sggx81ifW6FQfP9RIkyhUHzf+AFwRBF2rAghhgJGV5NHTdPm62NxjpUC/XwWdTtuCLI58ETknMNHhRDB+t0vAT8GUvR/8/TtzwMPn9grUSgU30eUCFMoFP2NSQjxXyFEpu5K+QAIIX4nhNgqhNgrhHhFHzV0NTAO+K8QIkMI4S2EGC+E2CCE2CWE2CKE8NePGyOE+EJ3ov7Wx3PfSOcYFIQQh4UQYbpDlymEeFUIsU8I8ZU+LqcLmqYd1jRtN53d+l3MBb7WNK1G07Ra4Gtgnj7HMkDTtE36iJX/AAv0Y+UDoUKIqBN9IxUKxfcLJcIUCkV/kwr8U9O0dKAB+Im+/QVN08ZrmjYM8AYu0TTtQ2AbcKOmaaMAB7AY+JmmaSOBC4FW/fGjgOuA4cB1QgjPwd0upgDb+zivFOBFTdOGAnXAVcfxmmKBQo+fi/Rtsfrt7ttd7NDPSaFQnAMoEaZQKPqbQk3T1uu33wGm6rdnCCE2CyH2ADOBob08NhUo1TRtK4CmaQ2aptn1+1ZqmlavaVobsB9I6OXx0UBlH+eVp2lahn57O5B4PC/qBKngFIVaFQrFdx8lwhQKRX/TfYCtpg/n/SdwtaZpw4FXAetxHtfmcdsBmHrZp/UIxz2Wx/dFMeDpvMXp24r12923u7DS6eQpFIqzHCXCFApFfzNACDFZv30DsI5OYVQlhPADPCsPGwFX3tcBIFoIMR5ACOEvhDgesZQJJJ/wmffNl8AcIUSwnpA/B/hS07RSoEEIMUmvirwFj5w0YDBwXNWiCoXi+4sSYQqFor85ANwjhMgEgoGX9ArFV/+/nTvEaSgKojD8T4BVdBfYGrZQwXIwra6qaRNEN9EQVBdQBAhc66qxKDIV9wpcQ8WbkP6ffLm5OfJkMvfRCskrsPt1fg0sI+IduKHtfS0i4oO2AP+XidkGeLg0eH8UcAQegVVEfAJk5hcw67l3wLR/g7bz9gzsgQPw0u+6oxXCt0vzSPpfoj3QkaTr0188boFxZv4UZ5kA95n5VJlD0nCchEm6Wpn5Tfuf1+jc2QHcAvPqEJKG4yRMkiSpgJMwSZKkApYwSZKkApYwSZKkApYwSZKkApYwSZKkAidHCtjY55Nd2wAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# draw the training loss and validation loss\n",
        "# make figure larger\n",
        "plt.figure(figsize=(10, 8))\n",
        "draw_loss(train_loss_list, valid_loss_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "由上圖看出，使用 L2 + L1 Loss 的模型，其 Training RMSE 與 Validation RMSE 的走向與 Q2 的模型非常類似，  \n",
        "都是在 20000 個 batches 後趨於平坦，不再往下掉。但 training RMSE 還在持續變小，代表可能開始產生 overfitting。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test RMSE Loss: 8.765939005064949\n"
          ]
        }
      ],
      "source": [
        "best_q7_mlp = torch.load('q7_mlp.ckpt', map_location=torch.device('cpu'))\n",
        "test_rmse_loss = calculate_test_rmse(best_q7_mlp, X_test, Y_test)\n",
        "print(f\"Test RMSE Loss: {test_rmse_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmwgqIVMPs3w",
        "outputId": "66a54ae4-4929-4a96-a260-a208998027a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training for z = 0.0\n",
            "Epoch: 0, Step: 100, Train Loss: 10.568008681423535, Valid Loss: 10.545305744468624\n",
            "Epoch: 0, Step: 200, Train Loss: 10.244463937791938, Valid Loss: 10.213576781455055\n",
            "Epoch: 0, Step: 300, Train Loss: 9.458251384860587, Valid Loss: 9.41430764362261\n",
            "Epoch: 0, Step: 400, Train Loss: 9.41171436190192, Valid Loss: 9.3738754802239\n",
            "Epoch: 1, Step: 500, Train Loss: 9.324685968835528, Valid Loss: 9.279818341572689\n",
            "Epoch: 1, Step: 600, Train Loss: 9.257497427823829, Valid Loss: 9.21137516502447\n",
            "Epoch: 1, Step: 700, Train Loss: 9.187845242226022, Valid Loss: 9.137731225882197\n",
            "Epoch: 1, Step: 800, Train Loss: 9.207033785250431, Valid Loss: 9.162330928393407\n",
            "Epoch: 2, Step: 900, Train Loss: 9.12120883401235, Valid Loss: 9.073539669715084\n",
            "Epoch: 2, Step: 1000, Train Loss: 9.125982152440171, Valid Loss: 9.08499711129202\n",
            "Epoch: 2, Step: 1100, Train Loss: 9.103000430873472, Valid Loss: 9.059953323932415\n",
            "Epoch: 2, Step: 1200, Train Loss: 9.051066813343297, Valid Loss: 9.00396355367468\n",
            "Epoch: 3, Step: 1300, Train Loss: 9.069172351417503, Valid Loss: 9.02543757492117\n",
            "Epoch: 3, Step: 1400, Train Loss: 9.059770462653955, Valid Loss: 9.016549739616597\n",
            "Epoch: 3, Step: 1500, Train Loss: 9.053576504374146, Valid Loss: 9.0120181001815\n",
            "Epoch: 3, Step: 1600, Train Loss: 9.034965820198536, Valid Loss: 8.991277202156516\n",
            "Epoch: 4, Step: 1700, Train Loss: 9.056558181945283, Valid Loss: 9.01236510926649\n",
            "Epoch: 4, Step: 1800, Train Loss: 9.048443046478402, Valid Loss: 9.011587814382588\n",
            "Epoch: 4, Step: 1900, Train Loss: 9.036841224221755, Valid Loss: 8.996869414751762\n",
            "Epoch: 4, Step: 2000, Train Loss: 9.001697685072601, Valid Loss: 8.965930447832383\n",
            "Epoch: 5, Step: 2100, Train Loss: 9.055739046530853, Valid Loss: 9.021797917576093\n",
            "Epoch: 5, Step: 2200, Train Loss: 9.018207074859253, Valid Loss: 8.985562985082927\n",
            "Epoch: 5, Step: 2300, Train Loss: 9.064531047605689, Valid Loss: 9.036850094400188\n",
            "Epoch: 5, Step: 2400, Train Loss: 8.989571650810467, Valid Loss: 8.957394646181877\n",
            "Epoch: 5, Step: 2500, Train Loss: 9.009571853418503, Valid Loss: 8.981133879516786\n",
            "Epoch: 6, Step: 2600, Train Loss: 8.962002016825334, Valid Loss: 8.933120347263088\n",
            "Epoch: 6, Step: 2700, Train Loss: 8.991058518932011, Valid Loss: 8.972025350830352\n",
            "Epoch: 6, Step: 2800, Train Loss: 9.000363095904605, Valid Loss: 8.974688633098191\n",
            "Epoch: 6, Step: 2900, Train Loss: 8.988594586148185, Valid Loss: 8.961012039370795\n",
            "Epoch: 7, Step: 3000, Train Loss: 9.005135716759375, Valid Loss: 8.979962767611395\n",
            "Epoch: 7, Step: 3100, Train Loss: 8.993230703765905, Valid Loss: 8.972871647363053\n",
            "Epoch: 7, Step: 3200, Train Loss: 9.024202071940154, Valid Loss: 9.004484468249217\n",
            "Epoch: 7, Step: 3300, Train Loss: 8.994631587248435, Valid Loss: 8.969933196202637\n",
            "Epoch: 8, Step: 3400, Train Loss: 8.97403634239809, Valid Loss: 8.95771432951646\n",
            "Epoch: 8, Step: 3500, Train Loss: 8.975843334736911, Valid Loss: 8.958588596025546\n",
            "Epoch: 8, Step: 3600, Train Loss: 8.983879257744496, Valid Loss: 8.960533535020986\n",
            "Epoch: 8, Step: 3700, Train Loss: 8.99121780303113, Valid Loss: 8.973219266726712\n",
            "Epoch: 9, Step: 3800, Train Loss: 8.990379487801233, Valid Loss: 8.971412866093829\n",
            "Epoch: 9, Step: 3900, Train Loss: 8.95808356714481, Valid Loss: 8.934445183617528\n",
            "Epoch: 9, Step: 4000, Train Loss: 8.95637098969878, Valid Loss: 8.938581298006095\n",
            "Epoch: 9, Step: 4100, Train Loss: 8.952853564872347, Valid Loss: 8.931529336355315\n",
            "Epoch: 10, Step: 4200, Train Loss: 8.962097680718792, Valid Loss: 8.939125753406596\n",
            "Epoch: 10, Step: 4300, Train Loss: 8.933568855016963, Valid Loss: 8.91351498894652\n",
            "Epoch: 10, Step: 4400, Train Loss: 8.969823513367507, Valid Loss: 8.950103255611387\n",
            "Epoch: 10, Step: 4500, Train Loss: 8.920566417274955, Valid Loss: 8.901471724391662\n",
            "Epoch: 11, Step: 4600, Train Loss: 8.957090530018815, Valid Loss: 8.93913247716872\n",
            "Epoch: 11, Step: 4700, Train Loss: 8.948823144835169, Valid Loss: 8.935012263665566\n",
            "Epoch: 11, Step: 4800, Train Loss: 8.927014300682197, Valid Loss: 8.914369665727829\n",
            "Epoch: 11, Step: 4900, Train Loss: 8.984068341783338, Valid Loss: 8.968731685761348\n",
            "Epoch: 11, Step: 5000, Train Loss: 8.929733924149563, Valid Loss: 8.91484853127954\n",
            "Epoch: 12, Step: 5100, Train Loss: 8.91356516626104, Valid Loss: 8.902764207347971\n",
            "Epoch: 12, Step: 5200, Train Loss: 8.942698493150553, Valid Loss: 8.929843184608103\n",
            "Epoch: 12, Step: 5300, Train Loss: 8.978610416815926, Valid Loss: 8.972268477786026\n",
            "Epoch: 12, Step: 5400, Train Loss: 8.93269558499123, Valid Loss: 8.91795616654454\n",
            "Epoch: 13, Step: 5500, Train Loss: 8.937219642690632, Valid Loss: 8.927636691590722\n",
            "Epoch: 13, Step: 5600, Train Loss: 8.90288572488514, Valid Loss: 8.896356925110426\n",
            "Epoch: 13, Step: 5700, Train Loss: 8.930622624781268, Valid Loss: 8.920176750112386\n",
            "Epoch: 13, Step: 5800, Train Loss: 8.878030880768737, Valid Loss: 8.87056223451555\n",
            "Epoch: 14, Step: 5900, Train Loss: 8.886619994514778, Valid Loss: 8.88176279372913\n",
            "Epoch: 14, Step: 6000, Train Loss: 8.939505932041943, Valid Loss: 8.934778095626116\n",
            "Epoch: 14, Step: 6100, Train Loss: 8.874316016997234, Valid Loss: 8.864625544629924\n",
            "Epoch: 14, Step: 6200, Train Loss: 8.936341308949435, Valid Loss: 8.929098233113967\n",
            "Epoch: 15, Step: 6300, Train Loss: 8.920965163910953, Valid Loss: 8.911768205531843\n",
            "Epoch: 15, Step: 6400, Train Loss: 8.929795806219163, Valid Loss: 8.922785081662154\n",
            "Epoch: 15, Step: 6500, Train Loss: 8.898481513855657, Valid Loss: 8.894358993469936\n",
            "Epoch: 15, Step: 6600, Train Loss: 8.910562060015575, Valid Loss: 8.90700826479098\n",
            "Epoch: 16, Step: 6700, Train Loss: 8.902065921166312, Valid Loss: 8.891053097393804\n",
            "Epoch: 16, Step: 6800, Train Loss: 8.913803043319371, Valid Loss: 8.910280561868591\n",
            "Epoch: 16, Step: 6900, Train Loss: 8.903202487058364, Valid Loss: 8.896049243148248\n",
            "Epoch: 16, Step: 7000, Train Loss: 8.914290223131161, Valid Loss: 8.918874696142485\n",
            "Epoch: 16, Step: 7100, Train Loss: 8.898645216098053, Valid Loss: 8.896307949249602\n",
            "Epoch: 17, Step: 7200, Train Loss: 8.867656819611824, Valid Loss: 8.865564004419127\n",
            "Epoch: 17, Step: 7300, Train Loss: 8.909363074432575, Valid Loss: 8.908687627912421\n",
            "Epoch: 17, Step: 7400, Train Loss: 8.890789141144817, Valid Loss: 8.885120989912929\n",
            "Epoch: 17, Step: 7500, Train Loss: 8.899167279657526, Valid Loss: 8.895200401166662\n",
            "Epoch: 18, Step: 7600, Train Loss: 8.901390486459727, Valid Loss: 8.895538034777418\n",
            "Epoch: 18, Step: 7700, Train Loss: 8.895090391308967, Valid Loss: 8.897100113552517\n",
            "Epoch: 18, Step: 7800, Train Loss: 8.904233718537341, Valid Loss: 8.898675937975915\n",
            "Epoch: 18, Step: 7900, Train Loss: 8.886684157945629, Valid Loss: 8.881609311866331\n",
            "Epoch: 19, Step: 8000, Train Loss: 8.872055574786039, Valid Loss: 8.870633163403193\n",
            "Epoch: 19, Step: 8100, Train Loss: 8.860985704822019, Valid Loss: 8.862927897825902\n",
            "Epoch: 19, Step: 8200, Train Loss: 8.865193204520997, Valid Loss: 8.863670919221716\n",
            "Epoch: 19, Step: 8300, Train Loss: 8.896491514883401, Valid Loss: 8.898058920300635\n",
            "Epoch: 20, Step: 8400, Train Loss: 8.864885954197842, Valid Loss: 8.864975284812923\n",
            "Epoch: 20, Step: 8500, Train Loss: 8.880775603646397, Valid Loss: 8.882478884858104\n",
            "Epoch: 20, Step: 8600, Train Loss: 8.898347898584372, Valid Loss: 8.902810216814112\n",
            "Epoch: 20, Step: 8700, Train Loss: 8.87213566380315, Valid Loss: 8.8772798737507\n",
            "Epoch: 21, Step: 8800, Train Loss: 8.938298672577728, Valid Loss: 8.952945731873863\n",
            "Epoch: 21, Step: 8900, Train Loss: 8.934087882809322, Valid Loss: 8.940713196006454\n",
            "Epoch: 21, Step: 9000, Train Loss: 8.919910653350893, Valid Loss: 8.924022713608714\n",
            "Epoch: 21, Step: 9100, Train Loss: 8.910220651126389, Valid Loss: 8.913645544864906\n",
            "Epoch: 22, Step: 9200, Train Loss: 8.871472788999448, Valid Loss: 8.876804801149587\n",
            "Epoch: 22, Step: 9300, Train Loss: 8.886135019007229, Valid Loss: 8.890171632706007\n",
            "Epoch: 22, Step: 9400, Train Loss: 8.892943207707248, Valid Loss: 8.896715665248037\n",
            "Epoch: 22, Step: 9500, Train Loss: 8.92579307335417, Valid Loss: 8.923205543230345\n",
            "Epoch: 22, Step: 9600, Train Loss: 8.85827448666793, Valid Loss: 8.85933727894103\n",
            "Epoch: 23, Step: 9700, Train Loss: 8.864152697779982, Valid Loss: 8.862904091009353\n",
            "Epoch: 23, Step: 9800, Train Loss: 8.904019080402247, Valid Loss: 8.904609050238369\n",
            "Epoch: 23, Step: 9900, Train Loss: 8.89566251326914, Valid Loss: 8.9059295109707\n",
            "Epoch: 23, Step: 10000, Train Loss: 8.883577676417175, Valid Loss: 8.888464729601766\n",
            "Epoch: 24, Step: 10100, Train Loss: 8.884095972427435, Valid Loss: 8.888430362302792\n",
            "Epoch: 24, Step: 10200, Train Loss: 8.871023313494943, Valid Loss: 8.883361959209463\n",
            "Epoch: 24, Step: 10300, Train Loss: 8.925198278434697, Valid Loss: 8.93792713726135\n",
            "Epoch: 24, Step: 10400, Train Loss: 8.857603215718129, Valid Loss: 8.864526315473894\n",
            "Epoch: 25, Step: 10500, Train Loss: 8.912770361911212, Valid Loss: 8.929837236827007\n",
            "Epoch: 25, Step: 10600, Train Loss: 8.87167833272151, Valid Loss: 8.88847224039674\n",
            "Epoch: 25, Step: 10700, Train Loss: 8.888528725632767, Valid Loss: 8.901165489748706\n",
            "Epoch: 25, Step: 10800, Train Loss: 8.896801433173886, Valid Loss: 8.913384090885202\n",
            "Epoch: 26, Step: 10900, Train Loss: 8.879480649208578, Valid Loss: 8.885908965915714\n",
            "Epoch: 26, Step: 11000, Train Loss: 8.868292406542873, Valid Loss: 8.881312643248945\n",
            "Epoch: 26, Step: 11100, Train Loss: 8.897561693973563, Valid Loss: 8.908224993946884\n",
            "Epoch: 26, Step: 11200, Train Loss: 8.866536858178966, Valid Loss: 8.870468997634735\n",
            "Epoch: 27, Step: 11300, Train Loss: 8.86329705976371, Valid Loss: 8.869391963693916\n",
            "Epoch: 27, Step: 11400, Train Loss: 8.86875420202654, Valid Loss: 8.877378056505886\n",
            "Epoch: 27, Step: 11500, Train Loss: 8.88331692423762, Valid Loss: 8.88989159564228\n",
            "Epoch: 27, Step: 11600, Train Loss: 8.872906055086828, Valid Loss: 8.883830785546898\n",
            "Epoch: 27, Step: 11700, Train Loss: 8.884999656386652, Valid Loss: 8.892254510142534\n",
            "Epoch: 28, Step: 11800, Train Loss: 8.86723865062248, Valid Loss: 8.87593411943448\n",
            "Epoch: 28, Step: 11900, Train Loss: 8.889435104012088, Valid Loss: 8.904751774605543\n",
            "Epoch: 28, Step: 12000, Train Loss: 8.873619442492744, Valid Loss: 8.884312773935314\n",
            "Epoch: 28, Step: 12100, Train Loss: 8.886760787454437, Valid Loss: 8.904007301914229\n",
            "Epoch: 29, Step: 12200, Train Loss: 8.894941897725573, Valid Loss: 8.907632750622186\n",
            "Epoch: 29, Step: 12300, Train Loss: 8.881257570006238, Valid Loss: 8.890953218086628\n",
            "Epoch: 29, Step: 12400, Train Loss: 8.897723710853036, Valid Loss: 8.906893396564309\n",
            "Epoch: 29, Step: 12500, Train Loss: 8.850966188697656, Valid Loss: 8.868042789555906\n",
            "Epoch: 30, Step: 12600, Train Loss: 8.872500095781321, Valid Loss: 8.88450170344299\n",
            "Epoch: 30, Step: 12700, Train Loss: 8.868220022815905, Valid Loss: 8.883274373342035\n",
            "Epoch: 30, Step: 12800, Train Loss: 8.86841239182982, Valid Loss: 8.881866653130034\n",
            "Epoch: 30, Step: 12900, Train Loss: 8.879965891697568, Valid Loss: 8.889306306679206\n",
            "Epoch: 31, Step: 13000, Train Loss: 8.831905098442016, Valid Loss: 8.847149862899913\n",
            "Epoch: 31, Step: 13100, Train Loss: 8.847924976091148, Valid Loss: 8.85613983724702\n",
            "Epoch: 31, Step: 13200, Train Loss: 8.881345439234785, Valid Loss: 8.889425062395409\n",
            "Epoch: 31, Step: 13300, Train Loss: 8.843219674968353, Valid Loss: 8.853624787111578\n",
            "Epoch: 32, Step: 13400, Train Loss: 8.87541148350965, Valid Loss: 8.88966818600768\n",
            "Epoch: 32, Step: 13500, Train Loss: 8.841751162809668, Valid Loss: 8.846155999011255\n",
            "Epoch: 32, Step: 13600, Train Loss: 8.842043979689, Valid Loss: 8.8499984496202\n",
            "Epoch: 32, Step: 13700, Train Loss: 8.843841371664816, Valid Loss: 8.846750379677921\n",
            "Epoch: 33, Step: 13800, Train Loss: 8.852511286068076, Valid Loss: 8.863530398811951\n",
            "Epoch: 33, Step: 13900, Train Loss: 8.863447786833568, Valid Loss: 8.875205023668032\n",
            "Epoch: 33, Step: 14000, Train Loss: 8.852796506870545, Valid Loss: 8.864239569090072\n",
            "Epoch: 33, Step: 14100, Train Loss: 8.866012646773978, Valid Loss: 8.87217372540107\n",
            "Epoch: 33, Step: 14200, Train Loss: 8.865090913690471, Valid Loss: 8.877146403529066\n",
            "Epoch: 34, Step: 14300, Train Loss: 8.818906681409885, Valid Loss: 8.832944479602974\n",
            "Epoch: 34, Step: 14400, Train Loss: 8.842553967503266, Valid Loss: 8.861217458031968\n",
            "Epoch: 34, Step: 14500, Train Loss: 8.888312950846096, Valid Loss: 8.904823426824377\n",
            "Epoch: 34, Step: 14600, Train Loss: 8.879742482721273, Valid Loss: 8.895437692084712\n",
            "Epoch: 35, Step: 14700, Train Loss: 8.827662963224327, Valid Loss: 8.844717139062656\n",
            "Epoch: 35, Step: 14800, Train Loss: 8.881993446401536, Valid Loss: 8.90130458796063\n",
            "Epoch: 35, Step: 14900, Train Loss: 8.862658128226293, Valid Loss: 8.880086155403923\n",
            "Epoch: 35, Step: 15000, Train Loss: 8.856217883575678, Valid Loss: 8.875160150081719\n",
            "Epoch: 36, Step: 15100, Train Loss: 8.858860831469807, Valid Loss: 8.877996037067986\n",
            "Epoch: 36, Step: 15200, Train Loss: 8.870377458621606, Valid Loss: 8.892136817713299\n",
            "Epoch: 36, Step: 15300, Train Loss: 8.840896482519794, Valid Loss: 8.862096603100834\n",
            "Epoch: 36, Step: 15400, Train Loss: 8.828855219343422, Valid Loss: 8.844857965506314\n",
            "Epoch: 37, Step: 15500, Train Loss: 8.841903788129134, Valid Loss: 8.863806671293148\n",
            "Epoch: 37, Step: 15600, Train Loss: 8.843970718829379, Valid Loss: 8.861339035693753\n",
            "Epoch: 37, Step: 15700, Train Loss: 8.88804580167539, Valid Loss: 8.91035420934277\n",
            "Epoch: 37, Step: 15800, Train Loss: 8.835901876140603, Valid Loss: 8.851913558440888\n",
            "Epoch: 38, Step: 15900, Train Loss: 8.878666131716807, Valid Loss: 8.899180003898318\n",
            "Epoch: 38, Step: 16000, Train Loss: 8.881421817881066, Valid Loss: 8.900581434291372\n",
            "Epoch: 38, Step: 16100, Train Loss: 8.822770956018227, Valid Loss: 8.845359550868125\n",
            "Epoch: 38, Step: 16200, Train Loss: 8.85635158807159, Valid Loss: 8.87791602947941\n",
            "Epoch: 38, Step: 16300, Train Loss: 8.882063401664006, Valid Loss: 8.901694977370942\n",
            "Epoch: 39, Step: 16400, Train Loss: 8.845068331202919, Valid Loss: 8.864200781496253\n",
            "Epoch: 39, Step: 16500, Train Loss: 8.864671282464418, Valid Loss: 8.882234898210417\n",
            "Epoch: 39, Step: 16600, Train Loss: 8.858954546511779, Valid Loss: 8.883852203646672\n",
            "Epoch: 39, Step: 16700, Train Loss: 8.860893508307061, Valid Loss: 8.884860815590296\n",
            "Epoch: 40, Step: 16800, Train Loss: 8.83000114479297, Valid Loss: 8.849469038718881\n",
            "Epoch: 40, Step: 16900, Train Loss: 8.894371600449556, Valid Loss: 8.918912936640591\n",
            "Epoch: 40, Step: 17000, Train Loss: 8.854861202286191, Valid Loss: 8.878998751301467\n",
            "Epoch: 40, Step: 17100, Train Loss: 8.812530634190281, Valid Loss: 8.833091911980263\n",
            "Epoch: 41, Step: 17200, Train Loss: 8.822343063851363, Valid Loss: 8.84116593334853\n",
            "Epoch: 41, Step: 17300, Train Loss: 8.85045131776686, Valid Loss: 8.86625214766686\n",
            "Epoch: 41, Step: 17400, Train Loss: 8.837586188639806, Valid Loss: 8.857770875396021\n",
            "Epoch: 41, Step: 17500, Train Loss: 8.82230746201547, Valid Loss: 8.842149830805909\n",
            "Epoch: 42, Step: 17600, Train Loss: 8.828142480416059, Valid Loss: 8.84297056909118\n",
            "Epoch: 42, Step: 17700, Train Loss: 8.846628486504564, Valid Loss: 8.86509790608256\n",
            "Epoch: 42, Step: 17800, Train Loss: 8.848738368863755, Valid Loss: 8.864523409949165\n",
            "Epoch: 42, Step: 17900, Train Loss: 8.817507783907965, Valid Loss: 8.836978933642136\n",
            "Epoch: 43, Step: 18000, Train Loss: 8.83562762984761, Valid Loss: 8.857451764778704\n",
            "Epoch: 43, Step: 18100, Train Loss: 8.844433064601803, Valid Loss: 8.868312481222706\n",
            "Epoch: 43, Step: 18200, Train Loss: 8.852551798910067, Valid Loss: 8.880733917315279\n",
            "Epoch: 43, Step: 18300, Train Loss: 8.830713343127202, Valid Loss: 8.850080917873955\n",
            "Epoch: 44, Step: 18400, Train Loss: 8.87139766883279, Valid Loss: 8.893590634160747\n",
            "Epoch: 44, Step: 18500, Train Loss: 8.86168905989651, Valid Loss: 8.884199153075906\n",
            "Epoch: 44, Step: 18600, Train Loss: 8.871362210314789, Valid Loss: 8.894518148594942\n",
            "Epoch: 44, Step: 18700, Train Loss: 8.898176423291812, Valid Loss: 8.918583016926881\n",
            "Epoch: 44, Step: 18800, Train Loss: 8.820856961952831, Valid Loss: 8.843221456957984\n",
            "Epoch: 45, Step: 18900, Train Loss: 8.839272753818758, Valid Loss: 8.870458067070471\n",
            "Epoch: 45, Step: 19000, Train Loss: 8.8243354942412, Valid Loss: 8.843532574794555\n",
            "Epoch: 45, Step: 19100, Train Loss: 8.80709584762339, Valid Loss: 8.828848164750552\n",
            "Epoch: 45, Step: 19200, Train Loss: 8.851389631134197, Valid Loss: 8.871386217245712\n",
            "Epoch: 46, Step: 19300, Train Loss: 8.83006116910327, Valid Loss: 8.852925473730652\n",
            "Epoch: 46, Step: 19400, Train Loss: 8.847172173716622, Valid Loss: 8.873339181641942\n",
            "Epoch: 46, Step: 19500, Train Loss: 8.812931756691393, Valid Loss: 8.842135680832888\n",
            "Epoch: 46, Step: 19600, Train Loss: 8.80836552934243, Valid Loss: 8.833811929422874\n",
            "Epoch: 47, Step: 19700, Train Loss: 8.8430476387344, Valid Loss: 8.86389461821396\n",
            "Epoch: 47, Step: 19800, Train Loss: 8.806986780392545, Valid Loss: 8.825027108043955\n",
            "Epoch: 47, Step: 19900, Train Loss: 8.891806340718494, Valid Loss: 8.912444106857949\n",
            "Epoch: 47, Step: 20000, Train Loss: 8.855622179844211, Valid Loss: 8.878972916932891\n",
            "Epoch: 48, Step: 20100, Train Loss: 8.850569355560532, Valid Loss: 8.875057486454784\n",
            "Epoch: 48, Step: 20200, Train Loss: 8.83311162786134, Valid Loss: 8.862649231217805\n",
            "Epoch: 48, Step: 20300, Train Loss: 8.850077995937585, Valid Loss: 8.87997179032771\n",
            "Epoch: 48, Step: 20400, Train Loss: 8.858542379926485, Valid Loss: 8.876005045730219\n",
            "Epoch: 49, Step: 20500, Train Loss: 8.838268570263637, Valid Loss: 8.86087393199393\n",
            "Epoch: 49, Step: 20600, Train Loss: 8.84899607249253, Valid Loss: 8.881254355484995\n",
            "Epoch: 49, Step: 20700, Train Loss: 8.841412549544259, Valid Loss: 8.867207205315383\n",
            "Epoch: 49, Step: 20800, Train Loss: 8.841770915273292, Valid Loss: 8.871590455602046\n",
            "Epoch: 49, Step: 20900, Train Loss: 8.861627572548562, Valid Loss: 8.890355939294166\n",
            "Epoch: 50, Step: 21000, Train Loss: 8.808433101788857, Valid Loss: 8.833225506663277\n",
            "Epoch: 50, Step: 21100, Train Loss: 8.828839639045151, Valid Loss: 8.857427446316906\n",
            "Epoch: 50, Step: 21200, Train Loss: 8.822120634762895, Valid Loss: 8.848792715784993\n",
            "Epoch: 50, Step: 21300, Train Loss: 8.838282995510244, Valid Loss: 8.86991412390779\n",
            "Epoch: 51, Step: 21400, Train Loss: 8.857257596544992, Valid Loss: 8.888386484273763\n",
            "Epoch: 51, Step: 21500, Train Loss: 8.804274861189338, Valid Loss: 8.83505212794878\n",
            "Epoch: 51, Step: 21600, Train Loss: 8.824741149707721, Valid Loss: 8.849203073160291\n",
            "Epoch: 51, Step: 21700, Train Loss: 8.832003826028254, Valid Loss: 8.855341212037224\n",
            "Epoch: 52, Step: 21800, Train Loss: 8.8197383918807, Valid Loss: 8.841755437974216\n",
            "Epoch: 52, Step: 21900, Train Loss: 8.843440408356823, Valid Loss: 8.87458920755763\n",
            "Epoch: 52, Step: 22000, Train Loss: 8.834261999061713, Valid Loss: 8.862129565898597\n",
            "Epoch: 52, Step: 22100, Train Loss: 8.820401781249712, Valid Loss: 8.841882256483457\n",
            "Epoch: 53, Step: 22200, Train Loss: 8.831663384203528, Valid Loss: 8.859697415837845\n",
            "Epoch: 53, Step: 22300, Train Loss: 8.810256332498673, Valid Loss: 8.83768009029118\n",
            "Epoch: 53, Step: 22400, Train Loss: 8.836142144266946, Valid Loss: 8.87134944548606\n",
            "Epoch: 53, Step: 22500, Train Loss: 8.833571654402466, Valid Loss: 8.86088106451141\n",
            "Epoch: 54, Step: 22600, Train Loss: 8.810319001435532, Valid Loss: 8.840628773206877\n",
            "Epoch: 54, Step: 22700, Train Loss: 8.811695808306748, Valid Loss: 8.83289286564912\n",
            "Epoch: 54, Step: 22800, Train Loss: 8.846210110414454, Valid Loss: 8.87356345806842\n",
            "Epoch: 54, Step: 22900, Train Loss: 8.841118566541006, Valid Loss: 8.868184373597893\n",
            "Epoch: 55, Step: 23000, Train Loss: 8.847562270749558, Valid Loss: 8.870730880810605\n",
            "Epoch: 55, Step: 23100, Train Loss: 8.843516021601369, Valid Loss: 8.866781033896675\n",
            "Epoch: 55, Step: 23200, Train Loss: 8.804157768949294, Valid Loss: 8.826417058318862\n",
            "Epoch: 55, Step: 23300, Train Loss: 8.824648659109803, Valid Loss: 8.861639650191554\n",
            "Epoch: 55, Step: 23400, Train Loss: 8.826520127811817, Valid Loss: 8.858234817798229\n",
            "Epoch: 56, Step: 23500, Train Loss: 8.824787760293415, Valid Loss: 8.858434951729643\n",
            "Epoch: 56, Step: 23600, Train Loss: 8.828520298361436, Valid Loss: 8.857710613880805\n",
            "Epoch: 56, Step: 23700, Train Loss: 8.826398094274513, Valid Loss: 8.852371805610026\n",
            "Epoch: 56, Step: 23800, Train Loss: 8.84404003294614, Valid Loss: 8.870992599571112\n",
            "Epoch: 57, Step: 23900, Train Loss: 8.82056922489569, Valid Loss: 8.849207513957595\n",
            "Epoch: 57, Step: 24000, Train Loss: 8.833092329157461, Valid Loss: 8.853012853160351\n",
            "Epoch: 57, Step: 24100, Train Loss: 8.821630947845131, Valid Loss: 8.845041809155411\n",
            "Epoch: 57, Step: 24200, Train Loss: 8.867764860677037, Valid Loss: 8.891859241762823\n",
            "Epoch: 58, Step: 24300, Train Loss: 8.833274742358249, Valid Loss: 8.864451154018019\n",
            "Epoch: 58, Step: 24400, Train Loss: 8.80282543286139, Valid Loss: 8.827903344687824\n",
            "Epoch: 58, Step: 24500, Train Loss: 8.852179761518428, Valid Loss: 8.881649526746699\n",
            "Epoch: 58, Step: 24600, Train Loss: 8.827361207994707, Valid Loss: 8.85888665219548\n",
            "Epoch: 59, Step: 24700, Train Loss: 8.802525498809786, Valid Loss: 8.838180324493559\n",
            "Epoch: 59, Step: 24800, Train Loss: 8.807143850008291, Valid Loss: 8.842418726006247\n",
            "Epoch: 59, Step: 24900, Train Loss: 8.83490560251111, Valid Loss: 8.866259755662517\n",
            "Epoch: 59, Step: 25000, Train Loss: 8.814616059920876, Valid Loss: 8.850250635007878\n",
            "Epoch: 60, Step: 25100, Train Loss: 8.843418274011588, Valid Loss: 8.873082729940435\n",
            "Epoch: 60, Step: 25200, Train Loss: 8.82143184344414, Valid Loss: 8.852268506568826\n",
            "Epoch: 60, Step: 25300, Train Loss: 8.823595508559679, Valid Loss: 8.849022055226705\n",
            "Epoch: 60, Step: 25400, Train Loss: 8.808526784590708, Valid Loss: 8.834985435249843\n",
            "Epoch: 61, Step: 25500, Train Loss: 8.80682266063159, Valid Loss: 8.835573800563438\n",
            "Epoch: 61, Step: 25600, Train Loss: 8.822995409237594, Valid Loss: 8.845986265217128\n",
            "Epoch: 61, Step: 25700, Train Loss: 8.823515619399043, Valid Loss: 8.861551923297537\n",
            "Epoch: 61, Step: 25800, Train Loss: 8.807699693054552, Valid Loss: 8.834995491985204\n",
            "Epoch: 61, Step: 25900, Train Loss: 8.815950775666021, Valid Loss: 8.839873916339842\n",
            "Epoch: 62, Step: 26000, Train Loss: 8.837176082898093, Valid Loss: 8.86708731127107\n",
            "Epoch: 62, Step: 26100, Train Loss: 8.83509585720291, Valid Loss: 8.870691808221316\n",
            "Epoch: 62, Step: 26200, Train Loss: 8.846171883036975, Valid Loss: 8.873317405892797\n",
            "Epoch: 62, Step: 26300, Train Loss: 8.84059586583839, Valid Loss: 8.871543507973543\n",
            "Epoch: 63, Step: 26400, Train Loss: 8.81282425441688, Valid Loss: 8.844262314693895\n",
            "Epoch: 63, Step: 26500, Train Loss: 8.796215241505818, Valid Loss: 8.83021150075764\n",
            "Epoch: 63, Step: 26600, Train Loss: 8.803167398472112, Valid Loss: 8.835423175108836\n",
            "Epoch: 63, Step: 26700, Train Loss: 8.831029038294941, Valid Loss: 8.865619608947185\n",
            "Epoch: 64, Step: 26800, Train Loss: 8.810855500991305, Valid Loss: 8.84675777115495\n",
            "Epoch: 64, Step: 26900, Train Loss: 8.801363525039045, Valid Loss: 8.839585507816894\n",
            "Epoch: 64, Step: 27000, Train Loss: 8.787787084091988, Valid Loss: 8.820229866604882\n",
            "Epoch: 64, Step: 27100, Train Loss: 8.82426256379351, Valid Loss: 8.865310092912543\n",
            "Epoch: 65, Step: 27200, Train Loss: 8.785604415525754, Valid Loss: 8.824966355661944\n",
            "Epoch: 65, Step: 27300, Train Loss: 8.83869008137424, Valid Loss: 8.872440189185815\n",
            "Epoch: 65, Step: 27400, Train Loss: 8.807308338396133, Valid Loss: 8.840464433796962\n",
            "Epoch: 65, Step: 27500, Train Loss: 8.811830174540862, Valid Loss: 8.852483460761622\n",
            "Epoch: 66, Step: 27600, Train Loss: 8.842549372053114, Valid Loss: 8.875926116374927\n",
            "Epoch: 66, Step: 27700, Train Loss: 8.810691726045308, Valid Loss: 8.847707059236054\n",
            "Epoch: 66, Step: 27800, Train Loss: 8.82400729664583, Valid Loss: 8.866811815543132\n",
            "Epoch: 66, Step: 27900, Train Loss: 8.82723603690499, Valid Loss: 8.866801806769503\n",
            "Epoch: 66, Step: 28000, Train Loss: 8.823639637179436, Valid Loss: 8.858827289744161\n",
            "Epoch: 67, Step: 28100, Train Loss: 8.779439794397137, Valid Loss: 8.819806933861472\n",
            "Epoch: 67, Step: 28200, Train Loss: 8.82984033631181, Valid Loss: 8.859949396467638\n",
            "Epoch: 67, Step: 28300, Train Loss: 8.835174231356891, Valid Loss: 8.869144031538553\n",
            "Epoch: 67, Step: 28400, Train Loss: 8.805357573299741, Valid Loss: 8.840615433116191\n",
            "Epoch: 68, Step: 28500, Train Loss: 8.817943620800872, Valid Loss: 8.856320961758636\n",
            "Epoch: 68, Step: 28600, Train Loss: 8.8092823406533, Valid Loss: 8.84900208072208\n",
            "Epoch: 68, Step: 28700, Train Loss: 8.83265831639364, Valid Loss: 8.86594362761038\n",
            "Epoch: 68, Step: 28800, Train Loss: 8.823560244859607, Valid Loss: 8.855825397838307\n",
            "Epoch: 69, Step: 28900, Train Loss: 8.849777969562473, Valid Loss: 8.884817732949113\n",
            "Epoch: 69, Step: 29000, Train Loss: 8.79732633100728, Valid Loss: 8.830641910133062\n",
            "Epoch: 69, Step: 29100, Train Loss: 8.799988282626597, Valid Loss: 8.831370895702138\n",
            "Epoch: 69, Step: 29200, Train Loss: 8.787737428121412, Valid Loss: 8.81823311917338\n",
            "Epoch: 70, Step: 29300, Train Loss: 8.830386258891659, Valid Loss: 8.869076111127391\n",
            "Epoch: 70, Step: 29400, Train Loss: 8.829442333724504, Valid Loss: 8.862049921238754\n",
            "Epoch: 70, Step: 29500, Train Loss: 8.83418755787512, Valid Loss: 8.877337200731121\n",
            "Epoch: 70, Step: 29600, Train Loss: 8.818534102086954, Valid Loss: 8.853995557817761\n",
            "Epoch: 71, Step: 29700, Train Loss: 8.827565127037534, Valid Loss: 8.865086638689469\n",
            "Epoch: 71, Step: 29800, Train Loss: 8.773100872542926, Valid Loss: 8.808087896341664\n",
            "Epoch: 71, Step: 29900, Train Loss: 8.836602724298793, Valid Loss: 8.875227167326562\n",
            "Epoch: 71, Step: 30000, Train Loss: 8.795573124456194, Valid Loss: 8.831899502927907\n",
            "Epoch: 72, Step: 30100, Train Loss: 8.81935501099401, Valid Loss: 8.850599558376034\n",
            "Epoch: 72, Step: 30200, Train Loss: 8.811286611685203, Valid Loss: 8.850781705205506\n",
            "Epoch: 72, Step: 30300, Train Loss: 8.799474180216624, Valid Loss: 8.83945071795428\n",
            "Epoch: 72, Step: 30400, Train Loss: 8.822519817691367, Valid Loss: 8.861999624607874\n",
            "Epoch: 72, Step: 30500, Train Loss: 8.78728125746263, Valid Loss: 8.829875168612354\n",
            "Epoch: 73, Step: 30600, Train Loss: 8.775477095235843, Valid Loss: 8.816386108385059\n",
            "Epoch: 73, Step: 30700, Train Loss: 8.765944893386052, Valid Loss: 8.814885881963953\n",
            "Epoch: 73, Step: 30800, Train Loss: 8.78895016322891, Valid Loss: 8.829546119824165\n",
            "Epoch: 73, Step: 30900, Train Loss: 8.775027852093892, Valid Loss: 8.812174123788228\n",
            "Epoch: 74, Step: 31000, Train Loss: 8.806389710311294, Valid Loss: 8.838605637596244\n",
            "Epoch: 74, Step: 31100, Train Loss: 8.827231033735641, Valid Loss: 8.861851204513334\n",
            "Epoch: 74, Step: 31200, Train Loss: 8.79825133010555, Valid Loss: 8.842207366492266\n",
            "Epoch: 74, Step: 31300, Train Loss: 8.79487398200472, Valid Loss: 8.83154849582627\n",
            "Epoch: 75, Step: 31400, Train Loss: 8.799447968023255, Valid Loss: 8.831924139668734\n",
            "Epoch: 75, Step: 31500, Train Loss: 8.792580030204705, Valid Loss: 8.827840560477075\n",
            "Epoch: 75, Step: 31600, Train Loss: 8.846434295862641, Valid Loss: 8.885521499746146\n",
            "Epoch: 75, Step: 31700, Train Loss: 8.822260539143894, Valid Loss: 8.860225547673721\n",
            "Epoch: 76, Step: 31800, Train Loss: 8.80442110523481, Valid Loss: 8.841130779537847\n",
            "Epoch: 76, Step: 31900, Train Loss: 8.823290722206515, Valid Loss: 8.86031145038354\n",
            "Epoch: 76, Step: 32000, Train Loss: 8.795916463260982, Valid Loss: 8.829958983936915\n",
            "Epoch: 76, Step: 32100, Train Loss: 8.81090136520667, Valid Loss: 8.8439932826652\n",
            "Epoch: 77, Step: 32200, Train Loss: 8.792519470850618, Valid Loss: 8.820327984829007\n",
            "Epoch: 77, Step: 32300, Train Loss: 8.83683973149157, Valid Loss: 8.868418701857282\n",
            "Epoch: 77, Step: 32400, Train Loss: 8.798057050422482, Valid Loss: 8.833428591315785\n",
            "Epoch: 77, Step: 32500, Train Loss: 8.795015513321564, Valid Loss: 8.831695333904692\n",
            "Epoch: 77, Step: 32600, Train Loss: 8.826158091122778, Valid Loss: 8.858372733389066\n",
            "Epoch: 78, Step: 32700, Train Loss: 8.825469643810543, Valid Loss: 8.861501816340331\n",
            "Epoch: 78, Step: 32800, Train Loss: 8.802344518951433, Valid Loss: 8.83556744132946\n",
            "Epoch: 78, Step: 32900, Train Loss: 8.836227742240428, Valid Loss: 8.871978838768682\n",
            "Epoch: 78, Step: 33000, Train Loss: 8.799190151416228, Valid Loss: 8.830621502936792\n",
            "Epoch: 79, Step: 33100, Train Loss: 8.795519778594025, Valid Loss: 8.837843876217281\n",
            "Epoch: 79, Step: 33200, Train Loss: 8.821107473578762, Valid Loss: 8.857663304999292\n",
            "Epoch: 79, Step: 33300, Train Loss: 8.78996181864922, Valid Loss: 8.82657795389398\n",
            "Epoch: 79, Step: 33400, Train Loss: 8.821907937659564, Valid Loss: 8.860642215933023\n",
            "Epoch: 80, Step: 33500, Train Loss: 8.795212235464327, Valid Loss: 8.830597592329896\n",
            "Epoch: 80, Step: 33600, Train Loss: 8.822733487051776, Valid Loss: 8.855539209200487\n",
            "Epoch: 80, Step: 33700, Train Loss: 8.819311471969211, Valid Loss: 8.847253919154973\n",
            "Epoch: 80, Step: 33800, Train Loss: 8.813895192097693, Valid Loss: 8.844318096007344\n",
            "Epoch: 81, Step: 33900, Train Loss: 8.784065887089117, Valid Loss: 8.815652859336982\n",
            "Epoch: 81, Step: 34000, Train Loss: 8.801013090434356, Valid Loss: 8.834260867281351\n",
            "Epoch: 81, Step: 34100, Train Loss: 8.802760589601826, Valid Loss: 8.831931982302452\n",
            "Epoch: 81, Step: 34200, Train Loss: 8.791700061575854, Valid Loss: 8.827920415961477\n",
            "Epoch: 82, Step: 34300, Train Loss: 8.796716330567238, Valid Loss: 8.83694259781463\n",
            "Epoch: 82, Step: 34400, Train Loss: 8.782481681543294, Valid Loss: 8.818136128768575\n",
            "Epoch: 82, Step: 34500, Train Loss: 8.84467382269541, Valid Loss: 8.880890034657648\n",
            "Epoch: 82, Step: 34600, Train Loss: 8.784674760755726, Valid Loss: 8.823624229391816\n",
            "Epoch: 83, Step: 34700, Train Loss: 8.80275499835086, Valid Loss: 8.844066184364479\n",
            "Epoch: 83, Step: 34800, Train Loss: 8.802152975763804, Valid Loss: 8.841171516784117\n",
            "Epoch: 83, Step: 34900, Train Loss: 8.811879681734322, Valid Loss: 8.852414282503567\n",
            "Epoch: 83, Step: 35000, Train Loss: 8.806330626030924, Valid Loss: 8.848183208594692\n",
            "Epoch: 83, Step: 35100, Train Loss: 8.77543758593707, Valid Loss: 8.815127855283986\n",
            "Epoch: 84, Step: 35200, Train Loss: 8.780451487853052, Valid Loss: 8.817464622698306\n",
            "Epoch: 84, Step: 35300, Train Loss: 8.811812207149647, Valid Loss: 8.853425441220422\n",
            "Epoch: 84, Step: 35400, Train Loss: 8.788000581731469, Valid Loss: 8.832542062543272\n",
            "Epoch: 84, Step: 35500, Train Loss: 8.79390193716532, Valid Loss: 8.83781932746604\n",
            "Epoch: 85, Step: 35600, Train Loss: 8.803160585562496, Valid Loss: 8.846582266542542\n",
            "Epoch: 85, Step: 35700, Train Loss: 8.816088064061812, Valid Loss: 8.860840893223441\n",
            "Epoch: 85, Step: 35800, Train Loss: 8.790589569784773, Valid Loss: 8.839195670354998\n",
            "Epoch: 85, Step: 35900, Train Loss: 8.812642750297696, Valid Loss: 8.864244349222805\n",
            "Epoch: 86, Step: 36000, Train Loss: 8.805476801528457, Valid Loss: 8.850245752134212\n",
            "Epoch: 86, Step: 36100, Train Loss: 8.798512467987946, Valid Loss: 8.848542452368598\n",
            "Epoch: 86, Step: 36200, Train Loss: 8.823601233255674, Valid Loss: 8.87072094768112\n",
            "Epoch: 86, Step: 36300, Train Loss: 8.811400464443652, Valid Loss: 8.857178494468732\n",
            "Epoch: 87, Step: 36400, Train Loss: 8.803070822440796, Valid Loss: 8.850607215518753\n",
            "Epoch: 87, Step: 36500, Train Loss: 8.817207926547619, Valid Loss: 8.863210445329393\n",
            "Epoch: 87, Step: 36600, Train Loss: 8.82201965025745, Valid Loss: 8.874999316595673\n",
            "Epoch: 87, Step: 36700, Train Loss: 8.796030350031042, Valid Loss: 8.846262252384893\n",
            "Epoch: 88, Step: 36800, Train Loss: 8.819309596682071, Valid Loss: 8.870977783402559\n",
            "Epoch: 88, Step: 36900, Train Loss: 8.801282490260961, Valid Loss: 8.85482316320175\n",
            "Epoch: 88, Step: 37000, Train Loss: 8.781368190702404, Valid Loss: 8.830498777351785\n",
            "Epoch: 88, Step: 37100, Train Loss: 8.769137302162301, Valid Loss: 8.81144407106834\n",
            "Epoch: 88, Step: 37200, Train Loss: 8.806521975791492, Valid Loss: 8.848071849060117\n",
            "Epoch: 89, Step: 37300, Train Loss: 8.791455792682697, Valid Loss: 8.832674749970698\n",
            "Epoch: 89, Step: 37400, Train Loss: 8.797659535089833, Valid Loss: 8.843156866795304\n",
            "Epoch: 89, Step: 37500, Train Loss: 8.79236428954397, Valid Loss: 8.834035631192924\n",
            "Epoch: 89, Step: 37600, Train Loss: 8.796496197665297, Valid Loss: 8.838604827477349\n",
            "Epoch: 90, Step: 37700, Train Loss: 8.762103770786226, Valid Loss: 8.801579171277869\n",
            "Epoch: 90, Step: 37800, Train Loss: 8.799057621751365, Valid Loss: 8.846889095604647\n",
            "Epoch: 90, Step: 37900, Train Loss: 8.837816021860032, Valid Loss: 8.880483788059275\n",
            "Epoch: 90, Step: 38000, Train Loss: 8.807556484320266, Valid Loss: 8.855385379848013\n",
            "Epoch: 91, Step: 38100, Train Loss: 8.778113853200368, Valid Loss: 8.831521504424037\n",
            "Epoch: 91, Step: 38200, Train Loss: 8.77920435631165, Valid Loss: 8.828062965533164\n",
            "Epoch: 91, Step: 38300, Train Loss: 8.794477048368389, Valid Loss: 8.84832492952783\n",
            "Epoch: 91, Step: 38400, Train Loss: 8.797770926835915, Valid Loss: 8.852775029319261\n",
            "Epoch: 92, Step: 38500, Train Loss: 8.790753917547356, Valid Loss: 8.84199028563072\n",
            "Epoch: 92, Step: 38600, Train Loss: 8.812940397270195, Valid Loss: 8.859563545241945\n",
            "Epoch: 92, Step: 38700, Train Loss: 8.797096732667097, Valid Loss: 8.846676983523253\n",
            "Epoch: 92, Step: 38800, Train Loss: 8.813014243697845, Valid Loss: 8.864001958557749\n",
            "Epoch: 93, Step: 38900, Train Loss: 8.775935704035456, Valid Loss: 8.828184870965414\n",
            "Epoch: 93, Step: 39000, Train Loss: 8.828533949971312, Valid Loss: 8.881484665920793\n",
            "Epoch: 93, Step: 39100, Train Loss: 8.817851222633555, Valid Loss: 8.868274134144954\n",
            "Epoch: 93, Step: 39200, Train Loss: 8.831177887970759, Valid Loss: 8.87370633527324\n",
            "Epoch: 94, Step: 39300, Train Loss: 8.80588010063854, Valid Loss: 8.857884412055295\n",
            "Epoch: 94, Step: 39400, Train Loss: 8.80361802122329, Valid Loss: 8.859373187745497\n",
            "Epoch: 94, Step: 39500, Train Loss: 8.759729385207558, Valid Loss: 8.811189744856708\n",
            "Epoch: 94, Step: 39600, Train Loss: 8.769296751846042, Valid Loss: 8.816431245287161\n",
            "Epoch: 94, Step: 39700, Train Loss: 8.80692123498795, Valid Loss: 8.860797674779608\n",
            "Epoch: 95, Step: 39800, Train Loss: 8.78291992099668, Valid Loss: 8.834785792739131\n",
            "Epoch: 95, Step: 39900, Train Loss: 8.807010196862663, Valid Loss: 8.861027012478772\n",
            "Epoch: 95, Step: 40000, Train Loss: 8.812552860719329, Valid Loss: 8.866222687108825\n",
            "Epoch: 95, Step: 40100, Train Loss: 8.789734332816169, Valid Loss: 8.843039706914134\n",
            "Epoch: 96, Step: 40200, Train Loss: 8.766214082454171, Valid Loss: 8.820619773487696\n",
            "Epoch: 96, Step: 40300, Train Loss: 8.771642448706247, Valid Loss: 8.82329858263838\n",
            "Epoch: 96, Step: 40400, Train Loss: 8.807071299012517, Valid Loss: 8.85833133112173\n",
            "Epoch: 96, Step: 40500, Train Loss: 8.81667312408654, Valid Loss: 8.864220918945174\n",
            "Epoch: 97, Step: 40600, Train Loss: 8.774147673653326, Valid Loss: 8.821878866593439\n",
            "Epoch: 97, Step: 40700, Train Loss: 8.79023714440909, Valid Loss: 8.835681015415147\n",
            "Epoch: 97, Step: 40800, Train Loss: 8.783089327096704, Valid Loss: 8.83428784793014\n",
            "Epoch: 97, Step: 40900, Train Loss: 8.775550451653622, Valid Loss: 8.826278073141308\n",
            "Epoch: 98, Step: 41000, Train Loss: 8.802948599661315, Valid Loss: 8.856291810588369\n",
            "Epoch: 98, Step: 41100, Train Loss: 8.794434204614294, Valid Loss: 8.84359251856453\n",
            "Epoch: 98, Step: 41200, Train Loss: 8.795890472973221, Valid Loss: 8.846590548486489\n",
            "Epoch: 98, Step: 41300, Train Loss: 8.805070684006111, Valid Loss: 8.856869615540564\n",
            "Epoch: 99, Step: 41400, Train Loss: 8.810235459752011, Valid Loss: 8.857877486346492\n",
            "Epoch: 99, Step: 41500, Train Loss: 8.769341018421033, Valid Loss: 8.821197620729238\n",
            "Epoch: 99, Step: 41600, Train Loss: 8.798845135433478, Valid Loss: 8.849903220865844\n",
            "Epoch: 99, Step: 41700, Train Loss: 8.809314070832569, Valid Loss: 8.857724884004426\n",
            "Epoch: 99, Step: 41800, Train Loss: 8.794312747035006, Valid Loss: 8.840079020920045\n",
            "Test RMSE Loss for z = 0.0: 8.994984392072922\n",
            "Start training for z = 0.1\n",
            "Epoch: 0, Step: 100, Train Loss: 9.872774428159994, Valid Loss: 9.84011229121659\n",
            "Epoch: 0, Step: 200, Train Loss: 9.19582112434116, Valid Loss: 9.140811729763236\n",
            "Epoch: 0, Step: 300, Train Loss: 9.072522483260178, Valid Loss: 9.013667700159004\n",
            "Epoch: 0, Step: 400, Train Loss: 9.01211538841923, Valid Loss: 8.958951386229138\n",
            "Epoch: 1, Step: 500, Train Loss: 8.991359428080404, Valid Loss: 8.935814359772548\n",
            "Epoch: 1, Step: 600, Train Loss: 8.955742175983232, Valid Loss: 8.899473897438446\n",
            "Epoch: 1, Step: 700, Train Loss: 8.912750423658064, Valid Loss: 8.862948121332476\n",
            "Epoch: 1, Step: 800, Train Loss: 8.858620785957873, Valid Loss: 8.803882103328446\n",
            "Epoch: 2, Step: 900, Train Loss: 8.864063567349993, Valid Loss: 8.816226725211305\n",
            "Epoch: 2, Step: 1000, Train Loss: 8.845156949352216, Valid Loss: 8.795314337692595\n",
            "Epoch: 2, Step: 1100, Train Loss: 8.81445969288273, Valid Loss: 8.768172900065162\n",
            "Epoch: 2, Step: 1200, Train Loss: 8.835859909609345, Valid Loss: 8.800825803588332\n",
            "Epoch: 3, Step: 1300, Train Loss: 8.80003729698262, Valid Loss: 8.758433573856763\n",
            "Epoch: 3, Step: 1400, Train Loss: 8.793751545163952, Valid Loss: 8.749784894089457\n",
            "Epoch: 3, Step: 1500, Train Loss: 8.815290842113518, Valid Loss: 8.775676995097184\n",
            "Epoch: 3, Step: 1600, Train Loss: 8.76646249688713, Valid Loss: 8.729261346044796\n",
            "Epoch: 4, Step: 1700, Train Loss: 8.769880535234703, Valid Loss: 8.733672807011898\n",
            "Epoch: 4, Step: 1800, Train Loss: 8.776106287197258, Valid Loss: 8.747663960133197\n",
            "Epoch: 4, Step: 1900, Train Loss: 8.757305119175928, Valid Loss: 8.72483871636164\n",
            "Epoch: 4, Step: 2000, Train Loss: 8.764740996666067, Valid Loss: 8.734037327407197\n",
            "Epoch: 5, Step: 2100, Train Loss: 8.7626222137638, Valid Loss: 8.739949835693093\n",
            "Epoch: 5, Step: 2200, Train Loss: 8.735245800071743, Valid Loss: 8.707244746548263\n",
            "Epoch: 5, Step: 2300, Train Loss: 8.743962322866288, Valid Loss: 8.711244894584212\n",
            "Epoch: 5, Step: 2400, Train Loss: 8.726019651934177, Valid Loss: 8.7073880648282\n",
            "Epoch: 5, Step: 2500, Train Loss: 8.711884481853675, Valid Loss: 8.687193854061743\n",
            "Epoch: 6, Step: 2600, Train Loss: 8.729185500111814, Valid Loss: 8.707711298678273\n",
            "Epoch: 6, Step: 2700, Train Loss: 8.717156734521526, Valid Loss: 8.699422771632348\n",
            "Epoch: 6, Step: 2800, Train Loss: 8.730438459188111, Valid Loss: 8.707555406783392\n",
            "Epoch: 6, Step: 2900, Train Loss: 8.709760357306017, Valid Loss: 8.691519462883823\n",
            "Epoch: 7, Step: 3000, Train Loss: 8.707880729733132, Valid Loss: 8.690454121970904\n",
            "Epoch: 7, Step: 3100, Train Loss: 8.710895697002467, Valid Loss: 8.695678246777753\n",
            "Epoch: 7, Step: 3200, Train Loss: 8.707086721290874, Valid Loss: 8.690489633257748\n",
            "Epoch: 7, Step: 3300, Train Loss: 8.712140805503823, Valid Loss: 8.697467654569252\n",
            "Epoch: 8, Step: 3400, Train Loss: 8.684208799478625, Valid Loss: 8.670902953825241\n",
            "Epoch: 8, Step: 3500, Train Loss: 8.69803648495565, Valid Loss: 8.683186305513486\n",
            "Epoch: 8, Step: 3600, Train Loss: 8.693789350227757, Valid Loss: 8.687681035757574\n",
            "Epoch: 8, Step: 3700, Train Loss: 8.702993492854654, Valid Loss: 8.69463691522619\n",
            "Epoch: 9, Step: 3800, Train Loss: 8.685381547812847, Valid Loss: 8.67304023585624\n",
            "Epoch: 9, Step: 3900, Train Loss: 8.701211217521365, Valid Loss: 8.691017526054226\n",
            "Epoch: 9, Step: 4000, Train Loss: 8.715136804354625, Valid Loss: 8.7044610655417\n",
            "Epoch: 9, Step: 4100, Train Loss: 8.68149888167179, Valid Loss: 8.67386152896653\n",
            "Epoch: 10, Step: 4200, Train Loss: 8.668241042824606, Valid Loss: 8.66850157443362\n",
            "Epoch: 10, Step: 4300, Train Loss: 8.686820148126921, Valid Loss: 8.686346126863988\n",
            "Epoch: 10, Step: 4400, Train Loss: 8.663857306511984, Valid Loss: 8.658942074999578\n",
            "Epoch: 10, Step: 4500, Train Loss: 8.680581720828457, Valid Loss: 8.681488729106954\n",
            "Epoch: 11, Step: 4600, Train Loss: 8.667316773981213, Valid Loss: 8.665127512121048\n",
            "Epoch: 11, Step: 4700, Train Loss: 8.684046080489244, Valid Loss: 8.684769148540292\n",
            "Epoch: 11, Step: 4800, Train Loss: 8.652603603267817, Valid Loss: 8.646434531761125\n",
            "Epoch: 11, Step: 4900, Train Loss: 8.661137732623686, Valid Loss: 8.66415239584023\n",
            "Epoch: 11, Step: 5000, Train Loss: 8.684513351667114, Valid Loss: 8.680384018400295\n",
            "Epoch: 12, Step: 5100, Train Loss: 8.65754869307008, Valid Loss: 8.65861995139076\n",
            "Epoch: 12, Step: 5200, Train Loss: 8.659306816852585, Valid Loss: 8.65816936878654\n",
            "Epoch: 12, Step: 5300, Train Loss: 8.655776983968341, Valid Loss: 8.657320417180005\n",
            "Epoch: 12, Step: 5400, Train Loss: 8.649955404763267, Valid Loss: 8.651328680398809\n",
            "Epoch: 13, Step: 5500, Train Loss: 8.664923035945536, Valid Loss: 8.673469167001201\n",
            "Epoch: 13, Step: 5600, Train Loss: 8.662349828453605, Valid Loss: 8.663603448627468\n",
            "Epoch: 13, Step: 5700, Train Loss: 8.644358116288874, Valid Loss: 8.654480026090177\n",
            "Epoch: 13, Step: 5800, Train Loss: 8.665524129251136, Valid Loss: 8.67210272997909\n",
            "Epoch: 14, Step: 5900, Train Loss: 8.652750330645437, Valid Loss: 8.6635359465796\n",
            "Epoch: 14, Step: 6000, Train Loss: 8.643415173286861, Valid Loss: 8.65656941771906\n",
            "Epoch: 14, Step: 6100, Train Loss: 8.640894670789715, Valid Loss: 8.649811181854139\n",
            "Epoch: 14, Step: 6200, Train Loss: 8.636018547065635, Valid Loss: 8.650217084480447\n",
            "Epoch: 15, Step: 6300, Train Loss: 8.621444301188395, Valid Loss: 8.63768568461754\n",
            "Epoch: 15, Step: 6400, Train Loss: 8.637255470165902, Valid Loss: 8.6444504255825\n",
            "Epoch: 15, Step: 6500, Train Loss: 8.641855498560819, Valid Loss: 8.650721624797491\n",
            "Epoch: 15, Step: 6600, Train Loss: 8.652855271226782, Valid Loss: 8.66088464645785\n",
            "Epoch: 16, Step: 6700, Train Loss: 8.627861793517699, Valid Loss: 8.639109289166887\n",
            "Epoch: 16, Step: 6800, Train Loss: 8.639394412580652, Valid Loss: 8.6557904025356\n",
            "Epoch: 16, Step: 6900, Train Loss: 8.616748175433043, Valid Loss: 8.635070091024998\n",
            "Epoch: 16, Step: 7000, Train Loss: 8.646549187006162, Valid Loss: 8.667654672293322\n",
            "Epoch: 16, Step: 7100, Train Loss: 8.642226385042923, Valid Loss: 8.65774042154838\n",
            "Epoch: 17, Step: 7200, Train Loss: 8.623063771152369, Valid Loss: 8.646281042461919\n",
            "Epoch: 17, Step: 7300, Train Loss: 8.63435698429465, Valid Loss: 8.658622845745729\n",
            "Epoch: 17, Step: 7400, Train Loss: 8.62469139722175, Valid Loss: 8.637296092518868\n",
            "Epoch: 17, Step: 7500, Train Loss: 8.634476437199599, Valid Loss: 8.649059111558657\n",
            "Epoch: 18, Step: 7600, Train Loss: 8.63276463929843, Valid Loss: 8.657858211088927\n",
            "Epoch: 18, Step: 7700, Train Loss: 8.631384136891489, Valid Loss: 8.647972625792786\n",
            "Epoch: 18, Step: 7800, Train Loss: 8.623971211485488, Valid Loss: 8.636519081908396\n",
            "Epoch: 18, Step: 7900, Train Loss: 8.602003172336511, Valid Loss: 8.621100270433764\n",
            "Epoch: 19, Step: 8000, Train Loss: 8.603713285302465, Valid Loss: 8.624032812858365\n",
            "Epoch: 19, Step: 8100, Train Loss: 8.602914574919858, Valid Loss: 8.620390058172124\n",
            "Epoch: 19, Step: 8200, Train Loss: 8.635135819843168, Valid Loss: 8.648922684343868\n",
            "Epoch: 19, Step: 8300, Train Loss: 8.63813035941587, Valid Loss: 8.66349213037341\n",
            "Epoch: 20, Step: 8400, Train Loss: 8.608349042320203, Valid Loss: 8.635851450863745\n",
            "Epoch: 20, Step: 8500, Train Loss: 8.61223313225871, Valid Loss: 8.640924891070808\n",
            "Epoch: 20, Step: 8600, Train Loss: 8.614870202239237, Valid Loss: 8.6526014724834\n",
            "Epoch: 20, Step: 8700, Train Loss: 8.619691273057366, Valid Loss: 8.650328420189282\n",
            "Epoch: 21, Step: 8800, Train Loss: 8.604010256820963, Valid Loss: 8.632446822794293\n",
            "Epoch: 21, Step: 8900, Train Loss: 8.60989131413811, Valid Loss: 8.631450896270813\n",
            "Epoch: 21, Step: 9000, Train Loss: 8.610269634242487, Valid Loss: 8.636318777530681\n",
            "Epoch: 21, Step: 9100, Train Loss: 8.616518934731584, Valid Loss: 8.64181716446116\n",
            "Epoch: 22, Step: 9200, Train Loss: 8.613988687896398, Valid Loss: 8.639178634836059\n",
            "Epoch: 22, Step: 9300, Train Loss: 8.612923950613904, Valid Loss: 8.641712330654121\n",
            "Epoch: 22, Step: 9400, Train Loss: 8.593711083356455, Valid Loss: 8.622876535527215\n",
            "Epoch: 22, Step: 9500, Train Loss: 8.612635277598988, Valid Loss: 8.644395413162966\n",
            "Epoch: 22, Step: 9600, Train Loss: 8.592898265575505, Valid Loss: 8.62131039652518\n",
            "Epoch: 23, Step: 9700, Train Loss: 8.60616499709577, Valid Loss: 8.629576737312977\n",
            "Epoch: 23, Step: 9800, Train Loss: 8.598914274829669, Valid Loss: 8.625186417830525\n",
            "Epoch: 23, Step: 9900, Train Loss: 8.610335084940425, Valid Loss: 8.640594098981493\n",
            "Epoch: 23, Step: 10000, Train Loss: 8.607176618147522, Valid Loss: 8.641351948345045\n",
            "Epoch: 24, Step: 10100, Train Loss: 8.620187889940242, Valid Loss: 8.656927269023557\n",
            "Epoch: 24, Step: 10200, Train Loss: 8.581610505016206, Valid Loss: 8.621718129470997\n",
            "Epoch: 24, Step: 10300, Train Loss: 8.587666286510927, Valid Loss: 8.622475202787447\n",
            "Epoch: 24, Step: 10400, Train Loss: 8.584581074300955, Valid Loss: 8.615351414784492\n",
            "Epoch: 25, Step: 10500, Train Loss: 8.581940863228853, Valid Loss: 8.620611783633027\n",
            "Epoch: 25, Step: 10600, Train Loss: 8.608698936486604, Valid Loss: 8.639618034593546\n",
            "Epoch: 25, Step: 10700, Train Loss: 8.5938080498015, Valid Loss: 8.625614185959213\n",
            "Epoch: 25, Step: 10800, Train Loss: 8.601410963257251, Valid Loss: 8.638006853013785\n",
            "Epoch: 26, Step: 10900, Train Loss: 8.581611071351402, Valid Loss: 8.622357530726909\n",
            "Epoch: 26, Step: 11000, Train Loss: 8.603447676765647, Valid Loss: 8.63912118037858\n",
            "Epoch: 26, Step: 11100, Train Loss: 8.601093190412834, Valid Loss: 8.64250319525344\n",
            "Epoch: 26, Step: 11200, Train Loss: 8.59301835416778, Valid Loss: 8.629355715653599\n",
            "Epoch: 27, Step: 11300, Train Loss: 8.613027905044424, Valid Loss: 8.650834139575787\n",
            "Epoch: 27, Step: 11400, Train Loss: 8.593947698979177, Valid Loss: 8.640406994757807\n",
            "Epoch: 27, Step: 11500, Train Loss: 8.589099588677012, Valid Loss: 8.63229640514416\n",
            "Epoch: 27, Step: 11600, Train Loss: 8.58071252359925, Valid Loss: 8.624207466817204\n",
            "Epoch: 27, Step: 11700, Train Loss: 8.579766412129127, Valid Loss: 8.62465971138778\n",
            "Epoch: 28, Step: 11800, Train Loss: 8.578088719283597, Valid Loss: 8.618387706417103\n",
            "Epoch: 28, Step: 11900, Train Loss: 8.590289553823267, Valid Loss: 8.62340591271405\n",
            "Epoch: 28, Step: 12000, Train Loss: 8.584136289670358, Valid Loss: 8.620523064817252\n",
            "Epoch: 28, Step: 12100, Train Loss: 8.601089841177751, Valid Loss: 8.641979293929145\n",
            "Epoch: 29, Step: 12200, Train Loss: 8.57103473738179, Valid Loss: 8.61585350907632\n",
            "Epoch: 29, Step: 12300, Train Loss: 8.588531610872842, Valid Loss: 8.629682082161557\n",
            "Epoch: 29, Step: 12400, Train Loss: 8.571656516628467, Valid Loss: 8.612266496201931\n",
            "Epoch: 29, Step: 12500, Train Loss: 8.572887182012721, Valid Loss: 8.62392838801801\n",
            "Epoch: 30, Step: 12600, Train Loss: 8.588830320498419, Valid Loss: 8.636179551997827\n",
            "Epoch: 30, Step: 12700, Train Loss: 8.57218888266186, Valid Loss: 8.617362160499026\n",
            "Epoch: 30, Step: 12800, Train Loss: 8.57224903237384, Valid Loss: 8.620895442703477\n",
            "Epoch: 30, Step: 12900, Train Loss: 8.574058416744432, Valid Loss: 8.62527478103353\n",
            "Epoch: 31, Step: 13000, Train Loss: 8.571819957251629, Valid Loss: 8.625829014244726\n",
            "Epoch: 31, Step: 13100, Train Loss: 8.580201909644343, Valid Loss: 8.626074984718755\n",
            "Epoch: 31, Step: 13200, Train Loss: 8.58253361121529, Valid Loss: 8.62757755020641\n",
            "Epoch: 31, Step: 13300, Train Loss: 8.582684116171542, Valid Loss: 8.631084353969179\n",
            "Epoch: 32, Step: 13400, Train Loss: 8.579129708631672, Valid Loss: 8.626199046412495\n",
            "Epoch: 32, Step: 13500, Train Loss: 8.579547188038271, Valid Loss: 8.636894403271102\n",
            "Epoch: 32, Step: 13600, Train Loss: 8.585536389703075, Valid Loss: 8.632434968741222\n",
            "Epoch: 32, Step: 13700, Train Loss: 8.563285345297729, Valid Loss: 8.605887309983883\n",
            "Epoch: 33, Step: 13800, Train Loss: 8.57933589459929, Valid Loss: 8.633134830102406\n",
            "Epoch: 33, Step: 13900, Train Loss: 8.56275524640682, Valid Loss: 8.612566272835785\n",
            "Epoch: 33, Step: 14000, Train Loss: 8.58277610596634, Valid Loss: 8.631245621226316\n",
            "Epoch: 33, Step: 14100, Train Loss: 8.572950205306546, Valid Loss: 8.62622491516019\n",
            "Epoch: 33, Step: 14200, Train Loss: 8.569615159237273, Valid Loss: 8.620073588699181\n",
            "Epoch: 34, Step: 14300, Train Loss: 8.574090956832245, Valid Loss: 8.619035271366048\n",
            "Epoch: 34, Step: 14400, Train Loss: 8.574172441600147, Valid Loss: 8.62493818050829\n",
            "Epoch: 34, Step: 14500, Train Loss: 8.54634277765035, Valid Loss: 8.606314265717367\n",
            "Epoch: 34, Step: 14600, Train Loss: 8.570944113564952, Valid Loss: 8.617578079161092\n",
            "Epoch: 35, Step: 14700, Train Loss: 8.562344936613597, Valid Loss: 8.619000061871414\n",
            "Epoch: 35, Step: 14800, Train Loss: 8.562702785738283, Valid Loss: 8.618235283974853\n",
            "Epoch: 35, Step: 14900, Train Loss: 8.575315522929712, Valid Loss: 8.629045481170948\n",
            "Epoch: 35, Step: 15000, Train Loss: 8.548861554587283, Valid Loss: 8.604318869209758\n",
            "Epoch: 36, Step: 15100, Train Loss: 8.580337154285177, Valid Loss: 8.631753156424322\n",
            "Epoch: 36, Step: 15200, Train Loss: 8.554520694461699, Valid Loss: 8.606465805487758\n",
            "Epoch: 36, Step: 15300, Train Loss: 8.598854898884765, Valid Loss: 8.648074393289372\n",
            "Epoch: 36, Step: 15400, Train Loss: 8.56024416579589, Valid Loss: 8.611080981006895\n",
            "Epoch: 37, Step: 15500, Train Loss: 8.572123025785336, Valid Loss: 8.627228760895159\n",
            "Epoch: 37, Step: 15600, Train Loss: 8.572262076393306, Valid Loss: 8.626516895091932\n",
            "Epoch: 37, Step: 15700, Train Loss: 8.56370145682594, Valid Loss: 8.615427136079397\n",
            "Epoch: 37, Step: 15800, Train Loss: 8.546748393172624, Valid Loss: 8.608248204673933\n",
            "Epoch: 38, Step: 15900, Train Loss: 8.560672093354578, Valid Loss: 8.619477466915395\n",
            "Epoch: 38, Step: 16000, Train Loss: 8.545503527294475, Valid Loss: 8.604878287285938\n",
            "Epoch: 38, Step: 16100, Train Loss: 8.545259992632685, Valid Loss: 8.60959286700223\n",
            "Epoch: 38, Step: 16200, Train Loss: 8.577902643275964, Valid Loss: 8.628643605527646\n",
            "Epoch: 38, Step: 16300, Train Loss: 8.558821294235468, Valid Loss: 8.619135655508945\n",
            "Epoch: 39, Step: 16400, Train Loss: 8.566327158096644, Valid Loss: 8.618444668652414\n",
            "Epoch: 39, Step: 16500, Train Loss: 8.576552799982203, Valid Loss: 8.63422482205727\n",
            "Epoch: 39, Step: 16600, Train Loss: 8.537818134860508, Valid Loss: 8.60104949132514\n",
            "Epoch: 39, Step: 16700, Train Loss: 8.56122530056381, Valid Loss: 8.615152363628804\n",
            "Epoch: 40, Step: 16800, Train Loss: 8.554988529177548, Valid Loss: 8.61639631574828\n",
            "Epoch: 40, Step: 16900, Train Loss: 8.560028646518845, Valid Loss: 8.617487882359113\n",
            "Epoch: 40, Step: 17000, Train Loss: 8.555753422911994, Valid Loss: 8.611853688993133\n",
            "Epoch: 40, Step: 17100, Train Loss: 8.571773245390661, Valid Loss: 8.630969024585069\n",
            "Epoch: 41, Step: 17200, Train Loss: 8.549638320656491, Valid Loss: 8.61238117616712\n",
            "Epoch: 41, Step: 17300, Train Loss: 8.54603896216538, Valid Loss: 8.60198246314315\n",
            "Epoch: 41, Step: 17400, Train Loss: 8.558376014868458, Valid Loss: 8.614639269117763\n",
            "Epoch: 41, Step: 17500, Train Loss: 8.553782536042169, Valid Loss: 8.605534513983761\n",
            "Epoch: 42, Step: 17600, Train Loss: 8.534071887863874, Valid Loss: 8.595828152698271\n",
            "Epoch: 42, Step: 17700, Train Loss: 8.554278940683439, Valid Loss: 8.61182557128282\n",
            "Epoch: 42, Step: 17800, Train Loss: 8.543966001568215, Valid Loss: 8.597479077695331\n",
            "Epoch: 42, Step: 17900, Train Loss: 8.559407317031926, Valid Loss: 8.607890617199041\n",
            "Epoch: 43, Step: 18000, Train Loss: 8.52942202814223, Valid Loss: 8.58700075085614\n",
            "Epoch: 43, Step: 18100, Train Loss: 8.52806278041846, Valid Loss: 8.58914524870058\n",
            "Epoch: 43, Step: 18200, Train Loss: 8.528079938051011, Valid Loss: 8.586931395839128\n",
            "Epoch: 43, Step: 18300, Train Loss: 8.547775712486867, Valid Loss: 8.602396346286644\n",
            "Epoch: 44, Step: 18400, Train Loss: 8.550563901286159, Valid Loss: 8.607247303021301\n",
            "Epoch: 44, Step: 18500, Train Loss: 8.55420351360307, Valid Loss: 8.61070022016267\n",
            "Epoch: 44, Step: 18600, Train Loss: 8.546562806635098, Valid Loss: 8.609056644724053\n",
            "Epoch: 44, Step: 18700, Train Loss: 8.549128782668818, Valid Loss: 8.608916144121858\n",
            "Epoch: 44, Step: 18800, Train Loss: 8.542496386138634, Valid Loss: 8.602846212330197\n",
            "Epoch: 45, Step: 18900, Train Loss: 8.540760469955265, Valid Loss: 8.605466419391737\n",
            "Epoch: 45, Step: 19000, Train Loss: 8.542837350382541, Valid Loss: 8.605772550823362\n",
            "Epoch: 45, Step: 19100, Train Loss: 8.561897646741267, Valid Loss: 8.62212783135154\n",
            "Epoch: 45, Step: 19200, Train Loss: 8.556963915266296, Valid Loss: 8.616693749045767\n",
            "Epoch: 46, Step: 19300, Train Loss: 8.538917638556102, Valid Loss: 8.600219621385712\n",
            "Epoch: 46, Step: 19400, Train Loss: 8.539671403517369, Valid Loss: 8.601459467652973\n",
            "Epoch: 46, Step: 19500, Train Loss: 8.545039438564332, Valid Loss: 8.606541893699198\n",
            "Epoch: 46, Step: 19600, Train Loss: 8.545726124365526, Valid Loss: 8.605037402505179\n",
            "Epoch: 47, Step: 19700, Train Loss: 8.540595101218438, Valid Loss: 8.600682111273212\n",
            "Epoch: 47, Step: 19800, Train Loss: 8.524257682272907, Valid Loss: 8.591046892607626\n",
            "Epoch: 47, Step: 19900, Train Loss: 8.526708392614836, Valid Loss: 8.595515034709116\n",
            "Epoch: 47, Step: 20000, Train Loss: 8.54741128549359, Valid Loss: 8.60352524605758\n",
            "Epoch: 48, Step: 20100, Train Loss: 8.52548458524476, Valid Loss: 8.58458112916628\n",
            "Epoch: 48, Step: 20200, Train Loss: 8.53033478530534, Valid Loss: 8.60138926454463\n",
            "Epoch: 48, Step: 20300, Train Loss: 8.560394186835707, Valid Loss: 8.621600918513794\n",
            "Epoch: 48, Step: 20400, Train Loss: 8.534021127704943, Valid Loss: 8.591758488692564\n",
            "Epoch: 49, Step: 20500, Train Loss: 8.524551523181252, Valid Loss: 8.593135890540394\n",
            "Epoch: 49, Step: 20600, Train Loss: 8.53897178294307, Valid Loss: 8.600195834217725\n",
            "Epoch: 49, Step: 20700, Train Loss: 8.553664801815835, Valid Loss: 8.603918247156862\n",
            "Epoch: 49, Step: 20800, Train Loss: 8.51821576526808, Valid Loss: 8.592295648344276\n",
            "Epoch: 49, Step: 20900, Train Loss: 8.51880492775877, Valid Loss: 8.594737900277392\n",
            "Epoch: 50, Step: 21000, Train Loss: 8.53093343811586, Valid Loss: 8.601929137734773\n",
            "Epoch: 50, Step: 21100, Train Loss: 8.524152254914645, Valid Loss: 8.591081841608002\n",
            "Epoch: 50, Step: 21200, Train Loss: 8.530865078294697, Valid Loss: 8.595248612914808\n",
            "Epoch: 50, Step: 21300, Train Loss: 8.54497120227098, Valid Loss: 8.607028034876864\n",
            "Epoch: 51, Step: 21400, Train Loss: 8.527678694710671, Valid Loss: 8.598822203446408\n",
            "Epoch: 51, Step: 21500, Train Loss: 8.53931516174126, Valid Loss: 8.5993567275648\n",
            "Epoch: 51, Step: 21600, Train Loss: 8.548969101757784, Valid Loss: 8.60910269985032\n",
            "Epoch: 51, Step: 21700, Train Loss: 8.544744558899458, Valid Loss: 8.600481214674575\n",
            "Epoch: 52, Step: 21800, Train Loss: 8.534837975950474, Valid Loss: 8.599339785411699\n",
            "Epoch: 52, Step: 21900, Train Loss: 8.550636608298909, Valid Loss: 8.6125144923359\n",
            "Epoch: 52, Step: 22000, Train Loss: 8.545907787867424, Valid Loss: 8.610025379651043\n",
            "Epoch: 52, Step: 22100, Train Loss: 8.530535394048531, Valid Loss: 8.602329798563932\n",
            "Epoch: 53, Step: 22200, Train Loss: 8.530722670298744, Valid Loss: 8.602816640438768\n",
            "Epoch: 53, Step: 22300, Train Loss: 8.543375937752252, Valid Loss: 8.614386562059895\n",
            "Epoch: 53, Step: 22400, Train Loss: 8.534288862423969, Valid Loss: 8.603395724279233\n",
            "Epoch: 53, Step: 22500, Train Loss: 8.537911659094608, Valid Loss: 8.60236819766405\n",
            "Epoch: 54, Step: 22600, Train Loss: 8.543828178611644, Valid Loss: 8.613274448007443\n",
            "Epoch: 54, Step: 22700, Train Loss: 8.524861318295736, Valid Loss: 8.596091597023458\n",
            "Epoch: 54, Step: 22800, Train Loss: 8.534377399635979, Valid Loss: 8.60277221386643\n",
            "Epoch: 54, Step: 22900, Train Loss: 8.525272898277175, Valid Loss: 8.595216836680308\n",
            "Epoch: 55, Step: 23000, Train Loss: 8.507420545198263, Valid Loss: 8.578777284825781\n",
            "Epoch: 55, Step: 23100, Train Loss: 8.540161869793069, Valid Loss: 8.607339672168596\n",
            "Epoch: 55, Step: 23200, Train Loss: 8.541985404734069, Valid Loss: 8.604520292983619\n",
            "Epoch: 55, Step: 23300, Train Loss: 8.524997899268795, Valid Loss: 8.585901452423492\n",
            "Epoch: 55, Step: 23400, Train Loss: 8.549458810485762, Valid Loss: 8.603854900576211\n",
            "Epoch: 56, Step: 23500, Train Loss: 8.533780082198925, Valid Loss: 8.598668667584294\n",
            "Epoch: 56, Step: 23600, Train Loss: 8.525494073490705, Valid Loss: 8.588604714144315\n",
            "Epoch: 56, Step: 23700, Train Loss: 8.530655726089144, Valid Loss: 8.592036195640766\n",
            "Epoch: 56, Step: 23800, Train Loss: 8.521237223294472, Valid Loss: 8.588936625832226\n",
            "Epoch: 57, Step: 23900, Train Loss: 8.521368176266677, Valid Loss: 8.58893155026415\n",
            "Epoch: 57, Step: 24000, Train Loss: 8.525548837233245, Valid Loss: 8.59339861112373\n",
            "Epoch: 57, Step: 24100, Train Loss: 8.513553727045418, Valid Loss: 8.584132018339165\n",
            "Epoch: 57, Step: 24200, Train Loss: 8.508588574859575, Valid Loss: 8.579998730491566\n",
            "Epoch: 58, Step: 24300, Train Loss: 8.52252086701603, Valid Loss: 8.59289446951194\n",
            "Epoch: 58, Step: 24400, Train Loss: 8.512766750037292, Valid Loss: 8.580663852299276\n",
            "Epoch: 58, Step: 24500, Train Loss: 8.52788718488698, Valid Loss: 8.596145201185422\n",
            "Epoch: 58, Step: 24600, Train Loss: 8.507264618146017, Valid Loss: 8.579775911293105\n",
            "Epoch: 59, Step: 24700, Train Loss: 8.510315099465902, Valid Loss: 8.593762955040512\n",
            "Epoch: 59, Step: 24800, Train Loss: 8.525211224262097, Valid Loss: 8.601337695879527\n",
            "Epoch: 59, Step: 24900, Train Loss: 8.534400157824996, Valid Loss: 8.608585954399313\n",
            "Epoch: 59, Step: 25000, Train Loss: 8.523158963354376, Valid Loss: 8.588998432279757\n",
            "Epoch: 60, Step: 25100, Train Loss: 8.51942617380794, Valid Loss: 8.596377833985382\n",
            "Epoch: 60, Step: 25200, Train Loss: 8.50456193431734, Valid Loss: 8.579918884624348\n",
            "Epoch: 60, Step: 25300, Train Loss: 8.515631751407552, Valid Loss: 8.597623206730004\n",
            "Epoch: 60, Step: 25400, Train Loss: 8.534161646082971, Valid Loss: 8.60892354654388\n",
            "Epoch: 61, Step: 25500, Train Loss: 8.528075166832183, Valid Loss: 8.596028072718848\n",
            "Epoch: 61, Step: 25600, Train Loss: 8.539060672229947, Valid Loss: 8.60789374635571\n",
            "Epoch: 61, Step: 25700, Train Loss: 8.522090428458252, Valid Loss: 8.596312212265568\n",
            "Epoch: 61, Step: 25800, Train Loss: 8.522918492490883, Valid Loss: 8.594276479910004\n",
            "Epoch: 61, Step: 25900, Train Loss: 8.51613397846998, Valid Loss: 8.595150042255458\n",
            "Epoch: 62, Step: 26000, Train Loss: 8.52029868351982, Valid Loss: 8.59365946610784\n",
            "Epoch: 62, Step: 26100, Train Loss: 8.523390658875774, Valid Loss: 8.603115676611536\n",
            "Epoch: 62, Step: 26200, Train Loss: 8.539660260663144, Valid Loss: 8.612827261280385\n",
            "Epoch: 62, Step: 26300, Train Loss: 8.524516818441969, Valid Loss: 8.596373419368861\n",
            "Epoch: 63, Step: 26400, Train Loss: 8.496383519931518, Valid Loss: 8.579060419747398\n",
            "Epoch: 63, Step: 26500, Train Loss: 8.5024463785459, Valid Loss: 8.586881896139849\n",
            "Epoch: 63, Step: 26600, Train Loss: 8.50178654340235, Valid Loss: 8.587821488243138\n",
            "Epoch: 63, Step: 26700, Train Loss: 8.511828673076304, Valid Loss: 8.589294051072592\n",
            "Epoch: 64, Step: 26800, Train Loss: 8.515499076062973, Valid Loss: 8.59631814582628\n",
            "Epoch: 64, Step: 26900, Train Loss: 8.526390689238507, Valid Loss: 8.607115796634233\n",
            "Epoch: 64, Step: 27000, Train Loss: 8.497747470850463, Valid Loss: 8.577392411325633\n",
            "Epoch: 64, Step: 27100, Train Loss: 8.53017935446949, Valid Loss: 8.610915383841705\n",
            "Epoch: 65, Step: 27200, Train Loss: 8.52310539388607, Valid Loss: 8.601168977852788\n",
            "Epoch: 65, Step: 27300, Train Loss: 8.506653594114146, Valid Loss: 8.583623377460672\n",
            "Epoch: 65, Step: 27400, Train Loss: 8.499065926398469, Valid Loss: 8.584280946864652\n",
            "Epoch: 65, Step: 27500, Train Loss: 8.52353519122817, Valid Loss: 8.595613294925407\n",
            "Epoch: 66, Step: 27600, Train Loss: 8.522308932905739, Valid Loss: 8.600726960007211\n",
            "Epoch: 66, Step: 27700, Train Loss: 8.511735556668885, Valid Loss: 8.591690007933614\n",
            "Epoch: 66, Step: 27800, Train Loss: 8.49397320891593, Valid Loss: 8.581985416249834\n",
            "Epoch: 66, Step: 27900, Train Loss: 8.513684519782792, Valid Loss: 8.597835501164244\n",
            "Epoch: 66, Step: 28000, Train Loss: 8.51270724572862, Valid Loss: 8.59462382291107\n",
            "Epoch: 67, Step: 28100, Train Loss: 8.537573524210252, Valid Loss: 8.621585282937732\n",
            "Epoch: 67, Step: 28200, Train Loss: 8.517638430937653, Valid Loss: 8.591649568291455\n",
            "Epoch: 67, Step: 28300, Train Loss: 8.5333486691355, Valid Loss: 8.614871739863215\n",
            "Epoch: 67, Step: 28400, Train Loss: 8.519757535399009, Valid Loss: 8.589764651120218\n",
            "Epoch: 68, Step: 28500, Train Loss: 8.502061056613437, Valid Loss: 8.585368102027788\n",
            "Epoch: 68, Step: 28600, Train Loss: 8.486111276080932, Valid Loss: 8.581193823632079\n",
            "Epoch: 68, Step: 28700, Train Loss: 8.535576452713553, Valid Loss: 8.616070015499854\n",
            "Epoch: 68, Step: 28800, Train Loss: 8.514222843078986, Valid Loss: 8.596224575399791\n",
            "Epoch: 69, Step: 28900, Train Loss: 8.503406810221763, Valid Loss: 8.589070303446173\n",
            "Epoch: 69, Step: 29000, Train Loss: 8.497215371294647, Valid Loss: 8.59057281058325\n",
            "Epoch: 69, Step: 29100, Train Loss: 8.50111808846677, Valid Loss: 8.59025719163982\n",
            "Epoch: 69, Step: 29200, Train Loss: 8.510733098646915, Valid Loss: 8.60575964689881\n",
            "Epoch: 70, Step: 29300, Train Loss: 8.504391339404275, Valid Loss: 8.585288438559754\n",
            "Epoch: 70, Step: 29400, Train Loss: 8.505498453389967, Valid Loss: 8.59140242347627\n",
            "Epoch: 70, Step: 29500, Train Loss: 8.50096675875583, Valid Loss: 8.59243711189151\n",
            "Epoch: 70, Step: 29600, Train Loss: 8.5069547689268, Valid Loss: 8.598561879815154\n",
            "Epoch: 71, Step: 29700, Train Loss: 8.505174157650922, Valid Loss: 8.58945328825826\n",
            "Epoch: 71, Step: 29800, Train Loss: 8.523302666970633, Valid Loss: 8.605242610146455\n",
            "Epoch: 71, Step: 29900, Train Loss: 8.506506918709183, Valid Loss: 8.597661325565241\n",
            "Epoch: 71, Step: 30000, Train Loss: 8.50974425302502, Valid Loss: 8.60043687153707\n",
            "Epoch: 72, Step: 30100, Train Loss: 8.506721118085016, Valid Loss: 8.58119567408129\n",
            "Epoch: 72, Step: 30200, Train Loss: 8.503394696899813, Valid Loss: 8.58730719723563\n",
            "Epoch: 72, Step: 30300, Train Loss: 8.502933285087888, Valid Loss: 8.582265939931728\n",
            "Epoch: 72, Step: 30400, Train Loss: 8.484441420495713, Valid Loss: 8.572877115768248\n",
            "Epoch: 72, Step: 30500, Train Loss: 8.493631972148446, Valid Loss: 8.583434113750087\n",
            "Epoch: 73, Step: 30600, Train Loss: 8.502529666436603, Valid Loss: 8.585641978194781\n",
            "Epoch: 73, Step: 30700, Train Loss: 8.501536713241665, Valid Loss: 8.583296690076724\n",
            "Epoch: 73, Step: 30800, Train Loss: 8.50316318989435, Valid Loss: 8.584896866021602\n",
            "Epoch: 73, Step: 30900, Train Loss: 8.492399639704216, Valid Loss: 8.583886381826158\n",
            "Epoch: 74, Step: 31000, Train Loss: 8.48827154111085, Valid Loss: 8.578611559979397\n",
            "Epoch: 74, Step: 31100, Train Loss: 8.499413693874903, Valid Loss: 8.58115958043298\n",
            "Epoch: 74, Step: 31200, Train Loss: 8.511293650006074, Valid Loss: 8.591834400757971\n",
            "Epoch: 74, Step: 31300, Train Loss: 8.508983748874163, Valid Loss: 8.59331078524771\n",
            "Epoch: 75, Step: 31400, Train Loss: 8.490275059593644, Valid Loss: 8.581804165547126\n",
            "Epoch: 75, Step: 31500, Train Loss: 8.483333377836136, Valid Loss: 8.57112602537369\n",
            "Epoch: 75, Step: 31600, Train Loss: 8.50477727963506, Valid Loss: 8.585546616060189\n",
            "Epoch: 75, Step: 31700, Train Loss: 8.524315973081523, Valid Loss: 8.606468661101863\n",
            "Epoch: 76, Step: 31800, Train Loss: 8.488700624018684, Valid Loss: 8.582714937383992\n",
            "Epoch: 76, Step: 31900, Train Loss: 8.49866407479928, Valid Loss: 8.588289579511398\n",
            "Epoch: 76, Step: 32000, Train Loss: 8.49038112561356, Valid Loss: 8.579229965316415\n",
            "Epoch: 76, Step: 32100, Train Loss: 8.494412152917874, Valid Loss: 8.5799624943544\n",
            "Epoch: 77, Step: 32200, Train Loss: 8.486913583212493, Valid Loss: 8.586566636961948\n",
            "Epoch: 77, Step: 32300, Train Loss: 8.479282580637433, Valid Loss: 8.579996477243037\n",
            "Epoch: 77, Step: 32400, Train Loss: 8.49888640364898, Valid Loss: 8.594190152888373\n",
            "Epoch: 77, Step: 32500, Train Loss: 8.50083641528051, Valid Loss: 8.58953863068191\n",
            "Epoch: 77, Step: 32600, Train Loss: 8.51431102638328, Valid Loss: 8.602842077657119\n",
            "Epoch: 78, Step: 32700, Train Loss: 8.49991575271068, Valid Loss: 8.597039760724561\n",
            "Epoch: 78, Step: 32800, Train Loss: 8.49862562115053, Valid Loss: 8.588591379835558\n",
            "Epoch: 78, Step: 32900, Train Loss: 8.506031719459985, Valid Loss: 8.594663741340328\n",
            "Epoch: 78, Step: 33000, Train Loss: 8.504315704283425, Valid Loss: 8.593381855730385\n",
            "Epoch: 79, Step: 33100, Train Loss: 8.488419896422853, Valid Loss: 8.588398304669333\n",
            "Epoch: 79, Step: 33200, Train Loss: 8.511168148881058, Valid Loss: 8.600705302084224\n",
            "Epoch: 79, Step: 33300, Train Loss: 8.494051647136079, Valid Loss: 8.587362528778216\n",
            "Epoch: 79, Step: 33400, Train Loss: 8.486076258618986, Valid Loss: 8.572543046880119\n",
            "Epoch: 80, Step: 33500, Train Loss: 8.482366959407035, Valid Loss: 8.572384679705486\n",
            "Epoch: 80, Step: 33600, Train Loss: 8.492002840857262, Valid Loss: 8.586258665534169\n",
            "Epoch: 80, Step: 33700, Train Loss: 8.494536310792176, Valid Loss: 8.591684061371913\n",
            "Epoch: 80, Step: 33800, Train Loss: 8.487917217707356, Valid Loss: 8.580855398230185\n",
            "Epoch: 81, Step: 33900, Train Loss: 8.495027989202068, Valid Loss: 8.594959032964733\n",
            "Epoch: 81, Step: 34000, Train Loss: 8.50561198715532, Valid Loss: 8.592863988420897\n",
            "Epoch: 81, Step: 34100, Train Loss: 8.501309375427773, Valid Loss: 8.590681881918584\n",
            "Epoch: 81, Step: 34200, Train Loss: 8.506552970360433, Valid Loss: 8.589884716312982\n",
            "Epoch: 82, Step: 34300, Train Loss: 8.49932218829259, Valid Loss: 8.592429383993434\n",
            "Epoch: 82, Step: 34400, Train Loss: 8.498059355243043, Valid Loss: 8.597113176524458\n",
            "Epoch: 82, Step: 34500, Train Loss: 8.489646699842467, Valid Loss: 8.585369968748175\n",
            "Epoch: 82, Step: 34600, Train Loss: 8.499871691799003, Valid Loss: 8.58612628335669\n",
            "Epoch: 83, Step: 34700, Train Loss: 8.502549320517218, Valid Loss: 8.583035208534296\n",
            "Epoch: 83, Step: 34800, Train Loss: 8.501216183435057, Valid Loss: 8.589271893562628\n",
            "Epoch: 83, Step: 34900, Train Loss: 8.500859804975805, Valid Loss: 8.58688030688649\n",
            "Epoch: 83, Step: 35000, Train Loss: 8.499330596798517, Valid Loss: 8.595661575671368\n",
            "Epoch: 83, Step: 35100, Train Loss: 8.515710760888494, Valid Loss: 8.608130071463904\n",
            "Epoch: 84, Step: 35200, Train Loss: 8.507866578309336, Valid Loss: 8.596113075477119\n",
            "Epoch: 84, Step: 35300, Train Loss: 8.490747702458748, Valid Loss: 8.579861646977502\n",
            "Epoch: 84, Step: 35400, Train Loss: 8.482901429969454, Valid Loss: 8.572435641315856\n",
            "Epoch: 84, Step: 35500, Train Loss: 8.508962591668004, Valid Loss: 8.595706433450149\n",
            "Epoch: 85, Step: 35600, Train Loss: 8.513477551609544, Valid Loss: 8.601409395386778\n",
            "Epoch: 85, Step: 35700, Train Loss: 8.490492964322923, Valid Loss: 8.586165610934106\n",
            "Epoch: 85, Step: 35800, Train Loss: 8.484140484060072, Valid Loss: 8.5876663088302\n",
            "Epoch: 85, Step: 35900, Train Loss: 8.497090634058221, Valid Loss: 8.589568155118648\n",
            "Epoch: 86, Step: 36000, Train Loss: 8.479878718582073, Valid Loss: 8.578851833940806\n",
            "Epoch: 86, Step: 36100, Train Loss: 8.493706482528795, Valid Loss: 8.598576344895093\n",
            "Epoch: 86, Step: 36200, Train Loss: 8.50300766305222, Valid Loss: 8.596091060490165\n",
            "Epoch: 86, Step: 36300, Train Loss: 8.476215478938046, Valid Loss: 8.571297609960197\n",
            "Epoch: 87, Step: 36400, Train Loss: 8.495571750362794, Valid Loss: 8.598380251104585\n",
            "Epoch: 87, Step: 36500, Train Loss: 8.479831356489058, Valid Loss: 8.579755037497982\n",
            "Epoch: 87, Step: 36600, Train Loss: 8.51102486502432, Valid Loss: 8.605654711554015\n",
            "Epoch: 87, Step: 36700, Train Loss: 8.48427825809242, Valid Loss: 8.58521945974113\n",
            "Epoch: 88, Step: 36800, Train Loss: 8.503336527948074, Valid Loss: 8.602146467121672\n",
            "Epoch: 88, Step: 36900, Train Loss: 8.483576183266141, Valid Loss: 8.588190472874972\n",
            "Epoch: 88, Step: 37000, Train Loss: 8.495129514104828, Valid Loss: 8.590239564706936\n",
            "Epoch: 88, Step: 37100, Train Loss: 8.481947557071749, Valid Loss: 8.583862964023265\n",
            "Epoch: 88, Step: 37200, Train Loss: 8.468351171770003, Valid Loss: 8.572675783754878\n",
            "Epoch: 89, Step: 37300, Train Loss: 8.51024509750924, Valid Loss: 8.604210266904854\n",
            "Epoch: 89, Step: 37400, Train Loss: 8.505133067232311, Valid Loss: 8.601019266846988\n",
            "Epoch: 89, Step: 37500, Train Loss: 8.482818468777277, Valid Loss: 8.580499928586395\n",
            "Epoch: 89, Step: 37600, Train Loss: 8.47604069345206, Valid Loss: 8.573425551771663\n",
            "Epoch: 90, Step: 37700, Train Loss: 8.500190318080408, Valid Loss: 8.594239625197455\n",
            "Epoch: 90, Step: 37800, Train Loss: 8.482501940518127, Valid Loss: 8.577549026314196\n",
            "Epoch: 90, Step: 37900, Train Loss: 8.486381158680006, Valid Loss: 8.592654409304808\n",
            "Epoch: 90, Step: 38000, Train Loss: 8.498743150612993, Valid Loss: 8.600177519963573\n",
            "Epoch: 91, Step: 38100, Train Loss: 8.4981439469479, Valid Loss: 8.604600860193935\n",
            "Epoch: 91, Step: 38200, Train Loss: 8.466108090604877, Valid Loss: 8.567698387996169\n",
            "Epoch: 91, Step: 38300, Train Loss: 8.488865770717, Valid Loss: 8.579127042414116\n",
            "Epoch: 91, Step: 38400, Train Loss: 8.506440120347886, Valid Loss: 8.609682948383458\n",
            "Epoch: 92, Step: 38500, Train Loss: 8.483768620931682, Valid Loss: 8.588636213240164\n",
            "Epoch: 92, Step: 38600, Train Loss: 8.479898990839088, Valid Loss: 8.579348778757076\n",
            "Epoch: 92, Step: 38700, Train Loss: 8.506684705028258, Valid Loss: 8.604682672416066\n",
            "Epoch: 92, Step: 38800, Train Loss: 8.481365870413478, Valid Loss: 8.577734314838633\n",
            "Epoch: 93, Step: 38900, Train Loss: 8.479390662873469, Valid Loss: 8.575404791653513\n",
            "Epoch: 93, Step: 39000, Train Loss: 8.474833128669204, Valid Loss: 8.568180809527043\n",
            "Epoch: 93, Step: 39100, Train Loss: 8.483551282409833, Valid Loss: 8.578974153180857\n",
            "Epoch: 93, Step: 39200, Train Loss: 8.49291118597474, Valid Loss: 8.592350719821003\n",
            "Epoch: 94, Step: 39300, Train Loss: 8.483367998119569, Valid Loss: 8.583167302241929\n",
            "Epoch: 94, Step: 39400, Train Loss: 8.486769883795239, Valid Loss: 8.585455616680987\n",
            "Epoch: 94, Step: 39500, Train Loss: 8.480921956412091, Valid Loss: 8.583631675133773\n",
            "Epoch: 94, Step: 39600, Train Loss: 8.500375833847103, Valid Loss: 8.596883971153318\n",
            "Epoch: 94, Step: 39700, Train Loss: 8.476235107351377, Valid Loss: 8.579359940292187\n",
            "Epoch: 95, Step: 39800, Train Loss: 8.47649867641418, Valid Loss: 8.580830254159013\n",
            "Epoch: 95, Step: 39900, Train Loss: 8.492586177035806, Valid Loss: 8.592909975932054\n",
            "Epoch: 95, Step: 40000, Train Loss: 8.463303776248008, Valid Loss: 8.570774938757241\n",
            "Epoch: 95, Step: 40100, Train Loss: 8.490424931515909, Valid Loss: 8.59531272366461\n",
            "Epoch: 96, Step: 40200, Train Loss: 8.485507248385252, Valid Loss: 8.58517461339967\n",
            "Epoch: 96, Step: 40300, Train Loss: 8.46960602323287, Valid Loss: 8.578724033658485\n",
            "Epoch: 96, Step: 40400, Train Loss: 8.472732295195403, Valid Loss: 8.572407133903965\n",
            "Epoch: 96, Step: 40500, Train Loss: 8.477993937525175, Valid Loss: 8.575531536293402\n",
            "Epoch: 97, Step: 40600, Train Loss: 8.465510016611043, Valid Loss: 8.57040142750925\n",
            "Epoch: 97, Step: 40700, Train Loss: 8.469926682611595, Valid Loss: 8.572397036892013\n",
            "Epoch: 97, Step: 40800, Train Loss: 8.499051434745693, Valid Loss: 8.5980642868225\n",
            "Epoch: 97, Step: 40900, Train Loss: 8.478095881965357, Valid Loss: 8.583468875383286\n",
            "Epoch: 98, Step: 41000, Train Loss: 8.518607606826725, Valid Loss: 8.610676718753806\n",
            "Epoch: 98, Step: 41100, Train Loss: 8.477544971947523, Valid Loss: 8.572354290286306\n",
            "Epoch: 98, Step: 41200, Train Loss: 8.47334592881982, Valid Loss: 8.585083678763167\n",
            "Epoch: 98, Step: 41300, Train Loss: 8.480612075952816, Valid Loss: 8.582177016117162\n",
            "Epoch: 99, Step: 41400, Train Loss: 8.482268304658389, Valid Loss: 8.581977185695692\n",
            "Epoch: 99, Step: 41500, Train Loss: 8.482265624922261, Valid Loss: 8.578642663704569\n",
            "Epoch: 99, Step: 41600, Train Loss: 8.477478338983605, Valid Loss: 8.585878827477623\n",
            "Epoch: 99, Step: 41700, Train Loss: 8.473997331127066, Valid Loss: 8.579428934310627\n",
            "Epoch: 99, Step: 41800, Train Loss: 8.45709818171573, Valid Loss: 8.576219804830812\n",
            "Test RMSE Loss for z = 0.1: 8.773821661899284\n",
            "Start training for z = 0.9\n",
            "Epoch: 0, Step: 100, Train Loss: 9.445454958162816, Valid Loss: 9.396020114341187\n",
            "Epoch: 0, Step: 200, Train Loss: 9.19632812569977, Valid Loss: 9.138639877189839\n",
            "Epoch: 0, Step: 300, Train Loss: 9.05424551949945, Valid Loss: 8.996248122297901\n",
            "Epoch: 0, Step: 400, Train Loss: 8.988765969027792, Valid Loss: 8.928933444082979\n",
            "Epoch: 1, Step: 500, Train Loss: 8.936637426067692, Valid Loss: 8.88648893315756\n",
            "Epoch: 1, Step: 600, Train Loss: 8.915203853286108, Valid Loss: 8.86372369874127\n",
            "Epoch: 1, Step: 700, Train Loss: 8.916342193648946, Valid Loss: 8.86665562133403\n",
            "Epoch: 1, Step: 800, Train Loss: 8.875693942903764, Valid Loss: 8.81963945808335\n",
            "Epoch: 2, Step: 900, Train Loss: 8.832997342076693, Valid Loss: 8.778490663116834\n",
            "Epoch: 2, Step: 1000, Train Loss: 8.847476654429457, Valid Loss: 8.802008619240068\n",
            "Epoch: 2, Step: 1100, Train Loss: 8.818285642557354, Valid Loss: 8.768040031548084\n",
            "Epoch: 2, Step: 1200, Train Loss: 8.804619288880216, Valid Loss: 8.76047808745241\n",
            "Epoch: 3, Step: 1300, Train Loss: 8.81106785382367, Valid Loss: 8.767529934549762\n",
            "Epoch: 3, Step: 1400, Train Loss: 8.800652498966883, Valid Loss: 8.763367725994206\n",
            "Epoch: 3, Step: 1500, Train Loss: 8.77706026645916, Valid Loss: 8.744282886705408\n",
            "Epoch: 3, Step: 1600, Train Loss: 8.766062020379113, Valid Loss: 8.733091061488924\n",
            "Epoch: 4, Step: 1700, Train Loss: 8.755418928488629, Valid Loss: 8.723972971808598\n",
            "Epoch: 4, Step: 1800, Train Loss: 8.753197531188945, Valid Loss: 8.730550065785806\n",
            "Epoch: 4, Step: 1900, Train Loss: 8.74112810290178, Valid Loss: 8.712494916357864\n",
            "Epoch: 4, Step: 2000, Train Loss: 8.77305262707814, Valid Loss: 8.746761987441722\n",
            "Epoch: 5, Step: 2100, Train Loss: 8.75090628188063, Valid Loss: 8.720272014644323\n",
            "Epoch: 5, Step: 2200, Train Loss: 8.75264264155578, Valid Loss: 8.729532560554407\n",
            "Epoch: 5, Step: 2300, Train Loss: 8.728038906370609, Valid Loss: 8.710924569708407\n",
            "Epoch: 5, Step: 2400, Train Loss: 8.71927326859292, Valid Loss: 8.698842575279661\n",
            "Epoch: 5, Step: 2500, Train Loss: 8.719759329211406, Valid Loss: 8.696097347864317\n",
            "Epoch: 6, Step: 2600, Train Loss: 8.720497040606492, Valid Loss: 8.705081990569873\n",
            "Epoch: 6, Step: 2700, Train Loss: 8.721997563069829, Valid Loss: 8.706114707056578\n",
            "Epoch: 6, Step: 2800, Train Loss: 8.706316103582832, Valid Loss: 8.691585368910854\n",
            "Epoch: 6, Step: 2900, Train Loss: 8.695886063492427, Valid Loss: 8.677191697879582\n",
            "Epoch: 7, Step: 3000, Train Loss: 8.693077371833313, Valid Loss: 8.671374249374749\n",
            "Epoch: 7, Step: 3100, Train Loss: 8.678532025785664, Valid Loss: 8.659511977411306\n",
            "Epoch: 7, Step: 3200, Train Loss: 8.684405875575926, Valid Loss: 8.66633131496346\n",
            "Epoch: 7, Step: 3300, Train Loss: 8.694709911215206, Valid Loss: 8.677276296304838\n",
            "Epoch: 8, Step: 3400, Train Loss: 8.702536547613601, Valid Loss: 8.684785133520462\n",
            "Epoch: 8, Step: 3500, Train Loss: 8.673306279329438, Valid Loss: 8.657253795401381\n",
            "Epoch: 8, Step: 3600, Train Loss: 8.695636887639068, Valid Loss: 8.682050027341813\n",
            "Epoch: 8, Step: 3700, Train Loss: 8.66811581184983, Valid Loss: 8.653293964088894\n",
            "Epoch: 9, Step: 3800, Train Loss: 8.667589624113184, Valid Loss: 8.652067682044457\n",
            "Epoch: 9, Step: 3900, Train Loss: 8.677145247260723, Valid Loss: 8.666164452788935\n",
            "Epoch: 9, Step: 4000, Train Loss: 8.680852031300104, Valid Loss: 8.672694115102393\n",
            "Epoch: 9, Step: 4100, Train Loss: 8.67144188405055, Valid Loss: 8.670685266287185\n",
            "Epoch: 10, Step: 4200, Train Loss: 8.656573031568314, Valid Loss: 8.657732880861559\n",
            "Epoch: 10, Step: 4300, Train Loss: 8.688332700275128, Valid Loss: 8.683122057748037\n",
            "Epoch: 10, Step: 4400, Train Loss: 8.664819957694634, Valid Loss: 8.660579728743537\n",
            "Epoch: 10, Step: 4500, Train Loss: 8.664631116300798, Valid Loss: 8.659202239645092\n",
            "Epoch: 11, Step: 4600, Train Loss: 8.65663609541747, Valid Loss: 8.65080313230633\n",
            "Epoch: 11, Step: 4700, Train Loss: 8.649289278213695, Valid Loss: 8.648082823938534\n",
            "Epoch: 11, Step: 4800, Train Loss: 8.654997705647938, Valid Loss: 8.64981344126324\n",
            "Epoch: 11, Step: 4900, Train Loss: 8.656342702866391, Valid Loss: 8.65282363814613\n",
            "Epoch: 11, Step: 5000, Train Loss: 8.669027892016517, Valid Loss: 8.66787106076899\n",
            "Epoch: 12, Step: 5100, Train Loss: 8.641508579162934, Valid Loss: 8.646161655681865\n",
            "Epoch: 12, Step: 5200, Train Loss: 8.637623050866997, Valid Loss: 8.636592149920105\n",
            "Epoch: 12, Step: 5300, Train Loss: 8.648279443295207, Valid Loss: 8.64900050039811\n",
            "Epoch: 12, Step: 5400, Train Loss: 8.632116533038035, Valid Loss: 8.634427623814029\n",
            "Epoch: 13, Step: 5500, Train Loss: 8.633347563998544, Valid Loss: 8.640214570487101\n",
            "Epoch: 13, Step: 5600, Train Loss: 8.64794211060598, Valid Loss: 8.652632884639939\n",
            "Epoch: 13, Step: 5700, Train Loss: 8.661993576682928, Valid Loss: 8.66266599601036\n",
            "Epoch: 13, Step: 5800, Train Loss: 8.62636595974198, Valid Loss: 8.627071216943072\n",
            "Epoch: 14, Step: 5900, Train Loss: 8.633071681745466, Valid Loss: 8.63654532940414\n",
            "Epoch: 14, Step: 6000, Train Loss: 8.633091654174903, Valid Loss: 8.635118836476938\n",
            "Epoch: 14, Step: 6100, Train Loss: 8.657223142872144, Valid Loss: 8.662891567127762\n",
            "Epoch: 14, Step: 6200, Train Loss: 8.624518288889274, Valid Loss: 8.631405858272494\n",
            "Epoch: 15, Step: 6300, Train Loss: 8.644456642762034, Valid Loss: 8.649296695625194\n",
            "Epoch: 15, Step: 6400, Train Loss: 8.623159330246503, Valid Loss: 8.637550152993859\n",
            "Epoch: 15, Step: 6500, Train Loss: 8.622864050224715, Valid Loss: 8.632484551216656\n",
            "Epoch: 15, Step: 6600, Train Loss: 8.633655147822997, Valid Loss: 8.643911502687393\n",
            "Epoch: 16, Step: 6700, Train Loss: 8.603896596664972, Valid Loss: 8.617558853645777\n",
            "Epoch: 16, Step: 6800, Train Loss: 8.625485743414334, Valid Loss: 8.632241081060515\n",
            "Epoch: 16, Step: 6900, Train Loss: 8.600670275440244, Valid Loss: 8.616881979174519\n",
            "Epoch: 16, Step: 7000, Train Loss: 8.627332908453923, Valid Loss: 8.636489659453513\n",
            "Epoch: 16, Step: 7100, Train Loss: 8.61456132458441, Valid Loss: 8.634614318587689\n",
            "Epoch: 17, Step: 7200, Train Loss: 8.595680636340642, Valid Loss: 8.617359985448761\n",
            "Epoch: 17, Step: 7300, Train Loss: 8.604988837622475, Valid Loss: 8.62245483536298\n",
            "Epoch: 17, Step: 7400, Train Loss: 8.5959172481084, Valid Loss: 8.614637105603261\n",
            "Epoch: 17, Step: 7500, Train Loss: 8.605042657917773, Valid Loss: 8.628491935385476\n",
            "Epoch: 18, Step: 7600, Train Loss: 8.60896313515894, Valid Loss: 8.6302222377003\n",
            "Epoch: 18, Step: 7700, Train Loss: 8.600623310810747, Valid Loss: 8.627869762625584\n",
            "Epoch: 18, Step: 7800, Train Loss: 8.605464965366174, Valid Loss: 8.627588768955826\n",
            "Epoch: 18, Step: 7900, Train Loss: 8.592965230013576, Valid Loss: 8.622359797329985\n",
            "Epoch: 19, Step: 8000, Train Loss: 8.579550453771258, Valid Loss: 8.61252163491392\n",
            "Epoch: 19, Step: 8100, Train Loss: 8.605432983872689, Valid Loss: 8.63189147988609\n",
            "Epoch: 19, Step: 8200, Train Loss: 8.580083835962478, Valid Loss: 8.61437693230022\n",
            "Epoch: 19, Step: 8300, Train Loss: 8.606417853623967, Valid Loss: 8.624943188494509\n",
            "Epoch: 20, Step: 8400, Train Loss: 8.608281775137861, Valid Loss: 8.633231137529291\n",
            "Epoch: 20, Step: 8500, Train Loss: 8.576841000852468, Valid Loss: 8.604821330693921\n",
            "Epoch: 20, Step: 8600, Train Loss: 8.580495193620893, Valid Loss: 8.614286755764768\n",
            "Epoch: 20, Step: 8700, Train Loss: 8.60298253902644, Valid Loss: 8.63009804838131\n",
            "Epoch: 21, Step: 8800, Train Loss: 8.577729822900073, Valid Loss: 8.60744577851069\n",
            "Epoch: 21, Step: 8900, Train Loss: 8.575650676301834, Valid Loss: 8.603203554163938\n",
            "Epoch: 21, Step: 9000, Train Loss: 8.588936229498463, Valid Loss: 8.615894072211036\n",
            "Epoch: 21, Step: 9100, Train Loss: 8.585545042734239, Valid Loss: 8.608218267243865\n",
            "Epoch: 22, Step: 9200, Train Loss: 8.573338754833015, Valid Loss: 8.599197634583227\n",
            "Epoch: 22, Step: 9300, Train Loss: 8.56436412573626, Valid Loss: 8.598736615176719\n",
            "Epoch: 22, Step: 9400, Train Loss: 8.581283032676856, Valid Loss: 8.616133609576927\n",
            "Epoch: 22, Step: 9500, Train Loss: 8.58639339972319, Valid Loss: 8.611836407054207\n",
            "Epoch: 22, Step: 9600, Train Loss: 8.578890942490643, Valid Loss: 8.607417538746333\n",
            "Epoch: 23, Step: 9700, Train Loss: 8.588639545383286, Valid Loss: 8.619527602806272\n",
            "Epoch: 23, Step: 9800, Train Loss: 8.567894404394721, Valid Loss: 8.60560946268611\n",
            "Epoch: 23, Step: 9900, Train Loss: 8.556105802398024, Valid Loss: 8.598066172830679\n",
            "Epoch: 23, Step: 10000, Train Loss: 8.599440694710694, Valid Loss: 8.63409009712451\n",
            "Epoch: 24, Step: 10100, Train Loss: 8.576803604043738, Valid Loss: 8.608941702754946\n",
            "Epoch: 24, Step: 10200, Train Loss: 8.57882784609917, Valid Loss: 8.610804010558779\n",
            "Epoch: 24, Step: 10300, Train Loss: 8.572065809654022, Valid Loss: 8.599155300395726\n",
            "Epoch: 24, Step: 10400, Train Loss: 8.589208852312488, Valid Loss: 8.621155118823292\n",
            "Epoch: 25, Step: 10500, Train Loss: 8.564304783204284, Valid Loss: 8.593495834402443\n",
            "Epoch: 25, Step: 10600, Train Loss: 8.57176005813011, Valid Loss: 8.599067499028122\n",
            "Epoch: 25, Step: 10700, Train Loss: 8.569795408459694, Valid Loss: 8.600797623958018\n",
            "Epoch: 25, Step: 10800, Train Loss: 8.589586668696553, Valid Loss: 8.611036176416906\n",
            "Epoch: 26, Step: 10900, Train Loss: 8.56378074470908, Valid Loss: 8.592564561158571\n",
            "Epoch: 26, Step: 11000, Train Loss: 8.559318788858988, Valid Loss: 8.597527384706417\n",
            "Epoch: 26, Step: 11100, Train Loss: 8.557946357656965, Valid Loss: 8.593438170093041\n",
            "Epoch: 26, Step: 11200, Train Loss: 8.566521215690642, Valid Loss: 8.599922779067493\n",
            "Epoch: 27, Step: 11300, Train Loss: 8.57704941262674, Valid Loss: 8.60438230782957\n",
            "Epoch: 27, Step: 11400, Train Loss: 8.563202750803242, Valid Loss: 8.600272492134641\n",
            "Epoch: 27, Step: 11500, Train Loss: 8.568101342753925, Valid Loss: 8.608016665775763\n",
            "Epoch: 27, Step: 11600, Train Loss: 8.559085334776547, Valid Loss: 8.59414267498481\n",
            "Epoch: 27, Step: 11700, Train Loss: 8.579244475320461, Valid Loss: 8.613821756865647\n",
            "Epoch: 28, Step: 11800, Train Loss: 8.558162611273813, Valid Loss: 8.595036805885801\n",
            "Epoch: 28, Step: 11900, Train Loss: 8.549802396480969, Valid Loss: 8.587573953689986\n",
            "Epoch: 28, Step: 12000, Train Loss: 8.558800816588755, Valid Loss: 8.601754082841065\n",
            "Epoch: 28, Step: 12100, Train Loss: 8.57096407242178, Valid Loss: 8.610053860349035\n",
            "Epoch: 29, Step: 12200, Train Loss: 8.552556030455651, Valid Loss: 8.59658502443068\n",
            "Epoch: 29, Step: 12300, Train Loss: 8.557867211765423, Valid Loss: 8.603309762922837\n",
            "Epoch: 29, Step: 12400, Train Loss: 8.568249752875454, Valid Loss: 8.613600323604732\n",
            "Epoch: 29, Step: 12500, Train Loss: 8.552186648559271, Valid Loss: 8.594292003440092\n",
            "Epoch: 30, Step: 12600, Train Loss: 8.558195191021273, Valid Loss: 8.599742550474351\n",
            "Epoch: 30, Step: 12700, Train Loss: 8.55655023948495, Valid Loss: 8.597912949238692\n",
            "Epoch: 30, Step: 12800, Train Loss: 8.546059337048819, Valid Loss: 8.592508844002907\n",
            "Epoch: 30, Step: 12900, Train Loss: 8.532990471160312, Valid Loss: 8.57951303620584\n",
            "Epoch: 31, Step: 13000, Train Loss: 8.561196801536383, Valid Loss: 8.604146009508119\n",
            "Epoch: 31, Step: 13100, Train Loss: 8.550514898788906, Valid Loss: 8.591095870685578\n",
            "Epoch: 31, Step: 13200, Train Loss: 8.559068374078127, Valid Loss: 8.59566615480649\n",
            "Epoch: 31, Step: 13300, Train Loss: 8.542622434471864, Valid Loss: 8.584474011599168\n",
            "Epoch: 32, Step: 13400, Train Loss: 8.56314863884765, Valid Loss: 8.59688496573058\n",
            "Epoch: 32, Step: 13500, Train Loss: 8.54495270798789, Valid Loss: 8.589631808105622\n",
            "Epoch: 32, Step: 13600, Train Loss: 8.567819146065005, Valid Loss: 8.608738459620561\n",
            "Epoch: 32, Step: 13700, Train Loss: 8.547895454172323, Valid Loss: 8.584994519201121\n",
            "Epoch: 33, Step: 13800, Train Loss: 8.549992350939657, Valid Loss: 8.582352094887407\n",
            "Epoch: 33, Step: 13900, Train Loss: 8.53744774366034, Valid Loss: 8.575998137382294\n",
            "Epoch: 33, Step: 14000, Train Loss: 8.5558184332538, Valid Loss: 8.58947522708914\n",
            "Epoch: 33, Step: 14100, Train Loss: 8.560007954167492, Valid Loss: 8.598285911995509\n",
            "Epoch: 33, Step: 14200, Train Loss: 8.566867478624404, Valid Loss: 8.611109981582837\n",
            "Epoch: 34, Step: 14300, Train Loss: 8.553912825274544, Valid Loss: 8.593879937939132\n",
            "Epoch: 34, Step: 14400, Train Loss: 8.54412590552069, Valid Loss: 8.591428664183036\n",
            "Epoch: 34, Step: 14500, Train Loss: 8.530404401311484, Valid Loss: 8.584307089129455\n",
            "Epoch: 34, Step: 14600, Train Loss: 8.555374891079733, Valid Loss: 8.602796708730756\n",
            "Epoch: 35, Step: 14700, Train Loss: 8.535699664766925, Valid Loss: 8.586105211608784\n",
            "Epoch: 35, Step: 14800, Train Loss: 8.536720930993512, Valid Loss: 8.593729029040384\n",
            "Epoch: 35, Step: 14900, Train Loss: 8.565882405764594, Valid Loss: 8.617401487268214\n",
            "Epoch: 35, Step: 15000, Train Loss: 8.538414666107116, Valid Loss: 8.583346993137782\n",
            "Epoch: 36, Step: 15100, Train Loss: 8.529902600790127, Valid Loss: 8.573117629053359\n",
            "Epoch: 36, Step: 15200, Train Loss: 8.539347880531187, Valid Loss: 8.592553205995776\n",
            "Epoch: 36, Step: 15300, Train Loss: 8.547266766764821, Valid Loss: 8.59395990895046\n",
            "Epoch: 36, Step: 15400, Train Loss: 8.530978068480188, Valid Loss: 8.579048926425891\n",
            "Epoch: 37, Step: 15500, Train Loss: 8.547569981789495, Valid Loss: 8.593587550000514\n",
            "Epoch: 37, Step: 15600, Train Loss: 8.545304610879054, Valid Loss: 8.599257738681418\n",
            "Epoch: 37, Step: 15700, Train Loss: 8.542689280248652, Valid Loss: 8.603899904110731\n",
            "Epoch: 37, Step: 15800, Train Loss: 8.528002880166957, Valid Loss: 8.579550008180846\n",
            "Epoch: 38, Step: 15900, Train Loss: 8.536342319975793, Valid Loss: 8.588877361645068\n",
            "Epoch: 38, Step: 16000, Train Loss: 8.550225683566463, Valid Loss: 8.604414351264978\n",
            "Epoch: 38, Step: 16100, Train Loss: 8.540989607763876, Valid Loss: 8.596526307830914\n",
            "Epoch: 38, Step: 16200, Train Loss: 8.546610644380152, Valid Loss: 8.594685973161527\n",
            "Epoch: 38, Step: 16300, Train Loss: 8.565063888357134, Valid Loss: 8.617072666258155\n",
            "Epoch: 39, Step: 16400, Train Loss: 8.55554648400146, Valid Loss: 8.604300877022522\n",
            "Epoch: 39, Step: 16500, Train Loss: 8.525496562884118, Valid Loss: 8.578038489873023\n",
            "Epoch: 39, Step: 16600, Train Loss: 8.53265177625152, Valid Loss: 8.586045246002458\n",
            "Epoch: 39, Step: 16700, Train Loss: 8.558277187220616, Valid Loss: 8.611861110899092\n",
            "Epoch: 40, Step: 16800, Train Loss: 8.529044556230835, Valid Loss: 8.575537896806283\n",
            "Epoch: 40, Step: 16900, Train Loss: 8.54773816579468, Valid Loss: 8.597827302929124\n",
            "Epoch: 40, Step: 17000, Train Loss: 8.519982123679167, Valid Loss: 8.575696227852283\n",
            "Epoch: 40, Step: 17100, Train Loss: 8.529662402070143, Valid Loss: 8.58092045832577\n",
            "Epoch: 41, Step: 17200, Train Loss: 8.546616055767613, Valid Loss: 8.599159346226656\n",
            "Epoch: 41, Step: 17300, Train Loss: 8.553478210346778, Valid Loss: 8.599784223024539\n",
            "Epoch: 41, Step: 17400, Train Loss: 8.54797375756102, Valid Loss: 8.601376958803522\n",
            "Epoch: 41, Step: 17500, Train Loss: 8.522089070414138, Valid Loss: 8.584292175585555\n",
            "Epoch: 42, Step: 17600, Train Loss: 8.526507604366452, Valid Loss: 8.582092341571684\n",
            "Epoch: 42, Step: 17700, Train Loss: 8.522371212747691, Valid Loss: 8.584654916456405\n",
            "Epoch: 42, Step: 17800, Train Loss: 8.526467476282148, Valid Loss: 8.583294854801924\n",
            "Epoch: 42, Step: 17900, Train Loss: 8.518668363409812, Valid Loss: 8.575469453510678\n",
            "Epoch: 43, Step: 18000, Train Loss: 8.514278666218619, Valid Loss: 8.573600281137418\n",
            "Epoch: 43, Step: 18100, Train Loss: 8.538951341822175, Valid Loss: 8.58773388489356\n",
            "Epoch: 43, Step: 18200, Train Loss: 8.529463774630806, Valid Loss: 8.58238772956098\n",
            "Epoch: 43, Step: 18300, Train Loss: 8.522183805416063, Valid Loss: 8.579824143704007\n",
            "Epoch: 44, Step: 18400, Train Loss: 8.509814599514865, Valid Loss: 8.572648584065233\n",
            "Epoch: 44, Step: 18500, Train Loss: 8.535061150013524, Valid Loss: 8.599621731507444\n",
            "Epoch: 44, Step: 18600, Train Loss: 8.534430305580349, Valid Loss: 8.588075170712099\n",
            "Epoch: 44, Step: 18700, Train Loss: 8.507694177196354, Valid Loss: 8.580079456779737\n",
            "Epoch: 44, Step: 18800, Train Loss: 8.529594470171594, Valid Loss: 8.598937165574526\n",
            "Epoch: 45, Step: 18900, Train Loss: 8.542366962317107, Valid Loss: 8.607134913349777\n",
            "Epoch: 45, Step: 19000, Train Loss: 8.524866224163944, Valid Loss: 8.588430925136057\n",
            "Epoch: 45, Step: 19100, Train Loss: 8.525101368479051, Valid Loss: 8.584168420875116\n",
            "Epoch: 45, Step: 19200, Train Loss: 8.526720743674208, Valid Loss: 8.59006405339798\n",
            "Epoch: 46, Step: 19300, Train Loss: 8.525027409445991, Valid Loss: 8.582001685926656\n",
            "Epoch: 46, Step: 19400, Train Loss: 8.528488822877643, Valid Loss: 8.597835150896218\n",
            "Epoch: 46, Step: 19500, Train Loss: 8.533565948354157, Valid Loss: 8.596426468112332\n",
            "Epoch: 46, Step: 19600, Train Loss: 8.531538797612527, Valid Loss: 8.595971971319914\n",
            "Epoch: 47, Step: 19700, Train Loss: 8.541985305843642, Valid Loss: 8.610323475728132\n",
            "Epoch: 47, Step: 19800, Train Loss: 8.505869062266372, Valid Loss: 8.578512184417425\n",
            "Epoch: 47, Step: 19900, Train Loss: 8.524247182473397, Valid Loss: 8.587659366288996\n",
            "Epoch: 47, Step: 20000, Train Loss: 8.519771323885669, Valid Loss: 8.577842832521922\n",
            "Epoch: 48, Step: 20100, Train Loss: 8.538026558048372, Valid Loss: 8.591108171542366\n",
            "Epoch: 48, Step: 20200, Train Loss: 8.520562725408771, Valid Loss: 8.578296956767375\n",
            "Epoch: 48, Step: 20300, Train Loss: 8.536450624080434, Valid Loss: 8.591463663106522\n",
            "Epoch: 48, Step: 20400, Train Loss: 8.511115605854371, Valid Loss: 8.57672135634949\n",
            "Epoch: 49, Step: 20500, Train Loss: 8.510726907256977, Valid Loss: 8.577574901836337\n",
            "Epoch: 49, Step: 20600, Train Loss: 8.531407405579115, Valid Loss: 8.58251068056344\n",
            "Epoch: 49, Step: 20700, Train Loss: 8.530063175735718, Valid Loss: 8.593861051497713\n",
            "Epoch: 49, Step: 20800, Train Loss: 8.526338565776973, Valid Loss: 8.590251258795796\n",
            "Epoch: 49, Step: 20900, Train Loss: 8.519070154733829, Valid Loss: 8.583552383203477\n",
            "Epoch: 50, Step: 21000, Train Loss: 8.497608406471935, Valid Loss: 8.574317083047907\n",
            "Epoch: 50, Step: 21100, Train Loss: 8.518365275807307, Valid Loss: 8.583101714471313\n",
            "Epoch: 50, Step: 21200, Train Loss: 8.528841571608934, Valid Loss: 8.595441526855609\n",
            "Epoch: 50, Step: 21300, Train Loss: 8.522297079833082, Valid Loss: 8.58746286871019\n",
            "Epoch: 51, Step: 21400, Train Loss: 8.504414437574098, Valid Loss: 8.57542376540358\n",
            "Epoch: 51, Step: 21500, Train Loss: 8.515710810074095, Valid Loss: 8.58485334759193\n",
            "Epoch: 51, Step: 21600, Train Loss: 8.500206242556796, Valid Loss: 8.56361156631756\n",
            "Epoch: 51, Step: 21700, Train Loss: 8.5170936554261, Valid Loss: 8.580958614545267\n",
            "Epoch: 52, Step: 21800, Train Loss: 8.523819470038479, Valid Loss: 8.587243075767029\n",
            "Epoch: 52, Step: 21900, Train Loss: 8.519793649519647, Valid Loss: 8.58442125698181\n",
            "Epoch: 52, Step: 22000, Train Loss: 8.522604178098613, Valid Loss: 8.59030651008587\n",
            "Epoch: 52, Step: 22100, Train Loss: 8.504778848994416, Valid Loss: 8.57283260528232\n",
            "Epoch: 53, Step: 22200, Train Loss: 8.504416870129733, Valid Loss: 8.584485041355423\n",
            "Epoch: 53, Step: 22300, Train Loss: 8.51118838647603, Valid Loss: 8.586722581837625\n",
            "Epoch: 53, Step: 22400, Train Loss: 8.503556875018178, Valid Loss: 8.570705988104587\n",
            "Epoch: 53, Step: 22500, Train Loss: 8.508657397461429, Valid Loss: 8.580902264913286\n",
            "Epoch: 54, Step: 22600, Train Loss: 8.518783115933521, Valid Loss: 8.581845525203791\n",
            "Epoch: 54, Step: 22700, Train Loss: 8.513167727320173, Valid Loss: 8.577786550807053\n",
            "Epoch: 54, Step: 22800, Train Loss: 8.516279420345315, Valid Loss: 8.586482280402707\n",
            "Epoch: 54, Step: 22900, Train Loss: 8.498907622006167, Valid Loss: 8.576602810991448\n",
            "Epoch: 55, Step: 23000, Train Loss: 8.511473718494088, Valid Loss: 8.580101839296645\n",
            "Epoch: 55, Step: 23100, Train Loss: 8.513839377760435, Valid Loss: 8.581502006275077\n",
            "Epoch: 55, Step: 23200, Train Loss: 8.496910884789493, Valid Loss: 8.574713480987393\n",
            "Epoch: 55, Step: 23300, Train Loss: 8.514368522618334, Valid Loss: 8.584772158391837\n",
            "Epoch: 55, Step: 23400, Train Loss: 8.531294980606907, Valid Loss: 8.59772813477272\n",
            "Epoch: 56, Step: 23500, Train Loss: 8.516709336593607, Valid Loss: 8.58593689576285\n",
            "Epoch: 56, Step: 23600, Train Loss: 8.534074764925549, Valid Loss: 8.605330304457272\n",
            "Epoch: 56, Step: 23700, Train Loss: 8.506382700636697, Valid Loss: 8.582703866093029\n",
            "Epoch: 56, Step: 23800, Train Loss: 8.513482243331593, Valid Loss: 8.588188773515755\n",
            "Epoch: 57, Step: 23900, Train Loss: 8.512555914230264, Valid Loss: 8.587552787399922\n",
            "Epoch: 57, Step: 24000, Train Loss: 8.491440561909128, Valid Loss: 8.575918467912391\n",
            "Epoch: 57, Step: 24100, Train Loss: 8.494710289305164, Valid Loss: 8.582838286107041\n",
            "Epoch: 57, Step: 24200, Train Loss: 8.496996878898091, Valid Loss: 8.573881892192908\n",
            "Epoch: 58, Step: 24300, Train Loss: 8.523013996607164, Valid Loss: 8.597141415841692\n",
            "Epoch: 58, Step: 24400, Train Loss: 8.49357497331313, Valid Loss: 8.573770833686686\n",
            "Epoch: 58, Step: 24500, Train Loss: 8.511531642143952, Valid Loss: 8.588448869650284\n",
            "Epoch: 58, Step: 24600, Train Loss: 8.510722451290038, Valid Loss: 8.58668673215599\n",
            "Epoch: 59, Step: 24700, Train Loss: 8.51130881856693, Valid Loss: 8.585754771857859\n",
            "Epoch: 59, Step: 24800, Train Loss: 8.503846525885264, Valid Loss: 8.575939626081931\n",
            "Epoch: 59, Step: 24900, Train Loss: 8.518357443995198, Valid Loss: 8.586799333057527\n",
            "Epoch: 59, Step: 25000, Train Loss: 8.507232017736214, Valid Loss: 8.575193668400141\n",
            "Epoch: 60, Step: 25100, Train Loss: 8.520781806181873, Valid Loss: 8.592195181022063\n",
            "Epoch: 60, Step: 25200, Train Loss: 8.51343907856665, Valid Loss: 8.585528008092636\n",
            "Epoch: 60, Step: 25300, Train Loss: 8.490607257362692, Valid Loss: 8.578974548405567\n",
            "Epoch: 60, Step: 25400, Train Loss: 8.514134821561166, Valid Loss: 8.58576676149379\n",
            "Epoch: 61, Step: 25500, Train Loss: 8.502536406776414, Valid Loss: 8.578267206956669\n",
            "Epoch: 61, Step: 25600, Train Loss: 8.472483956679444, Valid Loss: 8.56565089124681\n",
            "Epoch: 61, Step: 25700, Train Loss: 8.508835249123493, Valid Loss: 8.58841315958866\n",
            "Epoch: 61, Step: 25800, Train Loss: 8.501069587623807, Valid Loss: 8.583428053513062\n",
            "Epoch: 61, Step: 25900, Train Loss: 8.501348468440112, Valid Loss: 8.582043871607153\n",
            "Epoch: 62, Step: 26000, Train Loss: 8.495477144976144, Valid Loss: 8.572950672162234\n",
            "Epoch: 62, Step: 26100, Train Loss: 8.503645029968455, Valid Loss: 8.571379280599086\n",
            "Epoch: 62, Step: 26200, Train Loss: 8.526343239170169, Valid Loss: 8.594752303184586\n",
            "Epoch: 62, Step: 26300, Train Loss: 8.506014453531975, Valid Loss: 8.58793777716416\n",
            "Epoch: 63, Step: 26400, Train Loss: 8.487029577051945, Valid Loss: 8.57280771773013\n",
            "Epoch: 63, Step: 26500, Train Loss: 8.485526369420779, Valid Loss: 8.570937517683971\n",
            "Epoch: 63, Step: 26600, Train Loss: 8.501970631155231, Valid Loss: 8.579875971765972\n",
            "Epoch: 63, Step: 26700, Train Loss: 8.49426535841652, Valid Loss: 8.573707340508689\n",
            "Epoch: 64, Step: 26800, Train Loss: 8.49812093443973, Valid Loss: 8.58266732729627\n",
            "Epoch: 64, Step: 26900, Train Loss: 8.504657579363414, Valid Loss: 8.588732078063476\n",
            "Epoch: 64, Step: 27000, Train Loss: 8.505447139200564, Valid Loss: 8.58661657504624\n",
            "Epoch: 64, Step: 27100, Train Loss: 8.495324264869076, Valid Loss: 8.573228116297381\n",
            "Epoch: 65, Step: 27200, Train Loss: 8.478496000727812, Valid Loss: 8.565012191526154\n",
            "Epoch: 65, Step: 27300, Train Loss: 8.511048527768976, Valid Loss: 8.587728999894761\n",
            "Epoch: 65, Step: 27400, Train Loss: 8.4983968397781, Valid Loss: 8.586295382812851\n",
            "Epoch: 65, Step: 27500, Train Loss: 8.50588892980898, Valid Loss: 8.594130858765935\n",
            "Epoch: 66, Step: 27600, Train Loss: 8.499475714208545, Valid Loss: 8.584209526262384\n",
            "Epoch: 66, Step: 27700, Train Loss: 8.483071257705598, Valid Loss: 8.569482522422756\n",
            "Epoch: 66, Step: 27800, Train Loss: 8.497124448978216, Valid Loss: 8.575113849012912\n",
            "Epoch: 66, Step: 27900, Train Loss: 8.4778806353226, Valid Loss: 8.568995681555808\n",
            "Epoch: 66, Step: 28000, Train Loss: 8.48319533910503, Valid Loss: 8.565544650718408\n",
            "Epoch: 67, Step: 28100, Train Loss: 8.506613691733536, Valid Loss: 8.579556434444672\n",
            "Epoch: 67, Step: 28200, Train Loss: 8.498331265168712, Valid Loss: 8.582155564133448\n",
            "Epoch: 67, Step: 28300, Train Loss: 8.502641368468247, Valid Loss: 8.586506684388713\n",
            "Epoch: 67, Step: 28400, Train Loss: 8.484459815015008, Valid Loss: 8.569576792788943\n",
            "Epoch: 68, Step: 28500, Train Loss: 8.471454893410616, Valid Loss: 8.565487612059048\n",
            "Epoch: 68, Step: 28600, Train Loss: 8.496110072002972, Valid Loss: 8.576467595877782\n",
            "Epoch: 68, Step: 28700, Train Loss: 8.491853408733446, Valid Loss: 8.575256217340105\n",
            "Epoch: 68, Step: 28800, Train Loss: 8.501070195657993, Valid Loss: 8.58054443120577\n",
            "Epoch: 69, Step: 28900, Train Loss: 8.495467346944539, Valid Loss: 8.583927329093312\n",
            "Epoch: 69, Step: 29000, Train Loss: 8.499828775328677, Valid Loss: 8.586640159475428\n",
            "Epoch: 69, Step: 29100, Train Loss: 8.482971919891797, Valid Loss: 8.576676686564792\n",
            "Epoch: 69, Step: 29200, Train Loss: 8.497389400698603, Valid Loss: 8.586111500516727\n",
            "Epoch: 70, Step: 29300, Train Loss: 8.480995569749789, Valid Loss: 8.57785668438622\n",
            "Epoch: 70, Step: 29400, Train Loss: 8.502703048778312, Valid Loss: 8.589385635004177\n",
            "Epoch: 70, Step: 29500, Train Loss: 8.509533277711167, Valid Loss: 8.59793080293065\n",
            "Epoch: 70, Step: 29600, Train Loss: 8.494117112361772, Valid Loss: 8.580405876405544\n",
            "Epoch: 71, Step: 29700, Train Loss: 8.486928488480268, Valid Loss: 8.576801804100212\n",
            "Epoch: 71, Step: 29800, Train Loss: 8.481152037976635, Valid Loss: 8.572696161355992\n",
            "Epoch: 71, Step: 29900, Train Loss: 8.490200183135832, Valid Loss: 8.584369571665613\n",
            "Epoch: 71, Step: 30000, Train Loss: 8.491979928539498, Valid Loss: 8.591361627045952\n",
            "Epoch: 72, Step: 30100, Train Loss: 8.499160062897126, Valid Loss: 8.589472549711353\n",
            "Epoch: 72, Step: 30200, Train Loss: 8.471400914951696, Valid Loss: 8.573432572161801\n",
            "Epoch: 72, Step: 30300, Train Loss: 8.490901777770874, Valid Loss: 8.58368419907121\n",
            "Epoch: 72, Step: 30400, Train Loss: 8.48436374866943, Valid Loss: 8.58476913855876\n",
            "Epoch: 72, Step: 30500, Train Loss: 8.481874409674667, Valid Loss: 8.565890282265466\n",
            "Epoch: 73, Step: 30600, Train Loss: 8.497078863946143, Valid Loss: 8.586515930905996\n",
            "Epoch: 73, Step: 30700, Train Loss: 8.485802482408, Valid Loss: 8.576770961342387\n",
            "Epoch: 73, Step: 30800, Train Loss: 8.48959288528496, Valid Loss: 8.573420670902216\n",
            "Epoch: 73, Step: 30900, Train Loss: 8.481094820351718, Valid Loss: 8.58141829444373\n",
            "Epoch: 74, Step: 31000, Train Loss: 8.486274428608915, Valid Loss: 8.574416919857764\n",
            "Epoch: 74, Step: 31100, Train Loss: 8.479327690659623, Valid Loss: 8.57397184523706\n",
            "Epoch: 74, Step: 31200, Train Loss: 8.488757106759541, Valid Loss: 8.578711570205975\n",
            "Epoch: 74, Step: 31300, Train Loss: 8.471087734162523, Valid Loss: 8.563150805106531\n",
            "Epoch: 75, Step: 31400, Train Loss: 8.483236448345172, Valid Loss: 8.579267471187292\n",
            "Epoch: 75, Step: 31500, Train Loss: 8.48524239196589, Valid Loss: 8.576058340224087\n",
            "Epoch: 75, Step: 31600, Train Loss: 8.495351319300532, Valid Loss: 8.574943308439943\n",
            "Epoch: 75, Step: 31700, Train Loss: 8.489952162916959, Valid Loss: 8.56889799105398\n",
            "Epoch: 76, Step: 31800, Train Loss: 8.489542006640248, Valid Loss: 8.58248778657537\n",
            "Epoch: 76, Step: 31900, Train Loss: 8.487872503450474, Valid Loss: 8.576428241018489\n",
            "Epoch: 76, Step: 32000, Train Loss: 8.488027069829794, Valid Loss: 8.58227298744449\n",
            "Epoch: 76, Step: 32100, Train Loss: 8.496250827132055, Valid Loss: 8.578216044307794\n",
            "Epoch: 77, Step: 32200, Train Loss: 8.491125462598461, Valid Loss: 8.582850755835521\n",
            "Epoch: 77, Step: 32300, Train Loss: 8.477123988433313, Valid Loss: 8.577195909338206\n",
            "Epoch: 77, Step: 32400, Train Loss: 8.504924418471631, Valid Loss: 8.59398923685931\n",
            "Epoch: 77, Step: 32500, Train Loss: 8.486497570352363, Valid Loss: 8.577749036005748\n",
            "Epoch: 77, Step: 32600, Train Loss: 8.468577894535915, Valid Loss: 8.56963318248675\n",
            "Epoch: 78, Step: 32700, Train Loss: 8.491621635025286, Valid Loss: 8.580311602295119\n",
            "Epoch: 78, Step: 32800, Train Loss: 8.477265258923142, Valid Loss: 8.570904120529553\n",
            "Epoch: 78, Step: 32900, Train Loss: 8.478572800935645, Valid Loss: 8.570568235025538\n",
            "Epoch: 78, Step: 33000, Train Loss: 8.487627000626258, Valid Loss: 8.585947889300389\n",
            "Epoch: 79, Step: 33100, Train Loss: 8.488582534857303, Valid Loss: 8.580091736612834\n",
            "Epoch: 79, Step: 33200, Train Loss: 8.473175533344017, Valid Loss: 8.56223841462483\n",
            "Epoch: 79, Step: 33300, Train Loss: 8.49486892423193, Valid Loss: 8.590525423005447\n",
            "Epoch: 79, Step: 33400, Train Loss: 8.471734971458366, Valid Loss: 8.566205724647599\n",
            "Epoch: 80, Step: 33500, Train Loss: 8.487382112673279, Valid Loss: 8.5823187568747\n",
            "Epoch: 80, Step: 33600, Train Loss: 8.493285794809205, Valid Loss: 8.58842450060832\n",
            "Epoch: 80, Step: 33700, Train Loss: 8.478971127669642, Valid Loss: 8.576477354145894\n",
            "Epoch: 80, Step: 33800, Train Loss: 8.486355816337868, Valid Loss: 8.585554502235166\n",
            "Epoch: 81, Step: 33900, Train Loss: 8.483640707143447, Valid Loss: 8.574650102855662\n",
            "Epoch: 81, Step: 34000, Train Loss: 8.50425955077618, Valid Loss: 8.592568220395187\n",
            "Epoch: 81, Step: 34100, Train Loss: 8.490688181906968, Valid Loss: 8.580597300469213\n",
            "Epoch: 81, Step: 34200, Train Loss: 8.482587856708054, Valid Loss: 8.568747649858604\n",
            "Epoch: 82, Step: 34300, Train Loss: 8.484092821964373, Valid Loss: 8.579429003041716\n",
            "Epoch: 82, Step: 34400, Train Loss: 8.47704115812643, Valid Loss: 8.573090700980176\n",
            "Epoch: 82, Step: 34500, Train Loss: 8.44574139465566, Valid Loss: 8.559537378531186\n",
            "Epoch: 82, Step: 34600, Train Loss: 8.449163656993708, Valid Loss: 8.561529154220988\n",
            "Epoch: 83, Step: 34700, Train Loss: 8.480023264350129, Valid Loss: 8.581208403884865\n",
            "Epoch: 83, Step: 34800, Train Loss: 8.490366437516775, Valid Loss: 8.593355091635889\n",
            "Epoch: 83, Step: 34900, Train Loss: 8.467081796338007, Valid Loss: 8.563902764474523\n",
            "Epoch: 83, Step: 35000, Train Loss: 8.4871097256855, Valid Loss: 8.580638916285345\n",
            "Epoch: 83, Step: 35100, Train Loss: 8.487020182831069, Valid Loss: 8.58283071396477\n",
            "Epoch: 84, Step: 35200, Train Loss: 8.485977789276461, Valid Loss: 8.582945924638372\n",
            "Epoch: 84, Step: 35300, Train Loss: 8.458964793382716, Valid Loss: 8.566913358908234\n",
            "Epoch: 84, Step: 35400, Train Loss: 8.46464969603653, Valid Loss: 8.568121814706325\n",
            "Epoch: 84, Step: 35500, Train Loss: 8.46526926478353, Valid Loss: 8.566405082195933\n",
            "Epoch: 85, Step: 35600, Train Loss: 8.474432838897759, Valid Loss: 8.569678699952656\n",
            "Epoch: 85, Step: 35700, Train Loss: 8.475470538745364, Valid Loss: 8.572408209935196\n",
            "Epoch: 85, Step: 35800, Train Loss: 8.482319195483996, Valid Loss: 8.581960113528261\n",
            "Epoch: 85, Step: 35900, Train Loss: 8.486322007864706, Valid Loss: 8.58388247109432\n",
            "Epoch: 86, Step: 36000, Train Loss: 8.473512980303811, Valid Loss: 8.5690779000705\n",
            "Epoch: 86, Step: 36100, Train Loss: 8.479397236439736, Valid Loss: 8.573713061275827\n",
            "Epoch: 86, Step: 36200, Train Loss: 8.477818395458192, Valid Loss: 8.573768780218211\n",
            "Epoch: 86, Step: 36300, Train Loss: 8.476282441504697, Valid Loss: 8.572078859458733\n",
            "Epoch: 87, Step: 36400, Train Loss: 8.47982094903392, Valid Loss: 8.582085983462632\n",
            "Epoch: 87, Step: 36500, Train Loss: 8.452333122490757, Valid Loss: 8.565307327665828\n",
            "Epoch: 87, Step: 36600, Train Loss: 8.466477840715841, Valid Loss: 8.570178103224542\n",
            "Epoch: 87, Step: 36700, Train Loss: 8.503338578037107, Valid Loss: 8.588835584683537\n",
            "Epoch: 88, Step: 36800, Train Loss: 8.477342263221312, Valid Loss: 8.57510989989745\n",
            "Epoch: 88, Step: 36900, Train Loss: 8.48018128027947, Valid Loss: 8.580111608158406\n",
            "Epoch: 88, Step: 37000, Train Loss: 8.47535835492493, Valid Loss: 8.579837089089665\n",
            "Epoch: 88, Step: 37100, Train Loss: 8.493020925288251, Valid Loss: 8.599953967338045\n",
            "Epoch: 88, Step: 37200, Train Loss: 8.460842705643378, Valid Loss: 8.560566129674816\n",
            "Epoch: 89, Step: 37300, Train Loss: 8.496322011866207, Valid Loss: 8.592110845408532\n",
            "Epoch: 89, Step: 37400, Train Loss: 8.4608046440118, Valid Loss: 8.56550951638503\n",
            "Epoch: 89, Step: 37500, Train Loss: 8.458649835925042, Valid Loss: 8.567458914761396\n",
            "Epoch: 89, Step: 37600, Train Loss: 8.474693223587932, Valid Loss: 8.574906395193718\n",
            "Epoch: 90, Step: 37700, Train Loss: 8.456600344518927, Valid Loss: 8.565355255532577\n",
            "Epoch: 90, Step: 37800, Train Loss: 8.482448154838305, Valid Loss: 8.584116940117248\n",
            "Epoch: 90, Step: 37900, Train Loss: 8.477901604302843, Valid Loss: 8.57913698912896\n",
            "Epoch: 90, Step: 38000, Train Loss: 8.471113900863498, Valid Loss: 8.573576400463896\n",
            "Epoch: 91, Step: 38100, Train Loss: 8.473864021032602, Valid Loss: 8.569102333961402\n",
            "Epoch: 91, Step: 38200, Train Loss: 8.486233306787739, Valid Loss: 8.578553839807947\n",
            "Epoch: 91, Step: 38300, Train Loss: 8.469855848464764, Valid Loss: 8.569761764649924\n",
            "Epoch: 91, Step: 38400, Train Loss: 8.47752584753891, Valid Loss: 8.57659431253412\n",
            "Epoch: 92, Step: 38500, Train Loss: 8.483309943403764, Valid Loss: 8.582903564272312\n",
            "Epoch: 92, Step: 38600, Train Loss: 8.481957035258871, Valid Loss: 8.58455345692906\n",
            "Epoch: 92, Step: 38700, Train Loss: 8.456324464564359, Valid Loss: 8.569291933868262\n",
            "Epoch: 92, Step: 38800, Train Loss: 8.467971633819737, Valid Loss: 8.57645338326209\n",
            "Epoch: 93, Step: 38900, Train Loss: 8.47462087283737, Valid Loss: 8.579221768939924\n",
            "Epoch: 93, Step: 39000, Train Loss: 8.480169647566019, Valid Loss: 8.577715478778755\n",
            "Epoch: 93, Step: 39100, Train Loss: 8.461873865558674, Valid Loss: 8.57107234336623\n",
            "Epoch: 93, Step: 39200, Train Loss: 8.463317012232325, Valid Loss: 8.568298692261468\n",
            "Epoch: 94, Step: 39300, Train Loss: 8.463024211873982, Valid Loss: 8.572820536142798\n",
            "Epoch: 94, Step: 39400, Train Loss: 8.46705988802554, Valid Loss: 8.572536182987466\n",
            "Epoch: 94, Step: 39500, Train Loss: 8.471388985124758, Valid Loss: 8.576112506704327\n",
            "Epoch: 94, Step: 39600, Train Loss: 8.478648666528064, Valid Loss: 8.580416985023184\n",
            "Epoch: 94, Step: 39700, Train Loss: 8.458722273488501, Valid Loss: 8.5637945791939\n",
            "Epoch: 95, Step: 39800, Train Loss: 8.452136037604832, Valid Loss: 8.561620366092349\n",
            "Epoch: 95, Step: 39900, Train Loss: 8.471830429735475, Valid Loss: 8.583815409371615\n",
            "Epoch: 95, Step: 40000, Train Loss: 8.491847972644191, Valid Loss: 8.592571818357063\n",
            "Epoch: 95, Step: 40100, Train Loss: 8.463599875749178, Valid Loss: 8.568280540869145\n",
            "Epoch: 96, Step: 40200, Train Loss: 8.479675151154277, Valid Loss: 8.568543836750905\n",
            "Epoch: 96, Step: 40300, Train Loss: 8.481437214287524, Valid Loss: 8.58157361874154\n",
            "Epoch: 96, Step: 40400, Train Loss: 8.460870198057595, Valid Loss: 8.56575835729868\n",
            "Epoch: 96, Step: 40500, Train Loss: 8.464349903711193, Valid Loss: 8.569048595038447\n",
            "Epoch: 97, Step: 40600, Train Loss: 8.471064810023071, Valid Loss: 8.575570627587147\n",
            "Epoch: 97, Step: 40700, Train Loss: 8.476759664289762, Valid Loss: 8.577227440510615\n",
            "Epoch: 97, Step: 40800, Train Loss: 8.485915614591187, Valid Loss: 8.588196355646925\n",
            "Epoch: 97, Step: 40900, Train Loss: 8.475541371521112, Valid Loss: 8.576783351496307\n",
            "Epoch: 98, Step: 41000, Train Loss: 8.48187411283201, Valid Loss: 8.586713561182014\n",
            "Epoch: 98, Step: 41100, Train Loss: 8.471915093585352, Valid Loss: 8.573951512424097\n",
            "Epoch: 98, Step: 41200, Train Loss: 8.441405174885075, Valid Loss: 8.554306263546726\n",
            "Epoch: 98, Step: 41300, Train Loss: 8.46478573727665, Valid Loss: 8.572880066093632\n",
            "Epoch: 99, Step: 41400, Train Loss: 8.472615725875766, Valid Loss: 8.580327835874964\n",
            "Epoch: 99, Step: 41500, Train Loss: 8.464453730852982, Valid Loss: 8.579071557109827\n",
            "Epoch: 99, Step: 41600, Train Loss: 8.473363578615535, Valid Loss: 8.576709486663564\n",
            "Epoch: 99, Step: 41700, Train Loss: 8.476152433995592, Valid Loss: 8.575864585068011\n",
            "Epoch: 99, Step: 41800, Train Loss: 8.477216460134406, Valid Loss: 8.573700101730887\n",
            "Test RMSE Loss for z = 0.9: 8.771708431376377\n",
            "Start training for z = 1.0\n",
            "Epoch: 0, Step: 100, Train Loss: 9.47619966835244, Valid Loss: 9.436763585659312\n",
            "Epoch: 0, Step: 200, Train Loss: 9.137375808584684, Valid Loss: 9.084242674099885\n",
            "Epoch: 0, Step: 300, Train Loss: 9.052591974910708, Valid Loss: 8.994481097943282\n",
            "Epoch: 0, Step: 400, Train Loss: 8.978080713838326, Valid Loss: 8.916651783106085\n",
            "Epoch: 1, Step: 500, Train Loss: 8.959214307194229, Valid Loss: 8.913765873493325\n",
            "Epoch: 1, Step: 600, Train Loss: 8.918513323907767, Valid Loss: 8.865509302221673\n",
            "Epoch: 1, Step: 700, Train Loss: 8.8931395959791, Valid Loss: 8.841550134092314\n",
            "Epoch: 1, Step: 800, Train Loss: 8.884395697964836, Valid Loss: 8.842329238877218\n",
            "Epoch: 2, Step: 900, Train Loss: 8.864536053132715, Valid Loss: 8.822763399554773\n",
            "Epoch: 2, Step: 1000, Train Loss: 8.839690319470519, Valid Loss: 8.79956817111573\n",
            "Epoch: 2, Step: 1100, Train Loss: 8.834958078096165, Valid Loss: 8.796943450637876\n",
            "Epoch: 2, Step: 1200, Train Loss: 8.81271294540789, Valid Loss: 8.770001347198775\n",
            "Epoch: 3, Step: 1300, Train Loss: 8.807882744784434, Valid Loss: 8.771551919605692\n",
            "Epoch: 3, Step: 1400, Train Loss: 8.786721441868018, Valid Loss: 8.756185034250084\n",
            "Epoch: 3, Step: 1500, Train Loss: 8.779861987312936, Valid Loss: 8.750209256979558\n",
            "Epoch: 3, Step: 1600, Train Loss: 8.776839247586029, Valid Loss: 8.742782319704258\n",
            "Epoch: 4, Step: 1700, Train Loss: 8.764263153844784, Valid Loss: 8.732172042253323\n",
            "Epoch: 4, Step: 1800, Train Loss: 8.776770124192199, Valid Loss: 8.757261658415365\n",
            "Epoch: 4, Step: 1900, Train Loss: 8.75081523564211, Valid Loss: 8.72245098607132\n",
            "Epoch: 4, Step: 2000, Train Loss: 8.768331226447874, Valid Loss: 8.738822393872098\n",
            "Epoch: 5, Step: 2100, Train Loss: 8.749302094316102, Valid Loss: 8.72396989635786\n",
            "Epoch: 5, Step: 2200, Train Loss: 8.731969046830066, Valid Loss: 8.706171985489249\n",
            "Epoch: 5, Step: 2300, Train Loss: 8.727168447883152, Valid Loss: 8.702488601511524\n",
            "Epoch: 5, Step: 2400, Train Loss: 8.750548694292995, Valid Loss: 8.725672726313123\n",
            "Epoch: 5, Step: 2500, Train Loss: 8.716295927812268, Valid Loss: 8.693135043878389\n",
            "Epoch: 6, Step: 2600, Train Loss: 8.72426767363314, Valid Loss: 8.706187031326126\n",
            "Epoch: 6, Step: 2700, Train Loss: 8.741754458624893, Valid Loss: 8.723334425594388\n",
            "Epoch: 6, Step: 2800, Train Loss: 8.729346854438413, Valid Loss: 8.716537045577343\n",
            "Epoch: 6, Step: 2900, Train Loss: 8.707390528921968, Valid Loss: 8.694952015071737\n",
            "Epoch: 7, Step: 3000, Train Loss: 8.699731971894371, Valid Loss: 8.688227843573992\n",
            "Epoch: 7, Step: 3100, Train Loss: 8.701813460702894, Valid Loss: 8.694663304577\n",
            "Epoch: 7, Step: 3200, Train Loss: 8.699246178337303, Valid Loss: 8.690453518563725\n",
            "Epoch: 7, Step: 3300, Train Loss: 8.70003196889925, Valid Loss: 8.697600988585483\n",
            "Epoch: 8, Step: 3400, Train Loss: 8.69149645682349, Valid Loss: 8.685999801681787\n",
            "Epoch: 8, Step: 3500, Train Loss: 8.693084125394156, Valid Loss: 8.689696803296798\n",
            "Epoch: 8, Step: 3600, Train Loss: 8.682733717690736, Valid Loss: 8.675125858863503\n",
            "Epoch: 8, Step: 3700, Train Loss: 8.695186633076865, Valid Loss: 8.69082597761033\n",
            "Epoch: 9, Step: 3800, Train Loss: 8.678923579624218, Valid Loss: 8.675078566586187\n",
            "Epoch: 9, Step: 3900, Train Loss: 8.680749396656326, Valid Loss: 8.676965032554385\n",
            "Epoch: 9, Step: 4000, Train Loss: 8.669425322661116, Valid Loss: 8.663381612884258\n",
            "Epoch: 9, Step: 4100, Train Loss: 8.65487507156013, Valid Loss: 8.648026210340118\n",
            "Epoch: 10, Step: 4200, Train Loss: 8.665452597090706, Valid Loss: 8.662652987208796\n",
            "Epoch: 10, Step: 4300, Train Loss: 8.656048649158063, Valid Loss: 8.661270235422512\n",
            "Epoch: 10, Step: 4400, Train Loss: 8.65745546309574, Valid Loss: 8.655698603956658\n",
            "Epoch: 10, Step: 4500, Train Loss: 8.665501197515017, Valid Loss: 8.670530668796953\n",
            "Epoch: 11, Step: 4600, Train Loss: 8.674866199167626, Valid Loss: 8.67879919629579\n",
            "Epoch: 11, Step: 4700, Train Loss: 8.658398346127589, Valid Loss: 8.661470556044252\n",
            "Epoch: 11, Step: 4800, Train Loss: 8.681906475067532, Valid Loss: 8.693435372528672\n",
            "Epoch: 11, Step: 4900, Train Loss: 8.658826824064041, Valid Loss: 8.672588226685763\n",
            "Epoch: 11, Step: 5000, Train Loss: 8.663746478465773, Valid Loss: 8.674053799225273\n",
            "Epoch: 12, Step: 5100, Train Loss: 8.628812675440917, Valid Loss: 8.649390010871686\n",
            "Epoch: 12, Step: 5200, Train Loss: 8.637536540164716, Valid Loss: 8.649886861999024\n",
            "Epoch: 12, Step: 5300, Train Loss: 8.654256802979772, Valid Loss: 8.668819414709372\n",
            "Epoch: 12, Step: 5400, Train Loss: 8.666805053189028, Valid Loss: 8.676590808593273\n",
            "Epoch: 13, Step: 5500, Train Loss: 8.637152686455545, Valid Loss: 8.648746442685656\n",
            "Epoch: 13, Step: 5600, Train Loss: 8.632486744749098, Valid Loss: 8.64457739547321\n",
            "Epoch: 13, Step: 5700, Train Loss: 8.63275507487843, Valid Loss: 8.643989334617775\n",
            "Epoch: 13, Step: 5800, Train Loss: 8.63622437571056, Valid Loss: 8.652083615474382\n",
            "Epoch: 14, Step: 5900, Train Loss: 8.622049584218848, Valid Loss: 8.63276504399809\n",
            "Epoch: 14, Step: 6000, Train Loss: 8.64671680592409, Valid Loss: 8.663141194068055\n",
            "Epoch: 14, Step: 6100, Train Loss: 8.621535645639307, Valid Loss: 8.638896812053005\n",
            "Epoch: 14, Step: 6200, Train Loss: 8.617618413104005, Valid Loss: 8.628825568593529\n",
            "Epoch: 15, Step: 6300, Train Loss: 8.63913512312481, Valid Loss: 8.644209158649074\n",
            "Epoch: 15, Step: 6400, Train Loss: 8.624981730766043, Valid Loss: 8.640586555517523\n",
            "Epoch: 15, Step: 6500, Train Loss: 8.601332875112865, Valid Loss: 8.617932175815325\n",
            "Epoch: 15, Step: 6600, Train Loss: 8.65016845817827, Valid Loss: 8.667956437742905\n",
            "Epoch: 16, Step: 6700, Train Loss: 8.62618377960514, Valid Loss: 8.640028903461388\n",
            "Epoch: 16, Step: 6800, Train Loss: 8.636354339434481, Valid Loss: 8.658212540601928\n",
            "Epoch: 16, Step: 6900, Train Loss: 8.626246990210836, Valid Loss: 8.648472845713933\n",
            "Epoch: 16, Step: 7000, Train Loss: 8.6410197005207, Valid Loss: 8.661668851560464\n",
            "Epoch: 16, Step: 7100, Train Loss: 8.633431771155623, Valid Loss: 8.656627860858999\n",
            "Epoch: 17, Step: 7200, Train Loss: 8.607738807422061, Valid Loss: 8.636857311017435\n",
            "Epoch: 17, Step: 7300, Train Loss: 8.605284455284211, Valid Loss: 8.631376886850164\n",
            "Epoch: 17, Step: 7400, Train Loss: 8.606843486539894, Valid Loss: 8.635792584233826\n",
            "Epoch: 17, Step: 7500, Train Loss: 8.624935704279086, Valid Loss: 8.646839723405623\n",
            "Epoch: 18, Step: 7600, Train Loss: 8.584866391471673, Valid Loss: 8.615504910007742\n",
            "Epoch: 18, Step: 7700, Train Loss: 8.615510082321094, Valid Loss: 8.639756600206558\n",
            "Epoch: 18, Step: 7800, Train Loss: 8.597771637706524, Valid Loss: 8.625043530735734\n",
            "Epoch: 18, Step: 7900, Train Loss: 8.599070115822324, Valid Loss: 8.624801801499562\n",
            "Epoch: 19, Step: 8000, Train Loss: 8.605753967272461, Valid Loss: 8.625919949038531\n",
            "Epoch: 19, Step: 8100, Train Loss: 8.60736352686843, Valid Loss: 8.640634721457618\n",
            "Epoch: 19, Step: 8200, Train Loss: 8.613357878696412, Valid Loss: 8.645354208002122\n",
            "Epoch: 19, Step: 8300, Train Loss: 8.594903757603898, Valid Loss: 8.627210457647148\n",
            "Epoch: 20, Step: 8400, Train Loss: 8.591165939788096, Valid Loss: 8.629522362204847\n",
            "Epoch: 20, Step: 8500, Train Loss: 8.598807667454597, Valid Loss: 8.638868577587841\n",
            "Epoch: 20, Step: 8600, Train Loss: 8.63819411132275, Valid Loss: 8.661759320132642\n",
            "Epoch: 20, Step: 8700, Train Loss: 8.58189952361341, Valid Loss: 8.617213986999383\n",
            "Epoch: 21, Step: 8800, Train Loss: 8.568853140771688, Valid Loss: 8.604092389105382\n",
            "Epoch: 21, Step: 8900, Train Loss: 8.5888691252168, Valid Loss: 8.621246676968406\n",
            "Epoch: 21, Step: 9000, Train Loss: 8.606062858138419, Valid Loss: 8.63736434335678\n",
            "Epoch: 21, Step: 9100, Train Loss: 8.622554908848764, Valid Loss: 8.651759474178668\n",
            "Epoch: 22, Step: 9200, Train Loss: 8.61016189994978, Valid Loss: 8.643276522321782\n",
            "Epoch: 22, Step: 9300, Train Loss: 8.613643048127555, Valid Loss: 8.646016918160647\n",
            "Epoch: 22, Step: 9400, Train Loss: 8.58852182091829, Valid Loss: 8.62446652065578\n",
            "Epoch: 22, Step: 9500, Train Loss: 8.61594105433291, Valid Loss: 8.650407192545247\n",
            "Epoch: 22, Step: 9600, Train Loss: 8.604335545773361, Valid Loss: 8.642237822362071\n",
            "Epoch: 23, Step: 9700, Train Loss: 8.599958898235545, Valid Loss: 8.637691916465915\n",
            "Epoch: 23, Step: 9800, Train Loss: 8.584168517604384, Valid Loss: 8.626250974256243\n",
            "Epoch: 23, Step: 9900, Train Loss: 8.581659365127324, Valid Loss: 8.615945534198232\n",
            "Epoch: 23, Step: 10000, Train Loss: 8.599369295291883, Valid Loss: 8.630314922197798\n",
            "Epoch: 24, Step: 10100, Train Loss: 8.597497483315237, Valid Loss: 8.633874306013524\n",
            "Epoch: 24, Step: 10200, Train Loss: 8.590966511778728, Valid Loss: 8.627673724390647\n",
            "Epoch: 24, Step: 10300, Train Loss: 8.576898991856286, Valid Loss: 8.615762272490176\n",
            "Epoch: 24, Step: 10400, Train Loss: 8.581047384644954, Valid Loss: 8.621515180199314\n",
            "Epoch: 25, Step: 10500, Train Loss: 8.567814862353545, Valid Loss: 8.612704393400152\n",
            "Epoch: 25, Step: 10600, Train Loss: 8.589382850699927, Valid Loss: 8.625595202977534\n",
            "Epoch: 25, Step: 10700, Train Loss: 8.578639390435534, Valid Loss: 8.623038886790196\n",
            "Epoch: 25, Step: 10800, Train Loss: 8.580170829943349, Valid Loss: 8.616683398420172\n",
            "Epoch: 26, Step: 10900, Train Loss: 8.556193885225058, Valid Loss: 8.599948800319256\n",
            "Epoch: 26, Step: 11000, Train Loss: 8.588764790108177, Valid Loss: 8.630847476394468\n",
            "Epoch: 26, Step: 11100, Train Loss: 8.582060782596354, Valid Loss: 8.624420034779567\n",
            "Epoch: 26, Step: 11200, Train Loss: 8.579293361179861, Valid Loss: 8.61440301256885\n",
            "Epoch: 27, Step: 11300, Train Loss: 8.573428553652777, Valid Loss: 8.609243882338651\n",
            "Epoch: 27, Step: 11400, Train Loss: 8.562953133311243, Valid Loss: 8.605273644534213\n",
            "Epoch: 27, Step: 11500, Train Loss: 8.573189936333433, Valid Loss: 8.615851028107077\n",
            "Epoch: 27, Step: 11600, Train Loss: 8.552504237458097, Valid Loss: 8.598530319110964\n",
            "Epoch: 27, Step: 11700, Train Loss: 8.553623941330349, Valid Loss: 8.594540618694458\n",
            "Epoch: 28, Step: 11800, Train Loss: 8.561983948499622, Valid Loss: 8.611215681961397\n",
            "Epoch: 28, Step: 11900, Train Loss: 8.549567193661309, Valid Loss: 8.599781803538988\n",
            "Epoch: 28, Step: 12000, Train Loss: 8.5830317441998, Valid Loss: 8.629745922461435\n",
            "Epoch: 28, Step: 12100, Train Loss: 8.573588556848824, Valid Loss: 8.624749791677152\n",
            "Epoch: 29, Step: 12200, Train Loss: 8.555681056399303, Valid Loss: 8.605462954078414\n",
            "Epoch: 29, Step: 12300, Train Loss: 8.553967443731937, Valid Loss: 8.604911163442127\n",
            "Epoch: 29, Step: 12400, Train Loss: 8.568959494386306, Valid Loss: 8.61051780802303\n",
            "Epoch: 29, Step: 12500, Train Loss: 8.540083764389525, Valid Loss: 8.59500756485938\n",
            "Epoch: 30, Step: 12600, Train Loss: 8.550678432373159, Valid Loss: 8.605914093834672\n",
            "Epoch: 30, Step: 12700, Train Loss: 8.557742340606211, Valid Loss: 8.61224561801419\n",
            "Epoch: 30, Step: 12800, Train Loss: 8.55139543765992, Valid Loss: 8.609312536578082\n",
            "Epoch: 30, Step: 12900, Train Loss: 8.547817980069622, Valid Loss: 8.596345265723379\n",
            "Epoch: 31, Step: 13000, Train Loss: 8.571240982670592, Valid Loss: 8.619223787541783\n",
            "Epoch: 31, Step: 13100, Train Loss: 8.565189017215046, Valid Loss: 8.610460541426791\n",
            "Epoch: 31, Step: 13200, Train Loss: 8.550450866699093, Valid Loss: 8.592258415031864\n",
            "Epoch: 31, Step: 13300, Train Loss: 8.562615470270794, Valid Loss: 8.61278210877904\n",
            "Epoch: 32, Step: 13400, Train Loss: 8.57348058515032, Valid Loss: 8.620358228032018\n",
            "Epoch: 32, Step: 13500, Train Loss: 8.554485521290792, Valid Loss: 8.59992178729039\n",
            "Epoch: 32, Step: 13600, Train Loss: 8.561528505537414, Valid Loss: 8.613685451234279\n",
            "Epoch: 32, Step: 13700, Train Loss: 8.540368678717886, Valid Loss: 8.59128718157948\n",
            "Epoch: 33, Step: 13800, Train Loss: 8.557461741543307, Valid Loss: 8.618054182297927\n",
            "Epoch: 33, Step: 13900, Train Loss: 8.554819286942173, Valid Loss: 8.610804964396428\n",
            "Epoch: 33, Step: 14000, Train Loss: 8.566175724098464, Valid Loss: 8.623819112010043\n",
            "Epoch: 33, Step: 14100, Train Loss: 8.55294235587374, Valid Loss: 8.609290626282593\n",
            "Epoch: 33, Step: 14200, Train Loss: 8.547863763167983, Valid Loss: 8.604030544224441\n",
            "Epoch: 34, Step: 14300, Train Loss: 8.544870094297563, Valid Loss: 8.606010338816114\n",
            "Epoch: 34, Step: 14400, Train Loss: 8.547857755529302, Valid Loss: 8.607673134353824\n",
            "Epoch: 34, Step: 14500, Train Loss: 8.566074115479635, Valid Loss: 8.630601022143132\n",
            "Epoch: 34, Step: 14600, Train Loss: 8.567668046782991, Valid Loss: 8.630770606906708\n",
            "Epoch: 35, Step: 14700, Train Loss: 8.542643921375797, Valid Loss: 8.60189342732515\n",
            "Epoch: 35, Step: 14800, Train Loss: 8.5489654466214, Valid Loss: 8.609399497176465\n",
            "Epoch: 35, Step: 14900, Train Loss: 8.53627063894506, Valid Loss: 8.595376706529603\n",
            "Epoch: 35, Step: 15000, Train Loss: 8.537199243501183, Valid Loss: 8.6025068467336\n",
            "Epoch: 36, Step: 15100, Train Loss: 8.545532089885041, Valid Loss: 8.605355599644556\n",
            "Epoch: 36, Step: 15200, Train Loss: 8.544827286353211, Valid Loss: 8.61602229641358\n",
            "Epoch: 36, Step: 15300, Train Loss: 8.559791187593316, Valid Loss: 8.620561509912388\n",
            "Epoch: 36, Step: 15400, Train Loss: 8.547131184454066, Valid Loss: 8.607089708965429\n",
            "Epoch: 37, Step: 15500, Train Loss: 8.529989939762984, Valid Loss: 8.59825741191205\n",
            "Epoch: 37, Step: 15600, Train Loss: 8.557822280319725, Valid Loss: 8.625466132243657\n",
            "Epoch: 37, Step: 15700, Train Loss: 8.558777361597524, Valid Loss: 8.624323692448998\n",
            "Epoch: 37, Step: 15800, Train Loss: 8.541295651778665, Valid Loss: 8.607758271669077\n",
            "Epoch: 38, Step: 15900, Train Loss: 8.538058069057852, Valid Loss: 8.609593882126095\n",
            "Epoch: 38, Step: 16000, Train Loss: 8.54856607318508, Valid Loss: 8.619210661863368\n",
            "Epoch: 38, Step: 16100, Train Loss: 8.546796724874058, Valid Loss: 8.615969996499867\n",
            "Epoch: 38, Step: 16200, Train Loss: 8.530653590400652, Valid Loss: 8.597704201092112\n",
            "Epoch: 38, Step: 16300, Train Loss: 8.529813151633636, Valid Loss: 8.5987898697648\n",
            "Epoch: 39, Step: 16400, Train Loss: 8.547207055577756, Valid Loss: 8.604344326462774\n",
            "Epoch: 39, Step: 16500, Train Loss: 8.548240454157083, Valid Loss: 8.614602944003957\n",
            "Epoch: 39, Step: 16600, Train Loss: 8.535382967173971, Valid Loss: 8.603029776103549\n",
            "Epoch: 39, Step: 16700, Train Loss: 8.522096232954917, Valid Loss: 8.597421173101816\n",
            "Epoch: 40, Step: 16800, Train Loss: 8.533211280438547, Valid Loss: 8.609605134093451\n",
            "Epoch: 40, Step: 16900, Train Loss: 8.537668453008468, Valid Loss: 8.609890978053993\n",
            "Epoch: 40, Step: 17000, Train Loss: 8.545619666977355, Valid Loss: 8.616892947895998\n",
            "Epoch: 40, Step: 17100, Train Loss: 8.526592251386246, Valid Loss: 8.602501350754928\n",
            "Epoch: 41, Step: 17200, Train Loss: 8.53084340290768, Valid Loss: 8.601772290909679\n",
            "Epoch: 41, Step: 17300, Train Loss: 8.539381445364084, Valid Loss: 8.614267244201805\n",
            "Epoch: 41, Step: 17400, Train Loss: 8.524116857061834, Valid Loss: 8.600320376968096\n",
            "Epoch: 41, Step: 17500, Train Loss: 8.54491370711951, Valid Loss: 8.619178052822255\n",
            "Epoch: 42, Step: 17600, Train Loss: 8.550861555122141, Valid Loss: 8.626536955089984\n",
            "Epoch: 42, Step: 17700, Train Loss: 8.508223328557936, Valid Loss: 8.589768689115562\n",
            "Epoch: 42, Step: 17800, Train Loss: 8.532679078979372, Valid Loss: 8.611236205586472\n",
            "Epoch: 42, Step: 17900, Train Loss: 8.532480459238508, Valid Loss: 8.602967906447956\n",
            "Epoch: 43, Step: 18000, Train Loss: 8.511394046714548, Valid Loss: 8.597036781945471\n",
            "Epoch: 43, Step: 18100, Train Loss: 8.535106493256457, Valid Loss: 8.606853560827453\n",
            "Epoch: 43, Step: 18200, Train Loss: 8.556090030914602, Valid Loss: 8.623567279605748\n",
            "Epoch: 43, Step: 18300, Train Loss: 8.522779510458358, Valid Loss: 8.591993340851417\n",
            "Epoch: 44, Step: 18400, Train Loss: 8.528923491704361, Valid Loss: 8.595278002580676\n",
            "Epoch: 44, Step: 18500, Train Loss: 8.526177490359213, Valid Loss: 8.593451893890672\n",
            "Epoch: 44, Step: 18600, Train Loss: 8.533205706166449, Valid Loss: 8.597941266759218\n",
            "Epoch: 44, Step: 18700, Train Loss: 8.518525411176133, Valid Loss: 8.585631489564692\n",
            "Epoch: 44, Step: 18800, Train Loss: 8.556347914224922, Valid Loss: 8.622261580573515\n",
            "Epoch: 45, Step: 18900, Train Loss: 8.528557109832835, Valid Loss: 8.591234561948234\n",
            "Epoch: 45, Step: 19000, Train Loss: 8.546408953926864, Valid Loss: 8.603295371893942\n",
            "Epoch: 45, Step: 19100, Train Loss: 8.521913849057507, Valid Loss: 8.59257052181648\n",
            "Epoch: 45, Step: 19200, Train Loss: 8.545545133338651, Valid Loss: 8.605704306389264\n",
            "Epoch: 46, Step: 19300, Train Loss: 8.534592305478442, Valid Loss: 8.593439721373407\n",
            "Epoch: 46, Step: 19400, Train Loss: 8.504252422739826, Valid Loss: 8.579465469716569\n",
            "Epoch: 46, Step: 19500, Train Loss: 8.521296967626569, Valid Loss: 8.59656328992832\n",
            "Epoch: 46, Step: 19600, Train Loss: 8.542238023850105, Valid Loss: 8.61190932500954\n",
            "Epoch: 47, Step: 19700, Train Loss: 8.518122739009526, Valid Loss: 8.593884944419756\n",
            "Epoch: 47, Step: 19800, Train Loss: 8.50895504874939, Valid Loss: 8.590004922023814\n",
            "Epoch: 47, Step: 19900, Train Loss: 8.548830874436858, Valid Loss: 8.6121485501602\n",
            "Epoch: 47, Step: 20000, Train Loss: 8.53429165304528, Valid Loss: 8.605328145944233\n",
            "Epoch: 48, Step: 20100, Train Loss: 8.527068946888578, Valid Loss: 8.593418003422792\n",
            "Epoch: 48, Step: 20200, Train Loss: 8.508484353715447, Valid Loss: 8.57804232470662\n",
            "Epoch: 48, Step: 20300, Train Loss: 8.521883814906845, Valid Loss: 8.594980803376973\n",
            "Epoch: 48, Step: 20400, Train Loss: 8.50913173949734, Valid Loss: 8.585707195797056\n",
            "Epoch: 49, Step: 20500, Train Loss: 8.519073345317928, Valid Loss: 8.595011786609158\n",
            "Epoch: 49, Step: 20600, Train Loss: 8.519137962628923, Valid Loss: 8.598024362136751\n",
            "Epoch: 49, Step: 20700, Train Loss: 8.510083074070682, Valid Loss: 8.584213826922086\n",
            "Epoch: 49, Step: 20800, Train Loss: 8.511884193854256, Valid Loss: 8.59141810662854\n",
            "Epoch: 49, Step: 20900, Train Loss: 8.539453211806876, Valid Loss: 8.600771458358901\n",
            "Epoch: 50, Step: 21000, Train Loss: 8.499097519084986, Valid Loss: 8.580397762147687\n",
            "Epoch: 50, Step: 21100, Train Loss: 8.513112277582106, Valid Loss: 8.585510172761865\n",
            "Epoch: 50, Step: 21200, Train Loss: 8.514794673452204, Valid Loss: 8.585768010004426\n",
            "Epoch: 50, Step: 21300, Train Loss: 8.525574787653465, Valid Loss: 8.59138293347956\n",
            "Epoch: 51, Step: 21400, Train Loss: 8.526341725648606, Valid Loss: 8.593910942294032\n",
            "Epoch: 51, Step: 21500, Train Loss: 8.505916523113246, Valid Loss: 8.581413138351314\n",
            "Epoch: 51, Step: 21600, Train Loss: 8.500666884378475, Valid Loss: 8.582302129535405\n",
            "Epoch: 51, Step: 21700, Train Loss: 8.531985967967726, Valid Loss: 8.598509336542369\n",
            "Epoch: 52, Step: 21800, Train Loss: 8.493664292513275, Valid Loss: 8.578750168308265\n",
            "Epoch: 52, Step: 21900, Train Loss: 8.517109266969833, Valid Loss: 8.602351254097554\n",
            "Epoch: 52, Step: 22000, Train Loss: 8.538681719093775, Valid Loss: 8.611474522573898\n",
            "Epoch: 52, Step: 22100, Train Loss: 8.524858466671052, Valid Loss: 8.603426055456962\n",
            "Epoch: 53, Step: 22200, Train Loss: 8.50352105049206, Valid Loss: 8.57682318353456\n",
            "Epoch: 53, Step: 22300, Train Loss: 8.537074057150779, Valid Loss: 8.610387777520488\n",
            "Epoch: 53, Step: 22400, Train Loss: 8.515455699767314, Valid Loss: 8.588968069767803\n",
            "Epoch: 53, Step: 22500, Train Loss: 8.523576011197465, Valid Loss: 8.602728332976174\n",
            "Epoch: 54, Step: 22600, Train Loss: 8.513484369585093, Valid Loss: 8.590337537144292\n",
            "Epoch: 54, Step: 22700, Train Loss: 8.499472227472953, Valid Loss: 8.57966458788996\n",
            "Epoch: 54, Step: 22800, Train Loss: 8.53300876979805, Valid Loss: 8.608577622066365\n",
            "Epoch: 54, Step: 22900, Train Loss: 8.534541189795647, Valid Loss: 8.606794160642758\n",
            "Epoch: 55, Step: 23000, Train Loss: 8.49217341322814, Valid Loss: 8.58288596885723\n",
            "Epoch: 55, Step: 23100, Train Loss: 8.518980138074655, Valid Loss: 8.60224711444951\n",
            "Epoch: 55, Step: 23200, Train Loss: 8.493452405347114, Valid Loss: 8.580850690931884\n",
            "Epoch: 55, Step: 23300, Train Loss: 8.514225987384677, Valid Loss: 8.596832767416911\n",
            "Epoch: 55, Step: 23400, Train Loss: 8.52316671937416, Valid Loss: 8.586872963943538\n",
            "Epoch: 56, Step: 23500, Train Loss: 8.51657596945082, Valid Loss: 8.589601303177812\n",
            "Epoch: 56, Step: 23600, Train Loss: 8.512302938180172, Valid Loss: 8.58511076055762\n",
            "Epoch: 56, Step: 23700, Train Loss: 8.524769907178698, Valid Loss: 8.596943486355947\n",
            "Epoch: 56, Step: 23800, Train Loss: 8.5102008734102, Valid Loss: 8.58388533912807\n",
            "Epoch: 57, Step: 23900, Train Loss: 8.486543494562989, Valid Loss: 8.581311247832488\n",
            "Epoch: 57, Step: 24000, Train Loss: 8.496638891403988, Valid Loss: 8.576115067927104\n",
            "Epoch: 57, Step: 24100, Train Loss: 8.506519512284653, Valid Loss: 8.579604250802051\n",
            "Epoch: 57, Step: 24200, Train Loss: 8.517471793782061, Valid Loss: 8.598405270376869\n",
            "Epoch: 58, Step: 24300, Train Loss: 8.49876786764227, Valid Loss: 8.582133842166222\n",
            "Epoch: 58, Step: 24400, Train Loss: 8.51318066588858, Valid Loss: 8.588715708342823\n",
            "Epoch: 58, Step: 24500, Train Loss: 8.501266375383787, Valid Loss: 8.581400406397327\n",
            "Epoch: 58, Step: 24600, Train Loss: 8.518325281874978, Valid Loss: 8.592864307030593\n",
            "Epoch: 59, Step: 24700, Train Loss: 8.504673548301342, Valid Loss: 8.575199687794587\n",
            "Epoch: 59, Step: 24800, Train Loss: 8.50193098780092, Valid Loss: 8.579622103319315\n",
            "Epoch: 59, Step: 24900, Train Loss: 8.510348071349972, Valid Loss: 8.585510440132657\n",
            "Epoch: 59, Step: 25000, Train Loss: 8.516251680098696, Valid Loss: 8.597658148593846\n",
            "Epoch: 60, Step: 25100, Train Loss: 8.499172608441414, Valid Loss: 8.588120109803825\n",
            "Epoch: 60, Step: 25200, Train Loss: 8.523189219506165, Valid Loss: 8.601266042295142\n",
            "Epoch: 60, Step: 25300, Train Loss: 8.515469961259535, Valid Loss: 8.597543669063366\n",
            "Epoch: 60, Step: 25400, Train Loss: 8.502697728329808, Valid Loss: 8.585923361084854\n",
            "Epoch: 61, Step: 25500, Train Loss: 8.50826617793354, Valid Loss: 8.586596134828193\n",
            "Epoch: 61, Step: 25600, Train Loss: 8.501673879207324, Valid Loss: 8.593702621364807\n",
            "Epoch: 61, Step: 25700, Train Loss: 8.499373878640368, Valid Loss: 8.584835876376761\n",
            "Epoch: 61, Step: 25800, Train Loss: 8.49501627104654, Valid Loss: 8.578056183614002\n",
            "Epoch: 61, Step: 25900, Train Loss: 8.500922277467604, Valid Loss: 8.583839008857375\n",
            "Epoch: 62, Step: 26000, Train Loss: 8.494888796842263, Valid Loss: 8.580333377963987\n",
            "Epoch: 62, Step: 26100, Train Loss: 8.488881882102005, Valid Loss: 8.576686022254725\n",
            "Epoch: 62, Step: 26200, Train Loss: 8.504982947433687, Valid Loss: 8.590302200218543\n",
            "Epoch: 62, Step: 26300, Train Loss: 8.508551617572474, Valid Loss: 8.59868830271311\n",
            "Epoch: 63, Step: 26400, Train Loss: 8.504207411911088, Valid Loss: 8.594311501542261\n",
            "Epoch: 63, Step: 26500, Train Loss: 8.496370992301355, Valid Loss: 8.58880781826567\n",
            "Epoch: 63, Step: 26600, Train Loss: 8.531527041838476, Valid Loss: 8.610221675396666\n",
            "Epoch: 63, Step: 26700, Train Loss: 8.492952715737378, Valid Loss: 8.578437165907301\n",
            "Epoch: 64, Step: 26800, Train Loss: 8.504244293700419, Valid Loss: 8.586996137669844\n",
            "Epoch: 64, Step: 26900, Train Loss: 8.504321945190007, Valid Loss: 8.584931670794072\n",
            "Epoch: 64, Step: 27000, Train Loss: 8.47743183294511, Valid Loss: 8.56022077027483\n",
            "Epoch: 64, Step: 27100, Train Loss: 8.503719571760932, Valid Loss: 8.584059308485099\n",
            "Epoch: 65, Step: 27200, Train Loss: 8.493540062384495, Valid Loss: 8.581712000084341\n",
            "Epoch: 65, Step: 27300, Train Loss: 8.498006394684328, Valid Loss: 8.584344347116451\n",
            "Epoch: 65, Step: 27400, Train Loss: 8.503936500220133, Valid Loss: 8.593595325869801\n",
            "Epoch: 65, Step: 27500, Train Loss: 8.487305267838943, Valid Loss: 8.577181314898652\n",
            "Epoch: 66, Step: 27600, Train Loss: 8.494961823086738, Valid Loss: 8.585106600171564\n",
            "Epoch: 66, Step: 27700, Train Loss: 8.485148185170614, Valid Loss: 8.57265421464391\n",
            "Epoch: 66, Step: 27800, Train Loss: 8.510843467891815, Valid Loss: 8.59513068816698\n",
            "Epoch: 66, Step: 27900, Train Loss: 8.503801698369124, Valid Loss: 8.582677444124618\n",
            "Epoch: 66, Step: 28000, Train Loss: 8.505405197250989, Valid Loss: 8.584105841194376\n",
            "Epoch: 67, Step: 28100, Train Loss: 8.51703316894071, Valid Loss: 8.594464759936828\n",
            "Epoch: 67, Step: 28200, Train Loss: 8.484468556789686, Valid Loss: 8.573785915365919\n",
            "Epoch: 67, Step: 28300, Train Loss: 8.495334778931596, Valid Loss: 8.58202207819296\n",
            "Epoch: 67, Step: 28400, Train Loss: 8.483920979479274, Valid Loss: 8.570525570216022\n",
            "Epoch: 68, Step: 28500, Train Loss: 8.478620318813302, Valid Loss: 8.574446221279052\n",
            "Epoch: 68, Step: 28600, Train Loss: 8.486288834820442, Valid Loss: 8.572944400621742\n",
            "Epoch: 68, Step: 28700, Train Loss: 8.512894532661857, Valid Loss: 8.587749913163272\n",
            "Epoch: 68, Step: 28800, Train Loss: 8.47913085349431, Valid Loss: 8.569716537605942\n",
            "Epoch: 69, Step: 28900, Train Loss: 8.485314442294598, Valid Loss: 8.579426862558947\n",
            "Epoch: 69, Step: 29000, Train Loss: 8.470115880183906, Valid Loss: 8.567757771642494\n",
            "Epoch: 69, Step: 29100, Train Loss: 8.47618170593556, Valid Loss: 8.571125285798702\n",
            "Epoch: 69, Step: 29200, Train Loss: 8.47980930313277, Valid Loss: 8.580731240532659\n",
            "Epoch: 70, Step: 29300, Train Loss: 8.472066032435013, Valid Loss: 8.572274717157732\n",
            "Epoch: 70, Step: 29400, Train Loss: 8.48595523266875, Valid Loss: 8.576772876587325\n",
            "Epoch: 70, Step: 29500, Train Loss: 8.486972692408141, Valid Loss: 8.57766072335521\n",
            "Epoch: 70, Step: 29600, Train Loss: 8.499623832388828, Valid Loss: 8.586776894421043\n",
            "Epoch: 71, Step: 29700, Train Loss: 8.496192490843208, Valid Loss: 8.58628685715813\n",
            "Epoch: 71, Step: 29800, Train Loss: 8.495940672633711, Valid Loss: 8.585528584533048\n",
            "Epoch: 71, Step: 29900, Train Loss: 8.500125310076637, Valid Loss: 8.58864573454618\n",
            "Epoch: 71, Step: 30000, Train Loss: 8.482831962570264, Valid Loss: 8.580241879039507\n",
            "Epoch: 72, Step: 30100, Train Loss: 8.469609608737917, Valid Loss: 8.564979095831948\n",
            "Epoch: 72, Step: 30200, Train Loss: 8.489521402126002, Valid Loss: 8.583322884410645\n",
            "Epoch: 72, Step: 30300, Train Loss: 8.496253830194133, Valid Loss: 8.582676487161011\n",
            "Epoch: 72, Step: 30400, Train Loss: 8.491238795024126, Valid Loss: 8.578476946077298\n",
            "Epoch: 72, Step: 30500, Train Loss: 8.489625539648337, Valid Loss: 8.580603156540468\n",
            "Epoch: 73, Step: 30600, Train Loss: 8.475764379237035, Valid Loss: 8.56183289402623\n",
            "Epoch: 73, Step: 30700, Train Loss: 8.467281166423433, Valid Loss: 8.567686355869\n",
            "Epoch: 73, Step: 30800, Train Loss: 8.48809703558379, Valid Loss: 8.57296964643071\n",
            "Epoch: 73, Step: 30900, Train Loss: 8.492663132789225, Valid Loss: 8.57502954433532\n",
            "Epoch: 74, Step: 31000, Train Loss: 8.482149109375417, Valid Loss: 8.574341423412536\n",
            "Epoch: 74, Step: 31100, Train Loss: 8.482209487097153, Valid Loss: 8.573891768848458\n",
            "Epoch: 74, Step: 31200, Train Loss: 8.485948673282117, Valid Loss: 8.572888735278664\n",
            "Epoch: 74, Step: 31300, Train Loss: 8.48860967417678, Valid Loss: 8.568911201173371\n",
            "Epoch: 75, Step: 31400, Train Loss: 8.49889173198991, Valid Loss: 8.584103476170053\n",
            "Epoch: 75, Step: 31500, Train Loss: 8.479892311700604, Valid Loss: 8.568535416323602\n",
            "Epoch: 75, Step: 31600, Train Loss: 8.48060811571326, Valid Loss: 8.562907409811524\n",
            "Epoch: 75, Step: 31700, Train Loss: 8.48516975032835, Valid Loss: 8.569339473271112\n",
            "Epoch: 76, Step: 31800, Train Loss: 8.488012410464941, Valid Loss: 8.577521213539535\n",
            "Epoch: 76, Step: 31900, Train Loss: 8.501563228255558, Valid Loss: 8.588120651739489\n",
            "Epoch: 76, Step: 32000, Train Loss: 8.495026553562237, Valid Loss: 8.571209869667282\n",
            "Epoch: 76, Step: 32100, Train Loss: 8.496666614166047, Valid Loss: 8.58614017091271\n",
            "Epoch: 77, Step: 32200, Train Loss: 8.47633576140137, Valid Loss: 8.569781198148865\n",
            "Epoch: 77, Step: 32300, Train Loss: 8.47156854754553, Valid Loss: 8.572131670379509\n",
            "Epoch: 77, Step: 32400, Train Loss: 8.479113788608739, Valid Loss: 8.567819426076845\n",
            "Epoch: 77, Step: 32500, Train Loss: 8.471125095763883, Valid Loss: 8.567799555517823\n",
            "Epoch: 77, Step: 32600, Train Loss: 8.49179875008634, Valid Loss: 8.581750518433132\n",
            "Epoch: 78, Step: 32700, Train Loss: 8.476553986922136, Valid Loss: 8.559836696506164\n",
            "Epoch: 78, Step: 32800, Train Loss: 8.48020596062945, Valid Loss: 8.571123357006488\n",
            "Epoch: 78, Step: 32900, Train Loss: 8.469282555421003, Valid Loss: 8.56852154939055\n",
            "Epoch: 78, Step: 33000, Train Loss: 8.498974466354321, Valid Loss: 8.582640564142288\n",
            "Epoch: 79, Step: 33100, Train Loss: 8.465278369408974, Valid Loss: 8.566909696088306\n",
            "Epoch: 79, Step: 33200, Train Loss: 8.474224925324677, Valid Loss: 8.565283637780908\n",
            "Epoch: 79, Step: 33300, Train Loss: 8.468034166290224, Valid Loss: 8.567257430837687\n",
            "Epoch: 79, Step: 33400, Train Loss: 8.49076026373979, Valid Loss: 8.580605392449268\n",
            "Epoch: 80, Step: 33500, Train Loss: 8.50558933378418, Valid Loss: 8.594182960768613\n",
            "Epoch: 80, Step: 33600, Train Loss: 8.484443306641113, Valid Loss: 8.587798821762192\n",
            "Epoch: 80, Step: 33700, Train Loss: 8.476113665097046, Valid Loss: 8.57821483643351\n",
            "Epoch: 80, Step: 33800, Train Loss: 8.487056520897925, Valid Loss: 8.572620939639055\n",
            "Epoch: 81, Step: 33900, Train Loss: 8.468329248780893, Valid Loss: 8.562137565098181\n",
            "Epoch: 81, Step: 34000, Train Loss: 8.480891570713913, Valid Loss: 8.565142795329274\n",
            "Epoch: 81, Step: 34100, Train Loss: 8.467644817895437, Valid Loss: 8.559853195263422\n",
            "Epoch: 81, Step: 34200, Train Loss: 8.478637069245334, Valid Loss: 8.569553200719513\n",
            "Epoch: 82, Step: 34300, Train Loss: 8.463919899561366, Valid Loss: 8.563187542768414\n",
            "Epoch: 82, Step: 34400, Train Loss: 8.484472313623677, Valid Loss: 8.588023252354828\n",
            "Epoch: 82, Step: 34500, Train Loss: 8.489555620117772, Valid Loss: 8.585012973780644\n",
            "Epoch: 82, Step: 34600, Train Loss: 8.485523066950227, Valid Loss: 8.581808266184145\n",
            "Epoch: 83, Step: 34700, Train Loss: 8.47989739562785, Valid Loss: 8.572967261138523\n",
            "Epoch: 83, Step: 34800, Train Loss: 8.482635114034547, Valid Loss: 8.57979912670315\n",
            "Epoch: 83, Step: 34900, Train Loss: 8.497757116181614, Valid Loss: 8.590659870173607\n",
            "Epoch: 83, Step: 35000, Train Loss: 8.496459107792063, Valid Loss: 8.587220547549238\n",
            "Epoch: 83, Step: 35100, Train Loss: 8.491254427648308, Valid Loss: 8.579355597924083\n",
            "Epoch: 84, Step: 35200, Train Loss: 8.48606761197623, Valid Loss: 8.588936049620543\n",
            "Epoch: 84, Step: 35300, Train Loss: 8.484920845638674, Valid Loss: 8.580887144202118\n",
            "Epoch: 84, Step: 35400, Train Loss: 8.48190102367332, Valid Loss: 8.581436079380321\n",
            "Epoch: 84, Step: 35500, Train Loss: 8.49487240046153, Valid Loss: 8.581329509093745\n",
            "Epoch: 85, Step: 35600, Train Loss: 8.469837954873991, Valid Loss: 8.567723887692562\n",
            "Epoch: 85, Step: 35700, Train Loss: 8.480646208765096, Valid Loss: 8.578091271468598\n",
            "Epoch: 85, Step: 35800, Train Loss: 8.479793525510583, Valid Loss: 8.57266955377578\n",
            "Epoch: 85, Step: 35900, Train Loss: 8.497846034560677, Valid Loss: 8.589362131668732\n",
            "Epoch: 86, Step: 36000, Train Loss: 8.479902279219441, Valid Loss: 8.579844472418054\n",
            "Epoch: 86, Step: 36100, Train Loss: 8.47791351171721, Valid Loss: 8.572321274529253\n",
            "Epoch: 86, Step: 36200, Train Loss: 8.477237472161573, Valid Loss: 8.578085148535376\n",
            "Epoch: 86, Step: 36300, Train Loss: 8.48470671724868, Valid Loss: 8.582154018175103\n",
            "Epoch: 87, Step: 36400, Train Loss: 8.479311899723116, Valid Loss: 8.582076523588846\n",
            "Epoch: 87, Step: 36500, Train Loss: 8.471616295019304, Valid Loss: 8.572554836352202\n",
            "Epoch: 87, Step: 36600, Train Loss: 8.477678891006104, Valid Loss: 8.574245147164806\n",
            "Epoch: 87, Step: 36700, Train Loss: 8.45008091425379, Valid Loss: 8.556981494628621\n",
            "Epoch: 88, Step: 36800, Train Loss: 8.473209909307194, Valid Loss: 8.576381089597804\n",
            "Epoch: 88, Step: 36900, Train Loss: 8.47459984244972, Valid Loss: 8.584705925432123\n",
            "Epoch: 88, Step: 37000, Train Loss: 8.474764991134014, Valid Loss: 8.594176293043349\n",
            "Epoch: 88, Step: 37100, Train Loss: 8.463332942774931, Valid Loss: 8.563940059590335\n",
            "Epoch: 88, Step: 37200, Train Loss: 8.487911307960298, Valid Loss: 8.579092469389598\n",
            "Epoch: 89, Step: 37300, Train Loss: 8.48917540557061, Valid Loss: 8.586246392068125\n",
            "Epoch: 89, Step: 37400, Train Loss: 8.483946063766528, Valid Loss: 8.577808647144655\n",
            "Epoch: 89, Step: 37500, Train Loss: 8.492509943026253, Valid Loss: 8.59524633426207\n",
            "Epoch: 89, Step: 37600, Train Loss: 8.49098745932699, Valid Loss: 8.5860873210631\n",
            "Epoch: 90, Step: 37700, Train Loss: 8.477949557888413, Valid Loss: 8.57380917892693\n",
            "Epoch: 90, Step: 37800, Train Loss: 8.494600731309005, Valid Loss: 8.591899056621097\n",
            "Epoch: 90, Step: 37900, Train Loss: 8.465194246476546, Valid Loss: 8.570638383442633\n",
            "Epoch: 90, Step: 38000, Train Loss: 8.472761514879275, Valid Loss: 8.572021743894979\n",
            "Epoch: 91, Step: 38100, Train Loss: 8.473329436902313, Valid Loss: 8.575387826737629\n",
            "Epoch: 91, Step: 38200, Train Loss: 8.474049586282828, Valid Loss: 8.571231476811292\n",
            "Epoch: 91, Step: 38300, Train Loss: 8.489948413193257, Valid Loss: 8.584261249341704\n",
            "Epoch: 91, Step: 38400, Train Loss: 8.479110767609603, Valid Loss: 8.577483078857625\n",
            "Epoch: 92, Step: 38500, Train Loss: 8.470882012444633, Valid Loss: 8.563781621853517\n",
            "Epoch: 92, Step: 38600, Train Loss: 8.46697569629209, Valid Loss: 8.576154441412841\n",
            "Epoch: 92, Step: 38700, Train Loss: 8.467525830723877, Valid Loss: 8.571460970115506\n",
            "Epoch: 92, Step: 38800, Train Loss: 8.460559255436415, Valid Loss: 8.573506497159627\n",
            "Epoch: 93, Step: 38900, Train Loss: 8.455249842135007, Valid Loss: 8.56202204737065\n",
            "Epoch: 93, Step: 39000, Train Loss: 8.460432215972792, Valid Loss: 8.578381339808399\n",
            "Epoch: 93, Step: 39100, Train Loss: 8.480864175760226, Valid Loss: 8.587777638860292\n",
            "Epoch: 93, Step: 39200, Train Loss: 8.488737203806703, Valid Loss: 8.585062194793359\n",
            "Epoch: 94, Step: 39300, Train Loss: 8.488150442257393, Valid Loss: 8.588129374201552\n",
            "Epoch: 94, Step: 39400, Train Loss: 8.470583538487455, Valid Loss: 8.572839945511928\n",
            "Epoch: 94, Step: 39500, Train Loss: 8.474881923786748, Valid Loss: 8.578783436723404\n",
            "Epoch: 94, Step: 39600, Train Loss: 8.46776075568706, Valid Loss: 8.572089443288611\n",
            "Epoch: 94, Step: 39700, Train Loss: 8.468206067550556, Valid Loss: 8.566811406557708\n",
            "Epoch: 95, Step: 39800, Train Loss: 8.471582942305021, Valid Loss: 8.573447941834203\n",
            "Epoch: 95, Step: 39900, Train Loss: 8.467166475888979, Valid Loss: 8.578165814270251\n",
            "Epoch: 95, Step: 40000, Train Loss: 8.473542312172908, Valid Loss: 8.575864639093455\n",
            "Epoch: 95, Step: 40100, Train Loss: 8.475091203659295, Valid Loss: 8.576968855447898\n",
            "Epoch: 96, Step: 40200, Train Loss: 8.491066040044492, Valid Loss: 8.587225947852422\n",
            "Epoch: 96, Step: 40300, Train Loss: 8.470500175985777, Valid Loss: 8.584370226688419\n",
            "Epoch: 96, Step: 40400, Train Loss: 8.465678567700756, Valid Loss: 8.565581611684884\n",
            "Epoch: 96, Step: 40500, Train Loss: 8.464885591201009, Valid Loss: 8.566759226583756\n",
            "Epoch: 97, Step: 40600, Train Loss: 8.47463986795464, Valid Loss: 8.577782870533738\n",
            "Epoch: 97, Step: 40700, Train Loss: 8.491173768642398, Valid Loss: 8.589236159952089\n",
            "Epoch: 97, Step: 40800, Train Loss: 8.473438972513033, Valid Loss: 8.580346528752571\n",
            "Epoch: 97, Step: 40900, Train Loss: 8.458817972668236, Valid Loss: 8.568231075979249\n",
            "Epoch: 98, Step: 41000, Train Loss: 8.475232101901115, Valid Loss: 8.587280137082727\n",
            "Epoch: 98, Step: 41100, Train Loss: 8.48039793975545, Valid Loss: 8.590918690308609\n",
            "Epoch: 98, Step: 41200, Train Loss: 8.458875245819074, Valid Loss: 8.570745231637675\n",
            "Epoch: 98, Step: 41300, Train Loss: 8.460750036455622, Valid Loss: 8.577214117978308\n",
            "Epoch: 99, Step: 41400, Train Loss: 8.46463111886658, Valid Loss: 8.590103350689171\n",
            "Epoch: 99, Step: 41500, Train Loss: 8.45338320487048, Valid Loss: 8.566319737937814\n",
            "Epoch: 99, Step: 41600, Train Loss: 8.462545143606208, Valid Loss: 8.56896191801187\n",
            "Epoch: 99, Step: 41700, Train Loss: 8.455478975121826, Valid Loss: 8.563802923130364\n",
            "Epoch: 99, Step: 41800, Train Loss: 8.479630595706517, Valid Loss: 8.5741455441495\n",
            "Test RMSE Loss for z = 1.0: 8.756946007933966\n"
          ]
        }
      ],
      "source": [
        "q7_z_list = [0.0, 0.1, 0.9, 1.0]\n",
        "test_rmse_list = []\n",
        "for z in q7_z_list:\n",
        "    print(f\"Start training for z = {z}\")\n",
        "    mlp = MyMLP(\n",
        "        X_subtrain=X_subtrain,\n",
        "        Y_subtrain=Y_subtrain,\n",
        "        X_valid=X_valid,\n",
        "        Y_valid=Y_valid,\n",
        "        H=90,\n",
        "        lr=0.001,\n",
        "        wd=0,\n",
        "        mom=0,\n",
        "        loss_type=1,\n",
        "        optimizer_type=1,\n",
        "        use_dropout=True,\n",
        "        dropout_rate=0.5,\n",
        "        loss_z=z\n",
        "    )\n",
        "    mlp.fit(\n",
        "        max_epoch=100,\n",
        "        verbose=True,\n",
        "        patience_batch_num=5000,\n",
        "        model_path=f'q7_mlp_z_{z}.ckpt'\n",
        "    )\n",
        "    best_model = torch.load(f'q7_mlp_z_{z}.ckpt')\n",
        "    test_rmse_loss = calculate_test_rmse(best_model, X_test, Y_test)\n",
        "    test_rmse_list.append(test_rmse_loss)\n",
        "    print(f\"Test RMSE Loss for z = {z}: {test_rmse_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI2Vq0-TPs3x",
        "outputId": "2ffbbcb4-74b7-496e-d43f-98efae3c3ee5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test RMSE Loss for z = 0.0: 8.994984392072922\n",
            "Test RMSE Loss for z = 0.1: 8.773821661899284\n",
            "Test RMSE Loss for z = 0.9: 8.771708431376377\n",
            "Test RMSE Loss for z = 1.0: 8.756946007933966\n"
          ]
        }
      ],
      "source": [
        "# print rmse according to different z\n",
        "for z, rmse in zip(q7_z_list, test_rmse_list):\n",
        "    print(f\"Test RMSE Loss for z = {z}: {rmse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "這邊可以看出隨著 z 的增加，Test RMSE 也隨之減少，在 z = 1.0 時達到最小值，且與前面使用 SSE 的模型相比，Test RMSE 也有所下降，代表 L2 + L1 Loss 的模型或許有更好的效果。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1GhxtGVPs3x"
      },
      "source": [
        "\n",
        "#### Q8 L2 + Customerized Loss (15%)\n",
        "考慮另一個比較特別的Loss Function\n",
        "\n",
        "$$\n",
        "qloss(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\sum_{i=1}^n \\{ q (y_i - \\hat{y}_i)_+ + (1 - q) (\\hat{y}_i - y_i)_+ \\},\n",
        "$$\n",
        "其中q為參數且$0<=q<=1$，而$(y_i - \\hat{y}_i)_+$是取正值的意思。也就是說如果$(y_i - \\hat{y}_i) > 0$，則$(y_i - \\hat{y}_i)_+ = y_i - \\hat{y}_i$，否則$(y_i - \\hat{y}_i)_+ = 0$。\n",
        "\n",
        "令模型的Loss為$z \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + (1 - z) \\sum_{i=1}^n \\{ 0.5 (y_i - \\hat{y}_i)_+ + 0.5 (\\hat{y}_i - y_i)_+ \\} $。請使用Q5中的MLP with Dropout模型(H = 90)，令z = 0。並以Adam訓練模型。畫出Training and Validation RMSE，並報告Test RMSE。注意這裡繪圖時應使用RMSE而不是這個特殊的Loss。\n",
        "\n",
        "另外，使用z = 0.1, 0.5, 0.9, 1.0訓練模型(不須提供訓練過程的Loss圖形)，統整各個z值下的Test RMSE並討論。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Eisp1eKFPs3x",
        "outputId": "ae6f340b-9e08-40fa-bb62-3a407f8475ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Step: 100, Train Loss: 10.578546137578007, Valid Loss: 10.553734897612387\n",
            "Epoch: 0, Step: 200, Train Loss: 10.220429130390361, Valid Loss: 10.190562216087955\n",
            "Epoch: 0, Step: 300, Train Loss: 9.489304815806562, Valid Loss: 9.444216188322997\n",
            "Epoch: 0, Step: 400, Train Loss: 9.402771643210336, Valid Loss: 9.359018009944043\n",
            "Epoch: 1, Step: 500, Train Loss: 9.292622011718226, Valid Loss: 9.241579265413444\n",
            "Epoch: 1, Step: 600, Train Loss: 9.250229869662338, Valid Loss: 9.197656121735225\n",
            "Epoch: 1, Step: 700, Train Loss: 9.273896017248422, Valid Loss: 9.231596072532504\n",
            "Epoch: 1, Step: 800, Train Loss: 9.16856741989957, Valid Loss: 9.123681948756127\n",
            "Epoch: 2, Step: 900, Train Loss: 9.20184042981072, Valid Loss: 9.159848907032274\n",
            "Epoch: 2, Step: 1000, Train Loss: 9.12078132117568, Valid Loss: 9.077893984527691\n",
            "Epoch: 2, Step: 1100, Train Loss: 9.142978716929974, Valid Loss: 9.09921184196025\n",
            "Epoch: 2, Step: 1200, Train Loss: 9.142153313496227, Valid Loss: 9.097422560163482\n",
            "Epoch: 3, Step: 1300, Train Loss: 9.110512644157115, Valid Loss: 9.066744295147625\n",
            "Epoch: 3, Step: 1400, Train Loss: 9.117979312066744, Valid Loss: 9.07313485349105\n",
            "Epoch: 3, Step: 1500, Train Loss: 9.075064195127723, Valid Loss: 9.037978687278382\n",
            "Epoch: 3, Step: 1600, Train Loss: 9.051554215607279, Valid Loss: 9.005598264095527\n",
            "Epoch: 4, Step: 1700, Train Loss: 9.031024821110984, Valid Loss: 8.984610192703132\n",
            "Epoch: 4, Step: 1800, Train Loss: 9.051580820711967, Valid Loss: 9.012410084845884\n",
            "Epoch: 4, Step: 1900, Train Loss: 9.040260561064818, Valid Loss: 9.003295491354878\n",
            "Epoch: 4, Step: 2000, Train Loss: 9.058356582494433, Valid Loss: 9.022748881306102\n",
            "Epoch: 5, Step: 2100, Train Loss: 9.012111522571457, Valid Loss: 8.96331429631167\n",
            "Epoch: 5, Step: 2200, Train Loss: 9.023458093253318, Valid Loss: 8.983754767179587\n",
            "Epoch: 5, Step: 2300, Train Loss: 9.037702174505375, Valid Loss: 9.001669100935633\n",
            "Epoch: 5, Step: 2400, Train Loss: 9.041382022563395, Valid Loss: 9.009482191679064\n",
            "Epoch: 5, Step: 2500, Train Loss: 8.999047439657302, Valid Loss: 8.963409245881316\n",
            "Epoch: 6, Step: 2600, Train Loss: 8.985020057764828, Valid Loss: 8.946922349129913\n",
            "Epoch: 6, Step: 2700, Train Loss: 9.025490572736715, Valid Loss: 8.99028090612806\n",
            "Epoch: 6, Step: 2800, Train Loss: 9.012963441442235, Valid Loss: 8.979469939403351\n",
            "Epoch: 6, Step: 2900, Train Loss: 8.972819914754497, Valid Loss: 8.93989875031145\n",
            "Epoch: 7, Step: 3000, Train Loss: 8.973756017565595, Valid Loss: 8.940460902890326\n",
            "Epoch: 7, Step: 3100, Train Loss: 8.972716885096641, Valid Loss: 8.93814842882387\n",
            "Epoch: 7, Step: 3200, Train Loss: 9.008830512218948, Valid Loss: 8.977906046594507\n",
            "Epoch: 7, Step: 3300, Train Loss: 9.012469623275036, Valid Loss: 8.98882780849576\n",
            "Epoch: 8, Step: 3400, Train Loss: 9.035404014389274, Valid Loss: 9.007382936611167\n",
            "Epoch: 8, Step: 3500, Train Loss: 8.976832861643965, Valid Loss: 8.942516088285164\n",
            "Epoch: 8, Step: 3600, Train Loss: 8.97626863268382, Valid Loss: 8.953298925678453\n",
            "Epoch: 8, Step: 3700, Train Loss: 8.96474438587672, Valid Loss: 8.933745268487508\n",
            "Epoch: 9, Step: 3800, Train Loss: 8.968969265963718, Valid Loss: 8.944702531011439\n",
            "Epoch: 9, Step: 3900, Train Loss: 8.950575204311122, Valid Loss: 8.921853188397481\n",
            "Epoch: 9, Step: 4000, Train Loss: 8.989148164058935, Valid Loss: 8.963369545843364\n",
            "Epoch: 9, Step: 4100, Train Loss: 8.989179200514975, Valid Loss: 8.959797183011942\n",
            "Epoch: 10, Step: 4200, Train Loss: 8.956956408636337, Valid Loss: 8.92747717385932\n",
            "Epoch: 10, Step: 4300, Train Loss: 8.987267489598313, Valid Loss: 8.967937385468165\n",
            "Epoch: 10, Step: 4400, Train Loss: 8.950665800746304, Valid Loss: 8.91845622410827\n",
            "Epoch: 10, Step: 4500, Train Loss: 9.008140169696874, Valid Loss: 8.97927169165275\n",
            "Epoch: 11, Step: 4600, Train Loss: 8.97924883674572, Valid Loss: 8.95329894920025\n",
            "Epoch: 11, Step: 4700, Train Loss: 9.051872377971717, Valid Loss: 9.030514012007739\n",
            "Epoch: 11, Step: 4800, Train Loss: 8.93842727381337, Valid Loss: 8.917863467678057\n",
            "Epoch: 11, Step: 4900, Train Loss: 8.944904432721387, Valid Loss: 8.915498366617031\n",
            "Epoch: 11, Step: 5000, Train Loss: 8.938987849440366, Valid Loss: 8.91621571170254\n",
            "Epoch: 12, Step: 5100, Train Loss: 8.952552468915274, Valid Loss: 8.931285760634658\n",
            "Epoch: 12, Step: 5200, Train Loss: 8.94265163496108, Valid Loss: 8.916110797430955\n",
            "Epoch: 12, Step: 5300, Train Loss: 8.936588569687935, Valid Loss: 8.906066541373114\n",
            "Epoch: 12, Step: 5400, Train Loss: 8.957868271142555, Valid Loss: 8.938954079015863\n",
            "Epoch: 13, Step: 5500, Train Loss: 8.944808711561295, Valid Loss: 8.92354210071069\n",
            "Epoch: 13, Step: 5600, Train Loss: 8.97970820475936, Valid Loss: 8.953036844332912\n",
            "Epoch: 13, Step: 5700, Train Loss: 8.926105024879401, Valid Loss: 8.908053400182585\n",
            "Epoch: 13, Step: 5800, Train Loss: 8.965247443475334, Valid Loss: 8.944739931217145\n",
            "Epoch: 14, Step: 5900, Train Loss: 8.928983661520803, Valid Loss: 8.908135900734194\n",
            "Epoch: 14, Step: 6000, Train Loss: 8.955943229383525, Valid Loss: 8.937213721594775\n",
            "Epoch: 14, Step: 6100, Train Loss: 8.938344531045459, Valid Loss: 8.919853790151969\n",
            "Epoch: 14, Step: 6200, Train Loss: 8.93640539057931, Valid Loss: 8.914059194003421\n",
            "Epoch: 15, Step: 6300, Train Loss: 8.942065874575835, Valid Loss: 8.921811639305501\n",
            "Epoch: 15, Step: 6400, Train Loss: 8.938795286731812, Valid Loss: 8.922794458825605\n",
            "Epoch: 15, Step: 6500, Train Loss: 8.962128691533126, Valid Loss: 8.948286860211546\n",
            "Epoch: 15, Step: 6600, Train Loss: 8.940796102131884, Valid Loss: 8.92107588713009\n",
            "Epoch: 16, Step: 6700, Train Loss: 8.938632923722546, Valid Loss: 8.923352166368035\n",
            "Epoch: 16, Step: 6800, Train Loss: 8.950560386202074, Valid Loss: 8.934891169934685\n",
            "Epoch: 16, Step: 6900, Train Loss: 8.910712285898114, Valid Loss: 8.889507252542357\n",
            "Epoch: 16, Step: 7000, Train Loss: 8.93213580485434, Valid Loss: 8.923173207271905\n",
            "Epoch: 16, Step: 7100, Train Loss: 8.94357813473229, Valid Loss: 8.930613450100202\n",
            "Epoch: 17, Step: 7200, Train Loss: 8.926122222714415, Valid Loss: 8.9151821065643\n",
            "Epoch: 17, Step: 7300, Train Loss: 8.91687987464877, Valid Loss: 8.9035113811173\n",
            "Epoch: 17, Step: 7400, Train Loss: 8.923604438246553, Valid Loss: 8.916605455355302\n",
            "Epoch: 17, Step: 7500, Train Loss: 8.970874952683097, Valid Loss: 8.961596795278913\n",
            "Epoch: 18, Step: 7600, Train Loss: 8.903842851444777, Valid Loss: 8.89268708878833\n",
            "Epoch: 18, Step: 7700, Train Loss: 8.964544437440098, Valid Loss: 8.95357813211159\n",
            "Epoch: 18, Step: 7800, Train Loss: 8.908894145375795, Valid Loss: 8.89596315288431\n",
            "Epoch: 18, Step: 7900, Train Loss: 8.930637658595229, Valid Loss: 8.91935841514723\n",
            "Epoch: 19, Step: 8000, Train Loss: 8.909249765995883, Valid Loss: 8.903257104204854\n",
            "Epoch: 19, Step: 8100, Train Loss: 8.94886145431743, Valid Loss: 8.938801557940424\n",
            "Epoch: 19, Step: 8200, Train Loss: 8.914609014612594, Valid Loss: 8.905696176796695\n",
            "Epoch: 19, Step: 8300, Train Loss: 8.918033167913855, Valid Loss: 8.909675788141636\n",
            "Epoch: 20, Step: 8400, Train Loss: 8.922210704345252, Valid Loss: 8.918101947932673\n",
            "Epoch: 20, Step: 8500, Train Loss: 8.876856287269897, Valid Loss: 8.864700058419878\n",
            "Epoch: 20, Step: 8600, Train Loss: 8.920243386890295, Valid Loss: 8.908752274851318\n",
            "Epoch: 20, Step: 8700, Train Loss: 8.874918814174166, Valid Loss: 8.867737497507116\n",
            "Epoch: 21, Step: 8800, Train Loss: 8.89335730619388, Valid Loss: 8.884371160155307\n",
            "Epoch: 21, Step: 8900, Train Loss: 8.96723392929698, Valid Loss: 8.961153437685704\n",
            "Epoch: 21, Step: 9000, Train Loss: 8.888785514951278, Valid Loss: 8.886366739772805\n",
            "Epoch: 21, Step: 9100, Train Loss: 8.95559352980464, Valid Loss: 8.955878187284338\n",
            "Epoch: 22, Step: 9200, Train Loss: 8.895739651229578, Valid Loss: 8.896612154365462\n",
            "Epoch: 22, Step: 9300, Train Loss: 8.91121040485764, Valid Loss: 8.908186303308613\n",
            "Epoch: 22, Step: 9400, Train Loss: 8.8984721098692, Valid Loss: 8.89442274062713\n",
            "Epoch: 22, Step: 9500, Train Loss: 8.891173826361433, Valid Loss: 8.880213825719117\n",
            "Epoch: 22, Step: 9600, Train Loss: 8.895783723150222, Valid Loss: 8.889230542134008\n",
            "Epoch: 23, Step: 9700, Train Loss: 8.909395886124713, Valid Loss: 8.90469485588752\n",
            "Epoch: 23, Step: 9800, Train Loss: 8.893849472441753, Valid Loss: 8.896477421293074\n",
            "Epoch: 23, Step: 9900, Train Loss: 8.891244211793335, Valid Loss: 8.89050296443226\n",
            "Epoch: 23, Step: 10000, Train Loss: 8.88467642399718, Valid Loss: 8.88508202800964\n",
            "Epoch: 24, Step: 10100, Train Loss: 8.891353540267929, Valid Loss: 8.888985584638526\n",
            "Epoch: 24, Step: 10200, Train Loss: 8.889770022663837, Valid Loss: 8.897543756295505\n",
            "Epoch: 24, Step: 10300, Train Loss: 8.908974525969397, Valid Loss: 8.907127846096257\n",
            "Epoch: 24, Step: 10400, Train Loss: 8.924970591109401, Valid Loss: 8.923764584420953\n",
            "Epoch: 25, Step: 10500, Train Loss: 8.93206604501458, Valid Loss: 8.930698227716933\n",
            "Epoch: 25, Step: 10600, Train Loss: 8.883859012504312, Valid Loss: 8.88058355508576\n",
            "Epoch: 25, Step: 10700, Train Loss: 8.895892884044677, Valid Loss: 8.897994614594396\n",
            "Epoch: 25, Step: 10800, Train Loss: 8.872342849742335, Valid Loss: 8.87319040296294\n",
            "Epoch: 26, Step: 10900, Train Loss: 8.905924105381033, Valid Loss: 8.912021173859978\n",
            "Epoch: 26, Step: 11000, Train Loss: 8.873803309862993, Valid Loss: 8.880062283123237\n",
            "Epoch: 26, Step: 11100, Train Loss: 8.885702494723448, Valid Loss: 8.890187857164328\n",
            "Epoch: 26, Step: 11200, Train Loss: 8.899309948390718, Valid Loss: 8.89981296855449\n",
            "Epoch: 27, Step: 11300, Train Loss: 8.90092344080664, Valid Loss: 8.898455349864228\n",
            "Epoch: 27, Step: 11400, Train Loss: 8.857305712783687, Valid Loss: 8.855480649531948\n",
            "Epoch: 27, Step: 11500, Train Loss: 8.959699696290958, Valid Loss: 8.964896647668155\n",
            "Epoch: 27, Step: 11600, Train Loss: 8.889775436263783, Valid Loss: 8.889156816783574\n",
            "Epoch: 27, Step: 11700, Train Loss: 8.89064536397314, Valid Loss: 8.894886771973642\n",
            "Epoch: 28, Step: 11800, Train Loss: 8.931788403369834, Valid Loss: 8.93060471784913\n",
            "Epoch: 28, Step: 11900, Train Loss: 8.85424115593953, Valid Loss: 8.857683813946386\n",
            "Epoch: 28, Step: 12000, Train Loss: 8.888281480570072, Valid Loss: 8.892711777363788\n",
            "Epoch: 28, Step: 12100, Train Loss: 8.907832692109809, Valid Loss: 8.91079558036463\n",
            "Epoch: 29, Step: 12200, Train Loss: 8.893938654814736, Valid Loss: 8.90226696113445\n",
            "Epoch: 29, Step: 12300, Train Loss: 8.856626673766563, Valid Loss: 8.863188963091774\n",
            "Epoch: 29, Step: 12400, Train Loss: 8.925882678981823, Valid Loss: 8.936971627497787\n",
            "Epoch: 29, Step: 12500, Train Loss: 8.863353481734382, Valid Loss: 8.866425778411655\n",
            "Epoch: 30, Step: 12600, Train Loss: 8.897392701693493, Valid Loss: 8.901976445820152\n",
            "Epoch: 30, Step: 12700, Train Loss: 8.893752482475033, Valid Loss: 8.896904123071229\n",
            "Epoch: 30, Step: 12800, Train Loss: 8.863001966189676, Valid Loss: 8.86224022124\n",
            "Epoch: 30, Step: 12900, Train Loss: 8.866372638062403, Valid Loss: 8.8693173539783\n",
            "Epoch: 31, Step: 13000, Train Loss: 8.932849254714892, Valid Loss: 8.94546465676175\n",
            "Epoch: 31, Step: 13100, Train Loss: 8.857311710783742, Valid Loss: 8.864003277168733\n",
            "Epoch: 31, Step: 13200, Train Loss: 8.914706755146403, Valid Loss: 8.922663713418588\n",
            "Epoch: 31, Step: 13300, Train Loss: 8.910714776388861, Valid Loss: 8.921129672605383\n",
            "Epoch: 32, Step: 13400, Train Loss: 8.833804330721772, Valid Loss: 8.842132193948329\n",
            "Epoch: 32, Step: 13500, Train Loss: 8.870495697906513, Valid Loss: 8.885539050480531\n",
            "Epoch: 32, Step: 13600, Train Loss: 8.858385414159445, Valid Loss: 8.865128144815623\n",
            "Epoch: 32, Step: 13700, Train Loss: 8.853227356645032, Valid Loss: 8.860259606043169\n",
            "Epoch: 33, Step: 13800, Train Loss: 8.902332316319473, Valid Loss: 8.91635857150337\n",
            "Epoch: 33, Step: 13900, Train Loss: 8.850040196937304, Valid Loss: 8.86032245288677\n",
            "Epoch: 33, Step: 14000, Train Loss: 8.900869703966139, Valid Loss: 8.916836435122379\n",
            "Epoch: 33, Step: 14100, Train Loss: 8.86180044052946, Valid Loss: 8.879163347575462\n",
            "Epoch: 33, Step: 14200, Train Loss: 8.850406771052741, Valid Loss: 8.872059189492036\n",
            "Epoch: 34, Step: 14300, Train Loss: 8.843430478826496, Valid Loss: 8.863630033529304\n",
            "Epoch: 34, Step: 14400, Train Loss: 8.854753065085326, Valid Loss: 8.87894507588121\n",
            "Epoch: 34, Step: 14500, Train Loss: 8.878880632170459, Valid Loss: 8.900549801690714\n",
            "Epoch: 34, Step: 14600, Train Loss: 8.872606191431004, Valid Loss: 8.892143122281434\n",
            "Epoch: 35, Step: 14700, Train Loss: 8.902168005825827, Valid Loss: 8.922131953204376\n",
            "Epoch: 35, Step: 14800, Train Loss: 8.857697248126515, Valid Loss: 8.877850576814888\n",
            "Epoch: 35, Step: 14900, Train Loss: 8.876990154331722, Valid Loss: 8.894795490040451\n",
            "Epoch: 35, Step: 15000, Train Loss: 8.876723194768822, Valid Loss: 8.897936279899351\n",
            "Epoch: 36, Step: 15100, Train Loss: 8.906276381285972, Valid Loss: 8.919190468724585\n",
            "Epoch: 36, Step: 15200, Train Loss: 8.877753233210816, Valid Loss: 8.892726931497162\n",
            "Epoch: 36, Step: 15300, Train Loss: 8.8814206602146, Valid Loss: 8.893082168814122\n",
            "Epoch: 36, Step: 15400, Train Loss: 8.901908293152387, Valid Loss: 8.916685956605969\n",
            "Epoch: 37, Step: 15500, Train Loss: 8.909106222660188, Valid Loss: 8.920279822289467\n",
            "Epoch: 37, Step: 15600, Train Loss: 8.882403503981624, Valid Loss: 8.89558206929371\n",
            "Epoch: 37, Step: 15700, Train Loss: 8.849567126528886, Valid Loss: 8.862715157332431\n",
            "Epoch: 37, Step: 15800, Train Loss: 8.850164774758262, Valid Loss: 8.865663369237494\n",
            "Epoch: 38, Step: 15900, Train Loss: 8.884971632819317, Valid Loss: 8.903881306759631\n",
            "Epoch: 38, Step: 16000, Train Loss: 8.897006003019591, Valid Loss: 8.916795852252104\n",
            "Epoch: 38, Step: 16100, Train Loss: 8.86484039462369, Valid Loss: 8.887539808428986\n",
            "Epoch: 38, Step: 16200, Train Loss: 8.844798028207403, Valid Loss: 8.869618602705588\n",
            "Epoch: 38, Step: 16300, Train Loss: 8.847330150115129, Valid Loss: 8.870229153648825\n",
            "Epoch: 39, Step: 16400, Train Loss: 8.850757498512296, Valid Loss: 8.873599871777335\n",
            "Epoch: 39, Step: 16500, Train Loss: 8.873089950689172, Valid Loss: 8.895792589148627\n",
            "Epoch: 39, Step: 16600, Train Loss: 8.85870083108285, Valid Loss: 8.880196774341757\n",
            "Epoch: 39, Step: 16700, Train Loss: 8.874397829816814, Valid Loss: 8.898533890054376\n",
            "Epoch: 40, Step: 16800, Train Loss: 8.902160871513743, Valid Loss: 8.920479865039855\n",
            "Epoch: 40, Step: 16900, Train Loss: 8.862017252424469, Valid Loss: 8.88568666251691\n",
            "Epoch: 40, Step: 17000, Train Loss: 8.857149553591844, Valid Loss: 8.881697511681436\n",
            "Epoch: 40, Step: 17100, Train Loss: 8.84235840816175, Valid Loss: 8.862874659717267\n",
            "Epoch: 41, Step: 17200, Train Loss: 8.846759210626134, Valid Loss: 8.868802111430897\n",
            "Epoch: 41, Step: 17300, Train Loss: 8.85623726795692, Valid Loss: 8.88001285691073\n",
            "Epoch: 41, Step: 17400, Train Loss: 8.858766991819374, Valid Loss: 8.883338394440615\n",
            "Epoch: 41, Step: 17500, Train Loss: 8.83391114168787, Valid Loss: 8.862122130209341\n",
            "Epoch: 42, Step: 17600, Train Loss: 8.857053433814901, Valid Loss: 8.87465399609855\n",
            "Epoch: 42, Step: 17700, Train Loss: 8.886585346648774, Valid Loss: 8.904145613070535\n",
            "Epoch: 42, Step: 17800, Train Loss: 8.885172999021815, Valid Loss: 8.910130093175841\n",
            "Epoch: 42, Step: 17900, Train Loss: 8.909950116284435, Valid Loss: 8.930505523782532\n",
            "Epoch: 43, Step: 18000, Train Loss: 8.861008565319327, Valid Loss: 8.875052038237447\n",
            "Epoch: 43, Step: 18100, Train Loss: 8.901527205771828, Valid Loss: 8.922571316009991\n",
            "Epoch: 43, Step: 18200, Train Loss: 8.8895735383671, Valid Loss: 8.910633251169129\n",
            "Epoch: 43, Step: 18300, Train Loss: 8.854871706437923, Valid Loss: 8.872913352215583\n",
            "Epoch: 44, Step: 18400, Train Loss: 8.868157896827014, Valid Loss: 8.884839693839574\n",
            "Epoch: 44, Step: 18500, Train Loss: 8.858833990961065, Valid Loss: 8.87514336187771\n",
            "Epoch: 44, Step: 18600, Train Loss: 8.82723076149571, Valid Loss: 8.847157961021784\n",
            "Epoch: 44, Step: 18700, Train Loss: 8.902789169262881, Valid Loss: 8.926942338621565\n",
            "Epoch: 44, Step: 18800, Train Loss: 8.85328496871904, Valid Loss: 8.880140378871587\n",
            "Epoch: 45, Step: 18900, Train Loss: 8.845108932896741, Valid Loss: 8.86638685311712\n",
            "Epoch: 45, Step: 19000, Train Loss: 8.883925997566683, Valid Loss: 8.91333203071618\n",
            "Epoch: 45, Step: 19100, Train Loss: 8.850512666492587, Valid Loss: 8.878140306519704\n",
            "Epoch: 45, Step: 19200, Train Loss: 8.856703290946374, Valid Loss: 8.882669041184004\n",
            "Epoch: 46, Step: 19300, Train Loss: 8.8572330667578, Valid Loss: 8.884102458522472\n",
            "Epoch: 46, Step: 19400, Train Loss: 8.819788575255949, Valid Loss: 8.844664455657828\n",
            "Epoch: 46, Step: 19500, Train Loss: 8.884783988940397, Valid Loss: 8.908661009699797\n",
            "Epoch: 46, Step: 19600, Train Loss: 8.82394808238654, Valid Loss: 8.845719925912643\n",
            "Epoch: 47, Step: 19700, Train Loss: 8.89077462449, Valid Loss: 8.908476755589419\n",
            "Epoch: 47, Step: 19800, Train Loss: 8.858085420849621, Valid Loss: 8.876802407348078\n",
            "Epoch: 47, Step: 19900, Train Loss: 8.848046162122799, Valid Loss: 8.8715584964885\n",
            "Epoch: 47, Step: 20000, Train Loss: 8.871201724808726, Valid Loss: 8.89841917040222\n",
            "Epoch: 48, Step: 20100, Train Loss: 8.889463644437289, Valid Loss: 8.912098734346273\n",
            "Epoch: 48, Step: 20200, Train Loss: 8.876549088466522, Valid Loss: 8.898561842583607\n",
            "Epoch: 48, Step: 20300, Train Loss: 8.860015820254057, Valid Loss: 8.883594943607042\n",
            "Epoch: 48, Step: 20400, Train Loss: 8.859099315813571, Valid Loss: 8.888654473557231\n",
            "Epoch: 49, Step: 20500, Train Loss: 8.85093638442415, Valid Loss: 8.874589900485704\n",
            "Epoch: 49, Step: 20600, Train Loss: 8.862367706278532, Valid Loss: 8.885118349477867\n",
            "Epoch: 49, Step: 20700, Train Loss: 8.847487998860323, Valid Loss: 8.871618032499939\n",
            "Epoch: 49, Step: 20800, Train Loss: 8.862720807872895, Valid Loss: 8.892498974985925\n",
            "Epoch: 49, Step: 20900, Train Loss: 8.842534602875135, Valid Loss: 8.863294667744393\n",
            "Epoch: 50, Step: 21000, Train Loss: 8.83834732399746, Valid Loss: 8.859776171251044\n",
            "Epoch: 50, Step: 21100, Train Loss: 8.85372293345053, Valid Loss: 8.87124730038197\n",
            "Epoch: 50, Step: 21200, Train Loss: 8.87133683301434, Valid Loss: 8.893388544452419\n",
            "Epoch: 50, Step: 21300, Train Loss: 8.842620737012856, Valid Loss: 8.859944675818745\n",
            "Epoch: 51, Step: 21400, Train Loss: 8.848793710358274, Valid Loss: 8.87397835854893\n",
            "Epoch: 51, Step: 21500, Train Loss: 8.834957974274628, Valid Loss: 8.865734194793028\n",
            "Epoch: 51, Step: 21600, Train Loss: 8.88168227714363, Valid Loss: 8.910061270309587\n",
            "Epoch: 51, Step: 21700, Train Loss: 8.864764690069412, Valid Loss: 8.895513391925299\n",
            "Epoch: 52, Step: 21800, Train Loss: 8.843130558997588, Valid Loss: 8.867048607386064\n",
            "Epoch: 52, Step: 21900, Train Loss: 8.871074124154157, Valid Loss: 8.896930352750848\n",
            "Epoch: 52, Step: 22000, Train Loss: 8.901259127236997, Valid Loss: 8.928527892750232\n",
            "Epoch: 52, Step: 22100, Train Loss: 8.847641098855117, Valid Loss: 8.874352600306672\n",
            "Epoch: 53, Step: 22200, Train Loss: 8.83935259824941, Valid Loss: 8.87489822438007\n",
            "Epoch: 53, Step: 22300, Train Loss: 8.878154858138824, Valid Loss: 8.910899992370192\n",
            "Epoch: 53, Step: 22400, Train Loss: 8.865888055686607, Valid Loss: 8.890720886049909\n",
            "Epoch: 53, Step: 22500, Train Loss: 8.829130105604499, Valid Loss: 8.862018806989976\n",
            "Epoch: 54, Step: 22600, Train Loss: 8.84065830276732, Valid Loss: 8.877101731918348\n",
            "Epoch: 54, Step: 22700, Train Loss: 8.87924458176074, Valid Loss: 8.907090446373843\n",
            "Epoch: 54, Step: 22800, Train Loss: 8.846496302762064, Valid Loss: 8.870799044949791\n",
            "Epoch: 54, Step: 22900, Train Loss: 8.851066295699779, Valid Loss: 8.876533151565981\n",
            "Epoch: 55, Step: 23000, Train Loss: 8.83773629010611, Valid Loss: 8.862806282554804\n",
            "Epoch: 55, Step: 23100, Train Loss: 8.880517539020941, Valid Loss: 8.909477698732363\n",
            "Epoch: 55, Step: 23200, Train Loss: 8.860507447709017, Valid Loss: 8.889186694115347\n",
            "Epoch: 55, Step: 23300, Train Loss: 8.856699072983043, Valid Loss: 8.883105953332104\n",
            "Epoch: 55, Step: 23400, Train Loss: 8.831028788429464, Valid Loss: 8.861322031177426\n",
            "Epoch: 56, Step: 23500, Train Loss: 8.849160380268193, Valid Loss: 8.881965740723688\n",
            "Epoch: 56, Step: 23600, Train Loss: 8.830228301591147, Valid Loss: 8.86234951832182\n",
            "Epoch: 56, Step: 23700, Train Loss: 8.862249157619912, Valid Loss: 8.889690516326349\n",
            "Epoch: 56, Step: 23800, Train Loss: 8.860646856371616, Valid Loss: 8.89537477372229\n",
            "Epoch: 57, Step: 23900, Train Loss: 8.866597387548367, Valid Loss: 8.90189412712626\n",
            "Epoch: 57, Step: 24000, Train Loss: 8.851933873663793, Valid Loss: 8.881346559101033\n",
            "Epoch: 57, Step: 24100, Train Loss: 8.899951711561698, Valid Loss: 8.931459999300364\n",
            "Epoch: 57, Step: 24200, Train Loss: 8.852556332338125, Valid Loss: 8.878530405127195\n",
            "Epoch: 58, Step: 24300, Train Loss: 8.83675227633413, Valid Loss: 8.86427663643354\n",
            "Epoch: 58, Step: 24400, Train Loss: 8.865642647269627, Valid Loss: 8.900416178798874\n",
            "Epoch: 58, Step: 24500, Train Loss: 8.855768893119661, Valid Loss: 8.878130438610393\n",
            "Epoch: 58, Step: 24600, Train Loss: 8.859494275143382, Valid Loss: 8.88319344603039\n",
            "Epoch: 59, Step: 24700, Train Loss: 8.869583779589941, Valid Loss: 8.8920997433518\n",
            "Epoch: 59, Step: 24800, Train Loss: 8.855341057801198, Valid Loss: 8.878102303148172\n",
            "Epoch: 59, Step: 24900, Train Loss: 8.847006219328701, Valid Loss: 8.87394054619643\n",
            "Epoch: 59, Step: 25000, Train Loss: 8.865309530729833, Valid Loss: 8.892733297220122\n",
            "Epoch: 60, Step: 25100, Train Loss: 8.883812787441183, Valid Loss: 8.909734885081278\n",
            "Epoch: 60, Step: 25200, Train Loss: 8.860038286838046, Valid Loss: 8.8824500139523\n",
            "Epoch: 60, Step: 25300, Train Loss: 8.849752812755503, Valid Loss: 8.87586267769588\n",
            "Epoch: 60, Step: 25400, Train Loss: 8.842751620142542, Valid Loss: 8.864644315084426\n",
            "Epoch: 61, Step: 25500, Train Loss: 8.87801837086813, Valid Loss: 8.906830651339245\n",
            "Epoch: 61, Step: 25600, Train Loss: 8.876816973236279, Valid Loss: 8.90402589237555\n",
            "Epoch: 61, Step: 25700, Train Loss: 8.836488755090976, Valid Loss: 8.863411726210323\n",
            "Epoch: 61, Step: 25800, Train Loss: 8.851105935604538, Valid Loss: 8.884728774787899\n",
            "Epoch: 61, Step: 25900, Train Loss: 8.862655722183755, Valid Loss: 8.889899320806613\n",
            "Epoch: 62, Step: 26000, Train Loss: 8.86975793371117, Valid Loss: 8.899413563347897\n",
            "Epoch: 62, Step: 26100, Train Loss: 8.857405502184836, Valid Loss: 8.888262250867363\n",
            "Epoch: 62, Step: 26200, Train Loss: 8.859630174628974, Valid Loss: 8.888248043987995\n",
            "Epoch: 62, Step: 26300, Train Loss: 8.828374581277938, Valid Loss: 8.856114273871052\n",
            "Epoch: 63, Step: 26400, Train Loss: 8.861784582811307, Valid Loss: 8.89868481752277\n",
            "Epoch: 63, Step: 26500, Train Loss: 8.817544648134527, Valid Loss: 8.849243687571809\n",
            "Epoch: 63, Step: 26600, Train Loss: 8.838618984629248, Valid Loss: 8.8688455212027\n",
            "Epoch: 63, Step: 26700, Train Loss: 8.849465781491803, Valid Loss: 8.878047165828306\n",
            "Epoch: 64, Step: 26800, Train Loss: 8.870468300656777, Valid Loss: 8.901636534438783\n",
            "Epoch: 64, Step: 26900, Train Loss: 8.830627916404621, Valid Loss: 8.856021293899888\n",
            "Epoch: 64, Step: 27000, Train Loss: 8.820876759878173, Valid Loss: 8.840611266715886\n",
            "Epoch: 64, Step: 27100, Train Loss: 8.832284159833675, Valid Loss: 8.857851316961252\n",
            "Epoch: 65, Step: 27200, Train Loss: 8.853072010421542, Valid Loss: 8.883057817163355\n",
            "Epoch: 65, Step: 27300, Train Loss: 8.841062998881638, Valid Loss: 8.857680929952869\n",
            "Epoch: 65, Step: 27400, Train Loss: 8.845366113192394, Valid Loss: 8.869096980728745\n",
            "Epoch: 65, Step: 27500, Train Loss: 8.85108528899478, Valid Loss: 8.878168290048066\n",
            "Epoch: 66, Step: 27600, Train Loss: 8.850095754236191, Valid Loss: 8.879742892111624\n",
            "Epoch: 66, Step: 27700, Train Loss: 8.832038243184613, Valid Loss: 8.863452004647822\n",
            "Epoch: 66, Step: 27800, Train Loss: 8.881524068163156, Valid Loss: 8.9093307905281\n",
            "Epoch: 66, Step: 27900, Train Loss: 8.835028988978983, Valid Loss: 8.867245473881914\n",
            "Epoch: 66, Step: 28000, Train Loss: 8.883273232995785, Valid Loss: 8.919694518082725\n",
            "Epoch: 67, Step: 28100, Train Loss: 8.823874208396065, Valid Loss: 8.860259423023457\n",
            "Epoch: 67, Step: 28200, Train Loss: 8.85991911990632, Valid Loss: 8.892731719999011\n",
            "Epoch: 67, Step: 28300, Train Loss: 8.86408093571239, Valid Loss: 8.899041286618846\n",
            "Epoch: 67, Step: 28400, Train Loss: 8.847833306495199, Valid Loss: 8.888922554268204\n",
            "Epoch: 68, Step: 28500, Train Loss: 8.81617863395047, Valid Loss: 8.849476313695934\n",
            "Epoch: 68, Step: 28600, Train Loss: 8.84595575303074, Valid Loss: 8.886269746496215\n",
            "Epoch: 68, Step: 28700, Train Loss: 8.822434019086538, Valid Loss: 8.860664253379172\n",
            "Epoch: 68, Step: 28800, Train Loss: 8.890978542142618, Valid Loss: 8.923670718697776\n",
            "Epoch: 69, Step: 28900, Train Loss: 8.867890628103027, Valid Loss: 8.902136633412946\n",
            "Epoch: 69, Step: 29000, Train Loss: 8.836316990322628, Valid Loss: 8.868192131920107\n",
            "Epoch: 69, Step: 29100, Train Loss: 8.812821773700183, Valid Loss: 8.849049514444705\n",
            "Epoch: 69, Step: 29200, Train Loss: 8.841212660469733, Valid Loss: 8.86828193275904\n",
            "Epoch: 70, Step: 29300, Train Loss: 8.849108823436072, Valid Loss: 8.875100286641636\n",
            "Epoch: 70, Step: 29400, Train Loss: 8.826145450699524, Valid Loss: 8.853699203193978\n",
            "Epoch: 70, Step: 29500, Train Loss: 8.853459649601943, Valid Loss: 8.876488483986625\n",
            "Epoch: 70, Step: 29600, Train Loss: 8.85278864131649, Valid Loss: 8.888185603109692\n",
            "Epoch: 71, Step: 29700, Train Loss: 8.843432557765777, Valid Loss: 8.873361771403493\n",
            "Epoch: 71, Step: 29800, Train Loss: 8.843162255839465, Valid Loss: 8.88119730277326\n",
            "Epoch: 71, Step: 29900, Train Loss: 8.860279577971442, Valid Loss: 8.89612968184645\n",
            "Epoch: 71, Step: 30000, Train Loss: 8.854457653466847, Valid Loss: 8.88465371549841\n",
            "Epoch: 72, Step: 30100, Train Loss: 8.849233151531665, Valid Loss: 8.880129671280763\n",
            "Epoch: 72, Step: 30200, Train Loss: 8.863933166802944, Valid Loss: 8.89720318322832\n",
            "Epoch: 72, Step: 30300, Train Loss: 8.882296259267765, Valid Loss: 8.916276683196378\n",
            "Epoch: 72, Step: 30400, Train Loss: 8.879717764970964, Valid Loss: 8.919559245912533\n",
            "Epoch: 72, Step: 30500, Train Loss: 8.880933928453977, Valid Loss: 8.91468455135631\n",
            "Epoch: 73, Step: 30600, Train Loss: 8.851535750240041, Valid Loss: 8.889165817189307\n",
            "Epoch: 73, Step: 30700, Train Loss: 8.867989566698848, Valid Loss: 8.9034695950491\n",
            "Epoch: 73, Step: 30800, Train Loss: 8.867444472720557, Valid Loss: 8.903608127435941\n",
            "Epoch: 73, Step: 30900, Train Loss: 8.846397937147627, Valid Loss: 8.885705222576126\n",
            "Epoch: 74, Step: 31000, Train Loss: 8.860818129177543, Valid Loss: 8.895817494006353\n",
            "Epoch: 74, Step: 31100, Train Loss: 8.858235208507187, Valid Loss: 8.88839383875737\n",
            "Epoch: 74, Step: 31200, Train Loss: 8.8271931284195, Valid Loss: 8.860052306463816\n",
            "Epoch: 74, Step: 31300, Train Loss: 8.863574427566617, Valid Loss: 8.895481355405245\n",
            "Epoch: 75, Step: 31400, Train Loss: 8.865017556232516, Valid Loss: 8.903544237895305\n",
            "Epoch: 75, Step: 31500, Train Loss: 8.825068776193858, Valid Loss: 8.858026995338193\n",
            "Epoch: 75, Step: 31600, Train Loss: 8.843492394334453, Valid Loss: 8.87529661149359\n",
            "Epoch: 75, Step: 31700, Train Loss: 8.829680518686352, Valid Loss: 8.865648686688864\n",
            "Epoch: 76, Step: 31800, Train Loss: 8.847666434101106, Valid Loss: 8.874699624488965\n",
            "Epoch: 76, Step: 31900, Train Loss: 8.86291923949711, Valid Loss: 8.891495859957672\n",
            "Epoch: 76, Step: 32000, Train Loss: 8.850116632386843, Valid Loss: 8.885844048503946\n",
            "Epoch: 76, Step: 32100, Train Loss: 8.810133731063193, Valid Loss: 8.84057472427573\n",
            "Epoch: 77, Step: 32200, Train Loss: 8.822190343347923, Valid Loss: 8.852717897739883\n",
            "Epoch: 77, Step: 32300, Train Loss: 8.831603513591574, Valid Loss: 8.865440246985475\n",
            "Epoch: 77, Step: 32400, Train Loss: 8.856078172945347, Valid Loss: 8.888795388116838\n",
            "Epoch: 77, Step: 32500, Train Loss: 8.848810202733658, Valid Loss: 8.882186306828851\n",
            "Epoch: 77, Step: 32600, Train Loss: 8.852161314575538, Valid Loss: 8.886847967371885\n",
            "Epoch: 78, Step: 32700, Train Loss: 8.86157043746548, Valid Loss: 8.896456757954171\n",
            "Epoch: 78, Step: 32800, Train Loss: 8.840168749104388, Valid Loss: 8.878310946081188\n",
            "Epoch: 78, Step: 32900, Train Loss: 8.846908609221781, Valid Loss: 8.883461940462036\n",
            "Epoch: 78, Step: 33000, Train Loss: 8.842844899145984, Valid Loss: 8.879089033432885\n",
            "Epoch: 79, Step: 33100, Train Loss: 8.810627600385326, Valid Loss: 8.842109612526722\n",
            "Epoch: 79, Step: 33200, Train Loss: 8.876346430509628, Valid Loss: 8.912705165454236\n",
            "Epoch: 79, Step: 33300, Train Loss: 8.842990856199222, Valid Loss: 8.876555671513856\n",
            "Epoch: 79, Step: 33400, Train Loss: 8.841760395292967, Valid Loss: 8.881057749726594\n",
            "Epoch: 80, Step: 33500, Train Loss: 8.835917325875437, Valid Loss: 8.873152478075038\n",
            "Epoch: 80, Step: 33600, Train Loss: 8.824394380804998, Valid Loss: 8.865878029550217\n",
            "Epoch: 80, Step: 33700, Train Loss: 8.838577790545058, Valid Loss: 8.877640077470664\n",
            "Epoch: 80, Step: 33800, Train Loss: 8.835582533817137, Valid Loss: 8.874876910438415\n",
            "Epoch: 81, Step: 33900, Train Loss: 8.825276735933688, Valid Loss: 8.861730456141988\n",
            "Epoch: 81, Step: 34000, Train Loss: 8.853459191574572, Valid Loss: 8.892441430768157\n",
            "Epoch: 81, Step: 34100, Train Loss: 8.84087516785181, Valid Loss: 8.879573387656354\n",
            "Epoch: 81, Step: 34200, Train Loss: 8.842845446898263, Valid Loss: 8.881342326443912\n",
            "Epoch: 82, Step: 34300, Train Loss: 8.836602489684608, Valid Loss: 8.873997837792478\n",
            "Epoch: 82, Step: 34400, Train Loss: 8.85082199965246, Valid Loss: 8.888133594378125\n",
            "Epoch: 82, Step: 34500, Train Loss: 8.847929343956126, Valid Loss: 8.887946872231215\n",
            "Epoch: 82, Step: 34600, Train Loss: 8.838142279117966, Valid Loss: 8.880088930143293\n",
            "Epoch: 83, Step: 34700, Train Loss: 8.838276488452426, Valid Loss: 8.883029080903212\n",
            "Epoch: 83, Step: 34800, Train Loss: 8.839996779493152, Valid Loss: 8.88753514271779\n",
            "Epoch: 83, Step: 34900, Train Loss: 8.829906673543935, Valid Loss: 8.873047379790062\n",
            "Epoch: 83, Step: 35000, Train Loss: 8.842959610434313, Valid Loss: 8.886080259716273\n",
            "Epoch: 83, Step: 35100, Train Loss: 8.820493245423632, Valid Loss: 8.857935290766171\n",
            "Epoch: 84, Step: 35200, Train Loss: 8.813508931552633, Valid Loss: 8.851732039211159\n",
            "Epoch: 84, Step: 35300, Train Loss: 8.86952025726313, Valid Loss: 8.90679207050264\n",
            "Epoch: 84, Step: 35400, Train Loss: 8.823035231747973, Valid Loss: 8.85501267670884\n",
            "Epoch: 84, Step: 35500, Train Loss: 8.835494814357892, Valid Loss: 8.859776568211542\n",
            "Epoch: 85, Step: 35600, Train Loss: 8.826596436355606, Valid Loss: 8.85780706158983\n",
            "Epoch: 85, Step: 35700, Train Loss: 8.828230557066375, Valid Loss: 8.861886908262937\n",
            "Epoch: 85, Step: 35800, Train Loss: 8.841734850460083, Valid Loss: 8.873325183460778\n",
            "Epoch: 85, Step: 35900, Train Loss: 8.873011086539284, Valid Loss: 8.908805420822425\n",
            "Epoch: 86, Step: 36000, Train Loss: 8.822097668987164, Valid Loss: 8.855295907221038\n",
            "Epoch: 86, Step: 36100, Train Loss: 8.845914035763192, Valid Loss: 8.892128618452336\n",
            "Epoch: 86, Step: 36200, Train Loss: 8.890716719495451, Valid Loss: 8.931569168488585\n",
            "Epoch: 86, Step: 36300, Train Loss: 8.842602157928779, Valid Loss: 8.880080935571359\n",
            "Epoch: 87, Step: 36400, Train Loss: 8.845190892057888, Valid Loss: 8.881562227502885\n",
            "Epoch: 87, Step: 36500, Train Loss: 8.819608095029745, Valid Loss: 8.854829199423127\n",
            "Epoch: 87, Step: 36600, Train Loss: 8.86229566202671, Valid Loss: 8.898142938307252\n",
            "Epoch: 87, Step: 36700, Train Loss: 8.812933538016585, Valid Loss: 8.850725410219223\n",
            "Epoch: 88, Step: 36800, Train Loss: 8.83370734794663, Valid Loss: 8.86928475981638\n",
            "Epoch: 88, Step: 36900, Train Loss: 8.842960866808216, Valid Loss: 8.873933863220746\n",
            "Epoch: 88, Step: 37000, Train Loss: 8.854776195625549, Valid Loss: 8.892009531742197\n",
            "Epoch: 88, Step: 37100, Train Loss: 8.837532461982672, Valid Loss: 8.86963352561673\n",
            "Epoch: 88, Step: 37200, Train Loss: 8.827345822278463, Valid Loss: 8.858523113640066\n",
            "Epoch: 89, Step: 37300, Train Loss: 8.807065224020487, Valid Loss: 8.844770084068287\n",
            "Epoch: 89, Step: 37400, Train Loss: 8.85966159507639, Valid Loss: 8.900610196696116\n",
            "Epoch: 89, Step: 37500, Train Loss: 8.824953551821404, Valid Loss: 8.864078644370004\n",
            "Epoch: 89, Step: 37600, Train Loss: 8.828501462460606, Valid Loss: 8.861887027085078\n",
            "Epoch: 90, Step: 37700, Train Loss: 8.82492416268917, Valid Loss: 8.859351886373092\n",
            "Epoch: 90, Step: 37800, Train Loss: 8.833663452394058, Valid Loss: 8.875120325799942\n",
            "Epoch: 90, Step: 37900, Train Loss: 8.851451026492377, Valid Loss: 8.89757456879748\n",
            "Epoch: 90, Step: 38000, Train Loss: 8.811954904328102, Valid Loss: 8.860674326128045\n",
            "Epoch: 91, Step: 38100, Train Loss: 8.827451955394737, Valid Loss: 8.870208284337007\n",
            "Epoch: 91, Step: 38200, Train Loss: 8.845078519738733, Valid Loss: 8.89240922687393\n",
            "Epoch: 91, Step: 38300, Train Loss: 8.810187659331838, Valid Loss: 8.856786989362003\n",
            "Epoch: 91, Step: 38400, Train Loss: 8.824136689257461, Valid Loss: 8.865784505868227\n",
            "Epoch: 92, Step: 38500, Train Loss: 8.8391919098633, Valid Loss: 8.878661219304062\n",
            "Epoch: 92, Step: 38600, Train Loss: 8.815402804730528, Valid Loss: 8.854836657884833\n",
            "Epoch: 92, Step: 38700, Train Loss: 8.85360352399731, Valid Loss: 8.892281577712563\n",
            "Epoch: 92, Step: 38800, Train Loss: 8.842393779487663, Valid Loss: 8.881819932910574\n",
            "Epoch: 93, Step: 38900, Train Loss: 8.82863032008365, Valid Loss: 8.878476095922865\n",
            "Epoch: 93, Step: 39000, Train Loss: 8.822064222015248, Valid Loss: 8.861088836670485\n",
            "Epoch: 93, Step: 39100, Train Loss: 8.839958279868977, Valid Loss: 8.879998406788568\n",
            "Epoch: 93, Step: 39200, Train Loss: 8.82959467968335, Valid Loss: 8.868966436279125\n",
            "Epoch: 94, Step: 39300, Train Loss: 8.847654407035465, Valid Loss: 8.888256201810261\n",
            "Epoch: 94, Step: 39400, Train Loss: 8.841591456472, Valid Loss: 8.878896933757858\n",
            "Epoch: 94, Step: 39500, Train Loss: 8.850333976919215, Valid Loss: 8.890206490736961\n",
            "Epoch: 94, Step: 39600, Train Loss: 8.84440097678502, Valid Loss: 8.893051563349752\n",
            "Epoch: 94, Step: 39700, Train Loss: 8.835036118438998, Valid Loss: 8.883006423265009\n",
            "Epoch: 95, Step: 39800, Train Loss: 8.835375996501469, Valid Loss: 8.879267989239462\n",
            "Epoch: 95, Step: 39900, Train Loss: 8.86417951690426, Valid Loss: 8.90887647073673\n",
            "Epoch: 95, Step: 40000, Train Loss: 8.808948362799882, Valid Loss: 8.853774574856324\n",
            "Epoch: 95, Step: 40100, Train Loss: 8.806184513871829, Valid Loss: 8.850783389837982\n",
            "Epoch: 96, Step: 40200, Train Loss: 8.8396848050495, Valid Loss: 8.886148841722694\n",
            "Epoch: 96, Step: 40300, Train Loss: 8.84028603562516, Valid Loss: 8.88941722784111\n",
            "Epoch: 96, Step: 40400, Train Loss: 8.813461115964751, Valid Loss: 8.858915129199962\n",
            "Epoch: 96, Step: 40500, Train Loss: 8.81013876253368, Valid Loss: 8.858279634478942\n",
            "Epoch: 97, Step: 40600, Train Loss: 8.827237004456222, Valid Loss: 8.876388770580729\n",
            "Epoch: 97, Step: 40700, Train Loss: 8.84332407839489, Valid Loss: 8.890958703933459\n",
            "Epoch: 97, Step: 40800, Train Loss: 8.862653095945202, Valid Loss: 8.910378692975119\n",
            "Epoch: 97, Step: 40900, Train Loss: 8.827812705672807, Valid Loss: 8.878580831077798\n",
            "Epoch: 98, Step: 41000, Train Loss: 8.831630968348366, Valid Loss: 8.87743961262763\n",
            "Epoch: 98, Step: 41100, Train Loss: 8.827998190008833, Valid Loss: 8.875203101637629\n",
            "Epoch: 98, Step: 41200, Train Loss: 8.824920962038945, Valid Loss: 8.87693766679714\n",
            "Epoch: 98, Step: 41300, Train Loss: 8.829016542961725, Valid Loss: 8.876928393027397\n",
            "Epoch: 99, Step: 41400, Train Loss: 8.838497483224497, Valid Loss: 8.87942668672929\n",
            "Epoch: 99, Step: 41500, Train Loss: 8.825811698958901, Valid Loss: 8.871496403421258\n",
            "Epoch: 99, Step: 41600, Train Loss: 8.817071429726164, Valid Loss: 8.86507423337526\n",
            "Epoch: 99, Step: 41700, Train Loss: 8.84153438590435, Valid Loss: 8.886201094324699\n",
            "Epoch: 99, Step: 41800, Train Loss: 8.814200339178097, Valid Loss: 8.86235740294416\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm4AAAHgCAYAAAAc+uEmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeZxcdZ3v/9e3ek06WyckELKQoCyBJEQJiCKbCyIuuIM/GJGrMjqO28x4YRz3wRmY8V4dZlguKqCyuIAoCLKIQNghwQCBANnJQpZO0vve9f39caqXhE7oJOd0pcPr+Xj0o6pOnVP1rQqk3/l8txBjRJIkSXu/XLEbIEmSpIExuEmSJA0RBjdJkqQhwuAmSZI0RBjcJEmShgiDmyRJ0hBRWuwGDIb99tsvTps2rdjNkCRJek0LFiyoiTGO7++510VwmzZtGvPnzy92MyRJkl5TCGHVjp6zq1SSJGmIMLhJkiQNEQY3SZKkIeJ1McZNkiSlp6OjgzVr1tDa2lrspgxplZWVTJ48mbKysgFfY3CTJEm7ZM2aNYwcOZJp06YRQih2c4akGCObN29mzZo1TJ8+fcDX2VUqSZJ2SWtrK+PGjTO07YEQAuPGjdvlqqXBTZIk7TJD257bne/Q4CZJkoaU2tpaLr/88t269vTTT6e2tnbA53/3u9/lhz/84W69VxYMbpIkaUjZWXDr7Ozc6bV33HEHY8aMyaJZg8LgJkmShpQLL7yQZcuWMWfOHL7+9a9z//33c8IJJ/DBD36QI444AoAPfehDHH300Rx55JFcddVVPddOmzaNmpoaVq5cyYwZM/jc5z7HkUceyamnnkpLS8tO33fhwoUcd9xxzJ49mw9/+MNs3boVgEsvvZQjjjiC2bNnc9ZZZwHwwAMPMGfOHObMmcOb3vQmGhoaUvnsziqVJEm77Xu3Pcfz6+pTfc0jDhzFdz5w5A6fv/jii1m0aBELFy4E4P777+epp55i0aJFPTM0r776asaOHUtLSwvHHHMMH/3oRxk3btw2r7NkyRJuvPFGfvKTn/CJT3yCm2++mXPOOWeH7/upT32K//7v/+akk07i29/+Nt/73vf48Y9/zMUXX8yKFSuoqKjo6Yb94Q9/yGWXXcbxxx9PY2MjlZWVe/q1AFbcJEnSPuDYY4/dZlmNSy+9lKOOOorjjjuO1atXs2TJklddM336dObMmQPA0UcfzcqVK3f4+nV1ddTW1nLSSScBcO655zJv3jwAZs+ezdlnn811111HaWlSEzv++OP5h3/4By699FJqa2t7ju8pK26SJGm37awyNpiqqqp67t9///38+c9/5tFHH2X48OGcfPLJ/S67UVFR0XO/pKTkNbtKd+T2229n3rx53HbbbfzgBz/g2Wef5cILL+R973sfd9xxB8cffzx33XUXhx9++G69fl9W3CRJ0pAycuTInY4Zq6uro7q6muHDh/PCCy/w2GOP7fF7jh49murqah588EEAfvnLX3LSSSeRz+dZvXo1p5xyCpdccgl1dXU0NjaybNkyZs2axQUXXMAxxxzDCy+8sMdtACtukiRpiBk3bhzHH388M2fO5L3vfS/ve9/7tnn+tNNO48orr2TGjBkcdthhHHfccam8789//nM+//nP09zczMEHH8w111xDV1cX55xzDnV1dcQY+fKXv8yYMWP41re+xX333Ucul+PII4/kve99byptCDHGVF5obzZ37tw4f/78YjdDkqR9wuLFi5kxY0axm7FP6O+7DCEsiDHO7e98u0pT0NrRRV1zR7GbIUmS9nEGtxRc/KcXOOE//lLsZkiSpH2cwS0FpblAV37f73KWJEnFZXBLQUku0GlwkyRJGTO4paDEipskSRoEBrcUlOYCXa+D2bmSJKm4DG4peOPWh/hy7mbyVt0kSdorjRgxAoB169bxsY99rN9zTj75ZPpbPmxHx4vBBXhTcFD9k5xceged+Uh5LhS7OZIkaQcOPPBAbrrppmI3Y7dZcUtBzJVSQt5xbpIkDYILL7yQyy67rOfxd7/7XX74wx/S2NjIO9/5Tt785jcza9Ys/vCHP7zq2pUrVzJz5kwAWlpaOOuss5gxYwYf/vCHB7RX6Y033sisWbOYOXMmF1xwAQBdXV18+tOfZubMmcyaNYsf/ehHQLLR/RFHHMHs2bM566yz0vjoVtzSEHIllJKnPZ8HSordHEmSBs+fLoT1z6b7mgfMgvdevMOnzzzzTL761a/yxS9+EYDf/OY33HXXXVRWVnLLLbcwatQoampqOO644/jgBz9ICP33hl1xxRUMHz6cxYsX88wzz/DmN795p81at24dF1xwAQsWLKC6uppTTz2V3//+90yZMoW1a9eyaNEiAGprawG4+OKLWbFiBRUVFT3H9pQVtzTkSimhy4qbJEmD4E1vehMbN25k3bp1PP3001RXVzNlyhRijHzjG99g9uzZvOtd72Lt2rVs2LBhh68zb948zjnnHABmz57N7Nmzd/q+Tz75JCeffDLjx4+ntLSUs88+m3nz5nHwwQezfPlyvvSlL3HnnXcyatSontc8++yzue666ygtTadWZsUtDblSSkOezq58sVsiSdLg2kllLEsf//jHuemmm1i/fj1nnnkmANdffz2bNm1iwYIFlJWVMW3aNFpbWzNvS3V1NU8//TR33XUXV155Jb/5zW+4+uqruf3225k3bx633XYbP/jBD3j22Wf3OMBZcUtDLvlD6OrqLHJDJEl6fTjzzDP51a9+xU033cTHP/5xAOrq6pgwYQJlZWXcd999rFq1aqevceKJJ3LDDTcAsGjRIp555pmdnn/sscfywAMPUFNTQ1dXFzfeeCMnnXQSNTU15PN5PvrRj3LRRRfx1FNPkc/nWb16NaeccgqXXHIJdXV1NDY27vHntuKWgpBLxrV1drrRvCRJg+HII4+koaGBSZMmMXHiRADOPvtsPvCBDzBr1izmzp3L4YcfvtPX+MIXvsB5553HjBkzmDFjBkcfffROz584cSIXX3wxp5xyCjFG3ve+93HGGWfw9NNPc95555HPJz1v//7v/05XVxfnnHMOdXV1xBj58pe/zJgxY/b4c4f4Olg4du7cuTHL9Vee+fX3mb34/7D6b5cyZeL4zN5HkqS9weLFi5kxY0axm7FP6O+7DCEsiDHO7e/8zLpKQwhXhxA2hhAW9Tk2NoRwTwhhSeG2egfXdoUQFhZ+bu1zfHoI4fEQwtIQwq9DCOVZtX9XhJJCV2nerlJJkpSdLMe4XQuctt2xC4F7Y4yHAPcWHvenJcY4p/DzwT7HLwF+FGN8I7AV+EzKbd49hTFu+S67SiVJUnYyC24xxnnAlu0OnwH8vHD/58CHBvp6IVmE5R1A93LHu3R9lnLdkxMc4yZJkjI02LNK948xvlK4vx7YfwfnVYYQ5ocQHgshdIezcUBtjLG7P3INMCnDtg5cd1dpp12lkqTXh9fDGPms7c53WLRZpTHGGELYUYsPijGuDSEcDPwlhPAsULcrrx9COB84H2Dq1Kl71tjXkCsEt7wVN0nS60BlZSWbN29m3LhxO9yVQDsXY2Tz5s1UVlbu0nWDHdw2hBAmxhhfCSFMBDb2d1KMcW3hdnkI4X7gTcDNwJgQQmmh6jYZWLujN4oxXgVcBcms0nQ/xraC67hJkl5HJk+ezJo1a9i0aVOxmzKkVVZWMnny5F26ZrCD263AucDFhdtX7f5amGnaHGNsCyHsBxwP/EehQncf8DHgVzu6vhi6Z5VGJydIkl4HysrKmD59erGb8bqU5XIgNwKPAoeFENaEED5DEtjeHUJYAryr8JgQwtwQwk8Ll84A5ocQngbuAy6OMT5feO4C4B9CCEtJxrz9LKv274qe5UCsuEmSpAxlVnGLMX5yB0+9s59z5wOfLdx/BJi1g9dcDhybVhvT0jPGzeAmSZIy5F6lKQgGN0mSNAgMbinorrhFZ5VKkqQMGdxSEErKAIhueSVJkjJkcEtBSc6uUkmSlD2DWwp6xrjlu4rcEkmStC8zuKWgpNR13CRJUvYMbinIdY9xs6tUkiRlyOCWglxPxc3gJkmSsmNwS0GJs0olSdIgMLiloKTUrlJJkpQ9g1sKehbgteImSZIyZHBLgcFNkiQNBoNbCkrtKpUkSYPA4JaC7lmlwQV4JUlShgxuKXBWqSRJGgwGtxR0L8CLwU2SJGXI4JaGXOFrNLhJkqQMGdzSkEvGuOEYN0mSlCGDWxpy3cuBGNwkSVJ2DG5pyHXPKrWrVJIkZcfgloZQktwa3CRJUoYMbmnI5egi5xg3SZKUKYNbSrrIQbTiJkmSsmNwS0meHDkrbpIkKUMGt5R0UWLFTZIkZcrglpIuStyrVJIkZcrglpJ8KIFocJMkSdkxuKWkixKCXaWSJClDBreU5INdpZIkKVsGt5R0UULOrlJJkpQhg1tKYsjZVSpJkjJlcEtJPlhxkyRJ2TK4pSSPs0olSVK2DG4pseImSZKyZnBLicFNkiRlzeCWEoObJEnKmsEtJTGUGtwkSVKmDG4piSFncJMkSZkyuKUkhlIC+WI3Q5Ik7cMMbinJ50ooseImSZIyZHBLSQwl5DC4SZKk7BjcUuLkBEmSlDWDW0piroQSK26SJClDBreUxOAYN0mSlC2DW1oc4yZJkjJmcEtJzJWSczkQSZKUIYNbWkIJpVbcJElShjILbiGEq0MIG0MIi/ocGxtCuCeEsKRwW93PdXNCCI+GEJ4LITwTQjizz3PXhhBWhBAWFn7mZNX+XRVzpZSQJ8ZY7KZIkqR9VJYVt2uB07Y7diFwb4zxEODewuPtNQOfijEeWbj+xyGEMX2e/3qMcU7hZ2EG7d49uVJK6KIrb3CTJEnZyCy4xRjnAVu2O3wG8PPC/Z8DH+rnupdijEsK99cBG4HxWbUzNbmkq7TT4CZJkjIy2GPc9o8xvlK4vx7Yf2cnhxCOBcqBZX0O/6DQhfqjEEJFRu3cdbkSSshbcZMkSZkp2uSEmAwG22HKCSFMBH4JnBdj7J6u+c/A4cAxwFjggp1cf34IYX4IYf6mTZvSa/iO5EqtuEmSpEwNdnDbUAhk3cFsY38nhRBGAbcD/xJjfKz7eIzxlZhoA64Bjt3RG8UYr4oxzo0xzh0/fhB6WgvLgVhxkyRJWRns4HYrcG7h/rnAH7Y/IYRQDtwC/CLGeNN2z3WHvkAyPm7R9tcXTa6UUvJ05l3LTZIkZSPL5UBuBB4FDgshrAkhfAa4GHh3CGEJ8K7CY0IIc0MIPy1c+gngRODT/Sz7cX0I4VngWWA/4KKs2r/LciXkQqSry7XcJElSNkqzeuEY4yd38NQ7+zl3PvDZwv3rgOt28JrvSK2BKQu55Kvs6uwockskSdK+yp0T0lIIbnmDmyRJyojBLSUhVwJAR2dnkVsiSZL2VQa3tBQqbiFvxU2SJGXD4JaWQnCLziqVJEkZMbilpdBVmu+y4iZJkrJhcEtJ7K64uRyIJEnKiMEtJaEnuFlxkyRJ2TC4paUQ3MhbcZMkSdkwuKUll3yV0VmlkiQpIwa3tBQmJ0Q3mZckSRkxuKUkEADIR5cDkSRJ2TC4paVQcQvRMW6SJCkbBreU5EKh4mZXqSRJyojBLS2he3KCXaWSJCkbBreUhFCYnOAYN0mSlBGDW0pCLukqxYqbJEnKiMEtJaF7r1IrbpIkKSMGt5R0V9yiOydIkqSMGNzS4hg3SZKUMYNbSkJhVqlj3CRJUlYMbinp6Sq14iZJkjJicEtJoLur1AV4JUlSNgxuackVvkq3vJIkSRkxuKUk5Lp3TrDiJkmSsmFwS0nOipskScqYwS0lIVhxkyRJ2TK4paSnq9SKmyRJyojBLSU9FTeXA5EkSRkxuKWke69SXA5EkiRlxOCWkt69Sq24SZKkbBjcUpJzr1JJkpQxg1taco5xkyRJ2TK4pSS4jpskScqYwS0lucKsUlzHTZIkZcTglpLeiptdpZIkKRsGt5S4jpskScqawS0lVtwkSVLWDG4pyeW6lwNxjJskScqGwS0lOWeVSpKkjBnc0tJdcXNWqSRJyojBLSXdy4EEK26SJCkjBreU5Nw5QZIkZczglpbCJvM4OUGSJGXE4JYSN5mXJElZM7ilpHcdNytukiQpGwa3lHSv4+bkBEmSlBWDW0pcgFeSJGUt0+AWQrg6hLAxhLCoz7GxIYR7QghLCrfVO7j23MI5S0II5/Y5fnQI4dkQwtIQwqUhhJDlZxioXM/kBMe4SZKkbGRdcbsWOG27YxcC98YYDwHuLTzeRghhLPAd4C3AscB3+gS8K4DPAYcUfrZ//aIIhYqbwU2SJGUl0+AWY5wHbNnu8BnAzwv3fw58qJ9L3wPcE2PcEmPcCtwDnBZCmAiMijE+FpM+yV/s4PrBF9xkXpIkZasYY9z2jzG+Uri/Hti/n3MmAav7PF5TODapcH/743sBu0olSVK2ijo5oVA1y2Q0fwjh/BDC/BDC/E2bNmXxFtu9YffOCU5OkCRJ2ShGcNtQ6PKkcLuxn3PWAlP6PJ5cOLa2cH/7468SY7wqxjg3xjh3/PjxqTR8p+wqlSRJGStGcLsV6J4lei7wh37OuQs4NYRQXZiUcCpwV6GLtT6EcFxhNumndnD94OvZZN7gJkmSspH1ciA3Ao8Ch4UQ1oQQPgNcDLw7hLAEeFfhMSGEuSGEnwLEGLcA/wo8Wfj5fuEYwN8BPwWWAsuAP2X5GQbMipskScpYaZYvHmP85A6eemc/584HPtvn8dXA1Ts4b2ZabUxNcJN5SZKULXdOSEsI5AmAFTdJkpQNg1uKIo5xkyRJ2TG4pShPzuVAJElSZgxuKYoEJydIkqTMGNxSlCdncJMkSZkxuKUoEhzjJkmSMmNwS1Ge4HIgkiQpMwa3FEW7SiVJUoYMbilyHTdJkpQlg1uqHOMmSZKyY3BLUVJxkyRJyobBLUWRnBU3SZKUGYNbimJwAV5JkpQdg1uKnJwgSZKyZHBLUdJV6jpukiQpGwa3FLlXqSRJypLBLUVueSVJkrJkcEtRDDnArlJJkpQNg1uKkoqbwU2SJGXD4JYi9yqVJElZMrilKIZAcDkQSZKUEYNbivLkCI5xkyRJGTG4pchZpZIkKUsGtxQls0oNbpIkKRsGt5Q5q1SSJGXF4JaiZFapwU2SJGXD4JaiGHLOKpUkSZkxuKXIBXglSVKWDG4pcnKCJEnKksEtRVbcJElSlgxuKYo4xk2SJGXH4JamgMFNkiRlxuCWokjOrlJJkpQZg1uKkskJBjdJkpQNg1uq3KtUkiRlx+CWohhy5Ky4SZKkjBjcUuSsUkmSlCWDW4picB03SZKUHYNbmtyrVJIkZcjglqpAcIybJEnKiMEtRZGAy4FIkqSsGNxSFIML8EqSpOwY3NIUcuQc4yZJkjJicEtRdIybJEnKkMEtTc4qlSRJGTK4pcgxbpIkKUsGtzQ5xk2SJGWoKMEthPCVEMKiEMJzIYSv9vP810MICws/i0IIXSGEsYXnVoYQni08N3/wW79zjnGTJElZKR3sNwwhzAQ+BxwLtAN3hhD+GGNc2n1OjPE/gf8snP8B4Gsxxi19XuaUGGPNIDZ7QGLIGdwkSVJmilFxmwE8HmNsjjF2Ag8AH9nJ+Z8EbhyUlu0pg5skScpQMYLbIuCEEMK4EMJw4HRgSn8nFp4/Dbi5z+EI3B1CWBBCOD/z1u6CiLNKJUlSdga9qzTGuDiEcAlwN9AELAS6dnD6B4CHt+smfXuMcW0IYQJwTwjhhRjjvO0vLIS68wGmTp2a6mfYoZAj56xSSZKUkaJMTogx/izGeHSM8URgK/DSDk49i+26SWOMawu3G4FbSMbK9fceV8UY58YY544fPz69xu9McAFeSZKUnWLNKp1QuJ1KMr7thn7OGQ2cBPyhz7GqEMLI7vvAqSRdr3uF6HIgkiQpQ4PeVVpwcwhhHNABfDHGWBtC+DxAjPHKwjkfBu6OMTb1uW5/4JYQAiRtvyHGeOcgtvs1WHGTJEnZKUpwizGe0M+xK7d7fC1w7XbHlgNHZdm2PRJy5AxukiQpIwPqKi0smDsqJH4WQngqhHBq1o0bclwORJIkZWigY9z+V4yxnmRMWTXwN8DFmbVqqHKTeUmSlKGBBrdQuD0d+GWM8bk+x9TNrlJJkpShgQa3BSGEu0mC212FmZ2WlrYXAjki0bXcJElSBgY6OeEzwBxgeYyxubDh+3nZNWuIKoxxy0cosR4pSZJSNtCK21uBFwvLdpwDfBOoy65ZQ1MkWcctb8VNkiRlYKDB7QqgOYRwFPCPwDLgF5m1aogKha5Sg5skScrCQINbZ0wGbp0B/E+M8TJgZHbNGppioavU3CZJkrIw0DFuDSGEfyZZBuSEEEIOKMuuWUNUnzFukiRJaRtoxe1MoI1kPbf1wGTgPzNr1RBlV6kkScrSgIJbIaxdD4wOIbwfaI0xOsZte8HJCZIkKTsD3fLqE8ATwMeBTwCPhxA+lmXDhqIYcpQEu0olSVI2BjrG7V+AY2KMGwFCCOOBPwM3ZdWwoSgZ+gcx79rEkiQpfQMd45brDm0Fm3fh2tePkKy6mze4SZKkDAy04nZnCOEu4MbC4zOBO7Jp0tDVXXHL57uK3BJJkrQvGlBwizF+PYTwUeD4wqGrYoy3ZNesoSl2B7doxU2SJKVvoBU3Yow3Azdn2JahzzFukiQpQzsNbiGEBqC/OZIBiDHGUZm0aoiyq1SSJGVpp8Etxui2VruiENxcD0SSJGXBmaFpyllxkyRJ2TG4pal7jJuTEyRJUgYMbikKhXXcohU3SZKUAYNbinonJzjGTZIkpc/glibXcZMkSRkyuKWpZ1apXaWSJCl9BrcU9XaVWnGTJEnpM7ilKde9c4IVN0mSlD6DW5qcnCBJkjJkcEuRy4FIkqQsGdzSFEqSW2eVSpKkDBjcUtRTcYt2lUqSpPQZ3FIUcknFzXXcJElSFgxuaXKMmyRJypDBLUWhMMYtuo6bJEnKgMEtRaF7HTe7SiVJUgYMbinq3jkhdtlVKkmS0mdwS1PPrFIrbpIkKX0GtxT1dJU6xk2SJGXA4Jam7q5S13GTJEkZMLilyMkJkiQpSwa3NLnllSRJypDBLUU5F+CVJEkZMrilqHvLK7tKJUlSFgxuKepZx81ZpZIkKQMGtzTlurtKDW6SJCl9BrcU9exV6nIgkiQpAwa3FOUKy4EQnZwgSZLSV5TgFkL4SghhUQjhuRDCV/t5/uQQQl0IYWHh59t9njsthPBiCGFpCOHCwW35zvXunGDFTZIkpa90sN8whDAT+BxwLNAO3BlC+GOMcel2pz4YY3z/dteWAJcB7wbWAE+GEG6NMT4/CE1/bT07J1hxkyRJ6StGxW0G8HiMsTnG2Ak8AHxkgNceCyyNMS6PMbYDvwLOyKidu6xnVqnLgUiSpAwUI7gtAk4IIYwLIQwHTgem9HPeW0MIT4cQ/hRCOLJwbBKwus85awrHXiWEcH4IYX4IYf6mTZvSbP8OdXeVBoObJEnKwKB3lcYYF4cQLgHuBpqAhcD2fYtPAQfFGBtDCKcDvwcO2cX3uQq4CmDu3LmDMujMipskScpSUSYnxBh/FmM8OsZ4IrAVeGm75+tjjI2F+3cAZSGE/YC1bFudm1w4tlcoKUmWA8l3OcZNkiSlr1izSicUbqeSjG+7YbvnDwgh2fgzhHAsSTs3A08Ch4QQpocQyoGzgFsHs+07U1leBkBrh8FNkiSlb9C7SgtuDiGMAzqAL8YYa0MInweIMV4JfAz4QgihE2gBzorJqradIYS/B+4CSoCrY4zPFecjvNrwikJwa+8ockskSdK+qCjBLcZ4Qj/Hruxz/3+A/9nBtXcAd2TXut1XVpp0lba0GdwkSVL63DkhTYXJCS1W3CRJUgYMbmkqBLfW9s4iN0SSJO2LDG5p6gluVtwkSVL6DG6pCgC0dlhxkyRJ6TO4pcmuUkmSlCGDW5pCoeJmcJMkSRkwuKWpUHHr6Ggnnx+UXbYkSdLriMEtTRUjARhGK01W3SRJUsoMbmmqHA3AKJqpbzW4SZKkdBnc0lRSRmfJcEaHJupbXBJEkiSly+CWsq6KUUnFzeAmSZJSZnBLWawYlVTc7CqVJEkpM7ilrXIMo2iizoqbJElKmcEtZSXDxzAq2FUqSZLSZ3BLWcnwakbTRH2rwU2SJKXL4Jay3LCk4mZXqSRJSpvBLW2VoxkZmmlubS92SyRJ0j7G4Ja2YWPIEelqaSh2SyRJ0j7G4Ja2wu4JsbW2yA2RJEn7GoNb2irHAJBrM7hJkqR0GdzSVqi45drsKpUkSekyuKWtENxK2+uL3BBJkrSvMbilbVjSVVrRWVfkhkiSpH2NwS1thYpbeWdjkRsiSZL2NQa3tJWPJE+OYflGOrvyxW6NJEnahxjc0pbL0V46gtE00dzRVezWSJKkfYjBLQNdpVVU0UpTW2exmyJJkvYhBrcMdJUNZ3gwuEmSpHQZ3DIQy6qooo2mNrtKJUlSegxuWSivsuImSZJSZ3DLQKgYQRWtNBrcJElSigxuGchVjGA4rTS321UqSZLSY3DLQEnFCKpCmxU3SZKUKoNbBkqHjWS4y4FIkqSUlRa7Afui0soqSmgzuEmSpFQZ3DIQykcQQqStpanYTZEkSfsQu0qzUF4FQGdrQ5EbIkmS9iUGtyyUjwCgq7WxyA2RJEn7EoNbFgoVt3ybwU2SJKXH4JYFg5skScqAwS0Lha7SvGPcJElSigxuWShU3Fqa6ovcEEmStC8xuGWhENxob6aupaO4bZEkSfsMg1sWCl2lVaGV1Vuai9wYSZK0rzC4ZaF8OADDaeXexRs56nt38/JmA5wkSdozBrcslA4jEqgKbfzysVXUtXSwvMYZppIkac8UJbiFEL4SQlgUQnguhPDVfp4/O4TwTAjh2RDCIyGEo/o8t7JwfGEIYf7gtnyAcjlCeRVjStupaWwDoKmtq8iNkiRJQ92g71UaQpgJfA44FmgH7gwh/DHGuLTPaSuAk2KMW0MI7wWuAt7S5/lTYow1g9bo3VFexXg6oTV56IbzkiRpTxWj4jYDeDzG2Bxj7AQeAD7S94QY4yMxxq2Fh48Bkwe5jXuuvIpx5ftzVGcAACAASURBVL0zShsMbpIkaQ8VI7gtAk4IIYwLIQwHTgem7OT8zwB/6vM4AneHEBaEEM7PsJ17ptBV2s2KmyRJ2lOD3lUaY1wcQrgEuBtoAhYC/Q4ACyGcQhLc3t7n8NtjjGtDCBOAe0IIL8QY5/Vz7fnA+QBTp05N+VMMQPkIJuW6+Ozbp/OLx1YZ3CRJ0h4ryuSEGOPPYoxHxxhPBLYCL21/TghhNvBT4IwY4+Y+164t3G4EbiEZK9ffe1wVY5wbY5w7fvz4LD7GzpVXMSK08c33H8HIilIaDW6SJGkPFWtW6YTC7VSS8W03bPf8VOB3wN/EGF/qc7wqhDCy+z5wKknX696najw0bkzuGtwkSVIKBr2rtODmEMI4oAP4YoyxNoTweYAY45XAt4FxwOUhBIDOGONcYH/glsKxUuCGGOOdxfgAr2nMVKhfC611fC7/Wx5uPbvYLZIkSUNcUYJbjPGEfo5d2ef+Z4HP9nPOcuCo7Y/vlUZPgZiH+VfzN63Xs7JhFnB8sVslSZKGMHdOyMqYwoSIJX8GILS7c4IkSdozBresdAe31Y8BENqbitgYSZK0LzC4ZWXUJAg5yCeTEko6rLhJkqQ9Y3DLSmk5jJzY+7DTipskSdozBrcsjeld+Lcs30JXPhaxMZIkaagzuGWpT3AbQSufv24Bl923tIgNkiRJQ5nBLUvV04BAR8lwqmjhnuc38PiKLcVulSRJGqIMblk69nw4+yZah+3PiNAKQF1LR5EbJUmShiqDW5aq9oND3kUsq6KKFgDqmtuL3ChJkjRUGdwGQ/kIqqy4SZKkPWRwGwwVIxhBb3DLO7tUkiTtBoPbIMhVjuzpKs1HaGzvLHKLJEnSUGRwGwQllSOpCq1UlZcAUNdsd6kkSdp1BrdBUFE1ipGhjQ/OmQQ4zk2SJO0eg9sgyFWMooI2zhuzkLnhBeoNbpIkaTcY3AZDxQgA3vDoBfxt6R+pNbhJkqTdYHAbDOVJcCvpaGJ0aLKrVJIk7RaD22AoVNwARmNwkyRJu6e02A14XSgf2XN3dGhi3kubuOPZV5hSPZzvfOAIJoyqLGLjJEnSUGHFbTD0qbiNCU08smwzi9bWcfuzr/DnxRuL2DBJkjSUGNwGQ3lvcKuknXI6OO7gcVSVl/Di+voiNkySJA0ldpUOhvKqbR6OponjDh5HS0cXL25oKFKjJEnSUGPFbTBUFMa4VY0HYFRo4i3Tx3LY/iN5cX0DMbp3qSRJem0Gt8EwbCxMmgszPwbAfiXNHDVlDIfuP5KtzR28tKGRmsa2IjdSkiTt7Qxug6GkFD53L8z6OABfOX48lWUlHHZAUon74P88xNd+vbCYLZQkSUOAwW0wDRsDwNsOTDabP3T/JLi1deZZurGxaM2SJElDg8FtMFUmwY2WWgD2G1HOOw6fwOEHjGR9fSvtnfkiNk6SJO3tDG6DqXJ0cttaC12dhBC4+tPH8Jm3TydGWFfbUtz2SZKkvZrBbTCVlCa7KDx9I1w0Hn5xBjRtZsrY4QCs2WpwkyRJO2ZwG2zDxsDWlVBSAcvvh+d/3xPcVm9tLmrTJEnS3s3gNti6x7kd+h6oGA0bnuOAUZWU5gJrDG6SJGkn3DlhsBVmlnLwSdC4ATY+T0kucOCYYazeYlepJEnaMStug617gsL0k2DCEbDheYiRydXD7CqVJEk7ZXAbbGOnw7hDYOzBsP+R0FYH6/7K4SPbrLhJkqSdMrgNtnd8G86/D0JIghvAz07lMxsuoqaxjY0NrcVtnyRJ2msZ3AZbaXnvpvMTZiS3+Q72b34JiCxYubVoTZMkSXs3g1sxVY6Gd3wLjvwIpW21TCxtZP4qg5skSeqfwa3YTvwneNM5AJw6oe5Vwe3u59Zz+n896HZYkiTJ4LZXGH8YAMePquG5tXW0tHfx+PLNrKhp4ldPrub5V+qLuh3Wsk2NPPDSpqK9vyRJSriO295g1CQoH8ER5a/QmZ/D4ys28/c3/JUpY4ezfFMjkOxjOm2/qqI074r7lzHvpU088S/vKsr7S5KkhBW3vUEIsN+hTGxfRWkucPl9y2hs62TxK/W0FbpI1/apuHV05Wls6xy05m1paqehdfDeT5Ik9c/gtrcYfxglm5fw5qnVPLFyCyHAhJEVjKgoJQRYV5ssE/L06lpO/dE8PvjfDw1a07Y0tdPS0UVXPg7ae0qSpFczuO0txh8GDa9wyrRKAGZNGs1lZ7+Z//OJoxg/ooK1tc105SNfuG4BK2qaWF7TRNMAq24LV9fymWufpKNr9yY41Da3A9DS0bVb10uSpHQY3PYW+yUTFE4ZtwWA49+4H8dMG8t7jjyAA8cMY11tKw8trWFdXSvvnXkAAGu2DmzCwk0LVnPvCxtZO8Dzt3dE0xP8XcnvBxwUJUlSNgxue4vCzNJDS9bxtXcdyt8cd1DPU5PGDGNdbQu/mb+a6uFlnHf8dABWb0n2Nv3Foyv568s7Xv/tyRXJczWNbbvcrK585NSu+/lM6Z8MbpIkFZnBbW8x5iAoqSBX8xJfedchHDhmWHJ80e+46OVz2LC1gXue28AZcyZx8Phkdunqrc10duX5/m3P883fLyLGV49Bq21u58UNDQBsatj14FbX0sEYmhhBK83tdpVKklRMBre9RUkpjHsjbHpx2+PP/JrqtrVU5zfTmc9z3vHTGFdVzrCyElZvaeGVulY685Hn1tXzZD/bZS3os6Dvpt2ouG1pamdMaKQidNDUUry15CRJUpGCWwjhKyGERSGE50IIX+3n+RBCuDSEsDSE8EwI4c19njs3hLCk8HPu4LY8Y+MPhZpCcHvw/8Kim2HFPAD2o44z5kzioHFVBOCg6nJWb23m5UJ3KcDPH125zct1duW59el1lJUEcmH3Km61ze2MJllLrq25fnc+lSRJSsmgL8AbQpgJfA44FmgH7gwh/DHGuLTPae8FDin8vAW4AnhLCGEs8B1gLhCBBSGEW2OM+8YGn+MPh+d+D631cP+/Q74TYjIT9O0HdPGxdx6SnPfHr3FV6zzO3/JjVm1Ogtvcg6p5enXtNi/35V/9lTueXc+X5lZR8vzvWF8/ecBNyecj3/rDIsZVlXNeaAKgrakhhQ8pSZJ2VzEqbjOAx2OMzTHGTuAB4CPbnXMG8IuYeAwYE0KYCLwHuCfGuKUQ1u4BThvMxmdq/GFAhOduga72ntAG8E/HVyc7Jzx/Kyy4hqnty9iwtZ5VW5ooKwm85eCxrKtt6dnTNJ+P/OWFjZx1zBT+ccTdfDX/czpr1wy4KZub2rn+8Ze59pHljCYJbh0tBjdJkoqpGMFtEXBCCGFcCGE4cDowZbtzJgGr+zxeUzi2o+P7hqlvTW4fuzy5PfZv4W1fSu43boQY4c/fhVACwMj2jTy9upYp1cOZNq6KfOzdYWF9fSutHXlmTR4NS+4CoKtx4PuN9nSrttaTC8mkh44Wu0olSSqmQQ9uMcbFwCXA3cCdwEIg9emKIYTzQwjzQwjzN20aIhukjzwgWc9t0wtQPhJOuxhOvQiGVUPTRqh5CbYsgyM+CMCkUMNjy7cwZexwDhqXzDTtHvO2fFNSJZtRvhE2F3qhmwb+PWxsSHZqGFPoJgXItzbu8UeUJEm7ryiTE2KMP4sxHh1jPBHYCry03Slr2bYKN7lwbEfH+3uPq2KMc2OMc8ePH59e47N28EnJ7cTZkCv88YzYHxo3wIt3JI+P+SwAc0YlXZd/2/JTDt16PwAvb06C1vKaJGQdUvdIz0uXtGx51ZIhMUZ++uByLv7TCyx+pZ4YIx1d+Z6K2xh6w1pXq12lkiQVU7FmlU4o3E4lGd92w3an3Ap8qjC79DigLsb4CnAXcGoIoTqEUA2cWji275h+YnI78ajeY1XjoXETvPgnmDgHJh8DwDsOaKeMTo7b9FtG3/cNRpZ29kxWWL6piRHlgRHP/ypZIw4YE+uoa+nY5u2WbmzkotsXc+UDy7jsvqX8dv4ajvu3e3u6XMeE3uAW26y4SZJUTMVax+3mEMLzwG3AF2OMtSGEz4cQPl94/g5gObAU+AnwdwAxxi3AvwJPFn6+Xzi275h+Iow7BA59T++xERNg8xJY/QQcehqUVsCIA5g1soF3T+okR57Q8Aqfr3qQVd1dpTVNnD3qacLG5+Ed36IrV8a4UP+qJUFW1CQVuv1GVLBkQyOPLt/M5qZ25q/cylFla5iY6zNht/21g9vqLc0sWlu359+DJEl6lUFfDgQgxnhCP8eu7HM/Al/cwbVXA1dn17oiqxwNX5q/7bER+0Pz5uT+G9+Z3I6eTGXTWi5/337wC6B8BO+P9/OHzR+grqWD5Zsa+X78QxICZ36Ezju/ybj2er79h+f41vuP4IgDRwH0VOjefcT+3LRgNZ35ZFbq86te4bGSb9CQGw2Fya2ho4nXctHtz7NkQyN/+aeT9/SbkCRJ23HnhKGgqjBGr2IUHFhYi3j0ZKhbA7WrksfTTuCAzrW8tKGBo753N2u3NjG5fXlSucuVUDpyAoePauO5dXX88++e6RnrtnJzE2OGl3Hs9Go6uiLLCpMaRnRuoZxOxuU39zQjN4DgtmRDI2trW/rdfmtPbaxv5ZFlNam/riRJQ4XBbSgYsX9yO+2EZGss6A1uW1cly4NMO56KfDNfeks1F5x2OH8/dySl+TaongZAyYj9mF3dwQXvPZyn19Tx8Ssf5X2XPsiKmiamjavikAkjAXhTWMIlpVcxnj6L+ZaPpDVUUvoawa29M8+qLc20deapb+ncrZ0adubKB5bzv659co9C4ZINDVz78IoUW7WLujohn3/t8yRJ6ofBbSgYMSG5Pfjk3mNjpkJnK6xdAKMmJV2iwD/OLeMLJ7+Bf5xblpw3dnpyWzUemjbx0TdPZsLICuav2spz6+pZuGID32j7Lw7dcAe5EDm95HHOLL2fGbmXe99rWDVtueGUdPVur9Wfl7c00ZVPQtUDSzZx7L/9mQWrdjwEcdHaOp5cOfAhiltqXuGNnUtZX9/Kcf92Lw8v3fXq2y1PLOVnf7yPjq4ihaer3wP3/aA47y1JGvIMbkPBlLfAnHNgZp8NJrpnna54IAlxYw9OHm8pVJO2Fm67jw/fD5o2U1lWwnWffQs3fu44cgEOji9zbP1dlN/6eb4+8h6m5zYCMDP0qUoNr6Y9N4zyHQS3GCO/eHQljy7r7Va9/4WNxAjPrqmjoyvfb1D6l1ue5Ss3/nXAFbQTNlzHjeU/YOGqrayvb2Xhdlt8DcTM1Tdwe/k3qGtuS6pfGXTp7kzTKy/y8osLBvU9JUn7DoPbUFA5Cj50GVTt13ts8rEwcmKyLdaYqVB9EBB6A9uWFUkX6ujCsndV+0FHE8y/hkPLNvHWN4zj6IOqmRIKi/KWDeedZYs4tDypYs3OFV6npAKGVdNROpzyrpaet1+1ualnRupz65JJDz+4YzFvCGs5OrzI4yuSStrymiY+8/P5fO3XC7f5SM3tnSxaV8+6utaeRYNfy9jW1YwMLaxYt57PlNxO85ZXduFLTAxrWc+o0ELNulXU/+tUnr33+l1+jd0WI+X5ZlrrN7/2uZIk9cPgNlTlcjAj2UGB6oOSJUJGTdq24jZmCpQUuky7Q98fvwoP/CcA7zh8f6aGpMLGG97BIWE1UwqPDwlr6KiohpkfhYPeTmdpFeX5JLjFGPlf1z7JWVc9Skt7F48tT4JIa0ee71bcyI/LLu9ZB+6lDQ08tnwzdz+/gaa2zp7mL1xdy5S4jkPDah5dtpnGtk4+cvnDPLik/90dWju6mJBP2tb58pN8q+x6Dlp/Z8/zHV15rnl4Rc9erTtS3p5U6TYtXcAommha9dednp+m2NlKGV1Udrp1mCRp9xjchrIjP5zcdneHjp0OW5Yn97esgOrpvedW9dk9YtlfYO1TfLrsz5xxUDtx2FiY8hZC4wZCR1L9KgtdlIzcHz58BZz0dfKlwxlOC9+77Tl+/eRqlm1qYkN9Gz97aDmPLNvMqMpk0sSRuVVMzG1mOK18rfS3PLfyFdo787R35pn3UhLKYozMX7mVS8p+whUV/80jyzbzqyde5qmXa3lwSVLxa+3o4okVvePf1tW2MCkkz5VtfhGAkubekPfQkhq+d9vz3Pfixp1+ZZWdyRpzHRuTzTpyzYO3HVpbUxLYqrpSCG7P/6EngEuSXj+Kso6bUnLQW+Fvfg8HHZ88Hjsdnr0Zfv93ULMEZn+899yqwgSHilHQuB5uOJNhTZs44sA3JRW7CTNe9fK5Eb1hr6u0iipauebhlQCMqizl6IOquey+ZQB86fB66kr3Y+zipPr2kZIH+UrpLSxtn8RtvI3ykhx3P7+B0cPK+ML1T1ESOzg/t5xyOpj/0ss8viLZa3VVYcuu/3P3i/zkwRXc87UTOWT/kWysqeHgwr6pY5uXQwlUtvZOTli5uYkZYRXLNh4KRx6ww69seFeybVfJ1qTd5S2DF9yaG+uoBEbEhmRsXQi7/2LP/jZZkPmkr6fWPknS3s+K21D3hlOgtDy5f/DJUDYs2dO0vSHZsL7bpDfDp/4Af/do8rhpIxBh3VPJkiHjD+89tzvkdc9mBXKVI6kKrZz71oMYWVnK2ccdxCUfm824EeWM6tjEF5aczz+3/N+e89+eWwTAtLCeiyuv5cqJt3HLX9fyqaufoKq8hCkdK6mknRyRWSUr2VDfxvuHP0/Npo1sbWrn+seTWa13P78BgNpXlve89qFhddK8zs09Exvyqx7jTxX/TFz58A6/qhhjEpqA0U0rARjWMXgbb7Q2JdW+CjrIt732mnj87m9h0e/6f655K7Ts+uQMSdLQZnDbl8z8KPzvZfBPS+DTt8PR5/Y+F0IS7EZPhv1nJd2rucL4tzEHJcfLRwIBpr4lOV7VG9ymThzPhIpOvnfGTB7/xjv5+kHLmLD6bn75mbfwtdkdBCKsfLDn/LflngPg4NwrfCA8wikNt/KN97yB02dN5Pd/fzw3nF7ec+7lp5Tw0NlV/E/+It5a+0d++dgqKtq38ukRj3P3omQCQltN7yzXN4a1AIyLtSxYtZX5K7dQXpMExYrCbX/qWzsZTbJt18TONQCM6tz94NbQ0s61V17C04tfHND5rY29W4E11tZQ39LGDT/5T2ob+5mcEWNSVVt2b7+vtWXzBuhqg46Wfp/f5yz5MzQ5qWNAVj0K868pdiu0L9iyAu75jmtP7mUMbvuikjKY9vak+tafT94A5/4RDnxT8rh6WhLsxh8Gow7sHTPXp6u0pGIEJR1NECPDSyD3x6/CbV9h+pgyzprWJ3gMHwfAqJAcOz73HFWxkdDWwPlT1nDpJ9/EhJGVVG1amJw78kBK1i9k8rPJjmcH5Nfz2ydW8stRV/Ddzv8it24Br9S1ELeu7nmLkSEJK+NDLV+4/im+cP1TVDUkwW504/IdLi9S29DEqMK1+4dkD9Yx+drdWhIkxsjdP/0mn17/b9T/5UcDuqa9uXdsW8PWDSx59Hb+v7UXsfyJP7365NY6iF201e+gK7elsIfs66HqVrcGrv8ozH+Nne5a65JfMp3pLvycmqYa+NFMWJPxcjBPXAX3fDvb99Drw7M3wcM/7l2tQHsFg9vr0ZipMHpSMkYOCkuJAG/7EpzwD8nsVNh2QkP5CIhdyaK/y+9LulpbtiQVoU2LoWJ0UsGbdDRtFeN6LpsQ+gSLF25P1k57+FJYchdMmgsHzoEld8NLSXiZFGo4tfH3zGx/mkjgjJJH+H8PLCdf+zIdlFKXG9PzcuOoZ3NDC5sa2tivNelanRZXs76+lQeXbOK2p9cBUN/aAUBD7asX7B0W2mlq2LrLX+GTTzzEGTU/AUgC7QB0tDT03G+qraF9/WIAOhv6mVDRklQCN6xf1+9rdU9wyNethVu/BA/9GJr3sNt34Y2waQDVwz0Ji/k81O/iMi7L709u69fCyofgkf/e8XkP/xhefmz325elNU9C3WpYO/+1z90T9eugrR7aGrN9n2JaMQ9+9p70Ks6DvJ7jkNE92a1h15deUnYMbq9nM85IZp4eMDt5fOSH4JjPJlU32KarlPIRye2qh2HBtVA5JqmYPfPr5Jf9xNlwxmVw4tepGFcIgsMLS5CEHBxyajJe645/gnu+lVx/9Lkw/URorYfD3kfLpLdxYNjMSbmnaao+gjDjA3ys8gl++cgyyhvX0jL8QJrKqnuaVBryfL/0Gv6r7H84OJf8xXJoWMOFNz3DVdf8lMU3/4C1tS0c/a/38NCSGprr+q9e1W5cs8tfXcMTNxBDYFNuPBUtGwZ0TWdLb8WtraGGki1LAehqfHWgbKtPjlW09xMqO1qooB2A9hfvgad+AX/+Djx+5avPrV2dfL+tdTsPNO3N8Ie/2/Y12hrhJ++E7nGDbQ1w82fhkoPg5ce3vT7GZLbya1W7Fl4H/3VUUn0aqO7g1rghqbrd/U1Y30+XeHcVsn7twF97MG1Ihg9k/kuwvhD2Gwf23+VepXET/Ochr12VXPUIrH4MVj++8/MG4pnfwP89IvkHUFfHnr/evmRLMolrl/+xpUwZ3F7PJh8NX1m47cK+kHSzzjkbph7Xe2zSm5MxcNd9FF74I8z6OMz8GLxwR/ILafzhcNSZMOXYZP04SDa4Bxj3Rjj1B0mAW3ANzPoEfGk+HP4+OPZ8uHAVfPIGyg6cyaRQw6G5tVROng2zP8GIzq18ZMRzvG3YKkYeeBjt5Ulw68hVAHBmyf2cUfIIk0MNrRXjGB2aqVk6n1+WX8z/zl3HE8tr6OiKPLeu7lUL37bHEgBqN62lpb1rwF9ba3snh9Tcy/IRc9kw7A2M6tiuYrZ2Qb9jQrpaeytu7Q01jGgsdD+09KmUFa5rKoTM4Z294+J6tPSGueZ1i3vux8Z+gum1p8N9/wZP/hSuOX3H1bKNi5PFnGv7bHW26pGkOrT4tuTx41cm4+4ISdW1r+X3wy8/nAT5nVlyTzI2b+PinZ/XLZ/vDW4N66GuEMr6q7p1fy91hSD+8mNJV8+uiDEJqllUYHqC2/pt3y9N+Tw0FIJbff/V2r3apsVJNf+V11hfsanw3/qKB3d+3mtZswB+9zkoq0z+AXRHkWdpr5gHW1cWtw19bS4Et4Yh+N/SPszgplcbVg0fuhyG9XZLMnku/MPz8NGfwdk3wbu/D2/9IhChozkZH9ete7eGQ09Lbg+YBeMPhU/9Ho4+D07vs/5YrgQqRwNQWn0QI0IrB4QtlEw4DA55D4yaxH9UXM3Y9lcIR51FZ0US3BpHJ+9XFnoDV+chpwNw87greo49s/RlvlByK1tqNtLWkAS3+lzyfutKki7hh/98C9/88RU9+6wCXPvwCn7zZO+4ur4WPDGPqWEDuSM/RPvwiYzrqukdV7fhOfjJO5Jwu518n+DW1bSF8W1JSMoVAkfjXRfR9uM3Q1sDTbXJL6aq2Aj5bUNle0Nvtapr4wsAbI4jWb1uu0pTV0cSxLYsT8JM7Or9F/T2Njyb3Nb2+cwrHkhu1z2V3K56hM79ZrC+8mA6Vm1X6XjiJ4Vzt90hYxv5fFKxBah5acfn9bXifmjaRGuopK12XW8oW3TTq7uGu0NpXeEzPPTj/n8Rb3wBFt2chKb51yTVyG7P/CYJu4tvHVj7diSfT/6R82LvItFsfD657Q5u869Jxrx1tvee09706s/15M+SUDEQTZsg37nt+7yWlx+H35ybDGPo1tWRVGoHy7L74KEf9ba5cefrMfYEt5V7GNxe+lPyD8rP3ptM7nrh9uJ2m/72PLj/4mzfI8Yd/9nGCI9eVqjU10Fz4e+aoVRxq18H8344sD/HF+6A/3fStv8PDgEGNw1c5SiY9TE45N1QPjwZG3fs+clzfdeBm/KWZDuuN5ySVNve8M7k+MSj4AM/3jYQ9tVdqYOkgldaDm/9e0LTxqTbdsYHiYXu1/b9jug5tbN8JADDj0oWJK5sWkfjxGT8Xvmyu7ig7Fcc+MrddBVmJW6tTN5nS9UbADi/69d8v+n7zHs2Gc9R29zOkjsvZ9EdV/S7x2pYcC0dlDD97R+HUQcyNjSwuS75i7D+5SQA1Sx98tWfr72J9lhCSyyntGEt42PSnrL2ZIJE6/zrqahfQdvd36e90FWaIyaBpE8XTsPW3l9qo5tW0h5LeDnuT0fjdr/wC7/88o0boKHQbbZ5Of0qVINi7cs9f+F1LZ+X3K57GjrbiKuf5P6WN/CXxoOIa+b3VhVrX+4Zo8iGHc/qZdML0FyoetYs2fF53WqWwG8/TdvoN/DrjhMobdmUdDNOnJOEk00vbHN67Km4FQJs3Zqkmrl9EHr8yt61Dv/41WRsX7cFhdmYrzUR4rU0roelf4aHCkvkdLb1fubucPLyo1C/BtY80Xvdnf8M175/u/b+P7jv31/7F1G+a9tu4oFWSR67/P9n7zyj4yjPNny9WyWtdrXqvUuWLPeKcTfGGGNMDc2BhBogkACBEAgJJKSQkMKXAgkloYXeTa827r13S7LVrd7rlvl+PLO7kmyDCQHHzlzn6EianZ2dtvPecz9lYOfr4nYF+PR+uD8H3r/r6xEyG5+UbQys/+cKN11QVG34crl8+z6UxwdGxEDGyeL2NRZLOsiah4/sUFdugPqjvPk4Wnxe+X4cTZ7pl6H4Y/hdXuia0J+Wcnj/x+KcN/W7VhxPjtumf8Envxi4/kdi8zNQsxkaj+J69F+EIdwMvhwz74QFf5KLXoCis+DW3WB3wvc2wJhvHt2yovoLN93BG/stKZaYdD1YbETFSXPdyPRR8rolHMusH0OYG1PWZFjwZ7jqQ/zTfwRAZqcIKXt7BZo+gHc4MgDwxeTh1eQr4FC9FC95GoDX1xVzp+kpfur/O5vXLh2wiv6DOzip+U1WuM/G4ozHGpMGQFPFHuhupvaAuCoduoCr27uGxX/9Lr/601/o62qji3DalJO4li3BZdr6WvDV7SbOU02lFodtw6OYZZHGAQAAIABJREFUG0KDqG/Jb+RCWyY9+Dr7FVnYtF4aiKLT7MLqGRRW7RCB0NlQRUejDIj+xuKB89TuhJevRNPz35S3WwaPriZMtdvY50/F7OuBHa+h+tp5qzmDTVoeNk9b6GK370MJs+bNkdyzI7UO0N22Bi0KT91RDE6bnoa+Lt4d8yClWgpmzQeajzfa8uX1xoHuYVODDPa9TXq4N+C8DRaJnfVSZBNwHwNOWN1uEVPuTAnPNh7BnTwaAs5lxRpoKJbBWPPRrNz4AiHMwHqVfBJ6X8VaWZ9ArqCmiQBtr/5sl7J2J/wuVwRXgIBA/PheKeo4HJ5uOX4A1f3CkzWbwe+BVX8dKMaby2DL8wOX0VF/6D5+4wZ4+7Yjr+9gWislhB5wbAPCrb328OHQznqJDPi9UvTx79BRJ9uZf6r8nzpOX/cb4c2b4N3bRbwNpmk/PHkmvH/nv/e5R6KnBdDkvPsqxXLdDtnXg68FMPDmIiB8HPEDHTdPz+cLa5B0hsHnCsgNxlfZXuSgHj34vBxPbx+U6lGFo03d+C/BEG4GXw57JIy7XEKeX5aAcDPbZfAMLP/mbTD1FgASEqRwwpGUJwUTaeNF1P1gl7Q/GfdtSBuHM14E1ViTXIiieqtQPS34MNETKcLN5EykWUVRrRJoishibONbvL6pirKVrxKpevApE6lLbgmF0jSNrjduo10Lp/2kW2U94mRZUW9eRcufp+NtkMHe0bYP/D76XrqWWQ3PcFfzT4hv2Uy3CqfT5CTDIxfFCi2eCG8rteteB+AB34UoNNz1ocrDzq1vQk8L3ifPgZYKega1CGnCjccWdcgzUD2tcrGN8DTi18VCT+2gAXbzM7D9FdTBrdRoMTLtwHJ4fiEKjYe8+vNwVz8EQJVzJL6UCTKtQneKmkrRzGH8rnwIeDoHtg7w++C9H4uwKF1CszWR5f5h+I/GVTi4DRIKWVYXQZ0WcmkXNWfgV9ZDwr4+XZib26rEhenR3ZLBgicQZgsKNv2iveM1CZtd/CwoswjHo6W3fWC4pX+u4OZngo7mYu9wzL2tIpgCA2eJni/o6dHXVQsNmt3Nsk/7zzeYjjp47iKZV3c+WzWHuJO9HbDsD7D6b4d/b8ni0PL7h7mby0Lfx8DzjzUNXrtWfir6iaWP7oFn+j2lxeeFHa9LvtbREhC6gQKazjpxSp+YLyKpZsvA+TvrIXuG/H20g67PA69cE/qM4o/kd94c+Z04XK49FaulVVLqeGlmvnNRKKdS06SIwdMl++jfwdsHzy2UMF1/Am50b2voHP0qCDhthyuSCdyMtdeE3PnMyQPnXf4A/H3q54vLtY9ImH8wr1wNL172xdf7s+j/3QvcaHxeqkDlWmlUD6FrwXGCIdwM/ntwxIElXMKr5n5PYzOZQ4+HCvSWcybBlJskz04pCd32QznFmRuiJCcq2X8QT0cTnSoS5ZB2JTZnHGvzb2Hfyb/DOfkqxpn28eCLbzG5ezG9YQm8mHMf8T1l7H1gHs3Nzbz11O+JrFnJ77wXMWm4uD7RSSIwE/vKcXeXE98ogiu2r4q2ZX8nzbOfD2MWApDjL6fHFE6EkotMg+Ziv30oDn8b/r0fsN2fReEkydNzdx0Ibourt4Y6zY3F30P51k+DIdF6TXL12iwxeKwuHL5QDh1Ae71suxk/rj65Q/bVD7rLPrBMxArwsU/v6/fWzVCzlRdTbuc1/1QRATVbqNViyBtSRHzWMDq0MHw1W2X+plLqbSksaderkQN3vIG/Vz8IS+6DksWsMo+j2J+KvaNKKlk/i4PbIWkkmyqaBwi3Ci2BGlPiIY6YqVcEtsXXNdAlOkS4iWOp6QO+VrdLBqHqjWjxhWz3paPlngJbXxJnYHD+yzs/hHd/FPpf0+CRWfDRz0LTWnXhljJGcgXrduBVNtb6C0P7pbcNIhPF6epqggZx5YCQqGvtV/E8uCAE5H1PnS3blDUNAA8WdmkZeFurQ+K2bIVsS9VGqRQOOCi735Ic0/RJ4j4FtqelPCSMAsnyO14TR1KZ4NPfhsL3DXtl/kCOXN0O6OvQ8yqPwjny9gbd4dDvWnjvDmgpk1ZD/fetzwPdzaztTEQLj5b9djQUfwzbXpRQoKZJPpsrVVI4QFIzAn+PuUyKp2o2w2vXSSXz1hdFUB1YBlbH4bdv87MilAdP93klfxFE0Ox5W373p6tf8dRnpRJ01MNfxg8Uz1+EgAg7nHALfFfaa6CpBF9kCtu6Y9Haa0IuWd0OOT79iqRY9ZDsowCaJgJxsKvn80r7p4BL2tcJD08POV8B/H5JGdj64mdvi7cX3r0D7kuV3Lze9tCNRn/Hraf1UKFd/BGYLBCVYThuBgb/NkpB4jCpdj0SBfPh3Eek4GHG7VAw7/Dz2Z10qwhMSi6gGaoOrasJrz2KcJfkyTljEpn/zZuYcdo5WMcsRDNZ+Y37dWZbtmAfcyEXXHwli4f/mtyendT9aRazS+9nnb+A4rRvEO+UqlZ3YuaAj43zHqRZi8SERsSnP2eDP5/8c38MSCFFrymChkgRfT913I2KSsalteFu30tFRBFjhw+lV5MnWtT2EyvrbBPwa4rm8h34Opvo1azU6g5Zly0Or92NQ+uUxFy9VUd388ALs1czYWvr54Z1t0DNVjj5RlalX8MjPj23qrsZhp3LE93TUUpxo+d7bB72I77TdzNT8uMZkR5NsZZKd5WII19jCdu6YinWUvFhGuiOBIoRdi0CTyevdYygRNMF3mcNuB110FnH23WxlNZ30mYJ9QbsCktkV188fXXFcjHWB0RrXysdWpjMFGx9okKDoKcHNA1NdzP6qkRgqr52GYSrN1HjGMqZf1nO1th5kn/24AQZWPw+cRi9vTI473g9tK7N+8Wp6B+yaymXdjgZk8Vtq9lKtS2Dak3fjoAbNeYyQIPK9QNbnATWOSDcUsdLSHVwG5a1j0iu3yXPwUnXAlBPDDVaDP7WmtByuptFzC76vlQKb3xKBsd9H0irnrTx8vk+j+x7b7e0+AmPFuG2f6mEEJNGwswfQ/GH8It42Q/NZSI4AwNlYB09nQMH9yNxuPYtHXWSGlB4pjyPt+QT2Y8735B1Bt4o9tAdlffZuWaLvg/P66kaW/WwXdUG2PueDNyFZw58ZnDmySLKhp8PBXIThbdb3Li3fiDfF5DK+0BaQQC/Dz74qYSmBzdAfuc2EVtlKyWcrczibPcvEuifi3m4MGaAitVyvi3v1/i7fA38Mgnuy5C2S5omBUPPXiTn/eZnQy5w4DgdruBgUKj0oCWFl/b6UH5vqFAhIIAC56bfDyv+BGseCYX4e1okHDs4x/TgFhH1HbWy7eWr5Hox+Akx7TUikN+/KyR4+/PWD+DJBeIkr/mb5EAv+a2eEqCFlgGw9wP48xh4eFro5kLT5EYkc4qMN0fjuDWVSv7cfwGGcDP47+Ky12De/Ud+3RombUeO4gHt7bZQA+EY1UG+qsTqTKBg3Cw64seSVXRSaGZHHKrwDMZ1r8RkDYfJ3yfcZua0C65jw5hfkq8doMI5iuE3v8GTV4fapKgwFx1E0IONHl1wLVUiPC3+Xt5O+i5Z6em0I46gxxzB0O88Qcu1m/nb7degwmMJp49IrQN/bD5ZcU7KNOmftz8gcACVNJJq4lAN+6C7iWYi6bFIUYYnLA4tTO9v9+E9Elrq7aCvZWBC8XYtG7unLXQhLVsJaDBkLo9bL6ZcS6RNk/X0FsynuK6DqXlxLPOP5EeVU9mi5TE5N46RqW7JfWvcKxftpv3s8yUSHx3FbvOQgc7QgRUSfgL85jCWeoey1Z+DV1nhhcskhApyId3yPDw0WUSX7to9fcAFwLjhUvzSpoVz0tAcDmhJmFtK4G+T4fXrAbB7WtmtSeg6INxKrENkoH50NvwqCVY/hOqVwdLa0s+xK/4IOutZ0yPv/2NFnrS/aSyWpP13fgj/mCO5T30d4gwF2q8E+tzV7wk5LS0VNFkTaXQVSj7dgeXs9KVTq+nHKVANOfw8+X1wqwgTS7jkFAXcRH1wbDr1DxK6fO4iOX4bnhTHpWqDFPLkzISsqWjKRJU/mlotGnPnwYEC4LXrpHo4MlEGoOpN0FnPJvsE7llnlYG2Zou4XCDpCtFZsowXvy3FQ998CSbfCHN/DTaH5Md16vlOgdy98lWhz+wfMn7zJhEQ/elpGzgP0IdV9llrOSQUQc6s0P5982b5QRzrKnP6kW8AGktEoO5+S7Zr9zuSMxuZBC9fCd4eevLP4Cevb+PNLdV8sruW0mE3yvOcw92SZ5syVgqwZt4pYbWdr8myc085dPvKVoq4icmV0Gr/5sAln0ie4uPzJIJw5h8lh/DZCyXsWLtzoAgsXyUO4eEIfGf2vittfspWwdqHxTGMzpT9/OK3RCzufQ+W/Fq+I4FweSCEeLiCg/7CrWEf1eaU4A0iyx8QNyuwzQHBXbNZvg++XnF0YWDhQ39nvGxl6O+mktB3p2GQUA04xZ11UmUbcPs66uXmYtvLcjPxyS/l3P/mS3J83pKUGixhoXX45F65gehpDS23fJXckIxeKOdY84HDC8T+uXjL/iC5m0eT3/cVYwg3g/8uwlxHflTXF6QvXARQd3gyAENNFTgKZmGKzSbyhsXgiB34hnFXyO9TfgrOxODkiefcSPmVm8i95X3CY5IJsw7K50soxJM3j2JLHgDtaTPp0MJ4xzeRmacuAKDRIqFbr8WB2RGDOzkbAOWICS7GllhAjMNGlUnWt8OeSLcmz3S1JxVQZ88ksqMUU08LHcpJry7cNEcCpggRBFrVBvD1QfkqVEctbVpoX64JhOkCF8/9n+I32znz1R42lDVjM5uo0uLwWhz8dm8yfT4/C0alYDUr9tS2MzUvjhiHjfSYcMrMGYT3NkDtdsz+PqpIYv6IZN7pHS2CoK1aLnrlK6USOSqD6rgp9GLD60rjp9G/l1BhoOpyy/OSO1W3Q8IjgXBr4nDW3XUqc0Zm0qpFUKPFMrMgXoSbr1dE1M43YP9SwvxdbPdn6du4Ah8mPuopkIt/W7UUy+x9P7g/TH5PKK9Pv5N+vS6RMKuJT0s7qJz7GFzxLoTHwHo9V2dLvwrUQAuVgKvY2xocFH3NZaxucvB4iQhPNB/ru1NCId8Dy/Gb7dy1wosWna0Lt23UhmWz25ssYnHfR9BSRh9Wrn+/E875mwxAuxbJALX4V5KXljxalhkeTU/O6az1F1CnRWP2y3lAVAa40mTfjrlMRFdrObz3I0Dxz4M5vNUxBI89Wgb4g+Iq/WJFB6W+eLSyFeKcTP+hpChYwyVFIaEo6H4BMpD7fSKao+X8DroyPa0iNgOORUuFhJd/kx50p+qRfbPT369IKaFQxCOISO1uCgrFBi2K7X1JInj6P8c2ENpe+Wd5/B8KnrlQhMX4q2Dh8xIejkzkg/Zs/rW6nO89t4krn1jPra/vCz1JRim45hM4/TfiPoL0NLSEh57n3Foh4TmQ42IJC7VJCmx7W7WI4YIzIO9UuPojGH2pNCEvXyVi6B+nBcOUvqhMOc/+dZ6c2yACdNWDIi5qt+vNzRW8favMt+stGHkxXPS03Dzs+xBm3SXHfcWfZBkBJzwg3AY7bj1tIsCcKeKgdjex35/IAU2/Fq5+SIRSIHc0sH173wumWwS/C4GQNwy8eShbKfsIRNAF5g/k1pUslhBpIHSZNU2O43MXi+P1x0IRT72tYHeJ+J12GyQWwex7xMkLc4tDGgjvNhSLQwqhFIrNz0iO9NAFoY4IL34btrwgLvLi+yQf8ldJMg1CLvl/ounzl8QQbgYnLGEx0qfNnDsjOM00/JwjvyF3Fly/CiZec8hLWZnZmM2HL8CIvGoRzosepila8mOGDBvD2X2/4JHY25mWL2HZ9nBZF6/VMeC9lsiQeIzOGI5SirYwKazQwmNoRsRZTOZwuly5JHsrsPU102l20WcVUWByJWJ2iHBTgTvKksVYu+rYpYVCuW/4plBmSpc78tZK2PkGeyMnsL2ul8bOPk7OjeVZ3yn8TfsGj66qxm4xcXJOLFdPy+G6Gbk89u3x8hlK0e3WKzv3SDJ8T1QWRSku3veNlel735MWE93NvN+ZT/m5r/F43K1E2i1MzYvnk440GciKP5KBdun9kl+UdyrsX4pWtYEaYslMTSXeaScpKoxqLY4KLZ4RqVE02fXBPXu6DDTvSzh6v5bMZlMR9LZRTzQveKfTWnQpfGeJhNerBnbkL/Uns4dMqFqPpiys7kzi9rmFWM2Kp2ozJTF75EX6+aG3tQkUzgTE5YEV+ANPCVn3KPzrG6jWCiq1eJ4rtaPpA9UeLZ2IqAT6sIDfS601jWfWVlLnKBD3rHI9m/y5bOqKl/V85nxY9xgHiWV9eSttcSPFBVz2RxlY9y8VEZMyOrg9O6c/yO+8F7PRrx+f0iUQlwen/ARO/blUXQ9dAGkToHIdvtRxvH/ASyNRvDnkPhkcP7wHgGf3Kt6tCpMwGQRz6IIkFIbcNhDhtvEpcXJOvkGmBSp7qzYCmoh6n1dyBOt3y77UBcUan1SS7/Bnh5YZP1QKlBwJoWICnUZcLG3WhXfAdVvzMPwmQ8TQ1pdg5IWQPU2ExKTvyr5KGQPfXQ3XfMJHe5qIddh48sqJzBuexM7qNrz9WwApJT9R6SIIelolBzdwDqz8q7RNKV8jIivvVHFA+297IGw//Ydw6SsQmys5vPPuhzP/T4ph+tqhZDE92FhjGSeuXeIICfVWrBWh/v6PReDVbpeQ7rcXSU9Na4SI0rGXici9binctEVSSUZfom+HSc7XntZQMcpgx0132/ZEjAlO2tUbzx4tg5fHPiVtnvb260vYWinu77aX5bWEopAQG+C46aLM55HXhy4AlKxP1UZ5VGLTfjkvltwnbvS2l0XgfesNmHarPCJxxZ+linjrCxJq/tYbcMbvQ6Js6s1wzWLZn84kEXFtVRLSLjxT3lO7U643OxdB0dniGmfPgFEL5Xx87Tvw0rfh099IYYorWZzL/ctCTqMh3AwMvjrikuXiassT4eZ1Z8ud2GeRWHRUYdgB2J1gDSdi3MUsM42ncMQEFsyeyV3njEPpy/JE6mJskHCz6cUW3ZqN9GwZbPuisgDwh0XTboqiU7OTmZWHJWEI4fSR3FNCrzUKn10cCps7Gesg99Bfupjw3gYqtQRa9fBnrzODG7w3o3m65K62vYZF/qlkxkaQERPBxRPSedp3Gn/onMuZI5PZ/vO5pMdE8KPTC7ljXuEAp9GcqN+l7nkbAGtcHvkJToq1VDodGdIK4uUr6I3O5/ZtKdy/so1Fe7uZkhdLVmwEtW299OXMFgfpndtEMEz/oYQ9GvfBnnd4zzueohQRpylR4dzs+S6/9H6L1OhwuuNHstdawJL0G2jJnBMUUS2ag6f7ZgJQ6Y+lVEthVdFPxEGNyRaHrh8NRHFlzw/wRmVS5RxJn7Jx7phUpubF8c62GmmsPP02OPMBuOBxyfkadbE4GQe3SV5YazlP9shnsuyPUPwhJl8vVVocjd3+YLPo3f4MZg1N4C7PlXin3Mojru8BsKQ1UQZRTxcv901ir18Pkduc4O2hwheLz6+xan+rCMlAKDNQyKA7bq1dHqpbegDYQi4HbXIeEZsnA/jUm9lV28GFj22k7vxX4JSfsjnve/R5/djMJt5ozZXQUV8HzcpNpDOK8oDbEl8IzkRe3lDJ5Y+vpc/rl4E6iBKX5ON7IXMKN+4bS5+yh1yZwPNZPV2w6i9y3sz4UbAPZKOKDuY+btf09Tbb5ZgBxOQc0revUYtiXaeeDlG/R8Ju790pA/XyB0Sg5J0qA/+w82B2v7yzcDfeyBSW7KljVmECM4bEM3toIr1ePzuq23j40xKeX1tOZ68uWpUKuW5xeXIeWB1Smejrg6fPEaFw8g2hfpT9K2WtjtCjBQOMugjGXxEUelrdTho1J/erK+H7G+HCJ0XsPHGmuGgz7hDR1lQKCcNEsOTPgYUvypNpkkaE9lUgYjD221J4Mvn74nDrIrLZHIvWfnBgEcXW59FMFv5YMSQ4aXOnCOMt/lxp+dT/+9NYDP+cKwJ16i1ybpavkRBxIL8sMjHkuJUuEeE47DyIShOX3e+BoWfK711vhERR5VpxbU1mGH+lTNvwROjRi+kT5Wk+E68ZeL1OGQ1ZU0S4tR8MFVskDoO4IZKOUL5S9kXhfHktzAXn/g1u2gqXvw2XvwM/bYQ7K0Ucan5x/ALbMzjP9BhgCDeDExenhBxJHA6Jw7GMv/yLi7IvwPiTZzHt7o9xRUZy86lDmJAVCoOqGBGRWuDCo2OPEqfmACkkuCSsaYnLkRcjYmi1xlGqMoiJtBOVPkw2iy72uyej2aP02VIIc4WEW7UWg6luJzG+enyOBOq0aLo0OyOyU9nel0zH1B9D8340WyRPNg5l/ohklt4+izlFiZhNsn9mFSRgNR/58pCQlkunZoeaLXRrNmKSs8iJd2BSig/Sb5IWMfP/wIvjnqOVSN7aWkN9ey9njUolI1aEZEX0JHECNj4pd+wF88VBA/D7eMI3l6HJItzcEVbKLFl43VlYzSaSk5I4q+fnXP6Bj3+WOIPr1YqDt7wT8djcwVzBfbX6YBMdcnICIctGzUUV8aye9y73RP6UoUkuoh02zhiRTGVzN9uqWqXaefyVlLab0b63keqRN9ITVyRhymV/wGdx8EDXabSrSECTSkWk1YvZpNhiKqLJmow3PI6RaW5e8s2keuxtfNSRic1s4t1GWU+/O4uPOrJ4wTeLVdOehDk/04+nHNtl++pD7kL6JLCEoSkTZdYcFu+pY/yvPuS374m4KUh08aFd2lw8stPMY8tK0TSNe9/cydoDTby4qQ6m38bLzblE2i2cPTqFjeXN+Kf8AE2ZOeCPZ1ZBPOX6PiRrGpvKm7nz1a0s2VPPW1urgyEmzRKGFpMtT6PobqJ3xl18sKuOGuJDLkXlBqkOBQm3uTPFASs6W172x7Ddn00vNtb79R6OcUNYUdrMP5bvDwk4ZQargz4sJMbHU6XF0meNkuKOVQ9CZILkCAZy6TImy83ABY8fkoKxoayZth4vswtlG4cmy3l096Id3Pfubu54dRvPrOlXiagLry09CSx8bA3NNl0cuVJFkI68mPLI0ZT2OOW8bq3QH6G2TBLg+1XK939KC5EJYItEodGsOdlX1yk3DLG5MP/34qaN+IYIUIcuVBP7iea0cZJ3eDjc6XDV+zBMmpMHQtvr+rJQvr5QXl1HHWx8iqqMs9nqzwm+fVevnHs1rd3iVOp4YwvFfWvYC+c/Jo83HHaeiOWNT4mItUbITUX1Zskh2/ayhKjzZsu2dRyU7+SEq2WhH9wt73HJTS6x0iCdqDT9szW5rpx0PUz+3uG3F6hq6WZ5rUVCugFXPG6I7LPaHRIlMNshZya9Xh+7D+oFIiaTfL+ypsixUkoczPMelRB1ZKI4uDWbpeDjGGIIN4MTl4J5creWMBSuXyHtQ44R4fH6xXCQcHO45UJcH5YZdOfC00fTqkXQE1PIsiF38VreLwFIKxyLBzMrw2cy9aIf0BA3gaW+ETgTsoiICgm3h7xn04E4e2FRSdTjpk5zMzlPPmtt3PmQfxo1+Qvp9FsZlylhVovZRJJLwnrThgx6fu0g8hJdbPLn0WuN4i7PleQmOgmzmhma7OL51mGSfD3halaXtRNm1Zsc28ycUphAbrzsg0fXNaPlzRH34JLnaevz8cbBGLSIeA7ETqdMS6JQH0yVUqREhZMRI6IvLz6SHo+EtD5uCRVxhDnj6MXG44UP8xvPQkwKius78Ps1uiJDYeM9fhkcOix6XlVdD6sq+5iQJftiTlEiFpPiZ4t28M62GlYUN3DKHz7ljT3dXPn0Zp6uzRZncMerrI0/lzYi2eNLRVMm+PabvJd9J6vVKGYMiefeznO50fkAw1Ldwf1b2dxFVXM3Mwri2eHPRsNEU+65gKKLMFb7i2DEhfTZ3OzWMkhw2vlwZy010RKuZshcyJlFQ2QBM/60liseX4fHp1HZ3E2k3cKQRCcveKfRkzWbZxrz+cMHe3l8xQFWlTYSYTPz4vpK/H6NxbvrmJYfx6ScWNp7vOzujaVl6t086ZnD2IxoKm05tFrj0YrO5s5Xt5HgDCMnzsHflpTwp60iRBotiWxrjxTxEh7NBm8efV4/5b4YtBZdvFSuE4cjPEbCXVNvlmR6dzp9+fNZ6SviQ/84xvc8RImWgg8T+03pXPqPNfzirZ002kUME50FKWNo0qKYmBOLUib2uyZIlev+T+U7nz1dHJyY3AG5ql6fnzte2crHuySM99bWGuwWE1P1dIb8BCdWs2JLRQtFyS7yEiJZUdzIg4uLufDhVexS8h3+xy4LK0saKfHoN2Zn/1XyD+f9luv+tYFLH9+I5kwWxy3QUHnoWcH1aOrs4+T7PubxFXqFt1JBkdKkOens81HTqguDUZfAxc9Jnp3FJsIFQu7a0ZIwVEKSe0W4BcXZW7fA0t9Jfpe3l49jF1KPCGyPI5ke7JhNSpxcXbj1mcL5oClBjqPNCUP0yv6sKSKUlz8ALeV02OLYHTdHXNc/j5HH1A09Cyx2cYEBTvtlyLltqxQxn6OntgSEG4TcsbxTYd5vQv8fhpfWV/BGiR7uPrBcbhgc8eK6tZZLDlvODLDJebzgL8tp7fIccXkUniHi7bRfyQ2Try/UOucYYQg3gxOX6CwJcZml2vOrdNs+j9h0CYNaI1wDpjujE+nS7LRHFQanpaRlMqr3MbypJ3HrN2Zy96WnB+dV1y1n8m0vkxodgS91Apd77yQpxkWkO1RBW+Mazaye+3nBO5P6lJm8YV/A331nceaoZBJddh5dUQbffInX46V9xJiM6OB7cxMiGZXuJsEZ9pnbMyTRydWe27g+4V+86p8eFGPT8uPZWN5MR68XTdNYs7+RecOTGZPh5tyxqYTbzAyf3fm6AAAgAElEQVRLcXHdjFyeX1fBQwk/g+uWs6lBMeU3n3DTC1t5cdQ/eDDqVtKiw3GFWYOf+fOzh3H76YXB9QSYNzyJyPTheBARkZAgA/U7NU7qcTM+M4bl+xoYde8HXPiSJEz3ahbK9BCg2ZlIvNPO06vL6OrzMSFbBmN3hI1rZ+RQ2dzNDc9u5KevS1LzHz/cy+6D7fy6aTqd8/6Klncqv2udQ5jVxDPeU6gb/X2IzeVd+1xioyKZnBvL3iYvaw5qFKW4SNSF24ayZvwazBgSTz1uXhjzFBsypDjGbFLsq2uHMBcvTX2Xf/pO554Fw+jq83HaC220nfGQuBTn/p0/JtxHjMPG5ZOzuHOe7JvkqDBS3OHsabOydMJDlGlJdHt83PvWToanuvjZgmGUN3XxzxX7OdjWwymFCUwbEofdYuIfy/ezJW0hr/unkhMfiTs2ie8lP8vi3iHsPtjOD+YM4ZrpOeyr6+CB1S10WaIo88VR3Ku7aTmzWLFfWoBU+GPxN5dJqKyrge6ksfgzThYnfNTC4HEtnvV3fuu9BFC0E4EfE49bL+bB9unkxUeiFKxr0Ys6YvPom34Hv/ZcQrIrjKw4B6tMY8Th6euQZrp6Ll5H0kR2VLfS65WQ8pr9TTy/roJrn97A21trWLSlmrnDknDq55jNYgqex/NHJjMlN5a1+5v4+6clbChr5rJPndRmzGe5fwRJrjB29CVLhWrWdBi9kD2tZnbWtFHd2kOzNUkct9UPQVgUL3qnceuLW/D5NR74cC917b28v6NfAr8uZFqQz99Xp7vESolwiNBF4rTb4LLXKfYmhJ6PfDRY7BLq1fsLbtN04bZrkTigNVvgvEdZ2xaDFwvNyk17hIR8h6e4qG7tBncGRMRSb06k1KNfM3JngcXGugNNIohn/FDCpHvepaQ7krsPDA81UI/JhQlXyfvGXynPuy6cL9vmiJcimmm3ivsO1FlT6fHo6QATrpF8tkBvwc9gZ3Ub9YEioLIVEtpWSs4Lk0WcviLJdX5v+0E8Po3i+s95bNrIC+Qnexpc9vqhYe+vGUO4GRh8DcRnDqch91yGTVkwYHqUM5Krwv5A3/hrg9NGpEbx63NHMHd40iHLsSQVBYXoN8al8dJ1JxMbaccdGUG7XkE6b+pEzpk2lh95v0N0+lD2RU/nk4jTibBZuGZaDqtLm1hT2shrG6soSnYR47AFl/+HC0bx6Lc+o4+eTnJUGBa7gyWlUlGXow940/Pj8Pg01pQ2sqq0kYaOPk7KjuHV6yfzi7Mlv1ApxR3zCpmWH8ezGw7i1eAnr28n0m4h0WVnSX0kyyo9AwQliCgcnS4X5FFpURQlu7h2Ri6jsxPYrTto6SnizGyraiXeaWdkWhSNnX0kucJo1Ys+GnHRoLsK/og4rpmWTUWTtG4YnxkKb/9wbiGf3DaTlKhwShs6SY8Jp7xJmgZrmmJx+Gz2nPo4G5usXHpSJq/5p/FJ8lU0dPRS3dJNSlQ4k3LECfX5NYqSXSS6pDXK2gPSkmVospOUqDDW9qSzv0XyqSZkRbOpvIXbX97C6qpe7FYLZ4xI4vHLJ9De62NZ2CmUdZppxcGmRjNj0t387KxhXDopkwibmWR3OKnuMDw+jY931aEU/GT+UG6anc/L101mwagUEl12fv3OLpSCmQUJJDjD+PbkLF7dVMl720VQ5MY7yIiJoKKpi78tKSHVHc5Zo1O4aHw6T105kZy4SJ6Nvp6/9s2n2q/vt7xTWVnSSJjVxCYtH3NPk+S9AZd9GsXPtGvh6o840Orj+89t4meLdrCh/NBeb3/sPZu3W7OZlh/PhMwY3qnSbyRi82iKm8Ai/2RiIm0UJjl5s1N3bExWfJlT2eucgGay8ovdKcz/83Iu+PsqvD4/7+84SJjVxLAUFzc8u5HWbg/nj0sb8LmBnMr5I5I5OTeObo+P9h4vV0/LpsHn4PfOH9GEi1mF8fy653zar/w0GAJ9fXMVZpMi0WVnR6dL8tF2LYJxl/OvjQ28srGSK59YxzNrynDYzGwsbwkJE124NWniMBfXHUFIWMPYGzmeU//4KS+sqwhOrm/v5e43trPpMPsyyBm/D/65wZ9PZUQRnPsw3LJDihlGXhAMG77CbLbHi6s1PiuGli4P3R4/jF7IUvNJ1AT6EQ6Zi8fn56bnNnHLC5vxZM4Qgab5qPK6KKnrgKhUyS+8cS2kjOEfy/fzyJ4wiYAEbqbPe0SqfW0RkD8Hf+JwLv3Yxs/flCeOEO6WfDbTQMnS3uPhsWWltHaHHLMd1W0Uayn4lVlc4Dg9Zy99ouSt3bwdRi+krLGT3Qfl+lXyecItQFiUiNVBDd+/bgzhZmDwdWCxEXfZE9hThg2YbDYpnrvzMs6dGAoLKKVYeFIGkXbL4KUMIMxqZpwuNKLCrbTioFmLJCMlibvmF/HO96exYFQKk3PjmJon4aCFJ2WQ5Arj6ifXs6+ug2tn5AxYZrzT/rluW2Adx2ZG49dgfGZ0cF3HZUUTZjXx8zd3culja0hw2jllaAJKqWAoOMA3xqVR1dLNTS9sZkd1G3fMK+Sk7Fg+3VtPbVsvE7OiD/fRgDhi79w0jdHpbgqTnGzXKxFTkhO5dFIGPr9GVmwEpwxNYGyGmyevnEh6SjJNWiRNmosG/akTyhHPt07OItUtYdikqIHbHmm38OdLxnDumFQevlTClLMK4nGGWVi2t4HXN1VjNimunSG5Yj9btIOpv/2E3QfbSXWHU5TsIipchHZRivxtt5jYUCYDbGasg+x4B6UNnZQ1dhLrsDE2I5qa1h5eXF/Jm1uqSYuOQCnFyDQ3NouJdQeaOOfBFdz9xnZK6jvIT5TB3mG38MBFo7lpdh4n58rxfnVTJVmxDq6elsMtc4YQZjUTbjPzy3NG4NdgVJo72Ez6+hm5uMKsPL+ugqhwKzEOGxmxERxo7GTdgWYunyz5hSaTYvqQeIpSXPxf3VgW9xay15+OzxxGWfQktlS0cNaoFN73TcCnrLBrET3xI1nf4uCZ7Z2sbgznm4+t4aNdtTy9uoz735W8vCw999EZZqGrz0e3x0dOvIP5I5NZ3OjGb3OywZ/PJ7ulkjUmwsaQRCcbm+34U8dDzkx+u7iK0x4v49Mzl/FC11im5cextbKVp1eX8f6Og8wYEs9TV53EyLQoMmMjgt+LAJdPzuJHpxeSFefg5JxYlIK06HCumCzn14e7arGYFFPyJCRf1i03S5qmsWhzNdPy47hofDpbOlz6o/IUPWOvYmd1G5F2C5/urWf20ER+fd4I+rx+7n1rp4jICOkf2Kw5sZlNFNcNfApKfwLi7J8r9gddt8dX7OepVWWc+9BKXtlQefg3po6Fy9/hr+HX0UEE9yb9RQptotIgMp4ej48DjV3YLCZ+2X0+H9lPDQpdgJc3VNA4+af8uud8VviH0ZQ0BQrO4M0t1VS39tDW42X1/iZ57CBw0O+msbOP5s7Qk0e8Pj9//ngfz60Nic5VJY10pU+XUCaAK4XS899jryeBlzdUUtXSrx+ezu/e383VT67jnkU7+OXbu7jmyfX0eHy0dnmoaummUkvgnXGPwZDTpZlyAGu45P0pxYc7JWRuUl9AuP2X8Nkjg4GBwXGBxWyinUiatUiy4yS/LeAe3Da3IDhfhM3Cg98cy8WPrCIzNoL5I5L/7c/857fH0+3xBUNNAHaLmVkFCSzeU8flk7O56dT8oHAZzGlFSUTaLby9tYa5wxI5a1QKzZ19LNoibQrG9yvu+CyGJDq5xXc6e7U0Zjsc3DkvlZXFjYxKczM5N45XvyuDc1acg+3l2bQQyW5LIXv9qXhj8gizmvnn5ROCIbXBjMuMDuYB3n/+SMZkuPn9B3v4ZE8dFpNiWn4c8U47Q5OdrDsgg2qPx0+KOxyTSTExO4ZP99aTE+dAKcU5o1N5YX0FkXYLsQ4b2XEOFm2uJsxqIjM2grNHp1Le1EVLl4flxQ2kRYs4sFlkEH15QyUdvV7e3lqD169RkBTKm5w7LOTSFiW72FnTFjwP+jOnKJEfn1FIQVLotWiHjT9eOIqrnlxPTrysa0ZMBJomNxjnjEkdsIzCJCdvbZXqwTf9k5h72kI+XtOOzWLitrkFvL65mmL3ZAqaP2VH1HSoEIFz8SOrcYVZePHak/nzx/v4YGctZpNiVLqbA41djEiNYmWJJM3nxDuIjrDRTgRvz13Gj9/cg8e3I7i+BYlONA12nfI43V6NR5+QkPZD61oAxe1zC/Fru/j5m9K09vThSUSFW3ntu1Po9viCxTgBRqa5GZkmrm5UhJWrpmQzLFWcUneElZYuDzlxDnLiZJ//a3UZH+ys5bfnj5SbkNn5hNnMrNJ0QVh0NlvbnHj9Gr+/YBRZcREUJrlo6/FgUvDsGgldtpd08BTQjJORaVG8vbWGcKuFH80rwG4Z2IZoe5W4YntrO1jw1+UUJLpYXlzPtPw4Onq9/Oa93Zw+PAmvT6Ohs5e06HAu+8darpySzenDp/B4XxfQJ+HPfhTXdeDza0zOjWXZvgaW72sgOy6S5Cg5/376xg42VbTQ3uOlnWTeG/N3LrS7+fun28hLiKSquZv3dxxk2qkL8S25n2KPnC+lDR2M0/tVrjvQTGu3h85eL16fnwONnVzy6Gp+emYRV00VcezzaxxoEGfb49N45NMSfq679aX1HaRFR/DUqjLae8ShPik7hrUHmvjte7s5rSh0/m8zD+XMhS/Q3uNhW0kD7nDbgO/CBztrKUxy4tc0SuoO03x3EMV1HawqaeCyk7M+d96vGkO4GRicILxpm0d7H9wbafvM+cZlRvPcNZNwR1ixfEbl6OdhMZtwHub9D1w0Gq9f+1zHMNxm5u4FRbR09XH11ByUUsHwqDNMEuyPhtz4SIpJZ68vnfMjrDjsFt67eTpW88BBOTvWwfWem/FhoigjkdPKf8dP9KKOgqSj+6wLJ0jez9XTcrji8XV09Hr5kZ53991ZeRxo6OSjXbWsKG4k2S3u3e1zCzh/bFpwX/94/lCW7K0j0RWGUorsuEjaerxsKm/hvLGpFCQ5+evCsawqaWR5cQPp0aGwzKg0N5vKpQGqV69MzE84/LovGJUiwi35UOEG8J3puYdMmz00kT9cMIoY/RzK1ItBZg6JDzpzAfofHw0Ty6v8vL65iqun5ZDgDCM9Opz3w+ZRYFnDKz0TyIiJ4JTCBDaVN/N/F48hO87BqUMT+WBnLYlOe3A7R6a5Q8ItLpJohxWzSfHxvhbae0LiOtZhC67TrhYTK4sbiI6wYTUr1u5vwmY2UZDk5E8Xj+GFdRX4/RrzhsuNitmkPvf8BPjJmaHqzYJEJ2v2N5Ed5whWRr+4vgK/Bne+uhWl4JShCVQ1d/OYPwu/yYpp8o1sLBYxPyErmthIWV9XmJXxmTE0dPZy86lDuO05D+86ZrFZjeWOOUP4x/L9eg5iN3+5ZCy3vbSFxs4+7pxXyI7qVkalu2np6qO508MrG8Vh+/lZw0lw2TnvoZXM/P0Smjv78Gsa10zPYe1+6Vc3e2gCTV3igNW09FDZ3EWMw0aEzcKHO2tRCs4YkcyyfQ2UNnRy6aQMxma6uWZaNh/vruODHaH+bAdbu3lubTl7azt46JtjWbS5mg921PKzBcNYc/anPP8vaaJbXNcRjAx8pBeGeP0aNa09vK8vb1+tOIwvb6jkl2/v5Nu6OJpdmMArG6u4/fRCalq7mfPAUmYVJNDe4+XMkcm09Xj5+6Vj+cVbuwaIuUi7hfo2efzWDc9uYuneesKtZrbccxo2i4nGjl7WH2jixlPy2Vfbzp7aQx3OyuYurnpiPb8+bzjjMmN4bFkpz6+r4IwRycHjeKwwhJuBwQnCMpc8a3RwSPJwHK2b9e9wyJMlPoMLx6cP+H9osgubxcT4zOhD3JDP+rysOAel9Z24I0KJ5oPJjnPQibgHQxKdbCxvGZDf90WYkBXDm9+byttbq5k3Qu7yZxUkQIGEm1cUN5IZI85nfqIzGM4ECWu/8J2Tg8IrR3dIe71+LpuUFZxvUk4MV07J5owRIRdhTIabJ1bK7y0VIuDyEgZWKgc4Z0wKL62vYMaQ+MO+fiT6530VJruId9q5fErWIfMVJoVatZiV4vVNVfg1uFgXt5mxDt5tHc53f3iAN3+9lDNHxfKzswamCswqTEApSIqSQgOAsRnieDlsZhJddl3cOvhgZ+2A90Y7bLjDrYRZTWyvamV7dStj0t047BYWbammIMmJzWIiLtLODbPyvtA+OByFSSHhFnBLG/UwYENHH2Mz3MRF2rGYFFu1XB6fvpyrUgvZ8PF6smIjDhnsH7t8PCal8Hj98nSMzmvIjXcwOS+OyXlxPLq0lF+9s4vsuD28tkny5y56eBV9Pj+XTMzgngWyL3///h6W7atn9lBp4fOT+UPZUd1GUlQYz6wu4+FPSwFYu7+Jho5eNA1S3eFUtXQz7/+WMSE7hkcuG8fz68qZlh/P2H65peMzY7BbzNw1vwiTUjy8tDT4WnF9B0+uKmNybizzhidhMSne23GQVzdWAXb8eiZWSb24WR49zzA6wkpzl4eyxq5guDIQqnxy5QFaujy8vrmKqHAr18/M5ePddby9tYZ6fd0/2V2HzWLi/m+MJMImEua204bw9tZqXt5QSao7nASXndp2qczdXtVKjMNGU2cfe2vbGZ4axce76/BrcFpRIn6/xgc7a/loZy0j0qKCBUT3vbObPbXtPLS4hH9cHsNGPUS9raqVmQUJX/p8+jIYws3A4AThljn5mE3Hd9qqzWLi/vNHBsO9R0thkpPS+k6iI44sxALCwBVmCeay/bvCDUQI3nhK/iHT549IJuYaG5OyYw/zroHrElgOSO5c/1COUoq7FxQNeN/YDBG0F09Ix6wUzV19RxTKyVHhfHLbzC+ySYcQ47Cx7q5TD/taWnQ4ETYz+QmR9Hn9bKlsJTkqLLg92XEOVhQ38NzGWtp7vSJsBxHvtDO3KImsOAdnj04hN95BilvEdbYergVxuwYn7LvDxTEene5mRbE4RKcPSyLeaWfRlmqGp0Z9qW0fTCCsHCjEyYiNoLGzj/kjknl7Ww2nFkmlsjvChivMwv4WDwdbe1i+r4GzRqUcsrxgxbQd4iLtNHT0EusIiburp2XzysZKHlxcgtmkeOKKCXzrn2vRNBieEtq22+YWDEiHuHpaKG81KtzKb97dzbAUFzuq21ilO5mj091UtXTT3uvlk911fO+5TdS29XLv2RkD8jwDKQKB9wTIiXPw0a46+rx+bpiVh1KKOUWJjE5388BHezlvrIRJs+McLN5dR2uXB4fdQmVzN/eePYy739jB2gNNbK5owWpWlNZ3svtgm/RNBMoauxiVFsW4zGjyEiJ5avUBzEqRHBVGTWsPk3Njg6INIDbSzsvXT+ZAQyeFSS5+9c5OSus7aezopamzj8snZ/HEygNsrWxleGoUH+yoJdUdzrAUF/vq2vH5Na5+aj0Om5kfnFZAVLiVt7fVkOoO55M9dWyrbA1W+m7/LxBux/dV3sDAIMgphYlf2F35b+ScMamM6jdIHA0n58SS6hYhcSQyYiIwKRkk43T3o/9A+Z9CKcXk3DhMR+kYZsREcMOs3AFhuSORHhPBkttmcuH4dH5/wSj+csnYL7u6/zYmk+LyyVlcMC6dVD0P7+Tc2KDYumRiOl6/xt2LdpAT72D20MTDLufvl43jjnmFWM0mxmREE+OwoRRkx4WcxEBYNis2gpSosAFh/glZMezT87OKUqKClbxjMr7YOfR5TMyOxmY2BZebFevAYlL88pzh3LOgiG+eFOoTmBnroKyxi/vf243Pr32u4zckUbY1tl+ag1KK62dKOHtWQQLT8uM5Z7QIomGphw9/D+aKKVncd94I/nChPI4vkJMYEGFjM9wUJbt4d/tBJufGMrswAVeYhXCrmQSnPZhfCTBa325nmIWCJCd9Xj9hVlNQ3Cml+MGcIcHCmugIK0XJLvbVdfDC+gr+uWI/c4oSufSkTGwWE/9aLc2Nzx2TSmNnH48u3Y/VrIK9GrP0nNDvzsxle1UbWypbWTgxgwcuGsXtc0Ptk0L70Mlpw5LIiI0gwRlGXXtvUOzPKkzAHWFla2UL3X0+lhfXM6coEaUUE7NjyUuI5IdzC5iQHcMv3trJbS9tYbyeUmIxKW59aTOaJkWwAXF5LDEcNwMDg+OeSydlcumkzM8ME9ssJtKiI4hx2JiaF8ecosQjhhm/TkwmxQ8PMxAdifR+A9uxJtBXL9AHa3JuqEozL8HJJRPT+dfqcq6fkXvUoW+r2cTFEzI4pTDkagQKMIYmy5Mtdte0BV/r7woNS3GRHhPBGzdMCVZD/qfIS3Cy6xenB7fj2hk5zCyIJ9ph44op2QPmzYiNYOmeetp7vVw/MzeYE3ckhiQ6WVnSeIgDPH9EMhvKmrlgnISffzJ/KBOyYig4yvxPu8XMJRMz0DSNeKc9WJE7OS+WeKed78/OZ0xGNB29XlLdIZGWlxBJXkLkgO9TkiuMBKed2Eh70JWbkBUzwPGdnBuL026hvr2XwiQn18/MZUJWNLOHJvLS+gouPTkTk0nEWXFdB4VJTuYNT+bF9ZW8uqmS04oSSXKF8eSqMjJj5fw+b2xasDJ47vCko8p9TXTZae32sL1azpP8hEhGpEaxtbKVpfvq6fH4OU13SFPd4Xz0A+kPp2ka7++opbypkyumZGM1m7h8chaPLtsvrXOGxAeLQ44lhnAzMDA47jmavD6QwTbCJjlxj35r/Fe8Vv875CdEYjUrpuQNDA/ffnohhUkuzh1Ukfp53HfewCcDBAbrocnSvNnfr/ns2MxolJLQY8Ah+qKO7dHSX3wWJrmCeX6DyYyJoL3XS5jVxHem5Rx2nv4Eti92kHCzmE3ce3bo+cqxkXYWnpTxhddbKcUVU7K4/709gBS09A+BD678furKiYfkiQYcQKvZRHefFIn0F+qB9Z2UG8uHO2tJdIUxPDUqGLL+wWmhcG6mLtzOHp0abHqsaXDBuHQ6+7w8uaqM7LiQ2L1nQRHXz8wN5p99HoGWRqtKGnDYzCRHhTEiNYpHlpayaEs1UeHWYLPtwdt4+qD+mTefOoR3th0kKtzKpJxYFu+pp6mz70ulWXxZDOFmYGDwP0P/cJbBf45zx6QyKSc22DoigCvMyqWTvvw+z45z8Ktzh3NaUdIhgsIVZmVEahTREbajFvBfNZm6w3bh+HSij2KAD4RKv0oxcN30XBZtrqaxs++wxTv9OdI6B5zFwGPDDpeaMS0/jg931gYf73Y4Am7xglHJJEeFY7OYcIVZmFEg/eQuGp/O9PzQspVSRy3aAOL1RterShqDzuGknFgeWlLC21trOG9M6mc+i7k/DruF578zCZ9fo7qlW8/J6yDG8dUVeH0ehnAzMDAwMPhSWMymYAj3q0Ap9Zmi+5HLxh91KPbr4KTsWEamRXHNUbhtAMNTozhrVArTvsIcVZNJ8fb3p4We1vAlmFWQwEc/mE7eYVrRTNGbGgeeEnI4rpqazcTsGNL0FjDfGJfGkIRIrGYTVrOJ337jyz1SKi8+EotJ0dnnC67j9CHxPLhwLA8vLeGbk76Yaxk4t1Ojw9n+87mH9Nb7ulFf6Hlnxynjx4/X1q9ff6xXw8DAwMDA4IRG0zT+taacmUPiv1Ix/3lsKGvm8RX7WTgxg8mDnpBxPKCU2qBp2mHzOQzHzcDAwMDAwOA/glKKy/4D4fEvS/8nnpxoGO1ADAwMDAwMDAyOEwzhZmBgYGBgYGBwnGAINwMDAwMDAwOD4wRDuBkYGBgYGBgYHCcYws3AwMDAwMDA4DjBEG4GBgYGBgYGBscJhnAzMDAwMDAwMDhOOCZ93JRStwBXAxqwDbhC07Sefq8/AMzS/40AEjRNc+uv+fT3AJRrmnbW17biBgYGBgYGBgbHkK9duCmlUoHvA0WapnUrpV4ELgaeCMyjadot/eb/HjCm3yK6NU0b/TWtroGBgYGBgYHBfw3HKlRqAcKVUhbEUav+jHkvAZ77WtbKwMDAwMDAwOC/mK9duGmaVgX8HigHaoBWTdM+ONy8SqlMIBv4pN/kMKXUeqXUaqXUOV/5ChsYGBgYGBgY/JfwtQs3pVQ0cDYiyFIAh1Lq0iPMfjHwsqZpvn7TMvUHry4E/k8plXuEz/mOLvDW19fX/we3wMDAwMDAwMDg2HAsQqWnAvs1TavXNM0DvApMPsK8FzMoTKo7dmiaVgosYWD+W//5HtE0bbymaePj4+P/U+tuYGBgYGBgYHDMOBbCrRyYpJSKUEopYDawa/BMSqlCIBpY1W9atFLKrv8dB0wBdn4ta21gYGBgYGBgcIw5Fjlua4CXgY1IWw8T8IhS6l6lVP/WHhcDz2uapvWbNhRYr5TaAiwGfqNpmiHcDAwMDAwMDP4nUAN10YnJ+PHjtfXr1x/r1TAwMDAwMDAw+FyUUhv0fP5DMJ6cYGBgYGBgYGBwnGAINwMDAwMDAwOD44T/iVCpUqoeKPuKPyYOaPiKP8Pg2GMc5xMf4xj/b2Ac5/8NjtfjnKlp2mFbYvxPCLevA6XU+iPFow1OHIzjfOJjHOP/DYzj/L/BiXicjVCpgYGBgYGBgcFxgiHcDAwMDAwMDAyOEwzh9p/jkWO9AgZfC8ZxPvExjvH/BsZx/t/ghDvORo6bgYGBgYGBgcFxguG4GRgYGBgYGBgcJxjC7T+AUup0pdQepVSxUuqOY70+Bv8+Sql/KqXqlFLb+02LUUp9qJTap/+O1qcrpdSf9eO+VSk19titucHRopRKV0otVkrtVErtUErdpE83jvMJglIqTCm1Vim1RT/GP9enZyul1ujH8gWllE2fbtf/L9ZfzzqW62/wxVBKmZVSm5RSb+n/n9DH2RBuXxKllBl4EJgHFAGXKKWKju1aGXwJngBOHzTtDuBjTdPygY/1/0GOeb7+84w0pmkAAAZNSURBVB3gb1/TOhp8ObzArZqmFQGTgBv076xxnE8ceoFTNE0bBYwGTldKTQJ+CzygaVoe0Axcpc9/FdCsT39An8/g+OEmYFe//0/o42wIty/PRKBY07RSTdP6gOeBs4/xOhn8m2iathRoGjT5bOBJ/e8ngXP6TX9KE1YDbqVU8tezpgb/Lpqm1WiatlH/ux254KdiHOcTBv1Ydej/WvUfDTgFeFmfPvgYB479y8BspZT6mlbX4EuglEoD5gOP6f8rTvDjbAi3L08qUNHv/0p9msGJQ6KmaTX63weBRP1v49gf5+ihkjHAGozjfEKhh882A3XAh0AJ0KJpmlefpf9xDB5j/fVWIPbrXWODf5P/A24H/Pr/sZzgx9kQbgYGXwBNyrCNUuwTAKVUJPAKcLOmaW39XzOO8/GPpmk+TdNGA2lIZKTwGK+SwX8YpdSZQJ2maRuO9bp8nRjC7cvz/+3dW4hVVRzH8e8PNTSysgvkJZPIJOwilaQYYdJFQqJoysjQeuilHnrpoYIKil6igtKysiIqiySihC4W5UMXqjHTtIbISkuJrEwNMkH59bDXaU6DY+dM4rjH3wcOs87ae6/zP7Mezp+119prE3B80/sxpS4Gjp8bt8bK382lPn1fU5KGUCVti22/UqrTzwOQ7a3AcmAq1W3uweVQcz/+08fl+BHAb/s51GjfNOBSSeuppinNAB5igPdzErf/rxMYX1axHAJcDSzt55hi31oKzCvlecBrTfVzy6rDKcC2plttcYAqc1qeArpsP9h0KP08QEg6VtKRpTwMuJBqLuNyoKOc1rOPG33fAbznPOT0gGf7NttjbI+j+u19z/YcBng/5wG8+4CkS6jusw8CnrZ9bz+HFH0k6UVgOnAM8DNwF/AqsAQYC2wArrK9pSQAC6hWof4JXG97RX/EHa2TdC7wPrCG7nkxt1PNc0s/DwCSTqeahD6IaoBiie27JZ1INTJzFPA5cK3tnZKGAs9RzXfcAlxt+7v+iT76QtJ04BbbswZ6Pydxi4iIiKiJ3CqNiIiIqIkkbhERERE1kcQtIiIioiaSuEVERETURBK3iIiIiJpI4hYRERFRE0ncIqJWJI2TtLbNa66TNKqFcxa02N7L5VlRSHqj8bDXFq89T9JKSbskdfQ4Nk/SN+U1r6n+LElrJK2T9HBjY2xJ90ua0epnR0T9JXGLiIPBdcBeE7dWSZoIDGo8uNP2JWVbpVb9UOJ5oUe7R1E98Pkcqr0175I0ohxeCNwAjC+vmaV+PnBr375JRNRREreIqKPBkhZL6iqjX4cCSLpTUqektZKeKNtUdQBnA4slrZI0TNJkSR9JWi3pU0nDS7ujJL1VRrzu6+Wz59C9hQ6S1ks6powEdklaJOlLSW+X7Zb+xfZ621/QvWtDw8XAO7a32P4deAeYWfZNPdz2x2V7nmeBy0pbG4CjJR3X139kRNRLEreIqKMJwKO2TwG2AzeW+gW2J9s+FRgGzLL9MrACmGN7ErAbeAm42fYZwAXAjnL9JGA2cBowW1Lz5vIN04DPeolrPPCI7YnAVuCKNr7TaODHpvcbS93oUu5Z37CyxBQRB4EkbhFRRz/a/rCUnwfOLeXzJX0iaQ0wA5i4h2snAD/Z7gSwvd32rnLsXdvbbP8FfAWcsIfrRwK/9BLX97ZXlfJnwLh2vlQfbWYf3QaOiANfEreIqKOemyy7bCD9KNBh+zRgETC0zXZ3NpV3A4P3cM6OvbTbyvW92QQ0j/CNKXWbSrlnfcNQukcMI2KAS+IWEXU0VtLUUr4G+IDuZOpXSYcBzSs2/wAa89i+BkZKmgwgabikdhKsLuCkPkfeu2XARZJGlEUJFwHLbP8EbJc0pawmnUvTHDvgZKCtVbYRUV9J3CKijr4GbpLUBYwAFpaVnYuokphlQGfT+c8Aj0laBQyimsc2X9JqqkUA7YzMvQ5M72vgZWHERuBK4HFJXwLY3gLcU+LuBO4udVDN4XsSWAd8C7xZ2hpClUSu6Gs8EVEvqhYpRUREK8pK0eXANNu7+zmWy4Ezbd/Rn3FExP6TEbeIiDbY3kH1vLXR/3XufjAYeKC/g4iI/ScjbhERERE1kRG3iIiIiJpI4hYRERFRE0ncIiIiImoiiVtERERETSRxi4iIiKiJvwHTd3rJjC+ZKAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "q8_mlp = MyMLP(\n",
        "        X_subtrain=X_subtrain,\n",
        "        Y_subtrain=Y_subtrain,\n",
        "        X_valid=X_valid,\n",
        "        Y_valid=Y_valid,\n",
        "        H=90,\n",
        "        lr=0.001,\n",
        "        wd=0,\n",
        "        mom=0,\n",
        "        loss_type=2,\n",
        "        optimizer_type=1,\n",
        "        use_dropout=True,\n",
        "        dropout_rate=0.5,\n",
        "        loss_z=0\n",
        "    )\n",
        "train_loss_list, valid_loss_list = q8_mlp.fit(\n",
        "    max_epoch=100,\n",
        "    verbose=True,\n",
        "    patience_batch_num=5000,\n",
        "    model_path='q8_mlp.ckpt'\n",
        ")\n",
        "\n",
        "# draw the training loss and validation loss\n",
        "# make figure larger\n",
        "plt.figure(figsize=(10, 8))\n",
        "draw_loss(train_loss_list, valid_loss_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "從上圖看出，使用了 $qloss$ 的模型，其 validation 與 training RMSE 幾乎重疊在一起，而且都在持續下降，可以知道這個模型並沒有 overfitting 的問題，並在 validation set 上的表現相當成功。  \n",
        "而模型看起來非常穩定，如果繼續訓練下去，可能會有更好的效果。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKBRjCjFPs3y",
        "outputId": "d2e6c6c0-6a24-4216-ca58-4961c58756c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start training for z = 0.1\n",
            "Epoch: 0, Step: 100, Train Loss: 9.59568658071178, Valid Loss: 9.556662448029432\n",
            "Epoch: 0, Step: 200, Train Loss: 9.186549833133588, Valid Loss: 9.129590306425355\n",
            "Epoch: 0, Step: 300, Train Loss: 9.062005197028787, Valid Loss: 9.009085273488285\n",
            "Epoch: 0, Step: 400, Train Loss: 8.986552887575602, Valid Loss: 8.923914393712305\n",
            "Epoch: 1, Step: 500, Train Loss: 8.950722429326845, Valid Loss: 8.892893561319685\n",
            "Epoch: 1, Step: 600, Train Loss: 8.918929618030305, Valid Loss: 8.866361499803723\n",
            "Epoch: 1, Step: 700, Train Loss: 8.897622664489214, Valid Loss: 8.841943676250697\n",
            "Epoch: 1, Step: 800, Train Loss: 8.87349802768574, Valid Loss: 8.824858528580183\n",
            "Epoch: 2, Step: 900, Train Loss: 8.849314026710028, Valid Loss: 8.801689635569245\n",
            "Epoch: 2, Step: 1000, Train Loss: 8.842636055929189, Valid Loss: 8.798826581868157\n",
            "Epoch: 2, Step: 1100, Train Loss: 8.827808162983768, Valid Loss: 8.782592238933013\n",
            "Epoch: 2, Step: 1200, Train Loss: 8.840197516082108, Valid Loss: 8.801918831331728\n",
            "Epoch: 3, Step: 1300, Train Loss: 8.856347978154922, Valid Loss: 8.819353814500003\n",
            "Epoch: 3, Step: 1400, Train Loss: 8.7869670044932, Valid Loss: 8.747407902333821\n",
            "Epoch: 3, Step: 1500, Train Loss: 8.826471972061002, Valid Loss: 8.78980471095405\n",
            "Epoch: 3, Step: 1600, Train Loss: 8.798035959102705, Valid Loss: 8.763084415377222\n",
            "Epoch: 4, Step: 1700, Train Loss: 8.78279874085456, Valid Loss: 8.753283287772668\n",
            "Epoch: 4, Step: 1800, Train Loss: 8.7600313983853, Valid Loss: 8.728669810078573\n",
            "Epoch: 4, Step: 1900, Train Loss: 8.765319304433802, Valid Loss: 8.735401434220158\n",
            "Epoch: 4, Step: 2000, Train Loss: 8.776813640704948, Valid Loss: 8.748029994354852\n",
            "Epoch: 5, Step: 2100, Train Loss: 8.768370031761298, Valid Loss: 8.741115668563138\n",
            "Epoch: 5, Step: 2200, Train Loss: 8.74592100440797, Valid Loss: 8.717026090431471\n",
            "Epoch: 5, Step: 2300, Train Loss: 8.720062748190994, Valid Loss: 8.693011792100434\n",
            "Epoch: 5, Step: 2400, Train Loss: 8.741987783232496, Valid Loss: 8.714109356376714\n",
            "Epoch: 5, Step: 2500, Train Loss: 8.72210396051719, Valid Loss: 8.698012440415372\n",
            "Epoch: 6, Step: 2600, Train Loss: 8.716865927665607, Valid Loss: 8.696162153635013\n",
            "Epoch: 6, Step: 2700, Train Loss: 8.71147058130034, Valid Loss: 8.697091138257694\n",
            "Epoch: 6, Step: 2800, Train Loss: 8.732699884894831, Valid Loss: 8.714605451380182\n",
            "Epoch: 6, Step: 2900, Train Loss: 8.712621121572818, Valid Loss: 8.698641062253692\n",
            "Epoch: 7, Step: 3000, Train Loss: 8.706450987501615, Valid Loss: 8.691796860884248\n",
            "Epoch: 7, Step: 3100, Train Loss: 8.705703946037344, Valid Loss: 8.69231665481848\n",
            "Epoch: 7, Step: 3200, Train Loss: 8.680135329833057, Valid Loss: 8.670438538068977\n",
            "Epoch: 7, Step: 3300, Train Loss: 8.727031588206083, Valid Loss: 8.711750382336527\n",
            "Epoch: 8, Step: 3400, Train Loss: 8.707319645982345, Valid Loss: 8.70119859856597\n",
            "Epoch: 8, Step: 3500, Train Loss: 8.690036471293833, Valid Loss: 8.6810555606538\n",
            "Epoch: 8, Step: 3600, Train Loss: 8.697202712322522, Valid Loss: 8.690941193603878\n",
            "Epoch: 8, Step: 3700, Train Loss: 8.688249707417354, Valid Loss: 8.68077024778894\n",
            "Epoch: 9, Step: 3800, Train Loss: 8.674958448196216, Valid Loss: 8.671737842831748\n",
            "Epoch: 9, Step: 3900, Train Loss: 8.653632752871713, Valid Loss: 8.652839920659327\n",
            "Epoch: 9, Step: 4000, Train Loss: 8.678049595903797, Valid Loss: 8.678419918994454\n",
            "Epoch: 9, Step: 4100, Train Loss: 8.682057910485456, Valid Loss: 8.678549330569632\n",
            "Epoch: 10, Step: 4200, Train Loss: 8.664961506645389, Valid Loss: 8.662329469947096\n",
            "Epoch: 10, Step: 4300, Train Loss: 8.660454146669487, Valid Loss: 8.66651282693631\n",
            "Epoch: 10, Step: 4400, Train Loss: 8.652583241831719, Valid Loss: 8.657032643520608\n",
            "Epoch: 10, Step: 4500, Train Loss: 8.650886697777818, Valid Loss: 8.65688602481006\n",
            "Epoch: 11, Step: 4600, Train Loss: 8.660995765635516, Valid Loss: 8.663797400142794\n",
            "Epoch: 11, Step: 4700, Train Loss: 8.663349750471049, Valid Loss: 8.673029454697003\n",
            "Epoch: 11, Step: 4800, Train Loss: 8.64604909209815, Valid Loss: 8.658295052679449\n",
            "Epoch: 11, Step: 4900, Train Loss: 8.653445099685367, Valid Loss: 8.66631654257202\n",
            "Epoch: 11, Step: 5000, Train Loss: 8.642747236604452, Valid Loss: 8.652398741338542\n",
            "Epoch: 12, Step: 5100, Train Loss: 8.664713275148978, Valid Loss: 8.677492469117876\n",
            "Epoch: 12, Step: 5200, Train Loss: 8.66227332912044, Valid Loss: 8.677157998561173\n",
            "Epoch: 12, Step: 5300, Train Loss: 8.64520524367321, Valid Loss: 8.662734387317816\n",
            "Epoch: 12, Step: 5400, Train Loss: 8.635991744184846, Valid Loss: 8.650961904323326\n",
            "Epoch: 13, Step: 5500, Train Loss: 8.659271597231374, Valid Loss: 8.67368501222548\n",
            "Epoch: 13, Step: 5600, Train Loss: 8.630452951792728, Valid Loss: 8.639160322777558\n",
            "Epoch: 13, Step: 5700, Train Loss: 8.645220210028384, Valid Loss: 8.657564772922216\n",
            "Epoch: 13, Step: 5800, Train Loss: 8.631782202862594, Valid Loss: 8.650247392689993\n",
            "Epoch: 14, Step: 5900, Train Loss: 8.621646537805407, Valid Loss: 8.631033060326413\n",
            "Epoch: 14, Step: 6000, Train Loss: 8.624885406987305, Valid Loss: 8.641270190839728\n",
            "Epoch: 14, Step: 6100, Train Loss: 8.651535289561568, Valid Loss: 8.665010761976598\n",
            "Epoch: 14, Step: 6200, Train Loss: 8.630281718403177, Valid Loss: 8.638691331199226\n",
            "Epoch: 15, Step: 6300, Train Loss: 8.64986761449464, Valid Loss: 8.657387729298032\n",
            "Epoch: 15, Step: 6400, Train Loss: 8.644707435405142, Valid Loss: 8.656771941717935\n",
            "Epoch: 15, Step: 6500, Train Loss: 8.627414409206796, Valid Loss: 8.646776296707987\n",
            "Epoch: 15, Step: 6600, Train Loss: 8.611234077199596, Valid Loss: 8.631671544664153\n",
            "Epoch: 16, Step: 6700, Train Loss: 8.64914626373033, Valid Loss: 8.665739330361083\n",
            "Epoch: 16, Step: 6800, Train Loss: 8.624149134064782, Valid Loss: 8.642510142473819\n",
            "Epoch: 16, Step: 6900, Train Loss: 8.61874970812132, Valid Loss: 8.636145841397127\n",
            "Epoch: 16, Step: 7000, Train Loss: 8.611837903338609, Valid Loss: 8.626627721298663\n",
            "Epoch: 16, Step: 7100, Train Loss: 8.615085107636977, Valid Loss: 8.633937929937611\n",
            "Epoch: 17, Step: 7200, Train Loss: 8.61017326762735, Valid Loss: 8.629250715813637\n",
            "Epoch: 17, Step: 7300, Train Loss: 8.611055469564075, Valid Loss: 8.631079117746951\n",
            "Epoch: 17, Step: 7400, Train Loss: 8.621202007092691, Valid Loss: 8.644570924970132\n",
            "Epoch: 17, Step: 7500, Train Loss: 8.626458336815874, Valid Loss: 8.643340081908894\n",
            "Epoch: 18, Step: 7600, Train Loss: 8.611296899190021, Valid Loss: 8.632969368116925\n",
            "Epoch: 18, Step: 7700, Train Loss: 8.62340229140481, Valid Loss: 8.643561362405272\n",
            "Epoch: 18, Step: 7800, Train Loss: 8.612097753539727, Valid Loss: 8.640624302019091\n",
            "Epoch: 18, Step: 7900, Train Loss: 8.597489938871428, Valid Loss: 8.627686551602464\n",
            "Epoch: 19, Step: 8000, Train Loss: 8.605970732793505, Valid Loss: 8.635911950972638\n",
            "Epoch: 19, Step: 8100, Train Loss: 8.609100248324966, Valid Loss: 8.63802689120489\n",
            "Epoch: 19, Step: 8200, Train Loss: 8.61345736965147, Valid Loss: 8.636045220909272\n",
            "Epoch: 19, Step: 8300, Train Loss: 8.588654161589423, Valid Loss: 8.62577212763004\n",
            "Epoch: 20, Step: 8400, Train Loss: 8.59937094806795, Valid Loss: 8.631882201463073\n",
            "Epoch: 20, Step: 8500, Train Loss: 8.601428039052935, Valid Loss: 8.636612852207833\n",
            "Epoch: 20, Step: 8600, Train Loss: 8.60346165613772, Valid Loss: 8.625071644422606\n",
            "Epoch: 20, Step: 8700, Train Loss: 8.618663354135363, Valid Loss: 8.645047565904905\n",
            "Epoch: 21, Step: 8800, Train Loss: 8.59554555880013, Valid Loss: 8.62369171926728\n",
            "Epoch: 21, Step: 8900, Train Loss: 8.591865500573983, Valid Loss: 8.626783466856635\n",
            "Epoch: 21, Step: 9000, Train Loss: 8.592022702024556, Valid Loss: 8.623049913621083\n",
            "Epoch: 21, Step: 9100, Train Loss: 8.615751981086232, Valid Loss: 8.64240988849847\n",
            "Epoch: 22, Step: 9200, Train Loss: 8.613355751289067, Valid Loss: 8.641059621550593\n",
            "Epoch: 22, Step: 9300, Train Loss: 8.596935139017507, Valid Loss: 8.626297648014729\n",
            "Epoch: 22, Step: 9400, Train Loss: 8.600417929842214, Valid Loss: 8.634928116224831\n",
            "Epoch: 22, Step: 9500, Train Loss: 8.59145877241184, Valid Loss: 8.626590267536677\n",
            "Epoch: 22, Step: 9600, Train Loss: 8.597205072485425, Valid Loss: 8.627804109486451\n",
            "Epoch: 23, Step: 9700, Train Loss: 8.595325438869562, Valid Loss: 8.627139599935036\n",
            "Epoch: 23, Step: 9800, Train Loss: 8.614485590882087, Valid Loss: 8.643910386829793\n",
            "Epoch: 23, Step: 9900, Train Loss: 8.581762861469848, Valid Loss: 8.616110995578854\n",
            "Epoch: 23, Step: 10000, Train Loss: 8.605138612104062, Valid Loss: 8.637694783699805\n",
            "Epoch: 24, Step: 10100, Train Loss: 8.579505709248329, Valid Loss: 8.619111470963762\n",
            "Epoch: 24, Step: 10200, Train Loss: 8.578920615865291, Valid Loss: 8.623285005231327\n",
            "Epoch: 24, Step: 10300, Train Loss: 8.583746227731195, Valid Loss: 8.622882172384982\n",
            "Epoch: 24, Step: 10400, Train Loss: 8.590649699521878, Valid Loss: 8.618443297809852\n",
            "Epoch: 25, Step: 10500, Train Loss: 8.572080409898323, Valid Loss: 8.605857741201\n",
            "Epoch: 25, Step: 10600, Train Loss: 8.575688735726304, Valid Loss: 8.609519288548885\n",
            "Epoch: 25, Step: 10700, Train Loss: 8.575088305997983, Valid Loss: 8.609746335249293\n",
            "Epoch: 25, Step: 10800, Train Loss: 8.583983046340652, Valid Loss: 8.61088923431278\n",
            "Epoch: 26, Step: 10900, Train Loss: 8.569830694634943, Valid Loss: 8.600734846954651\n",
            "Epoch: 26, Step: 11000, Train Loss: 8.57334604022774, Valid Loss: 8.606780438544767\n",
            "Epoch: 26, Step: 11100, Train Loss: 8.583595147115282, Valid Loss: 8.615453374500483\n",
            "Epoch: 26, Step: 11200, Train Loss: 8.59161273323047, Valid Loss: 8.619598574401163\n",
            "Epoch: 27, Step: 11300, Train Loss: 8.568214943296503, Valid Loss: 8.600604668545845\n",
            "Epoch: 27, Step: 11400, Train Loss: 8.576797664692856, Valid Loss: 8.605084981823877\n",
            "Epoch: 27, Step: 11500, Train Loss: 8.591538144304849, Valid Loss: 8.62443511823705\n",
            "Epoch: 27, Step: 11600, Train Loss: 8.588007208001413, Valid Loss: 8.617492701611704\n",
            "Epoch: 27, Step: 11700, Train Loss: 8.575377326666205, Valid Loss: 8.607044068812193\n",
            "Epoch: 28, Step: 11800, Train Loss: 8.558907870333355, Valid Loss: 8.603725924713244\n",
            "Epoch: 28, Step: 11900, Train Loss: 8.571988743449586, Valid Loss: 8.603677476266085\n",
            "Epoch: 28, Step: 12000, Train Loss: 8.582070039246021, Valid Loss: 8.617123629973978\n",
            "Epoch: 28, Step: 12100, Train Loss: 8.56940263898953, Valid Loss: 8.604975542389603\n",
            "Epoch: 29, Step: 12200, Train Loss: 8.573878420892955, Valid Loss: 8.612909159186028\n",
            "Epoch: 29, Step: 12300, Train Loss: 8.567684100655176, Valid Loss: 8.606013419714472\n",
            "Epoch: 29, Step: 12400, Train Loss: 8.570552610045588, Valid Loss: 8.61029097244467\n",
            "Epoch: 29, Step: 12500, Train Loss: 8.571383978445596, Valid Loss: 8.60760590811337\n",
            "Epoch: 30, Step: 12600, Train Loss: 8.568029746355856, Valid Loss: 8.601528367652879\n",
            "Epoch: 30, Step: 12700, Train Loss: 8.570560655442998, Valid Loss: 8.606799294189486\n",
            "Epoch: 30, Step: 12800, Train Loss: 8.569674734769423, Valid Loss: 8.608101178280956\n",
            "Epoch: 30, Step: 12900, Train Loss: 8.569685335367147, Valid Loss: 8.598883985372519\n",
            "Epoch: 31, Step: 13000, Train Loss: 8.576764445721139, Valid Loss: 8.614486279189759\n",
            "Epoch: 31, Step: 13100, Train Loss: 8.580041963777925, Valid Loss: 8.62216191672892\n",
            "Epoch: 31, Step: 13200, Train Loss: 8.570297501396842, Valid Loss: 8.61463623530801\n",
            "Epoch: 31, Step: 13300, Train Loss: 8.582559687602862, Valid Loss: 8.621726962058746\n",
            "Epoch: 32, Step: 13400, Train Loss: 8.556954086187341, Valid Loss: 8.595867546137399\n",
            "Epoch: 32, Step: 13500, Train Loss: 8.569701937291777, Valid Loss: 8.615557435184057\n",
            "Epoch: 32, Step: 13600, Train Loss: 8.553596842239049, Valid Loss: 8.600685585858635\n",
            "Epoch: 32, Step: 13700, Train Loss: 8.574057603472626, Valid Loss: 8.615506369316341\n",
            "Epoch: 33, Step: 13800, Train Loss: 8.559220900232347, Valid Loss: 8.5960320146742\n",
            "Epoch: 33, Step: 13900, Train Loss: 8.552433598596824, Valid Loss: 8.59455776633686\n",
            "Epoch: 33, Step: 14000, Train Loss: 8.546319126381483, Valid Loss: 8.589133719835655\n",
            "Epoch: 33, Step: 14100, Train Loss: 8.553823176925597, Valid Loss: 8.595178288022538\n",
            "Epoch: 33, Step: 14200, Train Loss: 8.55684374071131, Valid Loss: 8.592780693637714\n",
            "Epoch: 34, Step: 14300, Train Loss: 8.567631357450281, Valid Loss: 8.606995741809174\n",
            "Epoch: 34, Step: 14400, Train Loss: 8.555824809986232, Valid Loss: 8.602059553163373\n",
            "Epoch: 34, Step: 14500, Train Loss: 8.549044951096144, Valid Loss: 8.59006813538213\n",
            "Epoch: 34, Step: 14600, Train Loss: 8.547259730990309, Valid Loss: 8.59810542375111\n",
            "Epoch: 35, Step: 14700, Train Loss: 8.561675515212823, Valid Loss: 8.604437402296794\n",
            "Epoch: 35, Step: 14800, Train Loss: 8.564511404486428, Valid Loss: 8.603031922953518\n",
            "Epoch: 35, Step: 14900, Train Loss: 8.553339660038551, Valid Loss: 8.603893011384063\n",
            "Epoch: 35, Step: 15000, Train Loss: 8.560524632381224, Valid Loss: 8.60619182118038\n",
            "Epoch: 36, Step: 15100, Train Loss: 8.547860494910136, Valid Loss: 8.597585785832678\n",
            "Epoch: 36, Step: 15200, Train Loss: 8.553938513478393, Valid Loss: 8.60525229907426\n",
            "Epoch: 36, Step: 15300, Train Loss: 8.561353522601419, Valid Loss: 8.60943019369102\n",
            "Epoch: 36, Step: 15400, Train Loss: 8.558652080218918, Valid Loss: 8.61228318306565\n",
            "Epoch: 37, Step: 15500, Train Loss: 8.53805908417863, Valid Loss: 8.592437165812752\n",
            "Epoch: 37, Step: 15600, Train Loss: 8.560341529375156, Valid Loss: 8.603914279442298\n",
            "Epoch: 37, Step: 15700, Train Loss: 8.543847093536073, Valid Loss: 8.596050536205295\n",
            "Epoch: 37, Step: 15800, Train Loss: 8.556083614998236, Valid Loss: 8.605548986921239\n",
            "Epoch: 38, Step: 15900, Train Loss: 8.557482104268876, Valid Loss: 8.605103324791198\n",
            "Epoch: 38, Step: 16000, Train Loss: 8.556440916234838, Valid Loss: 8.601996689888082\n",
            "Epoch: 38, Step: 16100, Train Loss: 8.531905125492491, Valid Loss: 8.590759921545391\n",
            "Epoch: 38, Step: 16200, Train Loss: 8.56346919389876, Valid Loss: 8.607977513872815\n",
            "Epoch: 38, Step: 16300, Train Loss: 8.551760527501957, Valid Loss: 8.596482911968694\n",
            "Epoch: 39, Step: 16400, Train Loss: 8.555433807620277, Valid Loss: 8.597509753082997\n",
            "Epoch: 39, Step: 16500, Train Loss: 8.540796876910706, Valid Loss: 8.587331971645598\n",
            "Epoch: 39, Step: 16600, Train Loss: 8.542776412347262, Valid Loss: 8.596141293580708\n",
            "Epoch: 39, Step: 16700, Train Loss: 8.558205485941954, Valid Loss: 8.605498960475675\n",
            "Epoch: 40, Step: 16800, Train Loss: 8.560038850120112, Valid Loss: 8.605748693353133\n",
            "Epoch: 40, Step: 16900, Train Loss: 8.55005507418628, Valid Loss: 8.600616786847082\n",
            "Epoch: 40, Step: 17000, Train Loss: 8.533814508796247, Valid Loss: 8.5901624323535\n",
            "Epoch: 40, Step: 17100, Train Loss: 8.552778103631494, Valid Loss: 8.604103251749223\n",
            "Epoch: 41, Step: 17200, Train Loss: 8.550279476108559, Valid Loss: 8.60879057106771\n",
            "Epoch: 41, Step: 17300, Train Loss: 8.54797972460427, Valid Loss: 8.601553923754306\n",
            "Epoch: 41, Step: 17400, Train Loss: 8.537548303127807, Valid Loss: 8.598132359208874\n",
            "Epoch: 41, Step: 17500, Train Loss: 8.541577693337567, Valid Loss: 8.598889429782448\n",
            "Epoch: 42, Step: 17600, Train Loss: 8.549166854514239, Valid Loss: 8.598961539111587\n",
            "Epoch: 42, Step: 17700, Train Loss: 8.543263141664921, Valid Loss: 8.601280463652264\n",
            "Epoch: 42, Step: 17800, Train Loss: 8.528186555122058, Valid Loss: 8.582175784261393\n",
            "Epoch: 42, Step: 17900, Train Loss: 8.532740124228132, Valid Loss: 8.586922559343353\n",
            "Epoch: 43, Step: 18000, Train Loss: 8.543726303396841, Valid Loss: 8.602343821538565\n",
            "Epoch: 43, Step: 18100, Train Loss: 8.536197882212166, Valid Loss: 8.590301123977161\n",
            "Epoch: 43, Step: 18200, Train Loss: 8.53661912309081, Valid Loss: 8.595671388100788\n",
            "Epoch: 43, Step: 18300, Train Loss: 8.542492746585257, Valid Loss: 8.592911108215784\n",
            "Epoch: 44, Step: 18400, Train Loss: 8.541037991504428, Valid Loss: 8.59218117089668\n",
            "Epoch: 44, Step: 18500, Train Loss: 8.540755444448974, Valid Loss: 8.600072237345426\n",
            "Epoch: 44, Step: 18600, Train Loss: 8.550782895064318, Valid Loss: 8.599385973394375\n",
            "Epoch: 44, Step: 18700, Train Loss: 8.522140256391417, Valid Loss: 8.577901205629129\n",
            "Epoch: 44, Step: 18800, Train Loss: 8.537392265517473, Valid Loss: 8.594444943631267\n",
            "Epoch: 45, Step: 18900, Train Loss: 8.538397752866505, Valid Loss: 8.586768955411886\n",
            "Epoch: 45, Step: 19000, Train Loss: 8.546887316714946, Valid Loss: 8.59830723556097\n",
            "Epoch: 45, Step: 19100, Train Loss: 8.532836305450981, Valid Loss: 8.591574738028546\n",
            "Epoch: 45, Step: 19200, Train Loss: 8.557338745209968, Valid Loss: 8.613773489892449\n",
            "Epoch: 46, Step: 19300, Train Loss: 8.519282941222981, Valid Loss: 8.592186759262939\n",
            "Epoch: 46, Step: 19400, Train Loss: 8.525348414989594, Valid Loss: 8.59199220844674\n",
            "Epoch: 46, Step: 19500, Train Loss: 8.52971783913737, Valid Loss: 8.586976235458142\n",
            "Epoch: 46, Step: 19600, Train Loss: 8.55496424180936, Valid Loss: 8.611323271483975\n",
            "Epoch: 47, Step: 19700, Train Loss: 8.537094823453828, Valid Loss: 8.58886173515308\n",
            "Epoch: 47, Step: 19800, Train Loss: 8.526833764353308, Valid Loss: 8.58463174606939\n",
            "Epoch: 47, Step: 19900, Train Loss: 8.536483939077126, Valid Loss: 8.598405263029077\n",
            "Epoch: 47, Step: 20000, Train Loss: 8.515780193365423, Valid Loss: 8.577502531691376\n",
            "Epoch: 48, Step: 20100, Train Loss: 8.525548472197686, Valid Loss: 8.58678343300015\n",
            "Epoch: 48, Step: 20200, Train Loss: 8.529338721652575, Valid Loss: 8.592858045122476\n",
            "Epoch: 48, Step: 20300, Train Loss: 8.524368434742975, Valid Loss: 8.588804329067107\n",
            "Epoch: 48, Step: 20400, Train Loss: 8.53604358336508, Valid Loss: 8.591077492907978\n",
            "Epoch: 49, Step: 20500, Train Loss: 8.5379664703545, Valid Loss: 8.58892573174691\n",
            "Epoch: 49, Step: 20600, Train Loss: 8.528976967564581, Valid Loss: 8.585592061423865\n",
            "Epoch: 49, Step: 20700, Train Loss: 8.534721697228065, Valid Loss: 8.593717018655308\n",
            "Epoch: 49, Step: 20800, Train Loss: 8.539695985096685, Valid Loss: 8.59752551817617\n",
            "Epoch: 49, Step: 20900, Train Loss: 8.512397119890128, Valid Loss: 8.57962596690059\n",
            "Epoch: 50, Step: 21000, Train Loss: 8.52100845119917, Valid Loss: 8.583001340730707\n",
            "Epoch: 50, Step: 21100, Train Loss: 8.5285531312504, Valid Loss: 8.597672095905942\n",
            "Epoch: 50, Step: 21200, Train Loss: 8.523382832504897, Valid Loss: 8.586504700187819\n",
            "Epoch: 50, Step: 21300, Train Loss: 8.537599245131792, Valid Loss: 8.595988290472848\n",
            "Epoch: 51, Step: 21400, Train Loss: 8.537942472212052, Valid Loss: 8.599725021348124\n",
            "Epoch: 51, Step: 21500, Train Loss: 8.545763569277135, Valid Loss: 8.603424640608178\n",
            "Epoch: 51, Step: 21600, Train Loss: 8.52684417342741, Valid Loss: 8.58761956734593\n",
            "Epoch: 51, Step: 21700, Train Loss: 8.519149796750138, Valid Loss: 8.582828554700681\n",
            "Epoch: 52, Step: 21800, Train Loss: 8.536551562051779, Valid Loss: 8.601263040495187\n",
            "Epoch: 52, Step: 21900, Train Loss: 8.527464283349744, Valid Loss: 8.591542908986511\n",
            "Epoch: 52, Step: 22000, Train Loss: 8.535914321853355, Valid Loss: 8.596915679983477\n",
            "Epoch: 52, Step: 22100, Train Loss: 8.52566158526561, Valid Loss: 8.593085392352148\n",
            "Epoch: 53, Step: 22200, Train Loss: 8.519637243611333, Valid Loss: 8.584021850538086\n",
            "Epoch: 53, Step: 22300, Train Loss: 8.513198814153963, Valid Loss: 8.58282899882209\n",
            "Epoch: 53, Step: 22400, Train Loss: 8.522489530289155, Valid Loss: 8.589877464199365\n",
            "Epoch: 53, Step: 22500, Train Loss: 8.512524401729236, Valid Loss: 8.587720740512962\n",
            "Epoch: 54, Step: 22600, Train Loss: 8.525158892124303, Valid Loss: 8.597442969123437\n",
            "Epoch: 54, Step: 22700, Train Loss: 8.532205396072074, Valid Loss: 8.5995717807831\n",
            "Epoch: 54, Step: 22800, Train Loss: 8.518044283729168, Valid Loss: 8.58541811821961\n",
            "Epoch: 54, Step: 22900, Train Loss: 8.521851859126487, Valid Loss: 8.588384626743014\n",
            "Epoch: 55, Step: 23000, Train Loss: 8.533343448412577, Valid Loss: 8.59117089142033\n",
            "Epoch: 55, Step: 23100, Train Loss: 8.530988421754378, Valid Loss: 8.60035904460298\n",
            "Epoch: 55, Step: 23200, Train Loss: 8.519516949575875, Valid Loss: 8.595396545188244\n",
            "Epoch: 55, Step: 23300, Train Loss: 8.528907493820219, Valid Loss: 8.590980497204777\n",
            "Epoch: 55, Step: 23400, Train Loss: 8.52206313820999, Valid Loss: 8.58611506684547\n",
            "Epoch: 56, Step: 23500, Train Loss: 8.530660533443534, Valid Loss: 8.58876643573999\n",
            "Epoch: 56, Step: 23600, Train Loss: 8.513528449372084, Valid Loss: 8.58086712965058\n",
            "Epoch: 56, Step: 23700, Train Loss: 8.530269336338156, Valid Loss: 8.59181602701122\n",
            "Epoch: 56, Step: 23800, Train Loss: 8.530258766308233, Valid Loss: 8.592086491654868\n",
            "Epoch: 57, Step: 23900, Train Loss: 8.50365352307966, Valid Loss: 8.57967048141703\n",
            "Epoch: 57, Step: 24000, Train Loss: 8.525737105396653, Valid Loss: 8.590725880724797\n",
            "Epoch: 57, Step: 24100, Train Loss: 8.515096892822738, Valid Loss: 8.582286037106394\n",
            "Epoch: 57, Step: 24200, Train Loss: 8.527598570658174, Valid Loss: 8.593987881720004\n",
            "Epoch: 58, Step: 24300, Train Loss: 8.505812720942302, Valid Loss: 8.575372949254822\n",
            "Epoch: 58, Step: 24400, Train Loss: 8.529658129097877, Valid Loss: 8.596466488412608\n",
            "Epoch: 58, Step: 24500, Train Loss: 8.507932035970688, Valid Loss: 8.575890315875144\n",
            "Epoch: 58, Step: 24600, Train Loss: 8.52176579346993, Valid Loss: 8.582934995958752\n",
            "Epoch: 59, Step: 24700, Train Loss: 8.52036170307875, Valid Loss: 8.581101106780757\n",
            "Epoch: 59, Step: 24800, Train Loss: 8.516594971002402, Valid Loss: 8.57317987625511\n",
            "Epoch: 59, Step: 24900, Train Loss: 8.539154055981461, Valid Loss: 8.60494797734363\n",
            "Epoch: 59, Step: 25000, Train Loss: 8.514722183627695, Valid Loss: 8.585813105890622\n",
            "Epoch: 60, Step: 25100, Train Loss: 8.51550337977172, Valid Loss: 8.579606194870081\n",
            "Epoch: 60, Step: 25200, Train Loss: 8.503578295805259, Valid Loss: 8.574034012550442\n",
            "Epoch: 60, Step: 25300, Train Loss: 8.504199121589666, Valid Loss: 8.5737010253079\n",
            "Epoch: 60, Step: 25400, Train Loss: 8.526225880761661, Valid Loss: 8.59577853518102\n",
            "Epoch: 61, Step: 25500, Train Loss: 8.514996290526668, Valid Loss: 8.585077256636698\n",
            "Epoch: 61, Step: 25600, Train Loss: 8.527905595449432, Valid Loss: 8.598044380838246\n",
            "Epoch: 61, Step: 25700, Train Loss: 8.508980155485794, Valid Loss: 8.580584685113534\n",
            "Epoch: 61, Step: 25800, Train Loss: 8.501029293219661, Valid Loss: 8.575583198747463\n",
            "Epoch: 61, Step: 25900, Train Loss: 8.520656462658659, Valid Loss: 8.591845622058367\n",
            "Epoch: 62, Step: 26000, Train Loss: 8.5101825958452, Valid Loss: 8.57825468637502\n",
            "Epoch: 62, Step: 26100, Train Loss: 8.510523055722848, Valid Loss: 8.581998520335844\n",
            "Epoch: 62, Step: 26200, Train Loss: 8.511668618725064, Valid Loss: 8.584896414647815\n",
            "Epoch: 62, Step: 26300, Train Loss: 8.525645768716666, Valid Loss: 8.591778669062649\n",
            "Epoch: 63, Step: 26400, Train Loss: 8.502310931638563, Valid Loss: 8.57746326996934\n",
            "Epoch: 63, Step: 26500, Train Loss: 8.527824494781228, Valid Loss: 8.59277335083519\n",
            "Epoch: 63, Step: 26600, Train Loss: 8.527051697488247, Valid Loss: 8.592127532199894\n",
            "Epoch: 63, Step: 26700, Train Loss: 8.505558681044755, Valid Loss: 8.58160205153\n",
            "Epoch: 64, Step: 26800, Train Loss: 8.499114971412832, Valid Loss: 8.576209589527464\n",
            "Epoch: 64, Step: 26900, Train Loss: 8.500462390520173, Valid Loss: 8.580061546306068\n",
            "Epoch: 64, Step: 27000, Train Loss: 8.505617971537337, Valid Loss: 8.579238728729159\n",
            "Epoch: 64, Step: 27100, Train Loss: 8.505366734738244, Valid Loss: 8.575600109249738\n",
            "Epoch: 65, Step: 27200, Train Loss: 8.495242796143533, Valid Loss: 8.57334230886287\n",
            "Epoch: 65, Step: 27300, Train Loss: 8.52294093177971, Valid Loss: 8.594919490796377\n",
            "Epoch: 65, Step: 27400, Train Loss: 8.493116486506686, Valid Loss: 8.576727983624574\n",
            "Epoch: 65, Step: 27500, Train Loss: 8.528622517066928, Valid Loss: 8.600828390350006\n",
            "Epoch: 66, Step: 27600, Train Loss: 8.50797156005288, Valid Loss: 8.583659475469732\n",
            "Epoch: 66, Step: 27700, Train Loss: 8.499907924821287, Valid Loss: 8.575011068270111\n",
            "Epoch: 66, Step: 27800, Train Loss: 8.50649570050304, Valid Loss: 8.582039886417656\n",
            "Epoch: 66, Step: 27900, Train Loss: 8.516627302034403, Valid Loss: 8.584604174576613\n",
            "Epoch: 66, Step: 28000, Train Loss: 8.495984186020289, Valid Loss: 8.564387780113677\n",
            "Epoch: 67, Step: 28100, Train Loss: 8.502114901515101, Valid Loss: 8.57694791098103\n",
            "Epoch: 67, Step: 28200, Train Loss: 8.50945025058213, Valid Loss: 8.577140458192865\n",
            "Epoch: 67, Step: 28300, Train Loss: 8.501673316629857, Valid Loss: 8.581975494921965\n",
            "Epoch: 67, Step: 28400, Train Loss: 8.532695238445463, Valid Loss: 8.603557593764181\n",
            "Epoch: 68, Step: 28500, Train Loss: 8.524494409927582, Valid Loss: 8.605782234260198\n",
            "Epoch: 68, Step: 28600, Train Loss: 8.504515677084438, Valid Loss: 8.579255381643442\n",
            "Epoch: 68, Step: 28700, Train Loss: 8.503849980834529, Valid Loss: 8.58379958475027\n",
            "Epoch: 68, Step: 28800, Train Loss: 8.510421198649551, Valid Loss: 8.582839822127308\n",
            "Epoch: 69, Step: 28900, Train Loss: 8.509405365966584, Valid Loss: 8.586244022727934\n",
            "Epoch: 69, Step: 29000, Train Loss: 8.503767800760137, Valid Loss: 8.585458582307531\n",
            "Epoch: 69, Step: 29100, Train Loss: 8.521612881665815, Valid Loss: 8.602613394583079\n",
            "Epoch: 69, Step: 29200, Train Loss: 8.50181002653869, Valid Loss: 8.587245601788199\n",
            "Epoch: 70, Step: 29300, Train Loss: 8.507319719001813, Valid Loss: 8.58501616524385\n",
            "Epoch: 70, Step: 29400, Train Loss: 8.505758575379879, Valid Loss: 8.580178328205042\n",
            "Epoch: 70, Step: 29500, Train Loss: 8.505273647727636, Valid Loss: 8.58151649520312\n",
            "Epoch: 70, Step: 29600, Train Loss: 8.505310699041788, Valid Loss: 8.590044074938262\n",
            "Epoch: 71, Step: 29700, Train Loss: 8.519944490481208, Valid Loss: 8.59505345269351\n",
            "Epoch: 71, Step: 29800, Train Loss: 8.526974936286898, Valid Loss: 8.596346529846576\n",
            "Epoch: 71, Step: 29900, Train Loss: 8.506990139169428, Valid Loss: 8.580446997363222\n",
            "Epoch: 71, Step: 30000, Train Loss: 8.498659628752169, Valid Loss: 8.57220568766881\n",
            "Epoch: 72, Step: 30100, Train Loss: 8.490384106774599, Valid Loss: 8.574508834617626\n",
            "Epoch: 72, Step: 30200, Train Loss: 8.489261057417126, Valid Loss: 8.57348413427023\n",
            "Epoch: 72, Step: 30300, Train Loss: 8.5091505987272, Valid Loss: 8.593429462822217\n",
            "Epoch: 72, Step: 30400, Train Loss: 8.503377693916365, Valid Loss: 8.581504435822644\n",
            "Epoch: 72, Step: 30500, Train Loss: 8.50236220420939, Valid Loss: 8.578526067115133\n",
            "Epoch: 73, Step: 30600, Train Loss: 8.497322872127931, Valid Loss: 8.580043429616513\n",
            "Epoch: 73, Step: 30700, Train Loss: 8.499252449020878, Valid Loss: 8.577227359485326\n",
            "Epoch: 73, Step: 30800, Train Loss: 8.511459377452862, Valid Loss: 8.599246245415362\n",
            "Epoch: 73, Step: 30900, Train Loss: 8.509179105698838, Valid Loss: 8.587811213171555\n",
            "Epoch: 74, Step: 31000, Train Loss: 8.501225988603284, Valid Loss: 8.588268888250996\n",
            "Epoch: 74, Step: 31100, Train Loss: 8.514934664365908, Valid Loss: 8.601265706857516\n",
            "Epoch: 74, Step: 31200, Train Loss: 8.510641052155709, Valid Loss: 8.590348137703359\n",
            "Epoch: 74, Step: 31300, Train Loss: 8.498976486108967, Valid Loss: 8.596210655137778\n",
            "Epoch: 75, Step: 31400, Train Loss: 8.48647455686872, Valid Loss: 8.57209106230879\n",
            "Epoch: 75, Step: 31500, Train Loss: 8.502526449829636, Valid Loss: 8.585085388549274\n",
            "Epoch: 75, Step: 31600, Train Loss: 8.502061875120805, Valid Loss: 8.589484492477082\n",
            "Epoch: 75, Step: 31700, Train Loss: 8.501263673010486, Valid Loss: 8.593747991740292\n",
            "Epoch: 76, Step: 31800, Train Loss: 8.504478174924401, Valid Loss: 8.583843044733412\n",
            "Epoch: 76, Step: 31900, Train Loss: 8.505883158542973, Valid Loss: 8.594164513627788\n",
            "Epoch: 76, Step: 32000, Train Loss: 8.499161999187242, Valid Loss: 8.592312535758838\n",
            "Epoch: 76, Step: 32100, Train Loss: 8.49770098762452, Valid Loss: 8.581078893756564\n",
            "Epoch: 77, Step: 32200, Train Loss: 8.485950757076376, Valid Loss: 8.574798930815845\n",
            "Epoch: 77, Step: 32300, Train Loss: 8.495773799663253, Valid Loss: 8.5758233660106\n",
            "Epoch: 77, Step: 32400, Train Loss: 8.492215334008176, Valid Loss: 8.574775785372704\n",
            "Epoch: 77, Step: 32500, Train Loss: 8.521664819304634, Valid Loss: 8.595004063476244\n",
            "Epoch: 77, Step: 32600, Train Loss: 8.494911244338274, Valid Loss: 8.578473248915484\n",
            "Epoch: 78, Step: 32700, Train Loss: 8.48100966377573, Valid Loss: 8.568154514827112\n",
            "Epoch: 78, Step: 32800, Train Loss: 8.508040205852577, Valid Loss: 8.58617820092067\n",
            "Epoch: 78, Step: 32900, Train Loss: 8.49799986635253, Valid Loss: 8.583164843721692\n",
            "Epoch: 78, Step: 33000, Train Loss: 8.506878752251726, Valid Loss: 8.586001115287093\n",
            "Epoch: 79, Step: 33100, Train Loss: 8.516400396724336, Valid Loss: 8.59717471842182\n",
            "Epoch: 79, Step: 33200, Train Loss: 8.495067403645782, Valid Loss: 8.5895322437596\n",
            "Epoch: 79, Step: 33300, Train Loss: 8.502225099162493, Valid Loss: 8.592887354734502\n",
            "Epoch: 79, Step: 33400, Train Loss: 8.503094254851973, Valid Loss: 8.588772649138981\n",
            "Epoch: 80, Step: 33500, Train Loss: 8.492042417034238, Valid Loss: 8.590639981352748\n",
            "Epoch: 80, Step: 33600, Train Loss: 8.505885017375777, Valid Loss: 8.600309188763683\n",
            "Epoch: 80, Step: 33700, Train Loss: 8.502865232658458, Valid Loss: 8.592897746277934\n",
            "Epoch: 80, Step: 33800, Train Loss: 8.492484977163775, Valid Loss: 8.574029399756183\n",
            "Epoch: 81, Step: 33900, Train Loss: 8.487483428527684, Valid Loss: 8.574671330468098\n",
            "Epoch: 81, Step: 34000, Train Loss: 8.488868572411564, Valid Loss: 8.581387352934529\n",
            "Epoch: 81, Step: 34100, Train Loss: 8.478529865246127, Valid Loss: 8.56865890034814\n",
            "Epoch: 81, Step: 34200, Train Loss: 8.480769174264408, Valid Loss: 8.576287083104045\n",
            "Epoch: 82, Step: 34300, Train Loss: 8.49925535025823, Valid Loss: 8.588687139721753\n",
            "Epoch: 82, Step: 34400, Train Loss: 8.490751690763949, Valid Loss: 8.578237211540511\n",
            "Epoch: 82, Step: 34500, Train Loss: 8.494533929122735, Valid Loss: 8.576087518166595\n",
            "Epoch: 82, Step: 34600, Train Loss: 8.502069864232965, Valid Loss: 8.590054131594181\n",
            "Epoch: 83, Step: 34700, Train Loss: 8.486186561135732, Valid Loss: 8.574207933715543\n",
            "Epoch: 83, Step: 34800, Train Loss: 8.48964845666651, Valid Loss: 8.57885038067131\n",
            "Epoch: 83, Step: 34900, Train Loss: 8.48643753774403, Valid Loss: 8.573472183954475\n",
            "Epoch: 83, Step: 35000, Train Loss: 8.489956313105363, Valid Loss: 8.586166793162347\n",
            "Epoch: 83, Step: 35100, Train Loss: 8.490803299352459, Valid Loss: 8.582953756768417\n",
            "Epoch: 84, Step: 35200, Train Loss: 8.497276824977897, Valid Loss: 8.583796947310542\n",
            "Epoch: 84, Step: 35300, Train Loss: 8.49245099140104, Valid Loss: 8.577482514152718\n",
            "Epoch: 84, Step: 35400, Train Loss: 8.491328677259899, Valid Loss: 8.579129995499711\n",
            "Epoch: 84, Step: 35500, Train Loss: 8.484672642414884, Valid Loss: 8.573267613581764\n",
            "Epoch: 85, Step: 35600, Train Loss: 8.480310169224119, Valid Loss: 8.574231969824996\n",
            "Epoch: 85, Step: 35700, Train Loss: 8.50266650458059, Valid Loss: 8.590204327893876\n",
            "Epoch: 85, Step: 35800, Train Loss: 8.501772119139522, Valid Loss: 8.587044405553241\n",
            "Epoch: 85, Step: 35900, Train Loss: 8.501719390610251, Valid Loss: 8.583198813931482\n",
            "Epoch: 86, Step: 36000, Train Loss: 8.475124414734935, Valid Loss: 8.569742574395091\n",
            "Epoch: 86, Step: 36100, Train Loss: 8.486012364028085, Valid Loss: 8.578236042949577\n",
            "Epoch: 86, Step: 36200, Train Loss: 8.4974573694852, Valid Loss: 8.578229993770435\n",
            "Epoch: 86, Step: 36300, Train Loss: 8.479147797748926, Valid Loss: 8.566921476104934\n",
            "Epoch: 87, Step: 36400, Train Loss: 8.488015232289886, Valid Loss: 8.579467740287255\n",
            "Epoch: 87, Step: 36500, Train Loss: 8.488181077090267, Valid Loss: 8.57377356018115\n",
            "Epoch: 87, Step: 36600, Train Loss: 8.483373911307446, Valid Loss: 8.574966426415356\n",
            "Epoch: 87, Step: 36700, Train Loss: 8.48183667967583, Valid Loss: 8.569302368770119\n",
            "Epoch: 88, Step: 36800, Train Loss: 8.497033719008364, Valid Loss: 8.588582888336862\n",
            "Epoch: 88, Step: 36900, Train Loss: 8.485044187393152, Valid Loss: 8.575979422748247\n",
            "Epoch: 88, Step: 37000, Train Loss: 8.48973585641212, Valid Loss: 8.578796079276497\n",
            "Epoch: 88, Step: 37100, Train Loss: 8.479556550796916, Valid Loss: 8.572052473530688\n",
            "Epoch: 88, Step: 37200, Train Loss: 8.485967744240746, Valid Loss: 8.56873604931018\n",
            "Epoch: 89, Step: 37300, Train Loss: 8.48681087949591, Valid Loss: 8.580611543036223\n",
            "Epoch: 89, Step: 37400, Train Loss: 8.496134849296311, Valid Loss: 8.585947094587304\n",
            "Epoch: 89, Step: 37500, Train Loss: 8.489326286983399, Valid Loss: 8.577985342143547\n",
            "Epoch: 89, Step: 37600, Train Loss: 8.494170977868945, Valid Loss: 8.58039423762192\n",
            "Epoch: 90, Step: 37700, Train Loss: 8.478112120904344, Valid Loss: 8.57147775361771\n",
            "Epoch: 90, Step: 37800, Train Loss: 8.483442870612288, Valid Loss: 8.571704893995085\n",
            "Epoch: 90, Step: 37900, Train Loss: 8.482530351672686, Valid Loss: 8.574279196924115\n",
            "Epoch: 90, Step: 38000, Train Loss: 8.492037795855234, Valid Loss: 8.581499574272732\n",
            "Epoch: 91, Step: 38100, Train Loss: 8.499640741846747, Valid Loss: 8.599974113784029\n",
            "Epoch: 91, Step: 38200, Train Loss: 8.490282399452411, Valid Loss: 8.579939208204852\n",
            "Epoch: 91, Step: 38300, Train Loss: 8.506872666982447, Valid Loss: 8.591557025555016\n",
            "Epoch: 91, Step: 38400, Train Loss: 8.501818225336715, Valid Loss: 8.586706406946778\n",
            "Epoch: 92, Step: 38500, Train Loss: 8.486234416067031, Valid Loss: 8.580088722495924\n",
            "Epoch: 92, Step: 38600, Train Loss: 8.502839079734532, Valid Loss: 8.592063995744994\n",
            "Epoch: 92, Step: 38700, Train Loss: 8.509225043892137, Valid Loss: 8.595649695389145\n",
            "Epoch: 92, Step: 38800, Train Loss: 8.48262043073136, Valid Loss: 8.571421167212097\n",
            "Epoch: 93, Step: 38900, Train Loss: 8.477100782779459, Valid Loss: 8.574451354543722\n",
            "Epoch: 93, Step: 39000, Train Loss: 8.491623153908575, Valid Loss: 8.591321744750822\n",
            "Epoch: 93, Step: 39100, Train Loss: 8.481702784055393, Valid Loss: 8.576795009912548\n",
            "Epoch: 93, Step: 39200, Train Loss: 8.473934021436905, Valid Loss: 8.573967566458728\n",
            "Epoch: 94, Step: 39300, Train Loss: 8.487558151682933, Valid Loss: 8.586765237294477\n",
            "Epoch: 94, Step: 39400, Train Loss: 8.463180627766054, Valid Loss: 8.570510544230403\n",
            "Epoch: 94, Step: 39500, Train Loss: 8.48455016261584, Valid Loss: 8.58273251351981\n",
            "Epoch: 94, Step: 39600, Train Loss: 8.46999358798278, Valid Loss: 8.5654473855491\n",
            "Epoch: 94, Step: 39700, Train Loss: 8.506223219150327, Valid Loss: 8.59406420293079\n",
            "Epoch: 95, Step: 39800, Train Loss: 8.48689147960698, Valid Loss: 8.582685848223504\n",
            "Epoch: 95, Step: 39900, Train Loss: 8.481223271635153, Valid Loss: 8.578773818551117\n",
            "Epoch: 95, Step: 40000, Train Loss: 8.483863674893136, Valid Loss: 8.573328567613292\n",
            "Epoch: 95, Step: 40100, Train Loss: 8.486253086293893, Valid Loss: 8.582206855437967\n",
            "Epoch: 96, Step: 40200, Train Loss: 8.48418724788045, Valid Loss: 8.586894453193086\n",
            "Epoch: 96, Step: 40300, Train Loss: 8.480453419633097, Valid Loss: 8.58175953448955\n",
            "Epoch: 96, Step: 40400, Train Loss: 8.481324955716321, Valid Loss: 8.58135309087464\n",
            "Epoch: 96, Step: 40500, Train Loss: 8.489596099350424, Valid Loss: 8.590074043836234\n",
            "Epoch: 97, Step: 40600, Train Loss: 8.491441814353227, Valid Loss: 8.584059065602684\n",
            "Epoch: 97, Step: 40700, Train Loss: 8.495612037049424, Valid Loss: 8.587480986913892\n",
            "Epoch: 97, Step: 40800, Train Loss: 8.471463390620274, Valid Loss: 8.577506401137837\n",
            "Epoch: 97, Step: 40900, Train Loss: 8.476828644000868, Valid Loss: 8.581391743356926\n",
            "Epoch: 98, Step: 41000, Train Loss: 8.473497336488316, Valid Loss: 8.572213701584186\n",
            "Epoch: 98, Step: 41100, Train Loss: 8.497873081239003, Valid Loss: 8.596217490327328\n",
            "Epoch: 98, Step: 41200, Train Loss: 8.487392729484213, Valid Loss: 8.58396882572307\n",
            "Epoch: 98, Step: 41300, Train Loss: 8.482670261559024, Valid Loss: 8.592587945442885\n",
            "Epoch: 99, Step: 41400, Train Loss: 8.475327336638289, Valid Loss: 8.581320578480442\n",
            "Epoch: 99, Step: 41500, Train Loss: 8.474393717535108, Valid Loss: 8.584224903625545\n",
            "Epoch: 99, Step: 41600, Train Loss: 8.487722111451754, Valid Loss: 8.588902661152398\n",
            "Epoch: 99, Step: 41700, Train Loss: 8.482774634076566, Valid Loss: 8.592071535239327\n",
            "Epoch: 99, Step: 41800, Train Loss: 8.487727387528839, Valid Loss: 8.592491466784157\n",
            "Test RMSE Loss for z = 0.1: 8.750706457881472\n",
            "Start training for z = 0.5\n",
            "Epoch: 0, Step: 100, Train Loss: 9.490357931614055, Valid Loss: 9.444253744420788\n",
            "Epoch: 0, Step: 200, Train Loss: 9.164225608333238, Valid Loss: 9.105387828471335\n",
            "Epoch: 0, Step: 300, Train Loss: 9.079614189532709, Valid Loss: 9.028139207309216\n",
            "Epoch: 0, Step: 400, Train Loss: 9.008762995365416, Valid Loss: 8.950999864096731\n",
            "Epoch: 1, Step: 500, Train Loss: 8.96564728835167, Valid Loss: 8.909482114214182\n",
            "Epoch: 1, Step: 600, Train Loss: 8.927022135174047, Valid Loss: 8.875153242599787\n",
            "Epoch: 1, Step: 700, Train Loss: 8.910255167168481, Valid Loss: 8.858913850244717\n",
            "Epoch: 1, Step: 800, Train Loss: 8.896152705784546, Valid Loss: 8.849100612923875\n",
            "Epoch: 2, Step: 900, Train Loss: 8.860111260850834, Valid Loss: 8.813257567049588\n",
            "Epoch: 2, Step: 1000, Train Loss: 8.832437626690584, Valid Loss: 8.786800102127755\n",
            "Epoch: 2, Step: 1100, Train Loss: 8.8192809043146, Valid Loss: 8.770810232988506\n",
            "Epoch: 2, Step: 1200, Train Loss: 8.801319595523866, Valid Loss: 8.753747617124493\n",
            "Epoch: 3, Step: 1300, Train Loss: 8.807642162711927, Valid Loss: 8.768718324658803\n",
            "Epoch: 3, Step: 1400, Train Loss: 8.7888837301206, Valid Loss: 8.744176049240764\n",
            "Epoch: 3, Step: 1500, Train Loss: 8.776021587948312, Valid Loss: 8.738080852646943\n",
            "Epoch: 3, Step: 1600, Train Loss: 8.810625231391457, Valid Loss: 8.770437703313107\n",
            "Epoch: 4, Step: 1700, Train Loss: 8.776461834457642, Valid Loss: 8.73908550138127\n",
            "Epoch: 4, Step: 1800, Train Loss: 8.761075730508395, Valid Loss: 8.72561859042292\n",
            "Epoch: 4, Step: 1900, Train Loss: 8.791083448687266, Valid Loss: 8.755428590428846\n",
            "Epoch: 4, Step: 2000, Train Loss: 8.747255667980347, Valid Loss: 8.714535473438824\n",
            "Epoch: 5, Step: 2100, Train Loss: 8.73380498169444, Valid Loss: 8.701435322936979\n",
            "Epoch: 5, Step: 2200, Train Loss: 8.726708478573649, Valid Loss: 8.698121905328696\n",
            "Epoch: 5, Step: 2300, Train Loss: 8.732009007115611, Valid Loss: 8.706297102201003\n",
            "Epoch: 5, Step: 2400, Train Loss: 8.719510617405332, Valid Loss: 8.696133419745829\n",
            "Epoch: 5, Step: 2500, Train Loss: 8.727908178639503, Valid Loss: 8.702204446325666\n",
            "Epoch: 6, Step: 2600, Train Loss: 8.738308999437352, Valid Loss: 8.72466918105199\n",
            "Epoch: 6, Step: 2700, Train Loss: 8.718416390224215, Valid Loss: 8.701897764953356\n",
            "Epoch: 6, Step: 2800, Train Loss: 8.717568949738403, Valid Loss: 8.702158603201926\n",
            "Epoch: 6, Step: 2900, Train Loss: 8.707614804129278, Valid Loss: 8.691089399248897\n",
            "Epoch: 7, Step: 3000, Train Loss: 8.702429841559459, Valid Loss: 8.686101693413043\n",
            "Epoch: 7, Step: 3100, Train Loss: 8.698436006329544, Valid Loss: 8.689716569610399\n",
            "Epoch: 7, Step: 3200, Train Loss: 8.713284005912271, Valid Loss: 8.700491165572595\n",
            "Epoch: 7, Step: 3300, Train Loss: 8.688193588385634, Valid Loss: 8.677757501661175\n",
            "Epoch: 8, Step: 3400, Train Loss: 8.691179381539492, Valid Loss: 8.679495488977052\n",
            "Epoch: 8, Step: 3500, Train Loss: 8.685560316473955, Valid Loss: 8.67900380521938\n",
            "Epoch: 8, Step: 3600, Train Loss: 8.695072947028585, Valid Loss: 8.698485973817183\n",
            "Epoch: 8, Step: 3700, Train Loss: 8.68775286511256, Valid Loss: 8.686842146726043\n",
            "Epoch: 9, Step: 3800, Train Loss: 8.687211647046844, Valid Loss: 8.693079828383258\n",
            "Epoch: 9, Step: 3900, Train Loss: 8.66696978489431, Valid Loss: 8.66577676802361\n",
            "Epoch: 9, Step: 4000, Train Loss: 8.655540462474058, Valid Loss: 8.66168637932748\n",
            "Epoch: 9, Step: 4100, Train Loss: 8.664187703138053, Valid Loss: 8.670621921725855\n",
            "Epoch: 10, Step: 4200, Train Loss: 8.642769605198396, Valid Loss: 8.64462589485788\n",
            "Epoch: 10, Step: 4300, Train Loss: 8.670377346402626, Valid Loss: 8.674804787762087\n",
            "Epoch: 10, Step: 4400, Train Loss: 8.683422245589705, Valid Loss: 8.689881961677624\n",
            "Epoch: 10, Step: 4500, Train Loss: 8.665690536504492, Valid Loss: 8.669494357247293\n",
            "Epoch: 11, Step: 4600, Train Loss: 8.635598747733638, Valid Loss: 8.648205527673152\n",
            "Epoch: 11, Step: 4700, Train Loss: 8.645335386625632, Valid Loss: 8.658050890247674\n",
            "Epoch: 11, Step: 4800, Train Loss: 8.654475376450671, Valid Loss: 8.662733837894562\n",
            "Epoch: 11, Step: 4900, Train Loss: 8.641809574399396, Valid Loss: 8.650987791529845\n",
            "Epoch: 11, Step: 5000, Train Loss: 8.645680006761408, Valid Loss: 8.657736845804157\n",
            "Epoch: 12, Step: 5100, Train Loss: 8.626656703471351, Valid Loss: 8.640116746815584\n",
            "Epoch: 12, Step: 5200, Train Loss: 8.641592192860676, Valid Loss: 8.662834085181737\n",
            "Epoch: 12, Step: 5300, Train Loss: 8.616932589119797, Valid Loss: 8.63390159088979\n",
            "Epoch: 12, Step: 5400, Train Loss: 8.63578874612543, Valid Loss: 8.66058992234467\n",
            "Epoch: 13, Step: 5500, Train Loss: 8.629764055081578, Valid Loss: 8.651395849263029\n",
            "Epoch: 13, Step: 5600, Train Loss: 8.637377339320784, Valid Loss: 8.655255621375378\n",
            "Epoch: 13, Step: 5700, Train Loss: 8.613381343440267, Valid Loss: 8.633345946385054\n",
            "Epoch: 13, Step: 5800, Train Loss: 8.632055804287962, Valid Loss: 8.647853527808957\n",
            "Epoch: 14, Step: 5900, Train Loss: 8.597571862957409, Valid Loss: 8.625273633465268\n",
            "Epoch: 14, Step: 6000, Train Loss: 8.615517363832005, Valid Loss: 8.635065674239826\n",
            "Epoch: 14, Step: 6100, Train Loss: 8.610039387941697, Valid Loss: 8.627195366836506\n",
            "Epoch: 14, Step: 6200, Train Loss: 8.614785309250005, Valid Loss: 8.635087992273622\n",
            "Epoch: 15, Step: 6300, Train Loss: 8.601713509866384, Valid Loss: 8.627319283043084\n",
            "Epoch: 15, Step: 6400, Train Loss: 8.611728930891072, Valid Loss: 8.641279298333147\n",
            "Epoch: 15, Step: 6500, Train Loss: 8.639907646069316, Valid Loss: 8.655004496844974\n",
            "Epoch: 15, Step: 6600, Train Loss: 8.628802730515808, Valid Loss: 8.65271343222888\n",
            "Epoch: 16, Step: 6700, Train Loss: 8.606609050024636, Valid Loss: 8.623305685737785\n",
            "Epoch: 16, Step: 6800, Train Loss: 8.6116479783775, Valid Loss: 8.636642127937787\n",
            "Epoch: 16, Step: 6900, Train Loss: 8.614820521152861, Valid Loss: 8.636624620058571\n",
            "Epoch: 16, Step: 7000, Train Loss: 8.626252946240228, Valid Loss: 8.645640022191978\n",
            "Epoch: 16, Step: 7100, Train Loss: 8.600133195812209, Valid Loss: 8.627544875149468\n",
            "Epoch: 17, Step: 7200, Train Loss: 8.609085450750785, Valid Loss: 8.640064300255455\n",
            "Epoch: 17, Step: 7300, Train Loss: 8.627091283962233, Valid Loss: 8.65518863332318\n",
            "Epoch: 17, Step: 7400, Train Loss: 8.605603039455728, Valid Loss: 8.633372288904933\n",
            "Epoch: 17, Step: 7500, Train Loss: 8.594728504272787, Valid Loss: 8.620288792038785\n",
            "Epoch: 18, Step: 7600, Train Loss: 8.625700823526664, Valid Loss: 8.650275414725154\n",
            "Epoch: 18, Step: 7700, Train Loss: 8.600602126284068, Valid Loss: 8.629409435326847\n",
            "Epoch: 18, Step: 7800, Train Loss: 8.607287617285662, Valid Loss: 8.637480879586928\n",
            "Epoch: 18, Step: 7900, Train Loss: 8.591386352855503, Valid Loss: 8.622840308614649\n",
            "Epoch: 19, Step: 8000, Train Loss: 8.608174544944458, Valid Loss: 8.64388084822382\n",
            "Epoch: 19, Step: 8100, Train Loss: 8.606435837091803, Valid Loss: 8.649143434865914\n",
            "Epoch: 19, Step: 8200, Train Loss: 8.599591818568344, Valid Loss: 8.636974019895073\n",
            "Epoch: 19, Step: 8300, Train Loss: 8.58527353551883, Valid Loss: 8.628532829620369\n",
            "Epoch: 20, Step: 8400, Train Loss: 8.61203631657354, Valid Loss: 8.65168701379651\n",
            "Epoch: 20, Step: 8500, Train Loss: 8.598401406738343, Valid Loss: 8.639903677790077\n",
            "Epoch: 20, Step: 8600, Train Loss: 8.572154116648857, Valid Loss: 8.613273614250055\n",
            "Epoch: 20, Step: 8700, Train Loss: 8.597325951183118, Valid Loss: 8.638364978386946\n",
            "Epoch: 21, Step: 8800, Train Loss: 8.570781137291162, Valid Loss: 8.611474830712847\n",
            "Epoch: 21, Step: 8900, Train Loss: 8.592259043063017, Valid Loss: 8.626294762344548\n",
            "Epoch: 21, Step: 9000, Train Loss: 8.573975999169264, Valid Loss: 8.614379425917619\n",
            "Epoch: 21, Step: 9100, Train Loss: 8.587412586827908, Valid Loss: 8.623825778794625\n",
            "Epoch: 22, Step: 9200, Train Loss: 8.58239892630089, Valid Loss: 8.612852798526612\n",
            "Epoch: 22, Step: 9300, Train Loss: 8.59216078951101, Valid Loss: 8.62946776711377\n",
            "Epoch: 22, Step: 9400, Train Loss: 8.575347226542716, Valid Loss: 8.615310399314467\n",
            "Epoch: 22, Step: 9500, Train Loss: 8.59653591128554, Valid Loss: 8.635107337286954\n",
            "Epoch: 22, Step: 9600, Train Loss: 8.570553851201796, Valid Loss: 8.610993293901576\n",
            "Epoch: 23, Step: 9700, Train Loss: 8.577787257401988, Valid Loss: 8.616902230238962\n",
            "Epoch: 23, Step: 9800, Train Loss: 8.576100767761933, Valid Loss: 8.610099027157677\n",
            "Epoch: 23, Step: 9900, Train Loss: 8.591945087127822, Valid Loss: 8.62362994900741\n",
            "Epoch: 23, Step: 10000, Train Loss: 8.582926054053363, Valid Loss: 8.616921572087357\n",
            "Epoch: 24, Step: 10100, Train Loss: 8.564066960596623, Valid Loss: 8.610018807357049\n",
            "Epoch: 24, Step: 10200, Train Loss: 8.566796317954214, Valid Loss: 8.604799714893073\n",
            "Epoch: 24, Step: 10300, Train Loss: 8.582623799943422, Valid Loss: 8.62603666917745\n",
            "Epoch: 24, Step: 10400, Train Loss: 8.567889372409143, Valid Loss: 8.613729885087578\n",
            "Epoch: 25, Step: 10500, Train Loss: 8.553027252872445, Valid Loss: 8.600257285491507\n",
            "Epoch: 25, Step: 10600, Train Loss: 8.563361112370917, Valid Loss: 8.613771372613497\n",
            "Epoch: 25, Step: 10700, Train Loss: 8.577572515816795, Valid Loss: 8.617488820793907\n",
            "Epoch: 25, Step: 10800, Train Loss: 8.572944506753366, Valid Loss: 8.614824943086063\n",
            "Epoch: 26, Step: 10900, Train Loss: 8.561155778568192, Valid Loss: 8.613987601317486\n",
            "Epoch: 26, Step: 11000, Train Loss: 8.582057701861162, Valid Loss: 8.634184886523355\n",
            "Epoch: 26, Step: 11100, Train Loss: 8.563839896328577, Valid Loss: 8.613434205192187\n",
            "Epoch: 26, Step: 11200, Train Loss: 8.557912462608906, Valid Loss: 8.612206839910666\n",
            "Epoch: 27, Step: 11300, Train Loss: 8.55987060463947, Valid Loss: 8.612541771523583\n",
            "Epoch: 27, Step: 11400, Train Loss: 8.55796555581388, Valid Loss: 8.606794067661426\n",
            "Epoch: 27, Step: 11500, Train Loss: 8.566837068632493, Valid Loss: 8.620249011817823\n",
            "Epoch: 27, Step: 11600, Train Loss: 8.549242731110553, Valid Loss: 8.603790204829233\n",
            "Epoch: 27, Step: 11700, Train Loss: 8.580628667354775, Valid Loss: 8.631060261464633\n",
            "Epoch: 28, Step: 11800, Train Loss: 8.571776689067288, Valid Loss: 8.627474898948227\n",
            "Epoch: 28, Step: 11900, Train Loss: 8.548638082502706, Valid Loss: 8.606456154630687\n",
            "Epoch: 28, Step: 12000, Train Loss: 8.555515764370908, Valid Loss: 8.614709432698596\n",
            "Epoch: 28, Step: 12100, Train Loss: 8.551899411760406, Valid Loss: 8.60579788875963\n",
            "Epoch: 29, Step: 12200, Train Loss: 8.555724646111242, Valid Loss: 8.605185305996999\n",
            "Epoch: 29, Step: 12300, Train Loss: 8.561447184946873, Valid Loss: 8.616146622620251\n",
            "Epoch: 29, Step: 12400, Train Loss: 8.557042853211561, Valid Loss: 8.613084508164848\n",
            "Epoch: 29, Step: 12500, Train Loss: 8.53956074429592, Valid Loss: 8.598183168212213\n",
            "Epoch: 30, Step: 12600, Train Loss: 8.529601465102424, Valid Loss: 8.594457286252078\n",
            "Epoch: 30, Step: 12700, Train Loss: 8.564735878981029, Valid Loss: 8.619358136458192\n",
            "Epoch: 30, Step: 12800, Train Loss: 8.553313720812177, Valid Loss: 8.611812387864854\n",
            "Epoch: 30, Step: 12900, Train Loss: 8.538245478478643, Valid Loss: 8.596866610127309\n",
            "Epoch: 31, Step: 13000, Train Loss: 8.551863978542125, Valid Loss: 8.614210088098499\n",
            "Epoch: 31, Step: 13100, Train Loss: 8.536478475490176, Valid Loss: 8.598593891058279\n",
            "Epoch: 31, Step: 13200, Train Loss: 8.54251151134655, Valid Loss: 8.60945152390045\n",
            "Epoch: 31, Step: 13300, Train Loss: 8.568602394560223, Valid Loss: 8.620021726253405\n",
            "Epoch: 32, Step: 13400, Train Loss: 8.559557252958061, Valid Loss: 8.615968495717174\n",
            "Epoch: 32, Step: 13500, Train Loss: 8.5526136091134, Valid Loss: 8.609268136188122\n",
            "Epoch: 32, Step: 13600, Train Loss: 8.546076561226704, Valid Loss: 8.607593824118347\n",
            "Epoch: 32, Step: 13700, Train Loss: 8.545004476928124, Valid Loss: 8.608198269755663\n",
            "Epoch: 33, Step: 13800, Train Loss: 8.537871309994651, Valid Loss: 8.599063039259967\n",
            "Epoch: 33, Step: 13900, Train Loss: 8.546959798476133, Valid Loss: 8.596521143651499\n",
            "Epoch: 33, Step: 14000, Train Loss: 8.549615153904218, Valid Loss: 8.60681630485247\n",
            "Epoch: 33, Step: 14100, Train Loss: 8.548921339616541, Valid Loss: 8.612704073079202\n",
            "Epoch: 33, Step: 14200, Train Loss: 8.544715939180811, Valid Loss: 8.604018195755309\n",
            "Epoch: 34, Step: 14300, Train Loss: 8.549923657445284, Valid Loss: 8.610434172836525\n",
            "Epoch: 34, Step: 14400, Train Loss: 8.53558540284403, Valid Loss: 8.595592444889867\n",
            "Epoch: 34, Step: 14500, Train Loss: 8.547623041779925, Valid Loss: 8.609288879718736\n",
            "Epoch: 34, Step: 14600, Train Loss: 8.53769394479358, Valid Loss: 8.594363690664805\n",
            "Epoch: 35, Step: 14700, Train Loss: 8.550160025336513, Valid Loss: 8.618120210282708\n",
            "Epoch: 35, Step: 14800, Train Loss: 8.549098251279059, Valid Loss: 8.61570506764049\n",
            "Epoch: 35, Step: 14900, Train Loss: 8.53851156471524, Valid Loss: 8.60439045821057\n",
            "Epoch: 35, Step: 15000, Train Loss: 8.538000942009763, Valid Loss: 8.596858900909233\n",
            "Epoch: 36, Step: 15100, Train Loss: 8.534351619652057, Valid Loss: 8.592463913158772\n",
            "Epoch: 36, Step: 15200, Train Loss: 8.553658551226043, Valid Loss: 8.613604824744932\n",
            "Epoch: 36, Step: 15300, Train Loss: 8.551848833484133, Valid Loss: 8.613548255926048\n",
            "Epoch: 36, Step: 15400, Train Loss: 8.52610034090645, Valid Loss: 8.598954201586999\n",
            "Epoch: 37, Step: 15500, Train Loss: 8.529885858857373, Valid Loss: 8.599024363348185\n",
            "Epoch: 37, Step: 15600, Train Loss: 8.528150523461644, Valid Loss: 8.591266919162038\n",
            "Epoch: 37, Step: 15700, Train Loss: 8.544123582040637, Valid Loss: 8.611668660155132\n",
            "Epoch: 37, Step: 15800, Train Loss: 8.547514245490483, Valid Loss: 8.614160884503963\n",
            "Epoch: 38, Step: 15900, Train Loss: 8.529682286067139, Valid Loss: 8.597532278834988\n",
            "Epoch: 38, Step: 16000, Train Loss: 8.532389589379472, Valid Loss: 8.594367135949282\n",
            "Epoch: 38, Step: 16100, Train Loss: 8.559110671190783, Valid Loss: 8.615272578570934\n",
            "Epoch: 38, Step: 16200, Train Loss: 8.56174419996213, Valid Loss: 8.612037222169576\n",
            "Epoch: 38, Step: 16300, Train Loss: 8.53326907956832, Valid Loss: 8.592884786256736\n",
            "Epoch: 39, Step: 16400, Train Loss: 8.530564090692623, Valid Loss: 8.593991341858276\n",
            "Epoch: 39, Step: 16500, Train Loss: 8.544023358767914, Valid Loss: 8.61080099985515\n",
            "Epoch: 39, Step: 16600, Train Loss: 8.54758837760664, Valid Loss: 8.608193227562962\n",
            "Epoch: 39, Step: 16700, Train Loss: 8.520113765783956, Valid Loss: 8.595128627550613\n",
            "Epoch: 40, Step: 16800, Train Loss: 8.53961734357737, Valid Loss: 8.608532706591893\n",
            "Epoch: 40, Step: 16900, Train Loss: 8.541319022728928, Valid Loss: 8.6031296101801\n",
            "Epoch: 40, Step: 17000, Train Loss: 8.533805725165603, Valid Loss: 8.602194566756555\n",
            "Epoch: 40, Step: 17100, Train Loss: 8.527395162786686, Valid Loss: 8.597871960803577\n",
            "Epoch: 41, Step: 17200, Train Loss: 8.513469369262092, Valid Loss: 8.590435528076583\n",
            "Epoch: 41, Step: 17300, Train Loss: 8.534962525325442, Valid Loss: 8.609599373577216\n",
            "Epoch: 41, Step: 17400, Train Loss: 8.535713781410411, Valid Loss: 8.606775011360886\n",
            "Epoch: 41, Step: 17500, Train Loss: 8.555028301385596, Valid Loss: 8.621915297252002\n",
            "Epoch: 42, Step: 17600, Train Loss: 8.53134536147951, Valid Loss: 8.60690130138562\n",
            "Epoch: 42, Step: 17700, Train Loss: 8.528274968013275, Valid Loss: 8.59602006140563\n",
            "Epoch: 42, Step: 17800, Train Loss: 8.522706316776551, Valid Loss: 8.596669046677832\n",
            "Epoch: 42, Step: 17900, Train Loss: 8.53294708643041, Valid Loss: 8.597383420635134\n",
            "Epoch: 43, Step: 18000, Train Loss: 8.50308934412766, Valid Loss: 8.584291130483464\n",
            "Epoch: 43, Step: 18100, Train Loss: 8.515196405186575, Valid Loss: 8.58597675646749\n",
            "Epoch: 43, Step: 18200, Train Loss: 8.52181058332777, Valid Loss: 8.596944304549318\n",
            "Epoch: 43, Step: 18300, Train Loss: 8.506572174200002, Valid Loss: 8.582043287570878\n",
            "Epoch: 44, Step: 18400, Train Loss: 8.522287513070756, Valid Loss: 8.59351728748535\n",
            "Epoch: 44, Step: 18500, Train Loss: 8.539536546219882, Valid Loss: 8.601411598954574\n",
            "Epoch: 44, Step: 18600, Train Loss: 8.524494879317047, Valid Loss: 8.590402929931244\n",
            "Epoch: 44, Step: 18700, Train Loss: 8.531561232878975, Valid Loss: 8.600793130806803\n",
            "Epoch: 44, Step: 18800, Train Loss: 8.515899575454808, Valid Loss: 8.580641834489933\n",
            "Epoch: 45, Step: 18900, Train Loss: 8.537246900192274, Valid Loss: 8.604346279626466\n",
            "Epoch: 45, Step: 19000, Train Loss: 8.551090241380745, Valid Loss: 8.614233826788759\n",
            "Epoch: 45, Step: 19100, Train Loss: 8.53170493896077, Valid Loss: 8.613037256668388\n",
            "Epoch: 45, Step: 19200, Train Loss: 8.503380652634407, Valid Loss: 8.586353605253914\n",
            "Epoch: 46, Step: 19300, Train Loss: 8.51881712382113, Valid Loss: 8.595850533407429\n",
            "Epoch: 46, Step: 19400, Train Loss: 8.507838427784673, Valid Loss: 8.58607067649398\n",
            "Epoch: 46, Step: 19500, Train Loss: 8.51556708660731, Valid Loss: 8.596422744372665\n",
            "Epoch: 46, Step: 19600, Train Loss: 8.511539308690066, Valid Loss: 8.592049446168954\n",
            "Epoch: 47, Step: 19700, Train Loss: 8.50800085673264, Valid Loss: 8.588160566065378\n",
            "Epoch: 47, Step: 19800, Train Loss: 8.51472068727728, Valid Loss: 8.583012217799054\n",
            "Epoch: 47, Step: 19900, Train Loss: 8.518230757232836, Valid Loss: 8.586313976853809\n",
            "Epoch: 47, Step: 20000, Train Loss: 8.524927616310743, Valid Loss: 8.597737663156343\n",
            "Epoch: 48, Step: 20100, Train Loss: 8.510962701480873, Valid Loss: 8.5826035981194\n",
            "Epoch: 48, Step: 20200, Train Loss: 8.510249421747126, Valid Loss: 8.589842669659694\n",
            "Epoch: 48, Step: 20300, Train Loss: 8.519725799603247, Valid Loss: 8.599318635836932\n",
            "Epoch: 48, Step: 20400, Train Loss: 8.525939914728529, Valid Loss: 8.605338250815235\n",
            "Epoch: 49, Step: 20500, Train Loss: 8.503913996200751, Valid Loss: 8.58019998885247\n",
            "Epoch: 49, Step: 20600, Train Loss: 8.518583767030028, Valid Loss: 8.59481743695586\n",
            "Epoch: 49, Step: 20700, Train Loss: 8.50988616518123, Valid Loss: 8.596601516354102\n",
            "Epoch: 49, Step: 20800, Train Loss: 8.5146886518495, Valid Loss: 8.586668918852896\n",
            "Epoch: 49, Step: 20900, Train Loss: 8.505186220017407, Valid Loss: 8.588666839293742\n",
            "Epoch: 50, Step: 21000, Train Loss: 8.514031395245462, Valid Loss: 8.595993239376638\n",
            "Epoch: 50, Step: 21100, Train Loss: 8.498902678852545, Valid Loss: 8.592398487037482\n",
            "Epoch: 50, Step: 21200, Train Loss: 8.50315144304797, Valid Loss: 8.582101497142284\n",
            "Epoch: 50, Step: 21300, Train Loss: 8.517267042673426, Valid Loss: 8.596847566116873\n",
            "Epoch: 51, Step: 21400, Train Loss: 8.516007847188696, Valid Loss: 8.602395274006483\n",
            "Epoch: 51, Step: 21500, Train Loss: 8.50870145495373, Valid Loss: 8.592706870737048\n",
            "Epoch: 51, Step: 21600, Train Loss: 8.49957850220084, Valid Loss: 8.59369727904492\n",
            "Epoch: 51, Step: 21700, Train Loss: 8.517608756427613, Valid Loss: 8.590308495860192\n",
            "Epoch: 52, Step: 21800, Train Loss: 8.514884193264647, Valid Loss: 8.594242928405533\n",
            "Epoch: 52, Step: 21900, Train Loss: 8.508983410627676, Valid Loss: 8.59317443855726\n",
            "Epoch: 52, Step: 22000, Train Loss: 8.516502582603735, Valid Loss: 8.604187871226271\n",
            "Epoch: 52, Step: 22100, Train Loss: 8.491669467042765, Valid Loss: 8.581330365587911\n",
            "Epoch: 53, Step: 22200, Train Loss: 8.501875972019624, Valid Loss: 8.584938662150153\n",
            "Epoch: 53, Step: 22300, Train Loss: 8.511329991384773, Valid Loss: 8.601493713281773\n",
            "Epoch: 53, Step: 22400, Train Loss: 8.521272231368826, Valid Loss: 8.603655027797457\n",
            "Epoch: 53, Step: 22500, Train Loss: 8.501558708034352, Valid Loss: 8.590401929700418\n",
            "Epoch: 54, Step: 22600, Train Loss: 8.501378111629736, Valid Loss: 8.59764241068817\n",
            "Epoch: 54, Step: 22700, Train Loss: 8.492633583442808, Valid Loss: 8.592324481930167\n",
            "Epoch: 54, Step: 22800, Train Loss: 8.501633684274731, Valid Loss: 8.599732813707893\n",
            "Epoch: 54, Step: 22900, Train Loss: 8.500015610454875, Valid Loss: 8.586953843882005\n",
            "Epoch: 55, Step: 23000, Train Loss: 8.525688342250175, Valid Loss: 8.614791346935553\n",
            "Epoch: 55, Step: 23100, Train Loss: 8.508504072694747, Valid Loss: 8.600634987528903\n",
            "Epoch: 55, Step: 23200, Train Loss: 8.515790723113751, Valid Loss: 8.592416168364757\n",
            "Epoch: 55, Step: 23300, Train Loss: 8.494311751631683, Valid Loss: 8.588119894010429\n",
            "Epoch: 55, Step: 23400, Train Loss: 8.503796047309462, Valid Loss: 8.596854016196138\n",
            "Epoch: 56, Step: 23500, Train Loss: 8.514785820735442, Valid Loss: 8.593430761683889\n",
            "Epoch: 56, Step: 23600, Train Loss: 8.486809677375792, Valid Loss: 8.580081324650745\n",
            "Epoch: 56, Step: 23700, Train Loss: 8.507792687005832, Valid Loss: 8.60948766038351\n",
            "Epoch: 56, Step: 23800, Train Loss: 8.503629093495363, Valid Loss: 8.595167068622871\n",
            "Epoch: 57, Step: 23900, Train Loss: 8.497038863728605, Valid Loss: 8.589636228638737\n",
            "Epoch: 57, Step: 24000, Train Loss: 8.510161564801175, Valid Loss: 8.599654662043969\n",
            "Epoch: 57, Step: 24100, Train Loss: 8.512011744242702, Valid Loss: 8.602013837356639\n",
            "Epoch: 57, Step: 24200, Train Loss: 8.493705542275352, Valid Loss: 8.592576676093396\n",
            "Epoch: 58, Step: 24300, Train Loss: 8.498720640275872, Valid Loss: 8.595121269602526\n",
            "Epoch: 58, Step: 24400, Train Loss: 8.488550237870719, Valid Loss: 8.590317091070661\n",
            "Epoch: 58, Step: 24500, Train Loss: 8.509716723827422, Valid Loss: 8.597484327032788\n",
            "Epoch: 58, Step: 24600, Train Loss: 8.514647959933047, Valid Loss: 8.593425379984055\n",
            "Epoch: 59, Step: 24700, Train Loss: 8.505156084978305, Valid Loss: 8.58938879296882\n",
            "Epoch: 59, Step: 24800, Train Loss: 8.4960650769762, Valid Loss: 8.581898487064766\n",
            "Epoch: 59, Step: 24900, Train Loss: 8.507814247297155, Valid Loss: 8.59570495852991\n",
            "Epoch: 59, Step: 25000, Train Loss: 8.493348150546222, Valid Loss: 8.586409865151543\n",
            "Epoch: 60, Step: 25100, Train Loss: 8.48428241022889, Valid Loss: 8.579319881924425\n",
            "Epoch: 60, Step: 25200, Train Loss: 8.512644533442977, Valid Loss: 8.601848325154831\n",
            "Epoch: 60, Step: 25300, Train Loss: 8.514075912482289, Valid Loss: 8.598268512162043\n",
            "Epoch: 60, Step: 25400, Train Loss: 8.519619929177276, Valid Loss: 8.608006179943317\n",
            "Epoch: 61, Step: 25500, Train Loss: 8.495673343133062, Valid Loss: 8.589903054963775\n",
            "Epoch: 61, Step: 25600, Train Loss: 8.493707117812779, Valid Loss: 8.589398602756864\n",
            "Epoch: 61, Step: 25700, Train Loss: 8.508811139669433, Valid Loss: 8.601482399287368\n",
            "Epoch: 61, Step: 25800, Train Loss: 8.501270482615226, Valid Loss: 8.602279839082865\n",
            "Epoch: 61, Step: 25900, Train Loss: 8.491921235007082, Valid Loss: 8.578027898652762\n",
            "Epoch: 62, Step: 26000, Train Loss: 8.494294783633181, Valid Loss: 8.592244007944243\n",
            "Epoch: 62, Step: 26100, Train Loss: 8.494929060048904, Valid Loss: 8.585033390784186\n",
            "Epoch: 62, Step: 26200, Train Loss: 8.506268110928982, Valid Loss: 8.596364693022544\n",
            "Epoch: 62, Step: 26300, Train Loss: 8.495596119661014, Valid Loss: 8.585273337838736\n",
            "Epoch: 63, Step: 26400, Train Loss: 8.510755230512741, Valid Loss: 8.599584537261297\n",
            "Epoch: 63, Step: 26500, Train Loss: 8.498689386496542, Valid Loss: 8.594622632044763\n",
            "Epoch: 63, Step: 26600, Train Loss: 8.476866156688255, Valid Loss: 8.579064941464283\n",
            "Epoch: 63, Step: 26700, Train Loss: 8.503560970411575, Valid Loss: 8.588866075167044\n",
            "Epoch: 64, Step: 26800, Train Loss: 8.47747624399816, Valid Loss: 8.579540955437388\n",
            "Epoch: 64, Step: 26900, Train Loss: 8.491244638247124, Valid Loss: 8.58795271870867\n",
            "Epoch: 64, Step: 27000, Train Loss: 8.500290360063708, Valid Loss: 8.593650555659815\n",
            "Epoch: 64, Step: 27100, Train Loss: 8.496440638177909, Valid Loss: 8.587540106238803\n",
            "Epoch: 65, Step: 27200, Train Loss: 8.50165072239808, Valid Loss: 8.586727450240808\n",
            "Epoch: 65, Step: 27300, Train Loss: 8.50818770987084, Valid Loss: 8.594968338995757\n",
            "Epoch: 65, Step: 27400, Train Loss: 8.4922808530671, Valid Loss: 8.59102980659582\n",
            "Epoch: 65, Step: 27500, Train Loss: 8.504641801891228, Valid Loss: 8.599548702015387\n",
            "Epoch: 66, Step: 27600, Train Loss: 8.485978596650964, Valid Loss: 8.586082052493872\n",
            "Epoch: 66, Step: 27700, Train Loss: 8.498111391675977, Valid Loss: 8.585989224090667\n",
            "Epoch: 66, Step: 27800, Train Loss: 8.512517510965473, Valid Loss: 8.598384804299155\n",
            "Epoch: 66, Step: 27900, Train Loss: 8.506163320529922, Valid Loss: 8.593598107340403\n",
            "Epoch: 66, Step: 28000, Train Loss: 8.479287271962876, Valid Loss: 8.577901046046394\n",
            "Epoch: 67, Step: 28100, Train Loss: 8.497333874985017, Valid Loss: 8.584481010687687\n",
            "Epoch: 67, Step: 28200, Train Loss: 8.498428323715094, Valid Loss: 8.596198594322315\n",
            "Epoch: 67, Step: 28300, Train Loss: 8.49083018878323, Valid Loss: 8.590089513669843\n",
            "Epoch: 67, Step: 28400, Train Loss: 8.501074164531953, Valid Loss: 8.594037362545006\n",
            "Epoch: 68, Step: 28500, Train Loss: 8.485600486603525, Valid Loss: 8.585850303379704\n",
            "Epoch: 68, Step: 28600, Train Loss: 8.499704652367287, Valid Loss: 8.586966135957098\n",
            "Epoch: 68, Step: 28700, Train Loss: 8.488743385875095, Valid Loss: 8.578746157039932\n",
            "Epoch: 68, Step: 28800, Train Loss: 8.486389174134668, Valid Loss: 8.584140693334112\n",
            "Epoch: 69, Step: 28900, Train Loss: 8.487241694594632, Valid Loss: 8.585621184890144\n",
            "Epoch: 69, Step: 29000, Train Loss: 8.469904828899205, Valid Loss: 8.575607879327023\n",
            "Epoch: 69, Step: 29100, Train Loss: 8.501982443760188, Valid Loss: 8.59175614538404\n",
            "Epoch: 69, Step: 29200, Train Loss: 8.48830526325401, Valid Loss: 8.580718917437123\n",
            "Epoch: 70, Step: 29300, Train Loss: 8.499561108638195, Valid Loss: 8.591610827141647\n",
            "Epoch: 70, Step: 29400, Train Loss: 8.473804870654414, Valid Loss: 8.572693456628361\n",
            "Epoch: 70, Step: 29500, Train Loss: 8.491060775136, Valid Loss: 8.588185323300554\n",
            "Epoch: 70, Step: 29600, Train Loss: 8.491420973604276, Valid Loss: 8.593653069995094\n",
            "Epoch: 71, Step: 29700, Train Loss: 8.488964367536905, Valid Loss: 8.589205570254933\n",
            "Epoch: 71, Step: 29800, Train Loss: 8.473785842461636, Valid Loss: 8.57989300880762\n",
            "Epoch: 71, Step: 29900, Train Loss: 8.470530045087926, Valid Loss: 8.571467805385632\n",
            "Epoch: 71, Step: 30000, Train Loss: 8.488445197869183, Valid Loss: 8.585952186636536\n",
            "Epoch: 72, Step: 30100, Train Loss: 8.47578846977856, Valid Loss: 8.5729359353859\n",
            "Epoch: 72, Step: 30200, Train Loss: 8.491499739070921, Valid Loss: 8.588442533411772\n",
            "Epoch: 72, Step: 30300, Train Loss: 8.476739139950729, Valid Loss: 8.578959448844893\n",
            "Epoch: 72, Step: 30400, Train Loss: 8.484736355701191, Valid Loss: 8.578364412693878\n",
            "Epoch: 72, Step: 30500, Train Loss: 8.48780049633967, Valid Loss: 8.582756866824555\n",
            "Epoch: 73, Step: 30600, Train Loss: 8.479868035243724, Valid Loss: 8.584956766056282\n",
            "Epoch: 73, Step: 30700, Train Loss: 8.500979694699504, Valid Loss: 8.594370777265514\n",
            "Epoch: 73, Step: 30800, Train Loss: 8.497231798127288, Valid Loss: 8.590855213552999\n",
            "Epoch: 73, Step: 30900, Train Loss: 8.502467506557446, Valid Loss: 8.592176474705385\n",
            "Epoch: 74, Step: 31000, Train Loss: 8.498514592948533, Valid Loss: 8.58180082564922\n",
            "Epoch: 74, Step: 31100, Train Loss: 8.479672062471995, Valid Loss: 8.575413772632599\n",
            "Epoch: 74, Step: 31200, Train Loss: 8.477690150656587, Valid Loss: 8.572274707330811\n",
            "Epoch: 74, Step: 31300, Train Loss: 8.47178938446812, Valid Loss: 8.57506692122229\n",
            "Epoch: 75, Step: 31400, Train Loss: 8.46966954650975, Valid Loss: 8.580740439278754\n",
            "Epoch: 75, Step: 31500, Train Loss: 8.489349707839915, Valid Loss: 8.590751169885399\n",
            "Epoch: 75, Step: 31600, Train Loss: 8.512219587303104, Valid Loss: 8.602828674839964\n",
            "Epoch: 75, Step: 31700, Train Loss: 8.47138853433773, Valid Loss: 8.574406965211299\n",
            "Epoch: 76, Step: 31800, Train Loss: 8.476919803053987, Valid Loss: 8.573877628107313\n",
            "Epoch: 76, Step: 31900, Train Loss: 8.484760554247437, Valid Loss: 8.582329745231485\n",
            "Epoch: 76, Step: 32000, Train Loss: 8.470799595400141, Valid Loss: 8.56951389028955\n",
            "Epoch: 76, Step: 32100, Train Loss: 8.494999565857144, Valid Loss: 8.587743657335125\n",
            "Epoch: 77, Step: 32200, Train Loss: 8.476465090047073, Valid Loss: 8.584359864074598\n",
            "Epoch: 77, Step: 32300, Train Loss: 8.470676765836968, Valid Loss: 8.577608764058432\n",
            "Epoch: 77, Step: 32400, Train Loss: 8.481622577982625, Valid Loss: 8.581575923111384\n",
            "Epoch: 77, Step: 32500, Train Loss: 8.47135849065401, Valid Loss: 8.582312322863817\n",
            "Epoch: 77, Step: 32600, Train Loss: 8.475345056299494, Valid Loss: 8.58060565260986\n",
            "Epoch: 78, Step: 32700, Train Loss: 8.468384942567484, Valid Loss: 8.572118246570048\n",
            "Epoch: 78, Step: 32800, Train Loss: 8.49214239822049, Valid Loss: 8.594575056065697\n",
            "Epoch: 78, Step: 32900, Train Loss: 8.4785474327705, Valid Loss: 8.586021537299288\n",
            "Epoch: 78, Step: 33000, Train Loss: 8.470915419021802, Valid Loss: 8.584790301899476\n",
            "Epoch: 79, Step: 33100, Train Loss: 8.496482617219593, Valid Loss: 8.599742621491973\n",
            "Epoch: 79, Step: 33200, Train Loss: 8.488285335258617, Valid Loss: 8.591977769048487\n",
            "Epoch: 79, Step: 33300, Train Loss: 8.480233845276802, Valid Loss: 8.583334401451983\n",
            "Epoch: 79, Step: 33400, Train Loss: 8.463994630311168, Valid Loss: 8.575616658720039\n",
            "Epoch: 80, Step: 33500, Train Loss: 8.474217523512083, Valid Loss: 8.582249528431111\n",
            "Epoch: 80, Step: 33600, Train Loss: 8.477652639407175, Valid Loss: 8.575762510750506\n",
            "Epoch: 80, Step: 33700, Train Loss: 8.483320336097051, Valid Loss: 8.58291742759863\n",
            "Epoch: 80, Step: 33800, Train Loss: 8.482213971836078, Valid Loss: 8.589037001317674\n",
            "Epoch: 81, Step: 33900, Train Loss: 8.492075274147899, Valid Loss: 8.589834522637123\n",
            "Epoch: 81, Step: 34000, Train Loss: 8.466561608742326, Valid Loss: 8.574542517373816\n",
            "Epoch: 81, Step: 34100, Train Loss: 8.47133030637327, Valid Loss: 8.572911193070016\n",
            "Epoch: 81, Step: 34200, Train Loss: 8.509358117893177, Valid Loss: 8.60541610858298\n",
            "Epoch: 82, Step: 34300, Train Loss: 8.498503576751943, Valid Loss: 8.589973668210485\n",
            "Epoch: 82, Step: 34400, Train Loss: 8.473591299188971, Valid Loss: 8.58184098041206\n",
            "Epoch: 82, Step: 34500, Train Loss: 8.480052570389415, Valid Loss: 8.58521901819641\n",
            "Epoch: 82, Step: 34600, Train Loss: 8.480655512096492, Valid Loss: 8.575236457337146\n",
            "Epoch: 83, Step: 34700, Train Loss: 8.493550782798799, Valid Loss: 8.586443167658564\n",
            "Epoch: 83, Step: 34800, Train Loss: 8.497640485363197, Valid Loss: 8.58115892271091\n",
            "Epoch: 83, Step: 34900, Train Loss: 8.498072289554724, Valid Loss: 8.589252930788227\n",
            "Epoch: 83, Step: 35000, Train Loss: 8.47246532543223, Valid Loss: 8.572465428561108\n",
            "Epoch: 83, Step: 35100, Train Loss: 8.478756879654858, Valid Loss: 8.579363506972767\n",
            "Epoch: 84, Step: 35200, Train Loss: 8.483932464731582, Valid Loss: 8.590038798989248\n",
            "Epoch: 84, Step: 35300, Train Loss: 8.494893912837117, Valid Loss: 8.593547690289043\n",
            "Epoch: 84, Step: 35400, Train Loss: 8.485509609976107, Valid Loss: 8.592105041299593\n",
            "Epoch: 84, Step: 35500, Train Loss: 8.469686289779078, Valid Loss: 8.580765782411358\n",
            "Epoch: 85, Step: 35600, Train Loss: 8.494102028801539, Valid Loss: 8.597459684758231\n",
            "Epoch: 85, Step: 35700, Train Loss: 8.465470705580964, Valid Loss: 8.577948682588351\n",
            "Epoch: 85, Step: 35800, Train Loss: 8.465925472899016, Valid Loss: 8.58619035675225\n",
            "Epoch: 85, Step: 35900, Train Loss: 8.484207655465127, Valid Loss: 8.587833928665383\n",
            "Epoch: 86, Step: 36000, Train Loss: 8.481837820704255, Valid Loss: 8.586946814926131\n",
            "Epoch: 86, Step: 36100, Train Loss: 8.489214402308214, Valid Loss: 8.591160056548778\n",
            "Epoch: 86, Step: 36200, Train Loss: 8.487542836188737, Valid Loss: 8.599585014802347\n",
            "Epoch: 86, Step: 36300, Train Loss: 8.475879616000679, Valid Loss: 8.585486369369\n",
            "Epoch: 87, Step: 36400, Train Loss: 8.466788142029177, Valid Loss: 8.575582498849396\n",
            "Epoch: 87, Step: 36500, Train Loss: 8.462119555026023, Valid Loss: 8.572200130498077\n",
            "Epoch: 87, Step: 36600, Train Loss: 8.488668703475179, Valid Loss: 8.603180357847712\n",
            "Epoch: 87, Step: 36700, Train Loss: 8.479317979383412, Valid Loss: 8.593186037974354\n",
            "Epoch: 88, Step: 36800, Train Loss: 8.472613427525951, Valid Loss: 8.580841359772982\n",
            "Epoch: 88, Step: 36900, Train Loss: 8.47346896656925, Valid Loss: 8.583223857827424\n",
            "Epoch: 88, Step: 37000, Train Loss: 8.469372055663374, Valid Loss: 8.58020178797234\n",
            "Epoch: 88, Step: 37100, Train Loss: 8.480562259795606, Valid Loss: 8.58926510923892\n",
            "Epoch: 88, Step: 37200, Train Loss: 8.484006722985756, Valid Loss: 8.583689932816782\n",
            "Epoch: 89, Step: 37300, Train Loss: 8.450038794413171, Valid Loss: 8.560179168363957\n",
            "Epoch: 89, Step: 37400, Train Loss: 8.47902099705417, Valid Loss: 8.583024753523482\n",
            "Epoch: 89, Step: 37500, Train Loss: 8.482909330927384, Valid Loss: 8.583236560087071\n",
            "Epoch: 89, Step: 37600, Train Loss: 8.475130831198701, Valid Loss: 8.58361346538649\n",
            "Epoch: 90, Step: 37700, Train Loss: 8.487273251342028, Valid Loss: 8.595251962288222\n",
            "Epoch: 90, Step: 37800, Train Loss: 8.46917512663951, Valid Loss: 8.579515284672022\n",
            "Epoch: 90, Step: 37900, Train Loss: 8.481772494372622, Valid Loss: 8.584775138973402\n",
            "Epoch: 90, Step: 38000, Train Loss: 8.457617466211529, Valid Loss: 8.571345285658742\n",
            "Epoch: 91, Step: 38100, Train Loss: 8.461956933372578, Valid Loss: 8.573013576466744\n",
            "Epoch: 91, Step: 38200, Train Loss: 8.45978037158026, Valid Loss: 8.573294209480117\n",
            "Epoch: 91, Step: 38300, Train Loss: 8.478752586823655, Valid Loss: 8.590739657884338\n",
            "Epoch: 91, Step: 38400, Train Loss: 8.495551807462281, Valid Loss: 8.602749917261296\n",
            "Epoch: 92, Step: 38500, Train Loss: 8.47262010025072, Valid Loss: 8.588607605121728\n",
            "Epoch: 92, Step: 38600, Train Loss: 8.471923605800642, Valid Loss: 8.58719069867324\n",
            "Epoch: 92, Step: 38700, Train Loss: 8.444975654076663, Valid Loss: 8.582044170987079\n",
            "Epoch: 92, Step: 38800, Train Loss: 8.45879050954572, Valid Loss: 8.576915713367327\n",
            "Epoch: 93, Step: 38900, Train Loss: 8.46167815880508, Valid Loss: 8.577181123383298\n",
            "Epoch: 93, Step: 39000, Train Loss: 8.464721144311387, Valid Loss: 8.573825660621848\n",
            "Epoch: 93, Step: 39100, Train Loss: 8.484629749727658, Valid Loss: 8.593372832262368\n",
            "Epoch: 93, Step: 39200, Train Loss: 8.460111453954779, Valid Loss: 8.581447014877057\n",
            "Epoch: 94, Step: 39300, Train Loss: 8.475053054992335, Valid Loss: 8.5897039755372\n",
            "Epoch: 94, Step: 39400, Train Loss: 8.47709508493693, Valid Loss: 8.5908312264048\n",
            "Epoch: 94, Step: 39500, Train Loss: 8.4550563272462, Valid Loss: 8.572794427634665\n",
            "Epoch: 94, Step: 39600, Train Loss: 8.465290514330526, Valid Loss: 8.576975899947007\n",
            "Epoch: 94, Step: 39700, Train Loss: 8.468648465551757, Valid Loss: 8.588103631019802\n",
            "Epoch: 95, Step: 39800, Train Loss: 8.47623880351273, Valid Loss: 8.59037822566666\n",
            "Epoch: 95, Step: 39900, Train Loss: 8.460657393909651, Valid Loss: 8.576895689548286\n",
            "Epoch: 95, Step: 40000, Train Loss: 8.463460153796207, Valid Loss: 8.584674723469265\n",
            "Epoch: 95, Step: 40100, Train Loss: 8.468906025552549, Valid Loss: 8.585630086500949\n",
            "Epoch: 96, Step: 40200, Train Loss: 8.48488212645676, Valid Loss: 8.60182936072576\n",
            "Epoch: 96, Step: 40300, Train Loss: 8.461398865936046, Valid Loss: 8.58034393688542\n",
            "Epoch: 96, Step: 40400, Train Loss: 8.485884268703995, Valid Loss: 8.595166485478309\n",
            "Epoch: 96, Step: 40500, Train Loss: 8.465200553265342, Valid Loss: 8.582343702730942\n",
            "Epoch: 97, Step: 40600, Train Loss: 8.478876760952465, Valid Loss: 8.598884548671952\n",
            "Epoch: 97, Step: 40700, Train Loss: 8.480280588423051, Valid Loss: 8.591076073574177\n",
            "Epoch: 97, Step: 40800, Train Loss: 8.48750935583165, Valid Loss: 8.599484980393301\n",
            "Epoch: 97, Step: 40900, Train Loss: 8.46955373955626, Valid Loss: 8.58676945083374\n",
            "Epoch: 98, Step: 41000, Train Loss: 8.465294478695393, Valid Loss: 8.582735131656744\n",
            "Epoch: 98, Step: 41100, Train Loss: 8.45114741522324, Valid Loss: 8.578011938145233\n",
            "Epoch: 98, Step: 41200, Train Loss: 8.469898632251999, Valid Loss: 8.584064234825936\n",
            "Epoch: 98, Step: 41300, Train Loss: 8.471585934224331, Valid Loss: 8.593405848012347\n",
            "Epoch: 99, Step: 41400, Train Loss: 8.460444238690513, Valid Loss: 8.578124662736824\n",
            "Epoch: 99, Step: 41500, Train Loss: 8.462100790074407, Valid Loss: 8.581993347414393\n",
            "Epoch: 99, Step: 41600, Train Loss: 8.469199598138898, Valid Loss: 8.579315551809618\n",
            "Epoch: 99, Step: 41700, Train Loss: 8.469294647908269, Valid Loss: 8.582574012917542\n",
            "Epoch: 99, Step: 41800, Train Loss: 8.44835183080585, Valid Loss: 8.568706069684543\n",
            "Test RMSE Loss for z = 0.5: 8.779484543662651\n",
            "Start training for z = 0.9\n",
            "Epoch: 0, Step: 100, Train Loss: 9.494006171034842, Valid Loss: 9.448323755702939\n",
            "Epoch: 0, Step: 200, Train Loss: 9.179156841233262, Valid Loss: 9.1152876560527\n",
            "Epoch: 0, Step: 300, Train Loss: 9.079232227295703, Valid Loss: 9.0132139851758\n",
            "Epoch: 0, Step: 400, Train Loss: 8.985776326627656, Valid Loss: 8.91956979756029\n",
            "Epoch: 1, Step: 500, Train Loss: 8.959552584317192, Valid Loss: 8.900519799239884\n",
            "Epoch: 1, Step: 600, Train Loss: 8.90676408023983, Valid Loss: 8.85406312771899\n",
            "Epoch: 1, Step: 700, Train Loss: 8.865035319710081, Valid Loss: 8.802384569093403\n",
            "Epoch: 1, Step: 800, Train Loss: 8.87743023198512, Valid Loss: 8.821787390226037\n",
            "Epoch: 2, Step: 900, Train Loss: 8.857689522444089, Valid Loss: 8.811718494414066\n",
            "Epoch: 2, Step: 1000, Train Loss: 8.83331595375595, Valid Loss: 8.784131792457298\n",
            "Epoch: 2, Step: 1100, Train Loss: 8.80211742465309, Valid Loss: 8.757598256679403\n",
            "Epoch: 2, Step: 1200, Train Loss: 8.814817044268356, Valid Loss: 8.769962630284736\n",
            "Epoch: 3, Step: 1300, Train Loss: 8.790916520104371, Valid Loss: 8.745135834451276\n",
            "Epoch: 3, Step: 1400, Train Loss: 8.789928491644622, Valid Loss: 8.746554504385536\n",
            "Epoch: 3, Step: 1500, Train Loss: 8.770642654052109, Valid Loss: 8.727834474675003\n",
            "Epoch: 3, Step: 1600, Train Loss: 8.780920118749705, Valid Loss: 8.745966964970046\n",
            "Epoch: 4, Step: 1700, Train Loss: 8.79928175018086, Valid Loss: 8.764527834398674\n",
            "Epoch: 4, Step: 1800, Train Loss: 8.773477275677774, Valid Loss: 8.741085766976534\n",
            "Epoch: 4, Step: 1900, Train Loss: 8.734458541254915, Valid Loss: 8.701489040523755\n",
            "Epoch: 4, Step: 2000, Train Loss: 8.737627713792618, Valid Loss: 8.710931607418269\n",
            "Epoch: 5, Step: 2100, Train Loss: 8.761176669380966, Valid Loss: 8.730987925649098\n",
            "Epoch: 5, Step: 2200, Train Loss: 8.766424671539772, Valid Loss: 8.742014330832895\n",
            "Epoch: 5, Step: 2300, Train Loss: 8.74845638747221, Valid Loss: 8.726265076030064\n",
            "Epoch: 5, Step: 2400, Train Loss: 8.736159689436024, Valid Loss: 8.714514845099815\n",
            "Epoch: 5, Step: 2500, Train Loss: 8.730618322350375, Valid Loss: 8.715293241059499\n",
            "Epoch: 6, Step: 2600, Train Loss: 8.712279901632105, Valid Loss: 8.694705396576298\n",
            "Epoch: 6, Step: 2700, Train Loss: 8.713094490232653, Valid Loss: 8.697978928298456\n",
            "Epoch: 6, Step: 2800, Train Loss: 8.716958926227012, Valid Loss: 8.705636225477264\n",
            "Epoch: 6, Step: 2900, Train Loss: 8.679771525556935, Valid Loss: 8.675381549741848\n",
            "Epoch: 7, Step: 3000, Train Loss: 8.712629017518902, Valid Loss: 8.704072139543179\n",
            "Epoch: 7, Step: 3100, Train Loss: 8.693954350355757, Valid Loss: 8.681056273881756\n",
            "Epoch: 7, Step: 3200, Train Loss: 8.691927863017398, Valid Loss: 8.68391449123925\n",
            "Epoch: 7, Step: 3300, Train Loss: 8.709920139606615, Valid Loss: 8.699839971209494\n",
            "Epoch: 8, Step: 3400, Train Loss: 8.672940369074047, Valid Loss: 8.665790756374543\n",
            "Epoch: 8, Step: 3500, Train Loss: 8.676907645960764, Valid Loss: 8.674386138894667\n",
            "Epoch: 8, Step: 3600, Train Loss: 8.668838765471149, Valid Loss: 8.663522389711074\n",
            "Epoch: 8, Step: 3700, Train Loss: 8.655669797707853, Valid Loss: 8.655243839924857\n",
            "Epoch: 9, Step: 3800, Train Loss: 8.667348840028996, Valid Loss: 8.666555789519075\n",
            "Epoch: 9, Step: 3900, Train Loss: 8.668252528156385, Valid Loss: 8.673389317208258\n",
            "Epoch: 9, Step: 4000, Train Loss: 8.687587130674528, Valid Loss: 8.68387856018904\n",
            "Epoch: 9, Step: 4100, Train Loss: 8.66317041949476, Valid Loss: 8.666152793093902\n",
            "Epoch: 10, Step: 4200, Train Loss: 8.647965280201788, Valid Loss: 8.654267529701135\n",
            "Epoch: 10, Step: 4300, Train Loss: 8.674808711209728, Valid Loss: 8.672245060266162\n",
            "Epoch: 10, Step: 4400, Train Loss: 8.66773612021427, Valid Loss: 8.67441903081241\n",
            "Epoch: 10, Step: 4500, Train Loss: 8.645757902220948, Valid Loss: 8.6492864083642\n",
            "Epoch: 11, Step: 4600, Train Loss: 8.657351355613585, Valid Loss: 8.65775792078146\n",
            "Epoch: 11, Step: 4700, Train Loss: 8.646360209348044, Valid Loss: 8.655148704489275\n",
            "Epoch: 11, Step: 4800, Train Loss: 8.638271111844979, Valid Loss: 8.639028558669004\n",
            "Epoch: 11, Step: 4900, Train Loss: 8.648502767637988, Valid Loss: 8.657002483133242\n",
            "Epoch: 11, Step: 5000, Train Loss: 8.6280565976708, Valid Loss: 8.641235247416269\n",
            "Epoch: 12, Step: 5100, Train Loss: 8.639835751780186, Valid Loss: 8.646921218795551\n",
            "Epoch: 12, Step: 5200, Train Loss: 8.634348120829854, Valid Loss: 8.641843266706749\n",
            "Epoch: 12, Step: 5300, Train Loss: 8.626342930420348, Valid Loss: 8.6338746767723\n",
            "Epoch: 12, Step: 5400, Train Loss: 8.615236240081728, Valid Loss: 8.624180625008199\n",
            "Epoch: 13, Step: 5500, Train Loss: 8.624049979447253, Valid Loss: 8.629536084746617\n",
            "Epoch: 13, Step: 5600, Train Loss: 8.637986268409694, Valid Loss: 8.641439261591787\n",
            "Epoch: 13, Step: 5700, Train Loss: 8.638468211775637, Valid Loss: 8.649047467762855\n",
            "Epoch: 13, Step: 5800, Train Loss: 8.608501163929722, Valid Loss: 8.62513215910348\n",
            "Epoch: 14, Step: 5900, Train Loss: 8.621126742113578, Valid Loss: 8.633247327707297\n",
            "Epoch: 14, Step: 6000, Train Loss: 8.653933487566471, Valid Loss: 8.658984379556632\n",
            "Epoch: 14, Step: 6100, Train Loss: 8.61274378187792, Valid Loss: 8.635478857029872\n",
            "Epoch: 14, Step: 6200, Train Loss: 8.627446040978532, Valid Loss: 8.646273918029303\n",
            "Epoch: 15, Step: 6300, Train Loss: 8.624418185381407, Valid Loss: 8.633183901163685\n",
            "Epoch: 15, Step: 6400, Train Loss: 8.610985780312996, Valid Loss: 8.624054556274883\n",
            "Epoch: 15, Step: 6500, Train Loss: 8.615236254748428, Valid Loss: 8.633620030041765\n",
            "Epoch: 15, Step: 6600, Train Loss: 8.621540259830567, Valid Loss: 8.63374190020711\n",
            "Epoch: 16, Step: 6700, Train Loss: 8.595171845923936, Valid Loss: 8.613590587833365\n",
            "Epoch: 16, Step: 6800, Train Loss: 8.617355396508014, Valid Loss: 8.628677167269126\n",
            "Epoch: 16, Step: 6900, Train Loss: 8.61480394261912, Valid Loss: 8.63204563004436\n",
            "Epoch: 16, Step: 7000, Train Loss: 8.623895081788156, Valid Loss: 8.632710754839586\n",
            "Epoch: 16, Step: 7100, Train Loss: 8.603638897560106, Valid Loss: 8.623651693439697\n",
            "Epoch: 17, Step: 7200, Train Loss: 8.60058768899856, Valid Loss: 8.61925361588929\n",
            "Epoch: 17, Step: 7300, Train Loss: 8.612872642076272, Valid Loss: 8.633045552060514\n",
            "Epoch: 17, Step: 7400, Train Loss: 8.603877147714295, Valid Loss: 8.627264312731917\n",
            "Epoch: 17, Step: 7500, Train Loss: 8.625285730775484, Valid Loss: 8.653846213000701\n",
            "Epoch: 18, Step: 7600, Train Loss: 8.615323475004727, Valid Loss: 8.641598995246316\n",
            "Epoch: 18, Step: 7700, Train Loss: 8.599421274865419, Valid Loss: 8.62974083673009\n",
            "Epoch: 18, Step: 7800, Train Loss: 8.584967134422676, Valid Loss: 8.616795636665506\n",
            "Epoch: 18, Step: 7900, Train Loss: 8.61694311447296, Valid Loss: 8.639758016418236\n",
            "Epoch: 19, Step: 8000, Train Loss: 8.610729897497855, Valid Loss: 8.626193475197903\n",
            "Epoch: 19, Step: 8100, Train Loss: 8.611185372668237, Valid Loss: 8.631630172452612\n",
            "Epoch: 19, Step: 8200, Train Loss: 8.584248089654134, Valid Loss: 8.60976262584793\n",
            "Epoch: 19, Step: 8300, Train Loss: 8.575077988231541, Valid Loss: 8.600137732424244\n",
            "Epoch: 20, Step: 8400, Train Loss: 8.60626025143373, Valid Loss: 8.634522053410677\n",
            "Epoch: 20, Step: 8500, Train Loss: 8.58820008857091, Valid Loss: 8.613767947314694\n",
            "Epoch: 20, Step: 8600, Train Loss: 8.584649574422773, Valid Loss: 8.616137173256035\n",
            "Epoch: 20, Step: 8700, Train Loss: 8.57673467349036, Valid Loss: 8.608762617041275\n",
            "Epoch: 21, Step: 8800, Train Loss: 8.602022886122027, Valid Loss: 8.633889302217588\n",
            "Epoch: 21, Step: 8900, Train Loss: 8.570235613246945, Valid Loss: 8.599447540658927\n",
            "Epoch: 21, Step: 9000, Train Loss: 8.577919047893145, Valid Loss: 8.604153631425309\n",
            "Epoch: 21, Step: 9100, Train Loss: 8.581219918698046, Valid Loss: 8.610268661067176\n",
            "Epoch: 22, Step: 9200, Train Loss: 8.584714379840882, Valid Loss: 8.613618643548492\n",
            "Epoch: 22, Step: 9300, Train Loss: 8.592029521944744, Valid Loss: 8.622833401720227\n",
            "Epoch: 22, Step: 9400, Train Loss: 8.567068583368776, Valid Loss: 8.602876263926325\n",
            "Epoch: 22, Step: 9500, Train Loss: 8.588565580699843, Valid Loss: 8.615263433807025\n",
            "Epoch: 22, Step: 9600, Train Loss: 8.583195678935612, Valid Loss: 8.619974722893868\n",
            "Epoch: 23, Step: 9700, Train Loss: 8.567705142896013, Valid Loss: 8.605005207204588\n",
            "Epoch: 23, Step: 9800, Train Loss: 8.572916996264523, Valid Loss: 8.612755121329435\n",
            "Epoch: 23, Step: 9900, Train Loss: 8.585453511325172, Valid Loss: 8.625264926591301\n",
            "Epoch: 23, Step: 10000, Train Loss: 8.568811005622681, Valid Loss: 8.60455539040292\n",
            "Epoch: 24, Step: 10100, Train Loss: 8.579427872564816, Valid Loss: 8.618962863287242\n",
            "Epoch: 24, Step: 10200, Train Loss: 8.596207148901195, Valid Loss: 8.629239177077219\n",
            "Epoch: 24, Step: 10300, Train Loss: 8.5822196997539, Valid Loss: 8.617104987548656\n",
            "Epoch: 24, Step: 10400, Train Loss: 8.5758736589479, Valid Loss: 8.61101930126526\n",
            "Epoch: 25, Step: 10500, Train Loss: 8.573972435475637, Valid Loss: 8.605525858108102\n",
            "Epoch: 25, Step: 10600, Train Loss: 8.56610605570962, Valid Loss: 8.609349451520542\n",
            "Epoch: 25, Step: 10700, Train Loss: 8.569758909264868, Valid Loss: 8.613334612803467\n",
            "Epoch: 25, Step: 10800, Train Loss: 8.565935889372161, Valid Loss: 8.612628249596392\n",
            "Epoch: 26, Step: 10900, Train Loss: 8.569887979751288, Valid Loss: 8.61592481891547\n",
            "Epoch: 26, Step: 11000, Train Loss: 8.560739314794583, Valid Loss: 8.609380864933163\n",
            "Epoch: 26, Step: 11100, Train Loss: 8.567992516189054, Valid Loss: 8.61030258549133\n",
            "Epoch: 26, Step: 11200, Train Loss: 8.56318955416977, Valid Loss: 8.595946310424372\n",
            "Epoch: 27, Step: 11300, Train Loss: 8.559987349240874, Valid Loss: 8.598779316350054\n",
            "Epoch: 27, Step: 11400, Train Loss: 8.552102059509771, Valid Loss: 8.60358903566739\n",
            "Epoch: 27, Step: 11500, Train Loss: 8.559782307057626, Valid Loss: 8.595594765101962\n",
            "Epoch: 27, Step: 11600, Train Loss: 8.559417501690744, Valid Loss: 8.60484101541641\n",
            "Epoch: 27, Step: 11700, Train Loss: 8.566728163457181, Valid Loss: 8.618496203377692\n",
            "Epoch: 28, Step: 11800, Train Loss: 8.555010646935267, Valid Loss: 8.603520854690986\n",
            "Epoch: 28, Step: 11900, Train Loss: 8.556316899109566, Valid Loss: 8.613747481064838\n",
            "Epoch: 28, Step: 12000, Train Loss: 8.567660393045616, Valid Loss: 8.615966733397098\n",
            "Epoch: 28, Step: 12100, Train Loss: 8.564949091203433, Valid Loss: 8.613357554387473\n",
            "Epoch: 29, Step: 12200, Train Loss: 8.577404755297797, Valid Loss: 8.625884626041277\n",
            "Epoch: 29, Step: 12300, Train Loss: 8.55881825461992, Valid Loss: 8.609505372659655\n",
            "Epoch: 29, Step: 12400, Train Loss: 8.544639408415675, Valid Loss: 8.604900250422448\n",
            "Epoch: 29, Step: 12500, Train Loss: 8.57030606032917, Valid Loss: 8.615274847037998\n",
            "Epoch: 30, Step: 12600, Train Loss: 8.539673682175287, Valid Loss: 8.591132736433039\n",
            "Epoch: 30, Step: 12700, Train Loss: 8.53531773429054, Valid Loss: 8.585245084014199\n",
            "Epoch: 30, Step: 12800, Train Loss: 8.561640586253926, Valid Loss: 8.6118686672976\n",
            "Epoch: 30, Step: 12900, Train Loss: 8.569661500318565, Valid Loss: 8.618509181087584\n",
            "Epoch: 31, Step: 13000, Train Loss: 8.558885170475827, Valid Loss: 8.61435510578299\n",
            "Epoch: 31, Step: 13100, Train Loss: 8.55559670647613, Valid Loss: 8.604702364798174\n",
            "Epoch: 31, Step: 13200, Train Loss: 8.533316154391516, Valid Loss: 8.594887590831735\n",
            "Epoch: 31, Step: 13300, Train Loss: 8.546561183893575, Valid Loss: 8.603206310497061\n",
            "Epoch: 32, Step: 13400, Train Loss: 8.540589332856515, Valid Loss: 8.599762185600138\n",
            "Epoch: 32, Step: 13500, Train Loss: 8.53734612669045, Valid Loss: 8.598455597714555\n",
            "Epoch: 32, Step: 13600, Train Loss: 8.562674788165056, Valid Loss: 8.622149762761014\n",
            "Epoch: 32, Step: 13700, Train Loss: 8.538918765380188, Valid Loss: 8.591892110146736\n",
            "Epoch: 33, Step: 13800, Train Loss: 8.556805452533116, Valid Loss: 8.609828306489408\n",
            "Epoch: 33, Step: 13900, Train Loss: 8.544830738424814, Valid Loss: 8.595148162957981\n",
            "Epoch: 33, Step: 14000, Train Loss: 8.564447453818104, Valid Loss: 8.614787919597692\n",
            "Epoch: 33, Step: 14100, Train Loss: 8.5446601201844, Valid Loss: 8.604573861769651\n",
            "Epoch: 33, Step: 14200, Train Loss: 8.569752415660812, Valid Loss: 8.627786498240418\n",
            "Epoch: 34, Step: 14300, Train Loss: 8.522764875176735, Valid Loss: 8.594424696009835\n",
            "Epoch: 34, Step: 14400, Train Loss: 8.554182822205796, Valid Loss: 8.612574909409458\n",
            "Epoch: 34, Step: 14500, Train Loss: 8.54431279707725, Valid Loss: 8.600963092717006\n",
            "Epoch: 34, Step: 14600, Train Loss: 8.54520124637096, Valid Loss: 8.607854180422372\n",
            "Epoch: 35, Step: 14700, Train Loss: 8.54835071497026, Valid Loss: 8.608567547907892\n",
            "Epoch: 35, Step: 14800, Train Loss: 8.539643095060361, Valid Loss: 8.603438368058516\n",
            "Epoch: 35, Step: 14900, Train Loss: 8.530953720983613, Valid Loss: 8.59426869975612\n",
            "Epoch: 35, Step: 15000, Train Loss: 8.556267580876504, Valid Loss: 8.613868688670815\n",
            "Epoch: 36, Step: 15100, Train Loss: 8.566027729859808, Valid Loss: 8.629999221767598\n",
            "Epoch: 36, Step: 15200, Train Loss: 8.533838505385587, Valid Loss: 8.602388585717165\n",
            "Epoch: 36, Step: 15300, Train Loss: 8.538929183837388, Valid Loss: 8.606451862638542\n",
            "Epoch: 36, Step: 15400, Train Loss: 8.534172077202264, Valid Loss: 8.594020449113595\n",
            "Epoch: 37, Step: 15500, Train Loss: 8.535859194226802, Valid Loss: 8.602572558292174\n",
            "Epoch: 37, Step: 15600, Train Loss: 8.55419619056873, Valid Loss: 8.612696294900415\n",
            "Epoch: 37, Step: 15700, Train Loss: 8.51726897264932, Valid Loss: 8.585240074948262\n",
            "Epoch: 37, Step: 15800, Train Loss: 8.538561426443707, Valid Loss: 8.609458695926888\n",
            "Epoch: 38, Step: 15900, Train Loss: 8.52942372684412, Valid Loss: 8.585652128311514\n",
            "Epoch: 38, Step: 16000, Train Loss: 8.542653694606757, Valid Loss: 8.60170024186838\n",
            "Epoch: 38, Step: 16100, Train Loss: 8.552517232248443, Valid Loss: 8.611599149206215\n",
            "Epoch: 38, Step: 16200, Train Loss: 8.532278574085675, Valid Loss: 8.600189406234849\n",
            "Epoch: 38, Step: 16300, Train Loss: 8.549254849538706, Valid Loss: 8.60291916242239\n",
            "Epoch: 39, Step: 16400, Train Loss: 8.538108178991642, Valid Loss: 8.597321395216241\n",
            "Epoch: 39, Step: 16500, Train Loss: 8.527372444350542, Valid Loss: 8.58722525626065\n",
            "Epoch: 39, Step: 16600, Train Loss: 8.531821198460628, Valid Loss: 8.600911991618418\n",
            "Epoch: 39, Step: 16700, Train Loss: 8.523285694641638, Valid Loss: 8.593903514688057\n",
            "Epoch: 40, Step: 16800, Train Loss: 8.556099314840987, Valid Loss: 8.61365317579083\n",
            "Epoch: 40, Step: 16900, Train Loss: 8.526940226592357, Valid Loss: 8.591943524708896\n",
            "Epoch: 40, Step: 17000, Train Loss: 8.522777894170567, Valid Loss: 8.600717373720157\n",
            "Epoch: 40, Step: 17100, Train Loss: 8.526124153373923, Valid Loss: 8.604105195177072\n",
            "Epoch: 41, Step: 17200, Train Loss: 8.524959543818246, Valid Loss: 8.59744383626004\n",
            "Epoch: 41, Step: 17300, Train Loss: 8.524188441552534, Valid Loss: 8.596543809182478\n",
            "Epoch: 41, Step: 17400, Train Loss: 8.526681828185403, Valid Loss: 8.60514443296435\n",
            "Epoch: 41, Step: 17500, Train Loss: 8.50939074975535, Valid Loss: 8.577669692151554\n",
            "Epoch: 42, Step: 17600, Train Loss: 8.532187881435528, Valid Loss: 8.599456239387258\n",
            "Epoch: 42, Step: 17700, Train Loss: 8.53606407914682, Valid Loss: 8.596450956560112\n",
            "Epoch: 42, Step: 17800, Train Loss: 8.519435446326833, Valid Loss: 8.590400892696275\n",
            "Epoch: 42, Step: 17900, Train Loss: 8.51443643913528, Valid Loss: 8.589605856122397\n",
            "Epoch: 43, Step: 18000, Train Loss: 8.511441071799599, Valid Loss: 8.58440708697355\n",
            "Epoch: 43, Step: 18100, Train Loss: 8.525802559240292, Valid Loss: 8.595660603002973\n",
            "Epoch: 43, Step: 18200, Train Loss: 8.530037569798237, Valid Loss: 8.593681815705793\n",
            "Epoch: 43, Step: 18300, Train Loss: 8.536404446996118, Valid Loss: 8.594007608410317\n",
            "Epoch: 44, Step: 18400, Train Loss: 8.507551598351709, Valid Loss: 8.583914978691904\n",
            "Epoch: 44, Step: 18500, Train Loss: 8.518539663408804, Valid Loss: 8.578558872417458\n",
            "Epoch: 44, Step: 18600, Train Loss: 8.506184495151963, Valid Loss: 8.574570327529916\n",
            "Epoch: 44, Step: 18700, Train Loss: 8.525291230222606, Valid Loss: 8.602624543069792\n",
            "Epoch: 44, Step: 18800, Train Loss: 8.523757913433606, Valid Loss: 8.591998372942253\n",
            "Epoch: 45, Step: 18900, Train Loss: 8.512887303006842, Valid Loss: 8.578533382833623\n",
            "Epoch: 45, Step: 19000, Train Loss: 8.50355243950926, Valid Loss: 8.57858628658533\n",
            "Epoch: 45, Step: 19100, Train Loss: 8.541126366703734, Valid Loss: 8.605490111231669\n",
            "Epoch: 45, Step: 19200, Train Loss: 8.54079745389744, Valid Loss: 8.603094169129882\n",
            "Epoch: 46, Step: 19300, Train Loss: 8.525129544617748, Valid Loss: 8.601930831931245\n",
            "Epoch: 46, Step: 19400, Train Loss: 8.525256677532953, Valid Loss: 8.593594191225588\n",
            "Epoch: 46, Step: 19500, Train Loss: 8.51329968039032, Valid Loss: 8.586998710361497\n",
            "Epoch: 46, Step: 19600, Train Loss: 8.516992392341978, Valid Loss: 8.58974041084961\n",
            "Epoch: 47, Step: 19700, Train Loss: 8.524051551686638, Valid Loss: 8.597458097460192\n",
            "Epoch: 47, Step: 19800, Train Loss: 8.504645233405874, Valid Loss: 8.579727678611057\n",
            "Epoch: 47, Step: 19900, Train Loss: 8.512310133420508, Valid Loss: 8.58498939714676\n",
            "Epoch: 47, Step: 20000, Train Loss: 8.507842754345923, Valid Loss: 8.581030896641174\n",
            "Epoch: 48, Step: 20100, Train Loss: 8.531486597266209, Valid Loss: 8.599984114749287\n",
            "Epoch: 48, Step: 20200, Train Loss: 8.508661353721083, Valid Loss: 8.576773483081466\n",
            "Epoch: 48, Step: 20300, Train Loss: 8.51128467818027, Valid Loss: 8.576838667446728\n",
            "Epoch: 48, Step: 20400, Train Loss: 8.530841838068945, Valid Loss: 8.591146459010782\n",
            "Epoch: 49, Step: 20500, Train Loss: 8.502496911919744, Valid Loss: 8.57851392251768\n",
            "Epoch: 49, Step: 20600, Train Loss: 8.523946054797356, Valid Loss: 8.592080839495647\n",
            "Epoch: 49, Step: 20700, Train Loss: 8.525563931851643, Valid Loss: 8.5943839482316\n",
            "Epoch: 49, Step: 20800, Train Loss: 8.513719959636353, Valid Loss: 8.579373703792617\n",
            "Epoch: 49, Step: 20900, Train Loss: 8.53051183994472, Valid Loss: 8.598183271083974\n",
            "Epoch: 50, Step: 21000, Train Loss: 8.517945208181963, Valid Loss: 8.591864343802156\n",
            "Epoch: 50, Step: 21100, Train Loss: 8.49295456775717, Valid Loss: 8.571430543023764\n",
            "Epoch: 50, Step: 21200, Train Loss: 8.50466140982021, Valid Loss: 8.582403117549804\n",
            "Epoch: 50, Step: 21300, Train Loss: 8.531664354259062, Valid Loss: 8.600549909478513\n",
            "Epoch: 51, Step: 21400, Train Loss: 8.49799369542363, Valid Loss: 8.572865830334388\n",
            "Epoch: 51, Step: 21500, Train Loss: 8.5086848049583, Valid Loss: 8.585080428451478\n",
            "Epoch: 51, Step: 21600, Train Loss: 8.536100385792396, Valid Loss: 8.603010356511602\n",
            "Epoch: 51, Step: 21700, Train Loss: 8.517886660051875, Valid Loss: 8.592984956320736\n",
            "Epoch: 52, Step: 21800, Train Loss: 8.506816983939512, Valid Loss: 8.584421306046895\n",
            "Epoch: 52, Step: 21900, Train Loss: 8.526370757223168, Valid Loss: 8.605667552008779\n",
            "Epoch: 52, Step: 22000, Train Loss: 8.536568956995444, Valid Loss: 8.614731752131874\n",
            "Epoch: 52, Step: 22100, Train Loss: 8.520078483528696, Valid Loss: 8.602402921956125\n",
            "Epoch: 53, Step: 22200, Train Loss: 8.50444492715025, Valid Loss: 8.590742521177354\n",
            "Epoch: 53, Step: 22300, Train Loss: 8.515276151857135, Valid Loss: 8.593239736233876\n",
            "Epoch: 53, Step: 22400, Train Loss: 8.516393387104213, Valid Loss: 8.587387690508837\n",
            "Epoch: 53, Step: 22500, Train Loss: 8.517057541670585, Valid Loss: 8.595661426218548\n",
            "Epoch: 54, Step: 22600, Train Loss: 8.495386201026445, Valid Loss: 8.58435075507158\n",
            "Epoch: 54, Step: 22700, Train Loss: 8.507123964929555, Valid Loss: 8.584395409456087\n",
            "Epoch: 54, Step: 22800, Train Loss: 8.521728348826421, Valid Loss: 8.597002178059453\n",
            "Epoch: 54, Step: 22900, Train Loss: 8.501580662860828, Valid Loss: 8.578842969484016\n",
            "Epoch: 55, Step: 23000, Train Loss: 8.487157215341648, Valid Loss: 8.575977247025119\n",
            "Epoch: 55, Step: 23100, Train Loss: 8.501014802711794, Valid Loss: 8.584685090609375\n",
            "Epoch: 55, Step: 23200, Train Loss: 8.512591372526494, Valid Loss: 8.588218074541233\n",
            "Epoch: 55, Step: 23300, Train Loss: 8.525990820154401, Valid Loss: 8.589600896183187\n",
            "Epoch: 55, Step: 23400, Train Loss: 8.505973676378813, Valid Loss: 8.585453563554319\n",
            "Epoch: 56, Step: 23500, Train Loss: 8.521972086704425, Valid Loss: 8.58910277669761\n",
            "Epoch: 56, Step: 23600, Train Loss: 8.514453468678132, Valid Loss: 8.587694407549668\n",
            "Epoch: 56, Step: 23700, Train Loss: 8.514261940284433, Valid Loss: 8.591816090740933\n",
            "Epoch: 56, Step: 23800, Train Loss: 8.5117847615266, Valid Loss: 8.582277357782889\n",
            "Epoch: 57, Step: 23900, Train Loss: 8.494936617625244, Valid Loss: 8.575330573582518\n",
            "Epoch: 57, Step: 24000, Train Loss: 8.494696635882999, Valid Loss: 8.571825364475377\n",
            "Epoch: 57, Step: 24100, Train Loss: 8.517199964681497, Valid Loss: 8.596588689305287\n",
            "Epoch: 57, Step: 24200, Train Loss: 8.51372036475663, Valid Loss: 8.59800969772863\n",
            "Epoch: 58, Step: 24300, Train Loss: 8.492536326140353, Valid Loss: 8.57136502023434\n",
            "Epoch: 58, Step: 24400, Train Loss: 8.50729664302281, Valid Loss: 8.581624383448668\n",
            "Epoch: 58, Step: 24500, Train Loss: 8.515903329413794, Valid Loss: 8.59628616530652\n",
            "Epoch: 58, Step: 24600, Train Loss: 8.49297111992506, Valid Loss: 8.5718575148745\n",
            "Epoch: 59, Step: 24700, Train Loss: 8.50272331340132, Valid Loss: 8.571603833989787\n",
            "Epoch: 59, Step: 24800, Train Loss: 8.514576486779884, Valid Loss: 8.591822417138113\n",
            "Epoch: 59, Step: 24900, Train Loss: 8.514827368570927, Valid Loss: 8.58441181194774\n",
            "Epoch: 59, Step: 25000, Train Loss: 8.507385457569919, Valid Loss: 8.582995699764275\n",
            "Epoch: 60, Step: 25100, Train Loss: 8.502133217065131, Valid Loss: 8.568879231420599\n",
            "Epoch: 60, Step: 25200, Train Loss: 8.505628200796036, Valid Loss: 8.57025260918944\n",
            "Epoch: 60, Step: 25300, Train Loss: 8.49692043413388, Valid Loss: 8.574668948105325\n",
            "Epoch: 60, Step: 25400, Train Loss: 8.517836608894303, Valid Loss: 8.59088680245355\n",
            "Epoch: 61, Step: 25500, Train Loss: 8.50000703580233, Valid Loss: 8.579681325889\n",
            "Epoch: 61, Step: 25600, Train Loss: 8.498299936165512, Valid Loss: 8.574398887030531\n",
            "Epoch: 61, Step: 25700, Train Loss: 8.486385576417943, Valid Loss: 8.571557368541912\n",
            "Epoch: 61, Step: 25800, Train Loss: 8.506200710452294, Valid Loss: 8.583509147472038\n",
            "Epoch: 61, Step: 25900, Train Loss: 8.50996406847693, Valid Loss: 8.587201393857974\n",
            "Epoch: 62, Step: 26000, Train Loss: 8.492178728973055, Valid Loss: 8.571378091417266\n",
            "Epoch: 62, Step: 26100, Train Loss: 8.498116995300933, Valid Loss: 8.578210250437726\n",
            "Epoch: 62, Step: 26200, Train Loss: 8.491047956082921, Valid Loss: 8.582067002356451\n",
            "Epoch: 62, Step: 26300, Train Loss: 8.509365661829422, Valid Loss: 8.590542696334905\n",
            "Epoch: 63, Step: 26400, Train Loss: 8.499480159828765, Valid Loss: 8.576974669799618\n",
            "Epoch: 63, Step: 26500, Train Loss: 8.481991302801944, Valid Loss: 8.56170298504098\n",
            "Epoch: 63, Step: 26600, Train Loss: 8.501238713588034, Valid Loss: 8.579402310790643\n",
            "Epoch: 63, Step: 26700, Train Loss: 8.492362018738032, Valid Loss: 8.578661409352188\n",
            "Epoch: 64, Step: 26800, Train Loss: 8.479510129283959, Valid Loss: 8.5632704638483\n",
            "Epoch: 64, Step: 26900, Train Loss: 8.485036949451619, Valid Loss: 8.568555469510915\n",
            "Epoch: 64, Step: 27000, Train Loss: 8.492799038565897, Valid Loss: 8.569904529065354\n",
            "Epoch: 64, Step: 27100, Train Loss: 8.496311486356637, Valid Loss: 8.571879203918678\n",
            "Epoch: 65, Step: 27200, Train Loss: 8.500091816239467, Valid Loss: 8.579551969443692\n",
            "Epoch: 65, Step: 27300, Train Loss: 8.486950766514497, Valid Loss: 8.565985464944648\n",
            "Epoch: 65, Step: 27400, Train Loss: 8.49224103111809, Valid Loss: 8.563057233781263\n",
            "Epoch: 65, Step: 27500, Train Loss: 8.490088252099161, Valid Loss: 8.56811179867172\n",
            "Epoch: 66, Step: 27600, Train Loss: 8.49809619157342, Valid Loss: 8.571152733570532\n",
            "Epoch: 66, Step: 27700, Train Loss: 8.48675372538886, Valid Loss: 8.574639971644112\n",
            "Epoch: 66, Step: 27800, Train Loss: 8.504190670296168, Valid Loss: 8.584303254641204\n",
            "Epoch: 66, Step: 27900, Train Loss: 8.471141602745677, Valid Loss: 8.562836261014837\n",
            "Epoch: 66, Step: 28000, Train Loss: 8.483091311042568, Valid Loss: 8.562261923545751\n",
            "Epoch: 67, Step: 28100, Train Loss: 8.488271581082731, Valid Loss: 8.574882294663185\n",
            "Epoch: 67, Step: 28200, Train Loss: 8.500839810895828, Valid Loss: 8.578628601987061\n",
            "Epoch: 67, Step: 28300, Train Loss: 8.501027921351348, Valid Loss: 8.574701949910901\n",
            "Epoch: 67, Step: 28400, Train Loss: 8.49300140261915, Valid Loss: 8.58024788507464\n",
            "Epoch: 68, Step: 28500, Train Loss: 8.48454777207586, Valid Loss: 8.56823147907317\n",
            "Epoch: 68, Step: 28600, Train Loss: 8.483343411648105, Valid Loss: 8.573493822242657\n",
            "Epoch: 68, Step: 28700, Train Loss: 8.481904659983618, Valid Loss: 8.571325801651955\n",
            "Epoch: 68, Step: 28800, Train Loss: 8.491639749729963, Valid Loss: 8.566755551406361\n",
            "Epoch: 69, Step: 28900, Train Loss: 8.492685319184401, Valid Loss: 8.573841680504191\n",
            "Epoch: 69, Step: 29000, Train Loss: 8.497315076271773, Valid Loss: 8.56927866289606\n",
            "Epoch: 69, Step: 29100, Train Loss: 8.48618613677819, Valid Loss: 8.567204621826974\n",
            "Epoch: 69, Step: 29200, Train Loss: 8.49520616724408, Valid Loss: 8.580115824965146\n",
            "Epoch: 70, Step: 29300, Train Loss: 8.485565430358141, Valid Loss: 8.57715002417941\n",
            "Epoch: 70, Step: 29400, Train Loss: 8.497306387642592, Valid Loss: 8.577355096133054\n",
            "Epoch: 70, Step: 29500, Train Loss: 8.48130344368652, Valid Loss: 8.564851036911742\n",
            "Epoch: 70, Step: 29600, Train Loss: 8.496653508070466, Valid Loss: 8.576434392145547\n",
            "Epoch: 71, Step: 29700, Train Loss: 8.483666573375427, Valid Loss: 8.57868260739964\n",
            "Epoch: 71, Step: 29800, Train Loss: 8.482648281511745, Valid Loss: 8.574250747221072\n",
            "Epoch: 71, Step: 29900, Train Loss: 8.481315111262935, Valid Loss: 8.571073537504619\n",
            "Epoch: 71, Step: 30000, Train Loss: 8.48326553089455, Valid Loss: 8.574073231026302\n",
            "Epoch: 72, Step: 30100, Train Loss: 8.506200172656975, Valid Loss: 8.58535393603527\n",
            "Epoch: 72, Step: 30200, Train Loss: 8.495617545922503, Valid Loss: 8.582387621592215\n",
            "Epoch: 72, Step: 30300, Train Loss: 8.480475415026367, Valid Loss: 8.577905305676827\n",
            "Epoch: 72, Step: 30400, Train Loss: 8.504219982477297, Valid Loss: 8.5845368756229\n",
            "Epoch: 72, Step: 30500, Train Loss: 8.502939377854727, Valid Loss: 8.583578846641561\n",
            "Epoch: 73, Step: 30600, Train Loss: 8.488516293691657, Valid Loss: 8.580398434654528\n",
            "Epoch: 73, Step: 30700, Train Loss: 8.479652758527173, Valid Loss: 8.57038893966727\n",
            "Epoch: 73, Step: 30800, Train Loss: 8.492891558907504, Valid Loss: 8.58562557314749\n",
            "Epoch: 73, Step: 30900, Train Loss: 8.488005253898056, Valid Loss: 8.571086628788633\n",
            "Epoch: 74, Step: 31000, Train Loss: 8.490458107691843, Valid Loss: 8.58121595291697\n",
            "Epoch: 74, Step: 31100, Train Loss: 8.503875187393074, Valid Loss: 8.5879251137373\n",
            "Epoch: 74, Step: 31200, Train Loss: 8.474338780325771, Valid Loss: 8.56962848131153\n",
            "Epoch: 74, Step: 31300, Train Loss: 8.520101050588805, Valid Loss: 8.60638693192613\n",
            "Epoch: 75, Step: 31400, Train Loss: 8.479837810782557, Valid Loss: 8.580245226907586\n",
            "Epoch: 75, Step: 31500, Train Loss: 8.47587034304036, Valid Loss: 8.572318176608475\n",
            "Epoch: 75, Step: 31600, Train Loss: 8.511856209738422, Valid Loss: 8.59563828305061\n",
            "Epoch: 75, Step: 31700, Train Loss: 8.485351765797667, Valid Loss: 8.57516572273508\n",
            "Epoch: 76, Step: 31800, Train Loss: 8.483098023796881, Valid Loss: 8.577953774475398\n",
            "Epoch: 76, Step: 31900, Train Loss: 8.476590044671644, Valid Loss: 8.572026372510411\n",
            "Epoch: 76, Step: 32000, Train Loss: 8.477955961173853, Valid Loss: 8.574396615117573\n",
            "Epoch: 76, Step: 32100, Train Loss: 8.484925291438845, Valid Loss: 8.578250124950731\n",
            "Epoch: 77, Step: 32200, Train Loss: 8.511400961481057, Valid Loss: 8.590676184708787\n",
            "Epoch: 77, Step: 32300, Train Loss: 8.490242669001137, Valid Loss: 8.579520822376614\n",
            "Epoch: 77, Step: 32400, Train Loss: 8.485491986547132, Valid Loss: 8.567969817971164\n",
            "Epoch: 77, Step: 32500, Train Loss: 8.484564377324086, Valid Loss: 8.569380322934\n",
            "Epoch: 77, Step: 32600, Train Loss: 8.478744980324352, Valid Loss: 8.56184281655751\n",
            "Epoch: 78, Step: 32700, Train Loss: 8.493061179166105, Valid Loss: 8.579296417290866\n",
            "Epoch: 78, Step: 32800, Train Loss: 8.493355696856147, Valid Loss: 8.579673441679413\n",
            "Epoch: 78, Step: 32900, Train Loss: 8.47769368168034, Valid Loss: 8.569043365146308\n",
            "Epoch: 78, Step: 33000, Train Loss: 8.476660924129849, Valid Loss: 8.564747257646033\n",
            "Epoch: 79, Step: 33100, Train Loss: 8.47318666172783, Valid Loss: 8.564991402232433\n",
            "Epoch: 79, Step: 33200, Train Loss: 8.496082332552518, Valid Loss: 8.589582750577916\n",
            "Epoch: 79, Step: 33300, Train Loss: 8.49458787710493, Valid Loss: 8.589356708190163\n",
            "Epoch: 79, Step: 33400, Train Loss: 8.466285527147454, Valid Loss: 8.567604202658664\n",
            "Epoch: 80, Step: 33500, Train Loss: 8.472392584740136, Valid Loss: 8.55715254083757\n",
            "Epoch: 80, Step: 33600, Train Loss: 8.482016434452515, Valid Loss: 8.565618152064507\n",
            "Epoch: 80, Step: 33700, Train Loss: 8.474407718719979, Valid Loss: 8.566468100941952\n",
            "Epoch: 80, Step: 33800, Train Loss: 8.473262959402666, Valid Loss: 8.560821001103504\n",
            "Epoch: 81, Step: 33900, Train Loss: 8.471937031048475, Valid Loss: 8.56713024913794\n",
            "Epoch: 81, Step: 34000, Train Loss: 8.492761026198453, Valid Loss: 8.584358845966742\n",
            "Epoch: 81, Step: 34100, Train Loss: 8.493735236106538, Valid Loss: 8.578170508311633\n",
            "Epoch: 81, Step: 34200, Train Loss: 8.480547387209814, Valid Loss: 8.569209557662553\n",
            "Epoch: 82, Step: 34300, Train Loss: 8.455116321968198, Valid Loss: 8.553451932465274\n",
            "Epoch: 82, Step: 34400, Train Loss: 8.475850006761545, Valid Loss: 8.569730395232968\n",
            "Epoch: 82, Step: 34500, Train Loss: 8.483352696299399, Valid Loss: 8.572283789858327\n",
            "Epoch: 82, Step: 34600, Train Loss: 8.484019982384119, Valid Loss: 8.573553246807643\n",
            "Epoch: 83, Step: 34700, Train Loss: 8.4862554542948, Valid Loss: 8.572309494562834\n",
            "Epoch: 83, Step: 34800, Train Loss: 8.468274608509207, Valid Loss: 8.558888414757886\n",
            "Epoch: 83, Step: 34900, Train Loss: 8.480371385330178, Valid Loss: 8.569174426067503\n",
            "Epoch: 83, Step: 35000, Train Loss: 8.488223973609836, Valid Loss: 8.574705395727289\n",
            "Epoch: 83, Step: 35100, Train Loss: 8.461667104330067, Valid Loss: 8.56150277267684\n",
            "Epoch: 84, Step: 35200, Train Loss: 8.460869051159316, Valid Loss: 8.566548681855739\n",
            "Epoch: 84, Step: 35300, Train Loss: 8.47695214518036, Valid Loss: 8.571038484805243\n",
            "Epoch: 84, Step: 35400, Train Loss: 8.479762671905728, Valid Loss: 8.573672305946372\n",
            "Epoch: 84, Step: 35500, Train Loss: 8.485574155275742, Valid Loss: 8.577111593205762\n",
            "Epoch: 85, Step: 35600, Train Loss: 8.464894862373221, Valid Loss: 8.564543673920902\n",
            "Epoch: 85, Step: 35700, Train Loss: 8.508193084366026, Valid Loss: 8.594731995029461\n",
            "Epoch: 85, Step: 35800, Train Loss: 8.47341727449556, Valid Loss: 8.568898443270497\n",
            "Epoch: 85, Step: 35900, Train Loss: 8.468290318618175, Valid Loss: 8.57298712708936\n",
            "Epoch: 86, Step: 36000, Train Loss: 8.497937876826278, Valid Loss: 8.591580897921176\n",
            "Epoch: 86, Step: 36100, Train Loss: 8.46528697481619, Valid Loss: 8.563935377421977\n",
            "Epoch: 86, Step: 36200, Train Loss: 8.48572853437027, Valid Loss: 8.569330242626055\n",
            "Epoch: 86, Step: 36300, Train Loss: 8.496015507588623, Valid Loss: 8.583347643331502\n",
            "Epoch: 87, Step: 36400, Train Loss: 8.478655608000066, Valid Loss: 8.568418601089048\n",
            "Epoch: 87, Step: 36500, Train Loss: 8.48051931210483, Valid Loss: 8.571254621997237\n",
            "Epoch: 87, Step: 36600, Train Loss: 8.453620393457276, Valid Loss: 8.553816914285562\n",
            "Epoch: 87, Step: 36700, Train Loss: 8.48769991428851, Valid Loss: 8.586586689733801\n",
            "Epoch: 88, Step: 36800, Train Loss: 8.469806292002183, Valid Loss: 8.570930912959811\n",
            "Epoch: 88, Step: 36900, Train Loss: 8.482186737569329, Valid Loss: 8.579006478083237\n",
            "Epoch: 88, Step: 37000, Train Loss: 8.484197797256988, Valid Loss: 8.58717684469825\n",
            "Epoch: 88, Step: 37100, Train Loss: 8.481352486300523, Valid Loss: 8.583912402622625\n",
            "Epoch: 88, Step: 37200, Train Loss: 8.47397964205978, Valid Loss: 8.579373551601364\n",
            "Epoch: 89, Step: 37300, Train Loss: 8.46262493877117, Valid Loss: 8.561730103895073\n",
            "Epoch: 89, Step: 37400, Train Loss: 8.481354661992818, Valid Loss: 8.571469171456219\n",
            "Epoch: 89, Step: 37500, Train Loss: 8.473405308846232, Valid Loss: 8.579343731882684\n",
            "Epoch: 89, Step: 37600, Train Loss: 8.494908786748987, Valid Loss: 8.592719480550029\n",
            "Epoch: 90, Step: 37700, Train Loss: 8.446844431301889, Valid Loss: 8.564439003239684\n",
            "Epoch: 90, Step: 37800, Train Loss: 8.481328233887519, Valid Loss: 8.581946064608356\n",
            "Epoch: 90, Step: 37900, Train Loss: 8.454092667324376, Valid Loss: 8.563917347115302\n",
            "Epoch: 90, Step: 38000, Train Loss: 8.472324330911736, Valid Loss: 8.577385853311377\n",
            "Epoch: 91, Step: 38100, Train Loss: 8.457132284092129, Valid Loss: 8.572059504872524\n",
            "Epoch: 91, Step: 38200, Train Loss: 8.458539041809903, Valid Loss: 8.570059459984849\n",
            "Epoch: 91, Step: 38300, Train Loss: 8.498520172904096, Valid Loss: 8.592154079524004\n",
            "Epoch: 91, Step: 38400, Train Loss: 8.4741002889945, Valid Loss: 8.576965923716692\n",
            "Epoch: 92, Step: 38500, Train Loss: 8.48377155670044, Valid Loss: 8.583979083313588\n",
            "Epoch: 92, Step: 38600, Train Loss: 8.485319379024366, Valid Loss: 8.588355262530373\n",
            "Epoch: 92, Step: 38700, Train Loss: 8.47621381208608, Valid Loss: 8.569259405204294\n",
            "Epoch: 92, Step: 38800, Train Loss: 8.482539789761425, Valid Loss: 8.580263640158668\n",
            "Epoch: 93, Step: 38900, Train Loss: 8.459203276591364, Valid Loss: 8.559360135392373\n",
            "Epoch: 93, Step: 39000, Train Loss: 8.475063341297512, Valid Loss: 8.57195700443705\n",
            "Epoch: 93, Step: 39100, Train Loss: 8.455332764147927, Valid Loss: 8.559301808069726\n",
            "Epoch: 93, Step: 39200, Train Loss: 8.47163175034267, Valid Loss: 8.579475641868552\n",
            "Epoch: 94, Step: 39300, Train Loss: 8.481010154609288, Valid Loss: 8.587686138325312\n",
            "Epoch: 94, Step: 39400, Train Loss: 8.48278666567763, Valid Loss: 8.589057227312896\n",
            "Epoch: 94, Step: 39500, Train Loss: 8.464818883188013, Valid Loss: 8.583723719392959\n",
            "Epoch: 94, Step: 39600, Train Loss: 8.456072587557538, Valid Loss: 8.557784205779305\n",
            "Epoch: 94, Step: 39700, Train Loss: 8.471993320905925, Valid Loss: 8.573169767870883\n",
            "Epoch: 95, Step: 39800, Train Loss: 8.456936714418015, Valid Loss: 8.568497423527983\n",
            "Epoch: 95, Step: 39900, Train Loss: 8.489017152138407, Valid Loss: 8.579274602169125\n",
            "Epoch: 95, Step: 40000, Train Loss: 8.470138020107917, Valid Loss: 8.57435452198783\n",
            "Epoch: 95, Step: 40100, Train Loss: 8.476216494287593, Valid Loss: 8.584332801990739\n",
            "Epoch: 96, Step: 40200, Train Loss: 8.503724059189283, Valid Loss: 8.59844980278694\n",
            "Epoch: 96, Step: 40300, Train Loss: 8.460129595193989, Valid Loss: 8.562953634446623\n",
            "Epoch: 96, Step: 40400, Train Loss: 8.473121163203697, Valid Loss: 8.574413267611565\n",
            "Epoch: 96, Step: 40500, Train Loss: 8.493886988875477, Valid Loss: 8.587736933111783\n",
            "Epoch: 97, Step: 40600, Train Loss: 8.455473695803244, Valid Loss: 8.563410318955013\n",
            "Epoch: 97, Step: 40700, Train Loss: 8.467204160164153, Valid Loss: 8.571497780265743\n",
            "Epoch: 97, Step: 40800, Train Loss: 8.498459070181124, Valid Loss: 8.59726448899323\n",
            "Epoch: 97, Step: 40900, Train Loss: 8.48821714912792, Valid Loss: 8.58327672776183\n",
            "Epoch: 98, Step: 41000, Train Loss: 8.462888015406072, Valid Loss: 8.570680679112474\n",
            "Epoch: 98, Step: 41100, Train Loss: 8.487935401794324, Valid Loss: 8.588913913254753\n",
            "Epoch: 98, Step: 41200, Train Loss: 8.459647481653487, Valid Loss: 8.576397676849874\n",
            "Epoch: 98, Step: 41300, Train Loss: 8.475056214382489, Valid Loss: 8.590607651195382\n",
            "Epoch: 99, Step: 41400, Train Loss: 8.484208493346099, Valid Loss: 8.582183793774629\n",
            "Epoch: 99, Step: 41500, Train Loss: 8.46857083842433, Valid Loss: 8.56801475436524\n",
            "Epoch: 99, Step: 41600, Train Loss: 8.451380847116623, Valid Loss: 8.555315283841757\n",
            "Epoch: 99, Step: 41700, Train Loss: 8.47368182839345, Valid Loss: 8.56935010717816\n",
            "Epoch: 99, Step: 41800, Train Loss: 8.475807045131997, Valid Loss: 8.574413508311093\n",
            "Test RMSE Loss for z = 0.9: 8.768688801255555\n",
            "Start training for z = 1.0\n",
            "Epoch: 0, Step: 100, Train Loss: 9.550879440183227, Valid Loss: 9.50922856098086\n",
            "Epoch: 0, Step: 200, Train Loss: 9.217018846101915, Valid Loss: 9.156410340337942\n",
            "Epoch: 0, Step: 300, Train Loss: 9.146988736536901, Valid Loss: 9.086645748500102\n",
            "Epoch: 0, Step: 400, Train Loss: 9.030321655961819, Valid Loss: 8.968610643424219\n",
            "Epoch: 1, Step: 500, Train Loss: 8.975745585000663, Valid Loss: 8.911044539731515\n",
            "Epoch: 1, Step: 600, Train Loss: 8.912386491768347, Valid Loss: 8.853704431444367\n",
            "Epoch: 1, Step: 700, Train Loss: 8.906195932464646, Valid Loss: 8.84500436358094\n",
            "Epoch: 1, Step: 800, Train Loss: 8.880856758854355, Valid Loss: 8.822855269198383\n",
            "Epoch: 2, Step: 900, Train Loss: 8.88165788910892, Valid Loss: 8.833661214325165\n",
            "Epoch: 2, Step: 1000, Train Loss: 8.843711248686482, Valid Loss: 8.790797133518128\n",
            "Epoch: 2, Step: 1100, Train Loss: 8.855148374966058, Valid Loss: 8.809172628697988\n",
            "Epoch: 2, Step: 1200, Train Loss: 8.845896821658988, Valid Loss: 8.802854732040739\n",
            "Epoch: 3, Step: 1300, Train Loss: 8.805560554177848, Valid Loss: 8.764575636389742\n",
            "Epoch: 3, Step: 1400, Train Loss: 8.802224136570015, Valid Loss: 8.756120160423958\n",
            "Epoch: 3, Step: 1500, Train Loss: 8.788209201366401, Valid Loss: 8.748935090752614\n",
            "Epoch: 3, Step: 1600, Train Loss: 8.780331384143112, Valid Loss: 8.738104247588849\n",
            "Epoch: 4, Step: 1700, Train Loss: 8.7525215659492, Valid Loss: 8.716609566047588\n",
            "Epoch: 4, Step: 1800, Train Loss: 8.760653218257668, Valid Loss: 8.728810221559916\n",
            "Epoch: 4, Step: 1900, Train Loss: 8.746846135106576, Valid Loss: 8.710086093832487\n",
            "Epoch: 4, Step: 2000, Train Loss: 8.755013877379088, Valid Loss: 8.730239724459933\n",
            "Epoch: 5, Step: 2100, Train Loss: 8.741445647618026, Valid Loss: 8.712922325746977\n",
            "Epoch: 5, Step: 2200, Train Loss: 8.739426599435737, Valid Loss: 8.718237754632247\n",
            "Epoch: 5, Step: 2300, Train Loss: 8.766347261862014, Valid Loss: 8.742899640378482\n",
            "Epoch: 5, Step: 2400, Train Loss: 8.738860255000429, Valid Loss: 8.718441633650222\n",
            "Epoch: 5, Step: 2500, Train Loss: 8.720433997986254, Valid Loss: 8.70145231317149\n",
            "Epoch: 6, Step: 2600, Train Loss: 8.742801914969633, Valid Loss: 8.720755503457093\n",
            "Epoch: 6, Step: 2700, Train Loss: 8.721028028054214, Valid Loss: 8.705216836090667\n",
            "Epoch: 6, Step: 2800, Train Loss: 8.698366511681131, Valid Loss: 8.68405016744557\n",
            "Epoch: 6, Step: 2900, Train Loss: 8.702705711873367, Valid Loss: 8.686026682788325\n",
            "Epoch: 7, Step: 3000, Train Loss: 8.717544983531392, Valid Loss: 8.704551522435846\n",
            "Epoch: 7, Step: 3100, Train Loss: 8.698552981654293, Valid Loss: 8.683582069996628\n",
            "Epoch: 7, Step: 3200, Train Loss: 8.693912959853119, Valid Loss: 8.68220450946359\n",
            "Epoch: 7, Step: 3300, Train Loss: 8.687300721298447, Valid Loss: 8.67840220176894\n",
            "Epoch: 8, Step: 3400, Train Loss: 8.69188557393192, Valid Loss: 8.678550990395411\n",
            "Epoch: 8, Step: 3500, Train Loss: 8.675384572428463, Valid Loss: 8.673554879785906\n",
            "Epoch: 8, Step: 3600, Train Loss: 8.67612932701455, Valid Loss: 8.668573313627464\n",
            "Epoch: 8, Step: 3700, Train Loss: 8.67683150069675, Valid Loss: 8.67221225238293\n",
            "Epoch: 9, Step: 3800, Train Loss: 8.679612216473325, Valid Loss: 8.67310224668714\n",
            "Epoch: 9, Step: 3900, Train Loss: 8.672394980186505, Valid Loss: 8.669093865986964\n",
            "Epoch: 9, Step: 4000, Train Loss: 8.671148872645281, Valid Loss: 8.670255879223589\n",
            "Epoch: 9, Step: 4100, Train Loss: 8.667128117658626, Valid Loss: 8.66469690416342\n",
            "Epoch: 10, Step: 4200, Train Loss: 8.672554257531377, Valid Loss: 8.670387469957097\n",
            "Epoch: 10, Step: 4300, Train Loss: 8.654853845442062, Valid Loss: 8.660871904888163\n",
            "Epoch: 10, Step: 4400, Train Loss: 8.68062477554178, Valid Loss: 8.678030862141595\n",
            "Epoch: 10, Step: 4500, Train Loss: 8.647769312654484, Valid Loss: 8.648905934248006\n",
            "Epoch: 11, Step: 4600, Train Loss: 8.655669947474447, Valid Loss: 8.655755812127863\n",
            "Epoch: 11, Step: 4700, Train Loss: 8.647146903166513, Valid Loss: 8.654750518045354\n",
            "Epoch: 11, Step: 4800, Train Loss: 8.6731229068734, Valid Loss: 8.68159717733919\n",
            "Epoch: 11, Step: 4900, Train Loss: 8.65017610221091, Valid Loss: 8.657867989514182\n",
            "Epoch: 11, Step: 5000, Train Loss: 8.635870045688035, Valid Loss: 8.64769535381887\n",
            "Epoch: 12, Step: 5100, Train Loss: 8.635396192958193, Valid Loss: 8.650760997102703\n",
            "Epoch: 12, Step: 5200, Train Loss: 8.63973589805975, Valid Loss: 8.652805622689993\n",
            "Epoch: 12, Step: 5300, Train Loss: 8.648939494838723, Valid Loss: 8.6574114468977\n",
            "Epoch: 12, Step: 5400, Train Loss: 8.645461781199302, Valid Loss: 8.656562569362928\n",
            "Epoch: 13, Step: 5500, Train Loss: 8.622541726796118, Valid Loss: 8.636938393424094\n",
            "Epoch: 13, Step: 5600, Train Loss: 8.64321408303845, Valid Loss: 8.656915258740543\n",
            "Epoch: 13, Step: 5700, Train Loss: 8.626598524804141, Valid Loss: 8.638884418348178\n",
            "Epoch: 13, Step: 5800, Train Loss: 8.623866966773237, Valid Loss: 8.639333651974315\n",
            "Epoch: 14, Step: 5900, Train Loss: 8.644523126599678, Valid Loss: 8.658491433758181\n",
            "Epoch: 14, Step: 6000, Train Loss: 8.61409467819867, Valid Loss: 8.627806618750459\n",
            "Epoch: 14, Step: 6100, Train Loss: 8.643539691180882, Valid Loss: 8.652214460294717\n",
            "Epoch: 14, Step: 6200, Train Loss: 8.626385219378568, Valid Loss: 8.644216740368575\n",
            "Epoch: 15, Step: 6300, Train Loss: 8.62879685244763, Valid Loss: 8.64357791332228\n",
            "Epoch: 15, Step: 6400, Train Loss: 8.62190228064017, Valid Loss: 8.636334657106968\n",
            "Epoch: 15, Step: 6500, Train Loss: 8.619606315976718, Valid Loss: 8.640304717722257\n",
            "Epoch: 15, Step: 6600, Train Loss: 8.620456467177165, Valid Loss: 8.633073351836131\n",
            "Epoch: 16, Step: 6700, Train Loss: 8.615462495146446, Valid Loss: 8.629236553525649\n",
            "Epoch: 16, Step: 6800, Train Loss: 8.604632311440284, Valid Loss: 8.631060993463938\n",
            "Epoch: 16, Step: 6900, Train Loss: 8.617934457670746, Valid Loss: 8.643369768554303\n",
            "Epoch: 16, Step: 7000, Train Loss: 8.609213802273764, Valid Loss: 8.627366839561523\n",
            "Epoch: 16, Step: 7100, Train Loss: 8.601523182897447, Valid Loss: 8.625624242649875\n",
            "Epoch: 17, Step: 7200, Train Loss: 8.610972502560427, Valid Loss: 8.636250767160035\n",
            "Epoch: 17, Step: 7300, Train Loss: 8.616164143592211, Valid Loss: 8.637585642843089\n",
            "Epoch: 17, Step: 7400, Train Loss: 8.595806269829929, Valid Loss: 8.619563469824124\n",
            "Epoch: 17, Step: 7500, Train Loss: 8.584759503617342, Valid Loss: 8.610803915175008\n",
            "Epoch: 18, Step: 7600, Train Loss: 8.598763212266007, Valid Loss: 8.622501996172193\n",
            "Epoch: 18, Step: 7700, Train Loss: 8.608496120343931, Valid Loss: 8.626958619270956\n",
            "Epoch: 18, Step: 7800, Train Loss: 8.610018701908842, Valid Loss: 8.629116918726538\n",
            "Epoch: 18, Step: 7900, Train Loss: 8.59227234891675, Valid Loss: 8.611053584690861\n",
            "Epoch: 19, Step: 8000, Train Loss: 8.60341907091621, Valid Loss: 8.625404601759717\n",
            "Epoch: 19, Step: 8100, Train Loss: 8.598843564138015, Valid Loss: 8.627816714381153\n",
            "Epoch: 19, Step: 8200, Train Loss: 8.601728694166656, Valid Loss: 8.626651977539572\n",
            "Epoch: 19, Step: 8300, Train Loss: 8.611193951557317, Valid Loss: 8.632781632690566\n",
            "Epoch: 20, Step: 8400, Train Loss: 8.602302185329995, Valid Loss: 8.625894333288002\n",
            "Epoch: 20, Step: 8500, Train Loss: 8.613726633498334, Valid Loss: 8.63564576341851\n",
            "Epoch: 20, Step: 8600, Train Loss: 8.58859049013174, Valid Loss: 8.615809883022354\n",
            "Epoch: 20, Step: 8700, Train Loss: 8.599186269193432, Valid Loss: 8.626821954847232\n",
            "Epoch: 21, Step: 8800, Train Loss: 8.584007838770567, Valid Loss: 8.620209319363777\n",
            "Epoch: 21, Step: 8900, Train Loss: 8.581883461705624, Valid Loss: 8.61977300792111\n",
            "Epoch: 21, Step: 9000, Train Loss: 8.586402306149665, Valid Loss: 8.618739829680742\n",
            "Epoch: 21, Step: 9100, Train Loss: 8.575594626722134, Valid Loss: 8.610758940386573\n",
            "Epoch: 22, Step: 9200, Train Loss: 8.571380131943187, Valid Loss: 8.60602971737925\n",
            "Epoch: 22, Step: 9300, Train Loss: 8.580438166125267, Valid Loss: 8.615101928490104\n",
            "Epoch: 22, Step: 9400, Train Loss: 8.572442700693228, Valid Loss: 8.609872789600695\n",
            "Epoch: 22, Step: 9500, Train Loss: 8.576989415527525, Valid Loss: 8.612819052863731\n",
            "Epoch: 22, Step: 9600, Train Loss: 8.587365799289769, Valid Loss: 8.624176075664932\n",
            "Epoch: 23, Step: 9700, Train Loss: 8.599374782594273, Valid Loss: 8.632957802637828\n",
            "Epoch: 23, Step: 9800, Train Loss: 8.55914539613977, Valid Loss: 8.596394240561985\n",
            "Epoch: 23, Step: 9900, Train Loss: 8.593398278639901, Valid Loss: 8.630119495886257\n",
            "Epoch: 23, Step: 10000, Train Loss: 8.581596026781941, Valid Loss: 8.616557745477527\n",
            "Epoch: 24, Step: 10100, Train Loss: 8.59284343926508, Valid Loss: 8.627509795582222\n",
            "Epoch: 24, Step: 10200, Train Loss: 8.572535323374632, Valid Loss: 8.616976153796648\n",
            "Epoch: 24, Step: 10300, Train Loss: 8.57154471729683, Valid Loss: 8.608909615014007\n",
            "Epoch: 24, Step: 10400, Train Loss: 8.581815985986443, Valid Loss: 8.618695795511549\n",
            "Epoch: 25, Step: 10500, Train Loss: 8.574368958125683, Valid Loss: 8.61313587202015\n",
            "Epoch: 25, Step: 10600, Train Loss: 8.578010137589416, Valid Loss: 8.62355037524912\n",
            "Epoch: 25, Step: 10700, Train Loss: 8.562889606317876, Valid Loss: 8.609389220950113\n",
            "Epoch: 25, Step: 10800, Train Loss: 8.573714187647258, Valid Loss: 8.616177048231625\n",
            "Epoch: 26, Step: 10900, Train Loss: 8.602995556574612, Valid Loss: 8.634212372864045\n",
            "Epoch: 26, Step: 11000, Train Loss: 8.559501876291048, Valid Loss: 8.606649832830877\n",
            "Epoch: 26, Step: 11100, Train Loss: 8.591446923773367, Valid Loss: 8.62841457628928\n",
            "Epoch: 26, Step: 11200, Train Loss: 8.565351349173293, Valid Loss: 8.612054156461193\n",
            "Epoch: 27, Step: 11300, Train Loss: 8.566610025070766, Valid Loss: 8.611083113621305\n",
            "Epoch: 27, Step: 11400, Train Loss: 8.580077198813775, Valid Loss: 8.626877643243096\n",
            "Epoch: 27, Step: 11500, Train Loss: 8.561945067845139, Valid Loss: 8.613167907340475\n",
            "Epoch: 27, Step: 11600, Train Loss: 8.552946767410834, Valid Loss: 8.60377141846977\n",
            "Epoch: 27, Step: 11700, Train Loss: 8.556107552960059, Valid Loss: 8.607578324577725\n",
            "Epoch: 28, Step: 11800, Train Loss: 8.566775205741846, Valid Loss: 8.617443441000672\n",
            "Epoch: 28, Step: 11900, Train Loss: 8.556707510815697, Valid Loss: 8.607903473890989\n",
            "Epoch: 28, Step: 12000, Train Loss: 8.573066640510282, Valid Loss: 8.620270239522293\n",
            "Epoch: 28, Step: 12100, Train Loss: 8.571856980254605, Valid Loss: 8.624911006455338\n",
            "Epoch: 29, Step: 12200, Train Loss: 8.544279891082152, Valid Loss: 8.60147247106751\n",
            "Epoch: 29, Step: 12300, Train Loss: 8.557126764758758, Valid Loss: 8.614466110454977\n",
            "Epoch: 29, Step: 12400, Train Loss: 8.553463027932636, Valid Loss: 8.60750882219028\n",
            "Epoch: 29, Step: 12500, Train Loss: 8.552753425592659, Valid Loss: 8.602632313218656\n",
            "Epoch: 30, Step: 12600, Train Loss: 8.548457464356808, Valid Loss: 8.611278299408022\n",
            "Epoch: 30, Step: 12700, Train Loss: 8.557360994161346, Valid Loss: 8.606326752818653\n",
            "Epoch: 30, Step: 12800, Train Loss: 8.563021487823168, Valid Loss: 8.609843677233053\n",
            "Epoch: 30, Step: 12900, Train Loss: 8.568204453395344, Valid Loss: 8.617919329197317\n",
            "Epoch: 31, Step: 13000, Train Loss: 8.557620798874392, Valid Loss: 8.619415693330705\n",
            "Epoch: 31, Step: 13100, Train Loss: 8.556388409468285, Valid Loss: 8.605027627677126\n",
            "Epoch: 31, Step: 13200, Train Loss: 8.566213479799405, Valid Loss: 8.617640950964383\n",
            "Epoch: 31, Step: 13300, Train Loss: 8.55383007982468, Valid Loss: 8.608451677113239\n",
            "Epoch: 32, Step: 13400, Train Loss: 8.557220080971398, Valid Loss: 8.611905471020782\n",
            "Epoch: 32, Step: 13500, Train Loss: 8.548241801476951, Valid Loss: 8.598468627715045\n",
            "Epoch: 32, Step: 13600, Train Loss: 8.568593577773324, Valid Loss: 8.619032603170659\n",
            "Epoch: 32, Step: 13700, Train Loss: 8.534586750752052, Valid Loss: 8.589862469613728\n",
            "Epoch: 33, Step: 13800, Train Loss: 8.571261121834512, Valid Loss: 8.61906292085865\n",
            "Epoch: 33, Step: 13900, Train Loss: 8.561680225618083, Valid Loss: 8.616447092821915\n",
            "Epoch: 33, Step: 14000, Train Loss: 8.554740948563213, Valid Loss: 8.605910917462754\n",
            "Epoch: 33, Step: 14100, Train Loss: 8.541770913638784, Valid Loss: 8.59288613421749\n",
            "Epoch: 33, Step: 14200, Train Loss: 8.54040085043244, Valid Loss: 8.595806176181673\n",
            "Epoch: 34, Step: 14300, Train Loss: 8.539880162337704, Valid Loss: 8.60186625636844\n",
            "Epoch: 34, Step: 14400, Train Loss: 8.536845274732112, Valid Loss: 8.598292094022458\n",
            "Epoch: 34, Step: 14500, Train Loss: 8.546329807696107, Valid Loss: 8.600147975627277\n",
            "Epoch: 34, Step: 14600, Train Loss: 8.534464756424036, Valid Loss: 8.59213206664847\n",
            "Epoch: 35, Step: 14700, Train Loss: 8.567041435578702, Valid Loss: 8.626211678066257\n",
            "Epoch: 35, Step: 14800, Train Loss: 8.549312495863724, Valid Loss: 8.6045605424179\n",
            "Epoch: 35, Step: 14900, Train Loss: 8.548548323271651, Valid Loss: 8.603807931282205\n",
            "Epoch: 35, Step: 15000, Train Loss: 8.545580877803303, Valid Loss: 8.604114567192612\n",
            "Epoch: 36, Step: 15100, Train Loss: 8.535054866060161, Valid Loss: 8.593078334100792\n",
            "Epoch: 36, Step: 15200, Train Loss: 8.551555811239712, Valid Loss: 8.608710566583225\n",
            "Epoch: 36, Step: 15300, Train Loss: 8.567828055686386, Valid Loss: 8.622962060037988\n",
            "Epoch: 36, Step: 15400, Train Loss: 8.525041457300766, Valid Loss: 8.586189824505746\n",
            "Epoch: 37, Step: 15500, Train Loss: 8.54287059609982, Valid Loss: 8.598014356442588\n",
            "Epoch: 37, Step: 15600, Train Loss: 8.537147570031165, Valid Loss: 8.598757374316781\n",
            "Epoch: 37, Step: 15700, Train Loss: 8.535099826340987, Valid Loss: 8.59846401333367\n",
            "Epoch: 37, Step: 15800, Train Loss: 8.543528631494029, Valid Loss: 8.603524438281285\n",
            "Epoch: 38, Step: 15900, Train Loss: 8.530583110734646, Valid Loss: 8.58876237520531\n",
            "Epoch: 38, Step: 16000, Train Loss: 8.561978024011651, Valid Loss: 8.618048263702564\n",
            "Epoch: 38, Step: 16100, Train Loss: 8.525263268706457, Valid Loss: 8.584131591458533\n",
            "Epoch: 38, Step: 16200, Train Loss: 8.5457891483566, Valid Loss: 8.596886065644703\n",
            "Epoch: 38, Step: 16300, Train Loss: 8.527917024756654, Valid Loss: 8.591272015412104\n",
            "Epoch: 39, Step: 16400, Train Loss: 8.540508670726265, Valid Loss: 8.597856005290819\n",
            "Epoch: 39, Step: 16500, Train Loss: 8.524598752373779, Valid Loss: 8.587669835276037\n",
            "Epoch: 39, Step: 16600, Train Loss: 8.537833066094061, Valid Loss: 8.596857132222063\n",
            "Epoch: 39, Step: 16700, Train Loss: 8.544219950611557, Valid Loss: 8.607167731951082\n",
            "Epoch: 40, Step: 16800, Train Loss: 8.528587125970217, Valid Loss: 8.59266353646511\n",
            "Epoch: 40, Step: 16900, Train Loss: 8.554737594583719, Valid Loss: 8.610545198706319\n",
            "Epoch: 40, Step: 17000, Train Loss: 8.535451249415877, Valid Loss: 8.603048402518324\n",
            "Epoch: 40, Step: 17100, Train Loss: 8.524854770180111, Valid Loss: 8.600595833855175\n",
            "Epoch: 41, Step: 17200, Train Loss: 8.5418042273349, Valid Loss: 8.609679262176046\n",
            "Epoch: 41, Step: 17300, Train Loss: 8.528049277525048, Valid Loss: 8.598976997854514\n",
            "Epoch: 41, Step: 17400, Train Loss: 8.535210578763976, Valid Loss: 8.605116486672141\n",
            "Epoch: 41, Step: 17500, Train Loss: 8.543182447535631, Valid Loss: 8.611578589763363\n",
            "Epoch: 42, Step: 17600, Train Loss: 8.526968189195461, Valid Loss: 8.602709529609589\n",
            "Epoch: 42, Step: 17700, Train Loss: 8.511467565856337, Valid Loss: 8.588189636682385\n",
            "Epoch: 42, Step: 17800, Train Loss: 8.5239355400647, Valid Loss: 8.596370134128348\n",
            "Epoch: 42, Step: 17900, Train Loss: 8.515642997696371, Valid Loss: 8.588147128075041\n",
            "Epoch: 43, Step: 18000, Train Loss: 8.518028077514241, Valid Loss: 8.595931382774413\n",
            "Epoch: 43, Step: 18100, Train Loss: 8.53081513866667, Valid Loss: 8.605204199399568\n",
            "Epoch: 43, Step: 18200, Train Loss: 8.529946691471567, Valid Loss: 8.59777926949129\n",
            "Epoch: 43, Step: 18300, Train Loss: 8.531570711624186, Valid Loss: 8.59513931775834\n",
            "Epoch: 44, Step: 18400, Train Loss: 8.525537456350317, Valid Loss: 8.597557224640225\n",
            "Epoch: 44, Step: 18500, Train Loss: 8.519687319346012, Valid Loss: 8.594468680555735\n",
            "Epoch: 44, Step: 18600, Train Loss: 8.518657789423317, Valid Loss: 8.590817432232116\n",
            "Epoch: 44, Step: 18700, Train Loss: 8.52366187162522, Valid Loss: 8.596203329964448\n",
            "Epoch: 44, Step: 18800, Train Loss: 8.508655411630695, Valid Loss: 8.591114113599998\n",
            "Epoch: 45, Step: 18900, Train Loss: 8.5297674598379, Valid Loss: 8.60328657668447\n",
            "Epoch: 45, Step: 19000, Train Loss: 8.527153646511225, Valid Loss: 8.6067217303275\n",
            "Epoch: 45, Step: 19100, Train Loss: 8.543802492327, Valid Loss: 8.615178649332158\n",
            "Epoch: 45, Step: 19200, Train Loss: 8.52297452580939, Valid Loss: 8.598099714314788\n",
            "Epoch: 46, Step: 19300, Train Loss: 8.512308669632851, Valid Loss: 8.598994947301094\n",
            "Epoch: 46, Step: 19400, Train Loss: 8.543273186437563, Valid Loss: 8.611260979630227\n",
            "Epoch: 46, Step: 19500, Train Loss: 8.505645340959783, Valid Loss: 8.584649259404213\n",
            "Epoch: 46, Step: 19600, Train Loss: 8.519031656133384, Valid Loss: 8.585332970323448\n",
            "Epoch: 47, Step: 19700, Train Loss: 8.516766103397588, Valid Loss: 8.589770152796467\n",
            "Epoch: 47, Step: 19800, Train Loss: 8.524487115996283, Valid Loss: 8.603938352500373\n",
            "Epoch: 47, Step: 19900, Train Loss: 8.54017142886209, Valid Loss: 8.614760921319387\n",
            "Epoch: 47, Step: 20000, Train Loss: 8.517605610074057, Valid Loss: 8.594867797553503\n",
            "Epoch: 48, Step: 20100, Train Loss: 8.524325680308477, Valid Loss: 8.60715705668281\n",
            "Epoch: 48, Step: 20200, Train Loss: 8.513479672367177, Valid Loss: 8.594098357954023\n",
            "Epoch: 48, Step: 20300, Train Loss: 8.530282034456338, Valid Loss: 8.600770319764207\n",
            "Epoch: 48, Step: 20400, Train Loss: 8.525619641210989, Valid Loss: 8.597850930089272\n",
            "Epoch: 49, Step: 20500, Train Loss: 8.523371654629498, Valid Loss: 8.59823714142738\n",
            "Epoch: 49, Step: 20600, Train Loss: 8.514808269914525, Valid Loss: 8.600040194873051\n",
            "Epoch: 49, Step: 20700, Train Loss: 8.526467385170308, Valid Loss: 8.599830134348071\n",
            "Epoch: 49, Step: 20800, Train Loss: 8.5167672933218, Valid Loss: 8.5930116259868\n",
            "Epoch: 49, Step: 20900, Train Loss: 8.51268034568747, Valid Loss: 8.587605414876778\n",
            "Epoch: 50, Step: 21000, Train Loss: 8.511873827239496, Valid Loss: 8.598255961922467\n",
            "Epoch: 50, Step: 21100, Train Loss: 8.50464798092775, Valid Loss: 8.588347832571717\n",
            "Epoch: 50, Step: 21200, Train Loss: 8.509155050017434, Valid Loss: 8.5939169804355\n",
            "Epoch: 50, Step: 21300, Train Loss: 8.526791014322027, Valid Loss: 8.606710937054798\n",
            "Epoch: 51, Step: 21400, Train Loss: 8.502277143125538, Valid Loss: 8.585608854095753\n",
            "Epoch: 51, Step: 21500, Train Loss: 8.51265685185613, Valid Loss: 8.594226559379768\n",
            "Epoch: 51, Step: 21600, Train Loss: 8.516878527835125, Valid Loss: 8.593839716900105\n",
            "Epoch: 51, Step: 21700, Train Loss: 8.530814600503025, Valid Loss: 8.602785271601107\n",
            "Epoch: 52, Step: 21800, Train Loss: 8.513854973833912, Valid Loss: 8.593502385023912\n",
            "Epoch: 52, Step: 21900, Train Loss: 8.5240164391828, Valid Loss: 8.614116396643535\n",
            "Epoch: 52, Step: 22000, Train Loss: 8.508007657919784, Valid Loss: 8.587131189450943\n",
            "Epoch: 52, Step: 22100, Train Loss: 8.52037695765818, Valid Loss: 8.596442225392856\n",
            "Epoch: 53, Step: 22200, Train Loss: 8.516107486016644, Valid Loss: 8.594642749332046\n",
            "Epoch: 53, Step: 22300, Train Loss: 8.524237943737148, Valid Loss: 8.597551125368135\n",
            "Epoch: 53, Step: 22400, Train Loss: 8.535699768390494, Valid Loss: 8.605408333605899\n",
            "Epoch: 53, Step: 22500, Train Loss: 8.508102958401798, Valid Loss: 8.58941422091666\n",
            "Epoch: 54, Step: 22600, Train Loss: 8.518376910200734, Valid Loss: 8.602537237351394\n",
            "Epoch: 54, Step: 22700, Train Loss: 8.514128459226587, Valid Loss: 8.603286992823167\n",
            "Epoch: 54, Step: 22800, Train Loss: 8.493187481512235, Valid Loss: 8.582026367679152\n",
            "Epoch: 54, Step: 22900, Train Loss: 8.499392644718652, Valid Loss: 8.58998039313458\n",
            "Epoch: 55, Step: 23000, Train Loss: 8.525383527496942, Valid Loss: 8.602351506256063\n",
            "Epoch: 55, Step: 23100, Train Loss: 8.495714358667751, Valid Loss: 8.575995035878385\n",
            "Epoch: 55, Step: 23200, Train Loss: 8.49717364206217, Valid Loss: 8.585710239829051\n",
            "Epoch: 55, Step: 23300, Train Loss: 8.51516894610187, Valid Loss: 8.587583969096277\n",
            "Epoch: 55, Step: 23400, Train Loss: 8.495080225447838, Valid Loss: 8.575762714576197\n",
            "Epoch: 56, Step: 23500, Train Loss: 8.516521622235068, Valid Loss: 8.594191336463211\n",
            "Epoch: 56, Step: 23600, Train Loss: 8.510651497806078, Valid Loss: 8.593652043185747\n",
            "Epoch: 56, Step: 23700, Train Loss: 8.496347489174704, Valid Loss: 8.57310913451067\n",
            "Epoch: 56, Step: 23800, Train Loss: 8.50827045779886, Valid Loss: 8.592253821825539\n",
            "Epoch: 57, Step: 23900, Train Loss: 8.500828200120944, Valid Loss: 8.589924458179162\n",
            "Epoch: 57, Step: 24000, Train Loss: 8.51356863432081, Valid Loss: 8.60238484986955\n",
            "Epoch: 57, Step: 24100, Train Loss: 8.524741940658721, Valid Loss: 8.603557833648345\n",
            "Epoch: 57, Step: 24200, Train Loss: 8.523165180579957, Valid Loss: 8.602760703263485\n",
            "Epoch: 58, Step: 24300, Train Loss: 8.499728551998002, Valid Loss: 8.582959238273727\n",
            "Epoch: 58, Step: 24400, Train Loss: 8.523327714971922, Valid Loss: 8.60837724488846\n",
            "Epoch: 58, Step: 24500, Train Loss: 8.506794827799208, Valid Loss: 8.582533682457365\n",
            "Epoch: 58, Step: 24600, Train Loss: 8.50602144859734, Valid Loss: 8.592015670272447\n",
            "Epoch: 59, Step: 24700, Train Loss: 8.501384704251368, Valid Loss: 8.579539450736966\n",
            "Epoch: 59, Step: 24800, Train Loss: 8.53107492593296, Valid Loss: 8.609889622971519\n",
            "Epoch: 59, Step: 24900, Train Loss: 8.497117791090435, Valid Loss: 8.58097595130882\n",
            "Epoch: 59, Step: 25000, Train Loss: 8.500583210752442, Valid Loss: 8.578802326900224\n",
            "Epoch: 60, Step: 25100, Train Loss: 8.491685071869487, Valid Loss: 8.586630535387979\n",
            "Epoch: 60, Step: 25200, Train Loss: 8.493746887707777, Valid Loss: 8.577043339254224\n",
            "Epoch: 60, Step: 25300, Train Loss: 8.486807484333289, Valid Loss: 8.572955961084118\n",
            "Epoch: 60, Step: 25400, Train Loss: 8.50692565321218, Valid Loss: 8.588846787833965\n",
            "Epoch: 61, Step: 25500, Train Loss: 8.496298105407154, Valid Loss: 8.578589721021785\n",
            "Epoch: 61, Step: 25600, Train Loss: 8.515407849950707, Valid Loss: 8.598070086908665\n",
            "Epoch: 61, Step: 25700, Train Loss: 8.500405665963681, Valid Loss: 8.576208057230913\n",
            "Epoch: 61, Step: 25800, Train Loss: 8.500765468465817, Valid Loss: 8.57287612331885\n",
            "Epoch: 61, Step: 25900, Train Loss: 8.495273512271737, Valid Loss: 8.582335678643839\n",
            "Epoch: 62, Step: 26000, Train Loss: 8.48910471191572, Valid Loss: 8.582070241539792\n",
            "Epoch: 62, Step: 26100, Train Loss: 8.504757256796324, Valid Loss: 8.586139297728995\n",
            "Epoch: 62, Step: 26200, Train Loss: 8.494543246443376, Valid Loss: 8.57513920140672\n",
            "Epoch: 62, Step: 26300, Train Loss: 8.502209902561324, Valid Loss: 8.578019453164092\n",
            "Epoch: 63, Step: 26400, Train Loss: 8.49751288778299, Valid Loss: 8.588503330702617\n",
            "Epoch: 63, Step: 26500, Train Loss: 8.523017137944173, Valid Loss: 8.602724925312737\n",
            "Epoch: 63, Step: 26600, Train Loss: 8.503522887001788, Valid Loss: 8.590698721024758\n",
            "Epoch: 63, Step: 26700, Train Loss: 8.487861693435582, Valid Loss: 8.580821583180146\n",
            "Epoch: 64, Step: 26800, Train Loss: 8.498889066580906, Valid Loss: 8.582177511804096\n",
            "Epoch: 64, Step: 26900, Train Loss: 8.49642392119263, Valid Loss: 8.57770281744675\n",
            "Epoch: 64, Step: 27000, Train Loss: 8.49726325774183, Valid Loss: 8.586830539563023\n",
            "Epoch: 64, Step: 27100, Train Loss: 8.504473452371574, Valid Loss: 8.591637327038448\n",
            "Epoch: 65, Step: 27200, Train Loss: 8.52740012098019, Valid Loss: 8.607786160452486\n",
            "Epoch: 65, Step: 27300, Train Loss: 8.490298015096577, Valid Loss: 8.57934874930062\n",
            "Epoch: 65, Step: 27400, Train Loss: 8.526317072910503, Valid Loss: 8.599054886278596\n",
            "Epoch: 65, Step: 27500, Train Loss: 8.489722452971229, Valid Loss: 8.578247247667857\n",
            "Epoch: 66, Step: 27600, Train Loss: 8.488019043240726, Valid Loss: 8.578503660847241\n",
            "Epoch: 66, Step: 27700, Train Loss: 8.500565709688349, Valid Loss: 8.59101183805607\n",
            "Epoch: 66, Step: 27800, Train Loss: 8.502874622857808, Valid Loss: 8.59181394353959\n",
            "Epoch: 66, Step: 27900, Train Loss: 8.496008545322164, Valid Loss: 8.597122590457053\n",
            "Epoch: 66, Step: 28000, Train Loss: 8.485311107201374, Valid Loss: 8.57236065069993\n",
            "Epoch: 67, Step: 28100, Train Loss: 8.482369489595047, Valid Loss: 8.573750085281038\n",
            "Epoch: 67, Step: 28200, Train Loss: 8.489706413740542, Valid Loss: 8.588160681318124\n",
            "Epoch: 67, Step: 28300, Train Loss: 8.501523118398973, Valid Loss: 8.581265578557307\n",
            "Epoch: 67, Step: 28400, Train Loss: 8.501912117235115, Valid Loss: 8.578011405391276\n",
            "Epoch: 68, Step: 28500, Train Loss: 8.485835943854955, Valid Loss: 8.567807165524595\n",
            "Epoch: 68, Step: 28600, Train Loss: 8.480069615450065, Valid Loss: 8.568615714813555\n",
            "Epoch: 68, Step: 28700, Train Loss: 8.48920792950937, Valid Loss: 8.577381465749058\n",
            "Epoch: 68, Step: 28800, Train Loss: 8.496214509936484, Valid Loss: 8.594515597973377\n",
            "Epoch: 69, Step: 28900, Train Loss: 8.480091841140041, Valid Loss: 8.573940749130069\n",
            "Epoch: 69, Step: 29000, Train Loss: 8.48145282580118, Valid Loss: 8.57615387170921\n",
            "Epoch: 69, Step: 29100, Train Loss: 8.485411661864584, Valid Loss: 8.582142376854435\n",
            "Epoch: 69, Step: 29200, Train Loss: 8.509032375104256, Valid Loss: 8.601980514339227\n",
            "Epoch: 70, Step: 29300, Train Loss: 8.482161031241194, Valid Loss: 8.572016125184884\n",
            "Epoch: 70, Step: 29400, Train Loss: 8.491194666766178, Valid Loss: 8.583142962369827\n",
            "Epoch: 70, Step: 29500, Train Loss: 8.483260567041459, Valid Loss: 8.572941457688117\n",
            "Epoch: 70, Step: 29600, Train Loss: 8.50283176199805, Valid Loss: 8.599870535522307\n",
            "Epoch: 71, Step: 29700, Train Loss: 8.48806724787195, Valid Loss: 8.584751282026113\n",
            "Epoch: 71, Step: 29800, Train Loss: 8.486504325079894, Valid Loss: 8.576905528357672\n",
            "Epoch: 71, Step: 29900, Train Loss: 8.501365003439334, Valid Loss: 8.593570848889684\n",
            "Epoch: 71, Step: 30000, Train Loss: 8.510679116910511, Valid Loss: 8.602871705764946\n",
            "Epoch: 72, Step: 30100, Train Loss: 8.510546096558693, Valid Loss: 8.588367503557148\n",
            "Epoch: 72, Step: 30200, Train Loss: 8.48596861614006, Valid Loss: 8.59174538478543\n",
            "Epoch: 72, Step: 30300, Train Loss: 8.467655699592012, Valid Loss: 8.57284248314674\n",
            "Epoch: 72, Step: 30400, Train Loss: 8.514485579192028, Valid Loss: 8.60476703165991\n",
            "Epoch: 72, Step: 30500, Train Loss: 8.494398988800942, Valid Loss: 8.588902668508322\n",
            "Epoch: 73, Step: 30600, Train Loss: 8.50122629330291, Valid Loss: 8.590969397343597\n",
            "Epoch: 73, Step: 30700, Train Loss: 8.508416951543921, Valid Loss: 8.588888809938824\n",
            "Epoch: 73, Step: 30800, Train Loss: 8.485442043785875, Valid Loss: 8.584944056529427\n",
            "Epoch: 73, Step: 30900, Train Loss: 8.49002975385112, Valid Loss: 8.58086461402308\n",
            "Epoch: 74, Step: 31000, Train Loss: 8.502266195653743, Valid Loss: 8.591699562623061\n",
            "Epoch: 74, Step: 31100, Train Loss: 8.487945582892152, Valid Loss: 8.582420316434975\n",
            "Epoch: 74, Step: 31200, Train Loss: 8.489008179371941, Valid Loss: 8.582026294060986\n",
            "Epoch: 74, Step: 31300, Train Loss: 8.485499222445979, Valid Loss: 8.57972795352584\n",
            "Epoch: 75, Step: 31400, Train Loss: 8.494000834868567, Valid Loss: 8.592743410958612\n",
            "Epoch: 75, Step: 31500, Train Loss: 8.502394990343953, Valid Loss: 8.602993683485227\n",
            "Epoch: 75, Step: 31600, Train Loss: 8.479832997243228, Valid Loss: 8.582763742173245\n",
            "Epoch: 75, Step: 31700, Train Loss: 8.487897491701613, Valid Loss: 8.597733697488453\n",
            "Epoch: 76, Step: 31800, Train Loss: 8.496070339334416, Valid Loss: 8.595213416238161\n",
            "Epoch: 76, Step: 31900, Train Loss: 8.500640734325495, Valid Loss: 8.593369974747164\n",
            "Epoch: 76, Step: 32000, Train Loss: 8.494845207052851, Valid Loss: 8.591755191882793\n",
            "Epoch: 76, Step: 32100, Train Loss: 8.486023199009546, Valid Loss: 8.59210824238381\n",
            "Epoch: 77, Step: 32200, Train Loss: 8.491849492864757, Valid Loss: 8.590741810257088\n",
            "Epoch: 77, Step: 32300, Train Loss: 8.478615334853771, Valid Loss: 8.58106760192982\n",
            "Epoch: 77, Step: 32400, Train Loss: 8.497207843007521, Valid Loss: 8.589513453185956\n",
            "Epoch: 77, Step: 32500, Train Loss: 8.48728364386914, Valid Loss: 8.582998180415911\n",
            "Epoch: 77, Step: 32600, Train Loss: 8.482629260743144, Valid Loss: 8.585094570364685\n",
            "Epoch: 78, Step: 32700, Train Loss: 8.517571462736532, Valid Loss: 8.605397817675136\n",
            "Epoch: 78, Step: 32800, Train Loss: 8.485822109602621, Valid Loss: 8.57588598402869\n",
            "Epoch: 78, Step: 32900, Train Loss: 8.47982389748304, Valid Loss: 8.586976213385423\n",
            "Epoch: 78, Step: 33000, Train Loss: 8.495924247711402, Valid Loss: 8.603792581576153\n",
            "Epoch: 79, Step: 33100, Train Loss: 8.479539800525512, Valid Loss: 8.589610190856838\n",
            "Epoch: 79, Step: 33200, Train Loss: 8.468666570267104, Valid Loss: 8.571468913475279\n",
            "Epoch: 79, Step: 33300, Train Loss: 8.501676569064728, Valid Loss: 8.591779828458154\n",
            "Epoch: 79, Step: 33400, Train Loss: 8.490962151125913, Valid Loss: 8.57780332929794\n",
            "Epoch: 80, Step: 33500, Train Loss: 8.486522032708532, Valid Loss: 8.59207770703113\n",
            "Epoch: 80, Step: 33600, Train Loss: 8.493850741464088, Valid Loss: 8.590952955893009\n",
            "Epoch: 80, Step: 33700, Train Loss: 8.486742452121574, Valid Loss: 8.592216308914619\n",
            "Epoch: 80, Step: 33800, Train Loss: 8.485666598039376, Valid Loss: 8.58882012485139\n",
            "Epoch: 81, Step: 33900, Train Loss: 8.485504555884086, Valid Loss: 8.59658070301236\n",
            "Epoch: 81, Step: 34000, Train Loss: 8.468152401490189, Valid Loss: 8.583082774869567\n",
            "Epoch: 81, Step: 34100, Train Loss: 8.499625909802145, Valid Loss: 8.602087166636645\n",
            "Epoch: 81, Step: 34200, Train Loss: 8.502241769514033, Valid Loss: 8.603270342363826\n",
            "Epoch: 82, Step: 34300, Train Loss: 8.47288237026798, Valid Loss: 8.578866008215185\n",
            "Epoch: 82, Step: 34400, Train Loss: 8.47868019095446, Valid Loss: 8.581885853989997\n",
            "Epoch: 82, Step: 34500, Train Loss: 8.48791609651808, Valid Loss: 8.589878437522609\n",
            "Epoch: 82, Step: 34600, Train Loss: 8.504453827775762, Valid Loss: 8.605199617997988\n",
            "Epoch: 83, Step: 34700, Train Loss: 8.486818837195926, Valid Loss: 8.591085153384277\n",
            "Epoch: 83, Step: 34800, Train Loss: 8.509305730881895, Valid Loss: 8.602766979995712\n",
            "Epoch: 83, Step: 34900, Train Loss: 8.475430130367073, Valid Loss: 8.578738650022308\n",
            "Epoch: 83, Step: 35000, Train Loss: 8.467682881388855, Valid Loss: 8.57779669303308\n",
            "Epoch: 83, Step: 35100, Train Loss: 8.482675778009284, Valid Loss: 8.58787088688821\n",
            "Epoch: 84, Step: 35200, Train Loss: 8.470694778096542, Valid Loss: 8.587699905647575\n",
            "Epoch: 84, Step: 35300, Train Loss: 8.45265448973086, Valid Loss: 8.570379017201342\n",
            "Epoch: 84, Step: 35400, Train Loss: 8.479276728067324, Valid Loss: 8.59307316294768\n",
            "Epoch: 84, Step: 35500, Train Loss: 8.4944995053027, Valid Loss: 8.592404200262125\n",
            "Epoch: 85, Step: 35600, Train Loss: 8.491845433431598, Valid Loss: 8.596109285453892\n",
            "Epoch: 85, Step: 35700, Train Loss: 8.473781049232482, Valid Loss: 8.591377930443466\n",
            "Epoch: 85, Step: 35800, Train Loss: 8.47734978237575, Valid Loss: 8.582536543580463\n",
            "Epoch: 85, Step: 35900, Train Loss: 8.478579080937827, Valid Loss: 8.58947077459961\n",
            "Epoch: 86, Step: 36000, Train Loss: 8.475166586605592, Valid Loss: 8.592133551986239\n",
            "Epoch: 86, Step: 36100, Train Loss: 8.498990201210459, Valid Loss: 8.599589834292086\n",
            "Epoch: 86, Step: 36200, Train Loss: 8.485069363777061, Valid Loss: 8.587270857054158\n",
            "Epoch: 86, Step: 36300, Train Loss: 8.47529593088358, Valid Loss: 8.584842055823538\n",
            "Epoch: 87, Step: 36400, Train Loss: 8.47407119487188, Valid Loss: 8.588891921498572\n",
            "Epoch: 87, Step: 36500, Train Loss: 8.478028217498771, Valid Loss: 8.583366422867682\n",
            "Epoch: 87, Step: 36600, Train Loss: 8.476736498492953, Valid Loss: 8.5894374421501\n",
            "Epoch: 87, Step: 36700, Train Loss: 8.463520803632782, Valid Loss: 8.581666298585269\n",
            "Epoch: 88, Step: 36800, Train Loss: 8.48325041121406, Valid Loss: 8.586944229956263\n",
            "Epoch: 88, Step: 36900, Train Loss: 8.476820024638627, Valid Loss: 8.586256597883144\n",
            "Epoch: 88, Step: 37000, Train Loss: 8.494521462672772, Valid Loss: 8.60210529076105\n",
            "Epoch: 88, Step: 37100, Train Loss: 8.478631772056563, Valid Loss: 8.588259501395875\n",
            "Epoch: 88, Step: 37200, Train Loss: 8.472984913627124, Valid Loss: 8.587101058228143\n",
            "Epoch: 89, Step: 37300, Train Loss: 8.467959771559862, Valid Loss: 8.579374369015637\n",
            "Epoch: 89, Step: 37400, Train Loss: 8.490597026542696, Valid Loss: 8.608652867192841\n",
            "Epoch: 89, Step: 37500, Train Loss: 8.480375691414098, Valid Loss: 8.596777634012513\n",
            "Epoch: 89, Step: 37600, Train Loss: 8.475386166621165, Valid Loss: 8.590813083398247\n",
            "Epoch: 90, Step: 37700, Train Loss: 8.468735048645945, Valid Loss: 8.577979113565556\n",
            "Epoch: 90, Step: 37800, Train Loss: 8.46472095661194, Valid Loss: 8.582306556297715\n",
            "Epoch: 90, Step: 37900, Train Loss: 8.49954455526334, Valid Loss: 8.60674141070353\n",
            "Epoch: 90, Step: 38000, Train Loss: 8.469573357438296, Valid Loss: 8.592188573030153\n",
            "Epoch: 91, Step: 38100, Train Loss: 8.465977411378717, Valid Loss: 8.58078752256754\n",
            "Epoch: 91, Step: 38200, Train Loss: 8.481991052585792, Valid Loss: 8.596721113699404\n",
            "Epoch: 91, Step: 38300, Train Loss: 8.47438557835382, Valid Loss: 8.580202961204469\n",
            "Epoch: 91, Step: 38400, Train Loss: 8.483416121606938, Valid Loss: 8.59131582244708\n",
            "Epoch: 92, Step: 38500, Train Loss: 8.479458440923558, Valid Loss: 8.587478414366725\n",
            "Epoch: 92, Step: 38600, Train Loss: 8.471494340466696, Valid Loss: 8.587347931997773\n",
            "Epoch: 92, Step: 38700, Train Loss: 8.493445500487672, Valid Loss: 8.596959882054344\n",
            "Epoch: 92, Step: 38800, Train Loss: 8.46919898698774, Valid Loss: 8.581072895665702\n",
            "Epoch: 93, Step: 38900, Train Loss: 8.50532835293547, Valid Loss: 8.614645837886712\n",
            "Epoch: 93, Step: 39000, Train Loss: 8.472800150259767, Valid Loss: 8.593674483475677\n",
            "Epoch: 93, Step: 39100, Train Loss: 8.4733642143201, Valid Loss: 8.594935667382716\n",
            "Epoch: 93, Step: 39200, Train Loss: 8.46721308169733, Valid Loss: 8.585229109940636\n",
            "Epoch: 94, Step: 39300, Train Loss: 8.470573874921337, Valid Loss: 8.587796923689993\n",
            "Epoch: 94, Step: 39400, Train Loss: 8.483183768725524, Valid Loss: 8.593996105672426\n",
            "Epoch: 94, Step: 39500, Train Loss: 8.51252184202061, Valid Loss: 8.61018289306964\n",
            "Epoch: 94, Step: 39600, Train Loss: 8.45681937011063, Valid Loss: 8.573654129085567\n",
            "Epoch: 94, Step: 39700, Train Loss: 8.465684415298673, Valid Loss: 8.587543083407299\n",
            "Epoch: 95, Step: 39800, Train Loss: 8.481985194987717, Valid Loss: 8.59336560759945\n",
            "Epoch: 95, Step: 39900, Train Loss: 8.489168652692785, Valid Loss: 8.597022657227283\n",
            "Epoch: 95, Step: 40000, Train Loss: 8.476873883018081, Valid Loss: 8.588907704861901\n",
            "Epoch: 95, Step: 40100, Train Loss: 8.473850208655723, Valid Loss: 8.584184818893851\n",
            "Epoch: 96, Step: 40200, Train Loss: 8.484798191819529, Valid Loss: 8.596133992847387\n",
            "Epoch: 96, Step: 40300, Train Loss: 8.466909019946353, Valid Loss: 8.58154900685506\n",
            "Epoch: 96, Step: 40400, Train Loss: 8.483729912764218, Valid Loss: 8.594315368321626\n",
            "Epoch: 96, Step: 40500, Train Loss: 8.480802248050082, Valid Loss: 8.591671013728497\n",
            "Epoch: 97, Step: 40600, Train Loss: 8.48235794319233, Valid Loss: 8.59507971165626\n",
            "Epoch: 97, Step: 40700, Train Loss: 8.486673167257555, Valid Loss: 8.589169144933216\n",
            "Epoch: 97, Step: 40800, Train Loss: 8.458505296989044, Valid Loss: 8.575668757786246\n",
            "Epoch: 97, Step: 40900, Train Loss: 8.47117961019826, Valid Loss: 8.591281872075731\n",
            "Epoch: 98, Step: 41000, Train Loss: 8.462475773446917, Valid Loss: 8.580623093168846\n",
            "Epoch: 98, Step: 41100, Train Loss: 8.468772259542044, Valid Loss: 8.59052743324427\n",
            "Epoch: 98, Step: 41200, Train Loss: 8.46978652050113, Valid Loss: 8.587334708548909\n",
            "Epoch: 98, Step: 41300, Train Loss: 8.481055200813918, Valid Loss: 8.587140207215365\n",
            "Epoch: 99, Step: 41400, Train Loss: 8.47693568208567, Valid Loss: 8.591510910691701\n",
            "Epoch: 99, Step: 41500, Train Loss: 8.476316254134074, Valid Loss: 8.590774375247808\n",
            "Epoch: 99, Step: 41600, Train Loss: 8.466668363450527, Valid Loss: 8.594734876594934\n",
            "Epoch: 99, Step: 41700, Train Loss: 8.483684330230483, Valid Loss: 8.595146298361252\n",
            "Epoch: 99, Step: 41800, Train Loss: 8.490738629277908, Valid Loss: 8.599434321121942\n",
            "Test RMSE Loss for z = 1.0: 8.776299084557824\n"
          ]
        }
      ],
      "source": [
        "q8_z_list = [0.1, 0.5, 0.9, 1.0]\n",
        "test_rmse_list = []\n",
        "for z in q8_z_list:\n",
        "    print(f\"Start training for z = {z}\")\n",
        "    mlp = MyMLP(\n",
        "        X_subtrain=X_subtrain,\n",
        "        Y_subtrain=Y_subtrain,\n",
        "        X_valid=X_valid,\n",
        "        Y_valid=Y_valid,\n",
        "        H=90,\n",
        "        lr=0.001,\n",
        "        wd=0,\n",
        "        mom=0,\n",
        "        loss_type=2,\n",
        "        optimizer_type=1,\n",
        "        use_dropout=True,\n",
        "        dropout_rate=0.5,\n",
        "        loss_z=z\n",
        "    )\n",
        "    mlp.fit(\n",
        "        max_epoch=100,\n",
        "        verbose=True,\n",
        "        patience_batch_num=5000,\n",
        "        model_path=f'q8_mlp_z_{z}.ckpt'\n",
        "    )\n",
        "    best_model = torch.load(f'q8_mlp_z_{z}.ckpt')\n",
        "    test_rmse_loss = calculate_test_rmse(best_model, X_test, Y_test)\n",
        "    test_rmse_list.append(test_rmse_loss)\n",
        "    print(f\"Test RMSE Loss for z = {z}: {test_rmse_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aro1S4SdPs3z",
        "outputId": "63ebb6d2-b52b-4387-e9a0-fd5d9158b5d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test RMSE Loss for z = 0.1: 8.750706457881472\n",
            "Test RMSE Loss for z = 0.5: 8.779484543662651\n",
            "Test RMSE Loss for z = 0.9: 8.768688801255555\n",
            "Test RMSE Loss for z = 1.0: 8.776299084557824\n"
          ]
        }
      ],
      "source": [
        "# print rmse according to different z\n",
        "for z, rmse in zip(q8_z_list, test_rmse_list):\n",
        "    print(f\"Test RMSE Loss for z = {z}: {rmse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdWucq2vPs3z"
      },
      "source": [
        "這裡不同的 z 值所產生的 Test RMSE 就沒有像前一題一樣有明顯的走向，有趣的是前一題 z 是最大值時，Test RMSE 最小，但這裡卻是 z = 0.1 時 Test RMSE 最小。  \n",
        "而且與前面使用 SSE 的模型相比，$8.75$ 的 Test RMSE 是最低的，代表 $qloss$ 的模型在這個資料集的下有最好的表現。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECM6JB6sPs3z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('python39')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "f1f0ca08df83ebe1228368c35aef2dd5bdef6efbf7330e5bb8a931893862d7fc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
